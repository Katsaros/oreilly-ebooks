<style>
	img {
		width: 100%;
	}
</style>
<figure id="cover-image">
            <img class="cover-image" src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8a10e3fd-2f00-4790-b508-e0e2f5a9db3f.png" alt="Hands-On Microservices with Spring Boot and Spring Cloud" width="810" height="1000" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8a10e3fd-2f00-4790-b508-e0e2f5a9db3f.png">
        </figure>
    <section>

            
            <article>
                
<div class="title-page-name"><strong>Hands-On Microservices with Spring Boot and Spring Cloud<br>
<br>
<br></strong>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</div>
<div class="title-page-tagline">Build and deploy Java microservices using Spring Cloud, Istio, and Kubernetes</div>
<p class="mce-root"></p>
<p>&nbsp;</p>
<p class="mce-root"></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<div class="title-page-author-name">Magnus Larsson</div>
<div class="title-page-author-name"></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p class="mce-root"><img class="alignnone size-full wp-image-110 image-border" src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/17353259-94e5-4412-9741-f5213cc74d1f.png" style="width:11.83em;height:4.00em;" width="645" height="207" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/17353259-94e5-4412-9741-f5213cc74d1f.png"></p>
<div class="CDPAlignLeft CDPAlign"><strong><strong>BIRMINGHAM - MUMBAI</strong></strong></div>


            </article>

            
        </section>
    <section>

            
            <article>
                


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Hands-On Microservices with Spring Boot and Spring Cloud</h1>
                </header>
            
            <article>
                
<p style="font-size: 11px">Copyright Â© 2019 Packt Publishing</p>
<p style="font-size: 11px"></p>
<p style="font-size: 11px">All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.</p>
<p style="font-size: 11px">Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author(s), nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.</p>
<p style="font-size: 11px">Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.</p>
<p style="font-size: 11px"><strong>Commissioning Editor:</strong><span> Richa Tripathi</span><br>
<strong>Acquisition Editor:</strong> <span>Shriram Shekhar</span><br>
<strong>Content Development Editor:</strong> <span>Tiksha Sarang</span><span><br>
<strong>Senior Editor: </strong>Rohit Singh</span><br>
<strong>Technical Editor:</strong><span>&nbsp;</span><span>Gaurav Gala</span><br>
<strong>Copy Editor:</strong><span>&nbsp;</span><span>Safis Editing</span><br>
<strong>Project Coordinator:</strong> <span>Prajakta Naik</span><br>
<strong>Proofreader:</strong><span> Safis Editing</span><br>
<strong>Indexer:</strong> <span>Rekha Nair</span><br>
<strong>Production Designer:</strong> <span>Jyoti Chauhan</span></p>
<p style="font-size: 11px">First published: September 2019</p>
<p style="font-size: 11px">Production reference: 1190919</p>
<p style="font-size: 11px">Published by Packt Publishing Ltd.<br>
Livery Place<br>
35 Livery Street<br>
Birmingham<br>
B3 2PB, UK.</p>
<p style="font-size: 11px">ISBN 978-1-78961-347-6</p>
<p style="font-size: 11px"><a href="http://www.packt.com" target="_blank">www.packt.com</a></p>


            </article>

            
        </section>
    <section>

            
            <article>
                
<div class="CDPAlignLeft CDPAlign">&nbsp;<img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cd16e0d5-fb8a-4fd4-8413-654373744bb9.png" style="width:20.08em;height:6.42em;" width="1102" height="351" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cd16e0d5-fb8a-4fd4-8413-654373744bb9.png"></div>
<div class="CDPAlignLeft CDPAlign"><span><a href="https://mapt.io/" target="_blank"></a></span></div>
<p><a href="https://subscribe.packtpub.com/">Packt.com</a></p>
<p>Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Why subscribe?</h1>
                </header>
            
            <article>
                
<ul>
<li>
<p>Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</p>
</li>
<li>
<p><span>Improve your learning with Skill Plans built especially for you</span></p>
</li>
<li>
<p><span>Get a free eBook or video every month</span></p>
</li>
<li>
<p>Fully searchable for easy access to vital information</p>
</li>
<li>
<p><span>Copy and paste, print, and bookmark content</span></p>
</li>
</ul>
<p>Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at <span><a href="http://www.packt.com" target="_blank">www.packt.com</a></span> and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at <kbd>customercare@packtpub.com</kbd> for more details.</p>
<p>At <span><a href="http://www.packt.com" target="_blank">www.packt.com</a></span>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks. </p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Contributors</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">About the author</h1>
                </header>
            
            <article>
                
<p><strong>Magnus Larsson</strong> has been in the IT industry for more than 30 years, working as a consultant for large companies in Sweden such as Volvo, Ericsson, and AstraZeneca. He has seen a lot of different communication technologies come and go over the years, such as RPC, CORBA, SOAP, and REST. In the past, he struggled with the challenges associated with distributed systems as there was no substantial help from the software available at that time. This has, however, changed dramatically over the last few years with the introduction of open source projects such as Spring Cloud, Netflix OSS, Docker, and Kubernetes. Over the last five years, Magnus has been helping customers use these new software technologies and has also done several presentations and blog posts on the subject.</p>
<p>&nbsp;</p>
<div class="packt_quote">I would like to thank the following people:<br>
<br>
Shriram Shekhar, Tiksha Sarang, and Gaurav Gala from Packt Publishing for their constant support.<br>
<br>
My college Erik Lupander, the technical reviewer of this book and a persistent troubleshooter.<br>
<br>
To my wife Maria, thank you for all of your support and understanding throughout the process of writing this book.<br>
<br>
And to our daughter Emma, who has reviewed each chapter and helped me to write proper English.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">About the reviewer</h1>
                </header>
            
            <article>
                
<p><strong>Erik Lupander</strong> is a software architect and developer with over 15 years of professional experience.</p>
<p class="mce-root">He holds an M.Sc. in applied informatics from the University of Gothenburg. While Java Virtual Machine-based languages and architecture have been his bread and butter, Erik is a polyglot software craftsman at heart who, among other technologies, has embraced Go and microservice architecture.</p>
<p class="mce-root">He has spoken at software conferences on topics ranging from OpenGL ES and big data to Go and microservices, and was a technical reviewer for <em>Building Microservices with Go</em>, by Nic Jackson.</p>
<p class="mce-root">He lives just outside Gothenburg, Sweden, with his wife and two children, and is currently employed by Callista Enterprise AB, a Swedish consultancy specializing in software architecture.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Packt is searching for authors like you</h1>
                </header>
            
            <article>
                
<p>If you're interested in becoming an author for Packt, please visit <a href="http://authors.packtpub.com" target="_blank">authors.packtpub.com</a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>


            </article>

            
        </section>
    <section>
            <header>
                <h1>Table of Contents</h1>
            </header>

            <article>
                <nav epub:type="toc" id="toc">
                    <ol><li style="list-style-type:none" hidden="hidden" class="front-matter miscellaneous ">
            <a href="4de0436a-1a8f-4b51-8247-89253f57cc29.xhtml" class="front-matter miscellaneous ">Title Page</a>
    
    
</li>
<li style="list-style-type:none" hidden="hidden" class="front-matter miscellaneous ">
            <a href="044c3e42-2514-4519-93ed-4a5ea9e8d665.xhtml" class="front-matter miscellaneous ">Copyright and Credits</a>
    
    <ol><li style="list-style-type:none" hidden="hidden" class="">
            <a href="8e2e3246-659e-41f6-98cf-2fb45ad67b5f.xhtml" class="">Hands-On Microservices with Spring Boot and Spring Cloud</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" hidden="hidden" class="front-matter miscellaneous ">
            <a href="fcfdcc7d-3188-4755-8d2e-10eccec503ac.xhtml" class="front-matter miscellaneous ">About Packt</a>
    
    <ol><li style="list-style-type:none" hidden="hidden" class="">
            <a href="ab4ebbec-6622-4253-a5a1-7cb4fd80c340.xhtml" class="">Why subscribe?</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" hidden="hidden" class="front-matter miscellaneous ">
            <a href="5eaf5fcf-5a8c-4f93-967d-85d5b23c6910.xhtml" class="front-matter miscellaneous ">Contributors</a>
    
    <ol><li style="list-style-type:none" hidden="hidden" class="">
            <a href="82b98532-221d-42f1-8e08-16d3d1869621.xhtml" class="">About the author</a>
    
    
</li>
<li style="list-style-type:none" hidden="hidden" class="">
            <a href="953462f0-e651-4b63-a868-9235a4e9fccc.xhtml" class="">About the reviewer</a>
    
    
</li>
<li style="list-style-type:none" hidden="hidden" class="">
            <a href="1a0c67d4-6f1b-446b-8346-57daf93b49fe.xhtml" class="">Packt is searching for authors like you</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="front-matter preface ">
            <a href="74635faf-20ac-4992-bec6-b51dd94a97ed.xhtml" class="front-matter preface ">Preface</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="6e9927b0-1885-4760-b7c0-482bac4f08d2.xhtml" class="">Who this book is for</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ab8f2b05-1e79-4957-8d17-17f3fbe6adfb.xhtml" class="">What this book covers</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e23949fb-e3e2-4b82-8b45-936d7ec6d722.xhtml" class="">To get the most out of this book</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="74f19d28-2dfb-41c1-be9b-f6272adcff74.xhtml" class="">Download the example code files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="15063023-93ea-4c20-b123-40fca8d52d11.xhtml" class="">Download the color images</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="cf36a724-e7d3-4835-9891-35c72b8283aa.xhtml" class="">Code in Action</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9b31071c-ebce-4290-82aa-38aff83fe37e.xhtml" class="">Conventions used</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="2aad90aa-27ed-425d-8110-65c4c123c8e2.xhtml" class="">Get in touch</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="9f743088-e932-4e8b-97fb-173a3d06a19b.xhtml" class="">Reviews</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="1" class="chapter section ">
            <a href="6c3283c6-b741-40bd-abb6-78e98c7c9831.xhtml" class="chapter section ">Section 1: Getting Started with Microservice Development Using Spring Boot</a>
    
    
</li>
<li style="list-style-type:decimal" value="1" class="chapter chapter ">
            <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml" class="chapter chapter ">Introduction to Microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="8db3622b-9977-49bb-ae49-3650a826964e.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="10649beb-d64b-4a27-a513-8385ca335d99.xhtml" class="">My way into microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="6fed5ea3-e5cd-442a-b114-ffcae1f6bc15.xhtml" class="">Benefits of autonomous software components</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="7372008a-2f9b-40d9-8590-edec37778683.xhtml" class="">Challenges with&amp;#xA0;autonomous software components</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="f66cf907-69d5-4363-8d88-d69e3dba73d8.xhtml" class="">Enter microservices</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="eaec078c-9fa9-449b-b5e5-838e4c2db3bd.xhtml" class="">A sample microservice landscape</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="9ab8a242-16f5-455e-8cb7-fa25f3fa5faa.xhtml" class="">Defining a microservice</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e4d58f66-7aaa-47d2-97d7-661608013585.xhtml" class="">Challenges with microservices</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="014148c9-6aa6-4f54-85a8-3a8227de40d9.xhtml" class="">Design patterns for microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f262ffcc-67ed-4504-b2e6-b901d76248c6.xhtml" class="">Service discovery</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="09f654f4-d7da-4848-af96-f5e6305a2d8f.xhtml" class="">Problem</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="5f78d381-5788-43f3-82e1-d560c9c2ee9a.xhtml" class="">Solution</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e3443d40-5fc2-401f-9d26-b63648fadd17.xhtml" class="">Solution requirements</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="b4ee1f41-894c-4eaa-9b72-2d965e60d609.xhtml" class="">Edge server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="3fcd16e3-9705-4686-9c9f-fd65d0e84ecc.xhtml" class="">Problem&amp;#xA0;</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="aa8daec2-560b-4bfc-84e5-f45a42f7ce9d.xhtml" class="">Solution</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="42b9ef7a-41a0-43ee-8477-45ef9799aa40.xhtml" class="">Solution requirements</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="369314de-5b84-4b7c-a06c-baeb558b71b8.xhtml" class="">Reactive microservice</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="eb4c0e30-3fcd-4712-acd4-5ab3b92af05a.xhtml" class="">Problem</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b2417e90-eb5e-46f6-a7f7-a2af6ca6c852.xhtml" class="">Solution</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e2288867-fd71-4506-8941-77d06b7ff9c4.xhtml" class="">Solution requirements</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="ec007af3-c2c1-421c-92a9-54b932f23a50.xhtml" class="">Central configuration</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="29aacd71-091c-4207-9555-b216241686ea.xhtml" class="">Problem</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="3bf2df73-484d-4226-b575-eaccf1127489.xhtml" class="">Solution</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="2e021541-a2b8-4756-a5b3-085a7cc1455d.xhtml" class="">Solution requirements</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="47a42540-fd55-4115-b356-0b32ce041e8f.xhtml" class="">Centralized log analysis</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="c23f90c7-33a8-4513-a0ba-61d025f36592.xhtml" class="">Problem</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="aa9ec16e-aa68-44e5-b720-2e092e02a631.xhtml" class="">Solution</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="9801aedf-b772-40e8-a0ce-d2a2b0bfffed.xhtml" class="">Distributed tracing</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="ade16246-1053-44a0-bda7-764130453a83.xhtml" class="">Problem</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="81041137-61d9-4828-86df-fcae8a56772e.xhtml" class="">Solution</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="7c1927fa-5110-44b4-b068-713e01701a8f.xhtml" class="">Solution requirements</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="58d25777-02aa-4c7b-b871-919ad04927bc.xhtml" class="">Circuit Breaker</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="39bbb67f-769d-4e25-a7bf-77695747c44b.xhtml" class="">Problem</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="3675dd8b-c778-4246-9a87-d497d385989a.xhtml" class="">Solution</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a59ca493-6fb9-4fe4-a067-db3721840ebe.xhtml" class="">Solution&amp;#xA0;requirements&amp;#xA0;</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="a828571f-cc09-4e69-868b-ee179b076d67.xhtml" class="">Control loop</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="60ed6835-13ae-40ab-aa16-3bb5d83f4225.xhtml" class="">Problem</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="15a7151e-36f6-4991-9f1e-88b0aac6fca3.xhtml" class="">Solution</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="428dc6ac-9aaf-4e5f-9672-2cba4623aaa2.xhtml" class="">Solution requirements</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="fefdeab9-cf40-4f4f-8038-41d6eaac74c1.xhtml" class="">Centralized monitoring and alarms</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="178c20bf-d9ec-4dcf-b6e5-57e26200fe3d.xhtml" class="">Problem</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="21da3eb1-b4c1-4331-969e-2c15db7f83f8.xhtml" class="">Solution</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ca886950-9acf-459d-b488-3aecf5559d92.xhtml" class="">Solution&amp;#xA0;requirements&amp;#xA0;</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="2ae28a9c-b36d-4cde-b329-4d61a698a141.xhtml" class="">Software enablers</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="dda3404c-7a72-4bec-86d5-0dd736f09b4b.xhtml" class="">Other important considerations</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a69cc4cb-ba1d-4ce0-b13f-8d7be8ab64fa.xhtml" class="">Summary</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="2" class="chapter chapter ">
            <a href="7d969006-ea94-4bbb-858d-30dce8177a2c.xhtml" class="chapter chapter ">Introduction to Spring Boot</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="b480235d-0f98-4cdf-8e95-770fd00b0050.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="852b8367-accc-4f3c-82b8-3de453827af4.xhtml" class="">Learning about Spring Boot</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="1b096695-036a-4dfe-97bb-373fb4b10d73.xhtml" class="">Convention over configuration and fat JAR files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b1e992bd-7d74-474b-961b-7acb6a81c7e7.xhtml" class="">Code examples for setting up a Spring Boot application</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="1e76c168-f477-4cef-ba4e-1acd036d4876.xhtml" class="">The magic&amp;#xA0;@SpringBootApplication annotation</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="4d3ecfe0-332f-4d97-92c1-ebbc6664fa2d.xhtml" class="">Component scanning</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a7c3400f-95d3-4069-a382-bd73868fc42a.xhtml" class="">Java-based configuration</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="47b3b43b-25e1-40ad-8d55-5e3aff32f606.xhtml" class="">Beginning with Spring WebFlux</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="000ab583-179c-44ff-accd-9f94700b28e8.xhtml" class="">Code examples of setting up a REST service using Spring WebFlux</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="845cbde4-2805-43aa-9f27-c72e0328686b.xhtml" class="">Starter dependencies</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="07675baf-3e6c-4e27-9ca8-941e863043bc.xhtml" class="">Property files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="950455cb-352e-4ce3-9b23-f9f46ca430ba.xhtml" class="">Sample RestController</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="8b12bcdb-dfda-4233-9567-f86385b29908.xhtml" class="">Exploring SpringFox</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e3ffa432-4abc-49c4-b0d7-05c8802cdbac.xhtml" class="">Understanding Spring Data</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="4e3207d9-705b-4ddb-87bf-744969dcfa43.xhtml" class="">Entity</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e4df23cb-ae70-4029-ae9e-b4c52bd8a3a9.xhtml" class="">Repositories</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="0942853a-5bab-48ed-8c4b-351ed4e7b498.xhtml" class="">Understanding Spring Cloud Stream</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="33d1de9f-7998-4330-93af-9deebbf253ff.xhtml" class="">Code examples for sending and receiving messages with Spring Cloud Stream</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="c69dcdce-4443-4079-8868-6a9c967f88b4.xhtml" class="">Learning about Docker</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ae0756ae-0662-4aa1-8627-b93b71c5a84f.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="648a138b-0e95-4d55-95f4-0068085958ed.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="3" class="chapter chapter ">
            <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml" class="chapter chapter ">Creating a Set of Cooperating Microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="dec06bf4-f19c-4d35-b820-0e83a59506db.xhtml" class="">Technical requirements</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="92066d85-699a-4d8c-a7a8-a22525a2a5f5.xhtml" class="">Tool installation</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="35f47cb6-035f-4645-8721-7fd69f51c78c.xhtml" class="">Installing Homebrew</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="558c6c22-d553-448b-92d0-892acdf2e5e7.xhtml" class="">Using Homebrew to install Java, curl, jq, and the Spring Boot CLI</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="5c41f36a-07b4-4683-943b-f3aea36fd848.xhtml" class="">Using an IDE</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="cded0738-d1ba-42da-a941-c95bce9ad624.xhtml" class="">Accessing the source code</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="c65cfc27-1779-46e1-a345-20050172efe3.xhtml" class="">Introducing the microservice landscape</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="39d40a2b-6e3f-423c-aecf-9b06d27fe6c8.xhtml" class="">Information handled by microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="874acd3b-f961-446d-b690-d517461f9a1a.xhtml" class="">Product service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="45f924a4-94dd-47c6-928e-060f7636a29a.xhtml" class="">Review service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="0498293f-b5f2-411c-a4b7-5e1c856787a6.xhtml" class="">Recommendation service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6577e816-3859-4b9b-9f2d-1eca439855ae.xhtml" class="">Product composite service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="bca449d8-4158-4084-bf18-f39a5122c7e9.xhtml" class="">Infrastructure-related information</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="7cd84f28-d070-41e4-bb1c-bcf18d4912ba.xhtml" class="">Temporarily replacing a discovery service</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="706ca023-7342-4117-97bc-224e4f25a829.xhtml" class="">Generating skeleton microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="961dc458-a475-4dbb-a188-04e014f17cca.xhtml" class="">Using&amp;#xA0;Spring Initializr&amp;#xA0;to generate skeleton code</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="955dc0a8-fbf7-4911-bd76-1d98d3dbe728.xhtml" class="">Setting up multi-project builds in Gradle</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="d03f7c72-a4e3-4154-9909-a4f6d679fcc0.xhtml" class="">Adding RESTful APIs</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="db782cc3-b08f-4b7b-98f5-717d490f72fb.xhtml" class="">Adding an API and a util project</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="ea52f321-2328-4b1c-b6fa-45fdbc5cd967.xhtml" class="">The api project</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6aba4e84-d9da-44b0-8e07-0f3ce16c53fb.xhtml" class="">The util project</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="29225038-3048-4a76-ab2c-05953a170c66.xhtml" class="">Implementing our API</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="22da1733-2f15-40b6-9eba-d2ef4155a594.xhtml" class="">Adding a composite microservice</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="7cf86534-963a-431b-9aee-17f25d04fa9b.xhtml" class="">API classes</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6f6a4e94-8378-4273-9aa4-d6fbc8f35b03.xhtml" class="">Properties</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="fd867f85-3b3f-4927-aa6c-be59ad214831.xhtml" class="">Integration component</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1aa7f43a-cddc-4ab2-ab67-b2d19ad45f26.xhtml" class="">Composite API implementation&amp;#xA0;</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="4debfdb9-fadf-4548-a3df-479c6215a626.xhtml" class="">Adding error handling</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="269de0a2-408c-4b22-bf63-89b6047d289f.xhtml" class="">The global REST controller exception handler</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="702611b9-1753-41b9-a311-8d20a54d7786.xhtml" class="">Error-handling in API implementations</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="cf011699-0ad6-4b74-8d6f-39eccf33f5e2.xhtml" class="">Error-handling in the API client</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="b14e6354-5575-4e69-b10e-1f64e0ed87c6.xhtml" class="">Testing APIs manually</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="31b9b7bc-faf5-4712-b4d5-6172cb638e60.xhtml" class="">Preventing slow lookup of the localhost hostname</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="2cbdf00e-9bab-4fca-a8c2-3e5dd5e3d87e.xhtml" class="">Adding automated microservice tests in isolation</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1d73dd9c-fee5-440b-b229-e5656230de8b.xhtml" class="">Adding semi-automated tests of a microservice landscape</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="bf271cff-cd60-4eaf-99b8-e3d408a55a1b.xhtml" class="">Trying out the test script</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="ee696807-d5f6-463a-bddd-a3244ff5f948.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="83e4df7f-68d2-4071-8ba6-c1185a93076a.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="4" class="chapter chapter ">
            <a href="ce37c0f1-ac54-4258-8cb6-7c6f50f26172.xhtml" class="chapter chapter ">Deploying Our Microservices Using Docker</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f2e3c9a5-74a7-4ad8-856e-c6420476722d.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="c974e63e-671c-492d-adfc-e9a1feed30ec.xhtml" class="">Introduction to Docker</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="9dc4d547-13b8-464d-8398-d00fadac6328.xhtml" class="">Running our first Docker commands</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="1cee3f9f-f274-401a-9f9e-3da4a0d42c0f.xhtml" class="">Challenges with running Java in Docker</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="77e9143e-79c8-412b-8406-b0c3fcfb83ce.xhtml" class="">Java without Docker</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="889a27c1-1877-4539-84f0-38ebc4eb1d90.xhtml" class="">Java in Docker</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="e835c329-2ac0-499f-873c-8487c99e11e6.xhtml" class="">CPU</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="2c69e514-b59a-400f-8246-29ec825e66b0.xhtml" class="">Memory</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="0bf631e0-5d7f-4dba-b7c7-3191ff78e66d.xhtml" class="">Problems with Docker and Java SE 9 (or older)</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="e65790c8-e465-43d7-9c16-9a916132b9cd.xhtml" class="">Using Docker with one microservice</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="ec683cb9-d4de-49ec-8e4b-6be934a791b5.xhtml" class="">Changes in source code</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="90d88bde-802e-4fac-878b-e81e5d14d3e6.xhtml" class="">Building a Docker image</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="89b4207f-0e89-4e65-9b35-6cdfb8e092bf.xhtml" class="">Starting up the service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9f8f9aef-25b1-40f1-a335-853ab0acd7a1.xhtml" class="">Running the container detached</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="31f506f2-59d6-497b-9dad-42bb79c22dd2.xhtml" class="">Managing a landscape of microservices using Docker Compose</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="9ffa0915-62b1-4eda-a583-6d8b17dcc3f0.xhtml" class="">Changes in the source code</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="c52c695e-d0b2-4f5c-8e91-6bc9693ca97b.xhtml" class="">Starting up the microservice landscape</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="6cab1eab-a116-4f4c-8719-3503c57d734d.xhtml" class="">Testing them all together automatically</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="485e0c65-e23d-42f7-8559-218fd6a7c460.xhtml" class="">Troubleshooting a test run</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="66c3d30d-9b7d-433f-90e2-6411573586e8.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="4b70b3a4-958d-4dca-bd2a-20c7ae9af330.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="5" class="chapter chapter ">
            <a href="ba24a656-10a1-4a3e-879e-6589621ef125.xhtml" class="chapter chapter ">Adding an API Description Using OpenAPI/Swagger</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="5ea57882-3d26-44d2-a6ce-193cc0f400c6.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="c6956195-0873-4148-b48e-d4830e2ccc1f.xhtml" class="">Introduction to using SpringFox</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1b17833f-f628-462f-b86b-bb304da7c743.xhtml" class="">Changes in the source code</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f398c000-6391-472a-aa17-0c4f6dfbf8ed.xhtml" class="">Adding dependencies to the Gradle build files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="f53ad581-d339-4b72-ae5e-e480e53692f2.xhtml" class="">Adding configuration and general API documentation to Product Composite Service Application</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="c0ca2294-2af5-4f69-8fa3-26e0eac5b5b1.xhtml" class="">Adding API-specific documentation to ProductCompositeService</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1e199467-d55a-49a7-8ab2-33fab5e6bcb1.xhtml" class="">Adding textual descriptions of the API to the property file&amp;#xA0;</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="83dd2b8e-8eee-4cd6-a037-e3fbc2e46284.xhtml" class="">Building and starting the microservice landscape</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="5c39a0b4-7f66-4d53-9ca6-29c7d634d854.xhtml" class="">Trying out the Swagger documentation</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e9360b5f-788f-4b4e-9937-0c5125391d52.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a59e765f-361d-41f1-b221-758275ae7a41.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="6" class="chapter chapter ">
            <a href="6c495e97-f473-4cb1-a404-11fd938f5478.xhtml" class="chapter chapter ">Adding Persistence</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="5788bc54-5b53-4b0d-bf71-6710b8a2cbcd.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="8e1784d2-49ad-4baf-afcd-a803288c6a65.xhtml" class="">But first, let's see where we are heading</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="375f1ce7-2f88-4107-969c-ecfa2404b6cf.xhtml" class="">Adding a persistence layer to the core microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f9bfdbd7-c9e7-48ab-8667-5ab626c91196.xhtml" class="">Adding dependencies</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="08bafe46-84a4-45f5-b0be-445be64ec930.xhtml" class="">Storing data with entity classes</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="643da5c5-9816-4e2d-b83d-feb54c14f413.xhtml" class="">Defining repositories in Spring Data</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="8d1ac89a-dc3f-453d-a520-599c66d70ce7.xhtml" class="">Writing automated tests that focus on persistence</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="c6f56a80-39a4-436d-936e-4083d80112b8.xhtml" class="">Using the persistence layer in the service layer</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="595b296b-075a-43f2-8c7e-1604d884f1b0.xhtml" class="">Log the database connection URL</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="dc33ae2b-86c2-4aad-949e-dfcd5151029f.xhtml" class="">Adding new APIs</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1a403a16-cd24-4e29-a155-a72c28eb0cbe.xhtml" class="">The use of the persistence layer</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="d6698ac9-e62c-4cdc-b9bf-4a546c219b25.xhtml" class="">Declaring a Java bean mapper</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="767052f2-4534-48ae-9b98-8a17acd2069a.xhtml" class="">Updating the service tests</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="4909d4dd-ebf3-46d6-9e4c-bdd3a437a26a.xhtml" class="">Extending the composite service API</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="6de8ed56-8a78-4fe0-b6f6-6d41f2d97217.xhtml" class="">Adding new operations in the composite service API</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="891b515d-b606-4c8d-88b8-a810489dbdf3.xhtml" class="">Adding methods in the integration layer&amp;#xA0;</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="16879842-0556-43d7-809e-c43079240e0a.xhtml" class="">Implementing the new composite API operations</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ab5c16eb-e6c6-483d-abf8-8db40491f638.xhtml" class="">Updating the composite service tests</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="d5b0a2d4-40a8-49c2-827d-de57cb80fbfc.xhtml" class="">Adding databases to the Docker Compose landscape</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="d09d5d7e-ae01-4e88-aa90-aeca975a65ed.xhtml" class="">The Docker Compose configuration</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="38428fde-51d6-4910-9db4-16632981507f.xhtml" class="">Database connect configuration</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="c9e41f90-e806-433c-bd43-40748bb6e6d7.xhtml" class="">The MongoDB and MySQL CLI tools</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="16f2907c-3ddb-40de-acd1-ad47b8f93ae9.xhtml" class="">Manual tests of the new APIs and the&amp;#xA0;persistence layer</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="cce10a05-0e76-49d5-a950-ea5a9f55c57a.xhtml" class="">Updating the&amp;#xA0;automated tests of the&amp;#xA0;microservice landscape</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ef64c159-8166-4617-b7f6-504dfcbbab4a.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="d07dbf89-a8b2-43cd-b01d-c69f50c18836.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="7" class="chapter chapter ">
            <a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml" class="chapter chapter ">Developing Reactive Microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="46dbe5c7-c11e-4e2b-aaa9-8be513ca104e.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="45b22692-ef94-4bf2-9605-0215479d9c8a.xhtml" class="">Choosing between non-blocking synchronous APIs and event-driven asynchronous services</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="df3cd1d2-7c61-4683-a50e-de2b2cc0086c.xhtml" class="">Developing non-blocking synchronous REST APIs using Spring</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="52398dae-1dc5-40d5-8c47-62b7ca86c0cf.xhtml" class="">An introduction to Spring Reactor</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1ce831a7-29c1-4e8f-a849-28890a4afcd3.xhtml" class="">Non-blocking persistence using&amp;#xA0;Spring Data for&amp;#xA0;MongoDB</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="fe4c254c-e1ef-43d5-8ac8-182a4fe06bb8.xhtml" class="">Changes in the test code</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="7c38050b-6cec-4b5e-b2b2-ab34ec70d347.xhtml" class="">Non-blocking REST APIs in the core services</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="c1f0e658-d95b-4811-ab48-38193d3637ec.xhtml" class="">Changes in the APIs</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="dc4f65a5-52c9-41f3-9658-0e744595debf.xhtml" class="">Changes in the&amp;#xA0;service implementations</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="5ea14d9c-e777-43ba-b583-e2e31e041ad0.xhtml" class="">Changes in the test code</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="90790607-0283-459e-94cc-af871c1950ae.xhtml" class="">Dealing with blocking code</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="a7471df5-dcf3-4ed3-8abc-3ec62c6c51fe.xhtml" class="">Non-blocking REST APIs in the composite services</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="a4ccc336-0c13-490e-92f6-325c322308fa.xhtml" class="">Changes in the API</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="01d86b2e-36ac-4e37-a0e9-91d39bda3467.xhtml" class="">Changes in the integration layer</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e85bc29b-08dd-4c3a-a762-f7ec8ad8fc10.xhtml" class="">Changes in the service implementation</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="cede6414-f57f-4101-97ca-2ad473c2a132.xhtml" class="">Changes in the test code</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="61749441-4a44-4fb9-954e-7fce8c85f019.xhtml" class="">Developing event-driven asynchronous services</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="2377aed8-acf5-4538-9304-eac003f5b5d0.xhtml" class="">Configuring Spring Cloud Stream to handle challenges with messaging</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="75f0b5a8-e845-47ad-870f-b7fe0af18805.xhtml" class="">Consumer groups</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="fcf53215-c51a-42b9-a887-60bf71222304.xhtml" class="">Retries and dead-letter queues&amp;#xA0;</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="3b7e34ad-cf25-40ee-b8d5-590e9610896d.xhtml" class="">Guaranteed order and partitions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="240b6736-a16e-4fb0-8e6e-ef76ad291825.xhtml" class="">Defining topics and events</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="454367a4-3b2f-499d-9348-6842fd140bf5.xhtml" class="">Changes in the Gradle build files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9d38c093-e24d-4794-9ce5-831fef204030.xhtml" class="">Publishing events in the composite service</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="5289d70d-b716-40f9-83ef-6ff9602f748f.xhtml" class="">Declaring message sources and publishing events in the integration layer</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="d3c5d285-4a9a-4066-9078-2b2276e158de.xhtml" class="">Adding configuration for publishing events</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="3b49255c-73d7-4374-9008-9850b8f192d5.xhtml" class="">Changes in the test code</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="3a8d3539-cce2-454c-8dae-a81d3e094383.xhtml" class="">Consuming events in the core services</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="39e619b1-9003-4195-9900-03bd6f3d0803.xhtml" class="">Declaring message processors</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="71aacbc3-ec46-46fc-9ad0-9c0864a02236.xhtml" class="">Changes in the service implementations</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="7477ce7a-936d-4f15-a34e-fdbc55376766.xhtml" class="">Adding configuration for consuming events</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="8c015d72-bc44-45ea-bd59-fa5a7d43ebbf.xhtml" class="">Changes in the test code</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="9e40f177-25ad-495e-9b0a-b8a08bb83983.xhtml" class="">Running manual tests of the reactive microservice landscape</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="686c1e75-a991-4c7b-8c2f-1716293c48f8.xhtml" class="">Saving events</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="2ace04dc-17c5-4429-894a-e98e3201dfd0.xhtml" class="">Adding a health API</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="27077f30-5aec-4e78-abc7-aefbf0ae6a22.xhtml" class="">Using RabbitMQ without using partitions</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="d58e0cff-c4f4-46da-a7a1-83f137315af2.xhtml" class="">Using RabbitMQ with two partitions per topic</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="87ebcc94-bbe5-44f6-a4aa-f4a3e7bf0d3d.xhtml" class="">Using Kafka with&amp;#xA0;two partitions per topic</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="62586a1e-c0e2-43f2-a8b8-ce2f029fad04.xhtml" class="">Running automated tests of the reactive microservice landscape</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6bb74d0c-6e44-4ba1-b8ad-60b5f02957bb.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a47d3c9e-d61d-4dda-82ab-b61c07c14692.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="2" class="chapter section ">
            <a href="be555c3a-09e9-4fbc-ab86-54daa9eb643a.xhtml" class="chapter section ">Section 2: Leveraging Spring Cloud to Manage Microservices</a>
    
    
</li>
<li style="list-style-type:decimal" value="8" class="chapter chapter ">
            <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml" class="chapter chapter ">Introduction to Spring Cloud</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="59f96b5d-a02a-4a02-a311-03976bbbfb23.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="2629cfb4-5ca4-42ab-806d-60fd6901e2ff.xhtml" class="">The evolution of Spring Cloud</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e3e3f81f-2616-4e50-a646-4195e12b5f42.xhtml" class="">Using&amp;#xA0;Netflix Eureka as a discovery service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="bc5afe13-b7f7-4ade-b4be-7e95ac760a52.xhtml" class="">Using Spring Cloud Gateway as an&amp;#xA0;edge server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ab4aaf45-2df1-47a5-a6e1-3836dfdd80dc.xhtml" class="">Using Spring Cloud Config for centralized configuration</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9c8de61b-60b4-4a00-a69d-c958985f8dde.xhtml" class="">Using Resilience4j for improved resilience</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="d1b006bb-8090-4891-8b31-3f0654cad03a.xhtml" class="">Sample usage of the circuit breaker in&amp;#xA0;Resilience4j</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="6dd67d88-b2e3-4667-b4f5-4564abdd8047.xhtml" class="">Using Spring Cloud Sleuth and Zipkin for distributed tracing</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ca67da5b-5132-407d-af9d-b3e3bcb18fd4.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6139c767-32a1-496b-bf12-e9ceb956de17.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="9" class="chapter chapter ">
            <a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml" class="chapter chapter ">Adding Service Discovery Using Netflix Eureka and Ribbon</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="38069eeb-1754-4ff4-b439-dc0184051a14.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="3ba979e5-a0b2-4931-b3dd-230bb33e1184.xhtml" class="">Introducing service discovery</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="d90e4114-ba33-4240-bdf0-9722bbd51212.xhtml" class="">The problem with DNS-based service discovery</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="84485cd6-d601-4e94-94e5-abdc1a24af42.xhtml" class="">Challenges with service discovery</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9ad99889-362e-4be1-97f8-03650e629b9c.xhtml" class="">Service discovery with Netflix Eureka in Spring Cloud</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="78de8b68-d2ad-4d8f-ad26-36bde6deac86.xhtml" class="">Setting up a Netflix Eureka server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9fc180b2-468c-4ea4-847b-7178180626aa.xhtml" class="">Connecting&amp;#xA0;microservices to a&amp;#xA0;Netflix Eureka server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b9309eb9-e43d-4735-b820-979700390df1.xhtml" class="">Setting up configuration for use in the development process</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="17b3e13d-e443-4a7e-a7c1-f6b131840063.xhtml" class="">Eureka configuration parameters</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="7819380b-1eac-4038-9b4a-d3391716b2ce.xhtml" class="">Configuring the Eureka server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="901dd19e-3c02-42e5-a297-fbfd8be0fdee.xhtml" class="">Configuring clients to the Eureka server</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="9ebccb44-fb89-40b8-aa5d-2cae93189f57.xhtml" class="">Trying out the discovery service</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="bdfeea23-b823-41c2-8e97-14eeee796fa8.xhtml" class="">Scaling up</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="5cfa7ee2-d313-43ad-8728-6ce34b85ba1b.xhtml" class="">Scaling down</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="5dbfcc3e-3f1f-4334-b8f4-ba0257a57444.xhtml" class="">Disruptive tests with the Eureka server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="43edd8f9-6e91-4160-b581-e48f98495173.xhtml" class="">Stopping the Eureka server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ea80018d-7393-4402-ba36-cc58e6d24f02.xhtml" class="">Stopping a review instance&amp;#xA0;</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a5e2445c-d2a4-4c8d-9c48-817ec0b46870.xhtml" class="">Starting up an extra instance of the product service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="4fd1b741-845a-4f9a-a99e-c16e90121dd6.xhtml" class="">Starting up the Eureka server again</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="49da99ac-7bcd-4230-abb3-568925ddcd7d.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="8d9e01ea-ca27-4cbb-90a7-470c8e8e4ed9.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="10" class="chapter chapter ">
            <a href="a3383211-405d-4319-b142-ddb8cf3674fd.xhtml" class="chapter chapter ">Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="71739452-e0a6-4de8-a324-5b1673baf3d2.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="faf47268-7f39-466c-b471-754688bad3d4.xhtml" class="">Adding an edge server to our system landscape</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b92fb391-6bfa-4aee-82c8-3abae1ef88a5.xhtml" class="">Setting up a Spring Cloud Gateway</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="a88e148d-f325-4ed9-95cc-041e75024f8b.xhtml" class="">Adding a&amp;#xA0;composite&amp;#xA0;health check</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="f180c0f5-798a-48ed-94b4-9d5c21c0b21d.xhtml" class="">Configuring a&amp;#xA0;Spring Cloud Gateway</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="4b01e05b-3459-4517-a9a2-f0dc6d487ecf.xhtml" class="">Routing rules</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="b2137e48-e436-462b-b110-e66dd0a790a1.xhtml" class="">Routing requests to the product-composite API</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="4f686c64-cf3e-435a-ad57-e842a806cabc.xhtml" class="">Routing requests to the Eureka server's API and web page</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="0834136b-05e6-4e79-9312-af6152834844.xhtml" class="">Routing requests with predicates and filters</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="b24ee5ee-8b96-4f95-b8c4-5a90b65f5624.xhtml" class="">Trying out the edge server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f9260f75-fc81-4a73-8864-d49525e205af.xhtml" class="">Examining what is exposed outside the Docker engine</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="92f77efd-5e57-408c-9214-888241ffb82d.xhtml" class="">Trying out the routing rules</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="a4e7c05b-f47e-44fc-9fe4-1450d4df09d2.xhtml" class="">Calling the product composite API through the edge server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="dbbff44b-5aa6-4bb3-9b4f-b4c20114a037.xhtml" class="">Calling Eureka through the edge server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="331d53a3-01cb-4fc9-84a9-bda0f4d754e8.xhtml" class="">Routing based on the host header</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="549b89f8-6f59-4fa5-9834-4ef3db5a7aa2.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1fd6e3b2-4b67-49cb-a7d7-4722236070d2.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="11" class="chapter chapter ">
            <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml" class="chapter chapter ">Securing Access to APIs</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="30415f77-158f-4f1d-9d2e-295e458dd42e.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="8d89ac2c-fe13-4b34-a534-0bad72d673f7.xhtml" class="">Introduction to OAuth 2.0 and OpenID Connect</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="a0a2e38d-0b9f-403e-9d23-fe9c42c6ca0b.xhtml" class="">Introduction to OAuth 2.0</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6676fe43-b6ee-48c1-8771-ca298cfb3931.xhtml" class="">Introducing OpenID Connect</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="5068f36d-91b5-44b4-a43c-ccb678bad85b.xhtml" class="">Securing the system landscape</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b04a42bd-8bf7-4341-888e-d6da5e5f97bc.xhtml" class="">Adding an authorization server to our system landscape</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="753f2205-afee-41aa-a1d1-6ce3220b4cae.xhtml" class="">Protecting external communication with HTTPS</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="d548df90-09f2-4dc4-9d66-5ff5cb242ae6.xhtml" class="">Replacing a self-signed certificate in runtime</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="84f2e00e-7a9e-48ab-ad8a-73ff73d35488.xhtml" class="">Securing access to the discovery service, Netflix Eureka</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f9e2c3f7-cb81-438c-acb8-3074af3702ef.xhtml" class="">Changes in the Eureka server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="2788ecae-9864-4c6a-b8be-2c02da1b8918.xhtml" class="">Changes in Eureka clients</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="95b5b481-1f8c-4806-85dd-d80ddb7f53a5.xhtml" class="">Testing the protected Eureka server</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="e3ae4ca0-3919-4ef3-bf2b-c56b922b2b57.xhtml" class="">Authenticating and authorizing API access using OAuth 2.0 and OpenID Connect</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f3923e48-a769-445e-92ed-88a1f7f30f6f.xhtml" class="">Changes in both the edge server and the product-composite service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1fb71eb8-de5a-451e-a952-6cfbd1d648cc.xhtml" class="">Changes in the product-composite service</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="53f51c86-3e3a-4592-a6af-d4e945f4f9ba.xhtml" class="">Changes in the test script</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="c51055cf-9e05-4ab9-9b93-64e65a4214c1.xhtml" class="">Testing with the local authorization server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="890132aa-f2c3-4469-a77e-05d8413a43bb.xhtml" class="">Building and running the automated tests</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="980dffdc-0bd3-4950-aa64-c53d461ae74d.xhtml" class="">Acquiring access tokens</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="23502b54-f728-4f79-a251-8f05e5d388e9.xhtml" class="">Acquiring access tokens using the password grant flow</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="390832ba-f5f0-4316-bf1b-2e3767770891.xhtml" class="">Acquiring access tokens using the implicit grant flow</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="24803036-713d-4aaf-b102-1835aaeda9a2.xhtml" class="">Acquiring access tokens using the code grant flow</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="e3707ff2-8a49-4890-9e6d-84e925e446b6.xhtml" class="">Calling protected APIs using access tokens</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="9abc3b9c-fd7b-4a84-b744-a035ce118ede.xhtml" class="">Testing with an OpenID Connect provider &amp;#x2013; Auth0</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="d5590c53-784c-4c69-885d-01f8b0fd4681.xhtml" class="">Setting up an account and OAuth 2.0 client in Auth0</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="f3401e59-f0c9-4e91-b6d3-2cb6c38109fe.xhtml" class="">Applying the necessary&amp;#xA0;changes to use&amp;#xA0;Auth0 as an OpenID provider</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="15e2f05c-8b26-417f-9adc-80e5e63bb2ba.xhtml" class="">Changing the configuration in the OAuth resource servers</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="0468bbd8-04c4-41d6-af3d-d1a9e466bb73.xhtml" class="">Changing the test script so it acquires access tokens from Auth0</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="94a573df-c630-48d6-b209-9e9b73d38a7b.xhtml" class="">Running the test script with Auth0 as the OpenID Connect provider</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="8ae0314c-d43c-4183-9e57-bfff1094a450.xhtml" class="">Acquiring access tokens using the password grant flow</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="74088191-40bf-4260-9ae2-22d3ad11793d.xhtml" class="">Acquiring access tokens using the&amp;#xA0;implicit grant flow</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9ce74b39-d6ee-476d-b2bd-cb99fb2d9ad6.xhtml" class="">Acquiring access tokens using the&amp;#xA0;authorization code grant flow</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b533d5a3-eb8e-4579-acbd-56732afe3e45.xhtml" class="">Calling protected APIs using the Auth0 access tokens</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="282ce5ac-912d-4845-861c-4cbb164dfe16.xhtml" class="">Getting extra information about the user</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="eca6e3a8-2569-4248-814b-c9e882dd173c.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b8760299-a69a-4565-9de2-3e893a4b4fcb.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="12" class="chapter chapter ">
            <a href="a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml" class="chapter chapter ">Centralized Configuration</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="bcd7ed4c-66d8-4399-9fdb-dc424c39e895.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="198ee65e-7921-422d-bfb2-cdf27ac2920f.xhtml" class="">Introduction to the Spring Cloud Configuration server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="9134c695-af98-46a7-bc06-59296244e6c7.xhtml" class="">Selecting the storage type of the configuration repository</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="c9a72427-cb0d-4262-845e-802da2254798.xhtml" class="">Deciding on the initial client connection</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="86d2f3d5-fe8d-4550-b916-a40934f6e608.xhtml" class="">Securing the configuration</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="7a1d4005-ca72-4e5c-ab41-502827f24b73.xhtml" class="">Securing the configuration in transit</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="bdce0150-ce58-4eb0-8f2d-1ffd74eb6063.xhtml" class="">Securing the configuration at rest</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="05474884-3931-4130-8364-d7cb0be2014a.xhtml" class="">Introducing the config server API</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="b7818778-8904-43ee-ac22-309858b5067c.xhtml" class="">Setting up a config server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="38085d49-19f2-42b4-bfb7-3f210b472cd7.xhtml" class="">Setting up a routing rule in the edge server&amp;#xA0;</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a0cf673b-187c-4277-b480-4465cf49e038.xhtml" class="">Configuring the config server for use with Docker</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="5bb0b31c-02ea-4cd0-9199-72457c2319d8.xhtml" class="">Configuring clients of a&amp;#xA0;config&amp;#xA0;server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="a705313e-4d67-4030-9313-15e2f267e3d0.xhtml" class="">Configuring connection information</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="82af7a35-3a05-4e9f-b724-a28620b29263.xhtml" class="">Moving the partitioning configuration from Docker Compose files to the configuration repository</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="699625bd-6727-4724-b2e3-15d18cd9eeef.xhtml" class="">Structuring the configuration repository</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="86e363ee-dd96-4909-a275-ea2370b03553.xhtml" class="">Trying out the&amp;#xA0;Spring Cloud Configuration server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="1908a5c7-bcab-4d56-8d4b-660ed6e6ff3e.xhtml" class="">Building and running automated tests</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ddd464cc-9d82-4d1c-8c87-3aca8087aaee.xhtml" class="">Getting the configuration using the config server API</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="98e624e2-6439-4a65-a5a3-3935752f25f6.xhtml" class="">Encrypting and decrypting sensitive information</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="1eff5db2-f899-4d73-9c2a-727df6675365.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="822bd01f-ac86-4f1d-add5-289a9e99af12.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="13" class="chapter chapter ">
            <a href="23795d34-4068-4961-842d-989cde26b642.xhtml" class="chapter chapter ">Improving Resilience Using Resilience4j</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="c56574a4-f2a6-4197-a4c1-bc1ab4783d1d.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="716e8d47-5f20-45c3-b55f-b5c379331e9c.xhtml" class="">Introducing the Resilience4j circuit breaker and retry mechanism</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="19b903fb-0d77-4fad-9d36-caff3338e658.xhtml" class="">Introducing the circuit breaker</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="599d0875-1212-494b-ae12-7d6e89061a08.xhtml" class="">Introducing the retry mechanism</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="5d8e1e56-866a-468e-8ca7-2008cfc88943.xhtml" class="">Adding a&amp;#xA0;circuit breaker and retry&amp;#xA0;mechanism to the source code</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="b18ccabc-dddb-491f-b582-8d7605e7de7c.xhtml" class="">Adding programmable delays and random errors</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="5e1deac3-58bb-4acf-811c-0c6a3bb4cb70.xhtml" class="">Changes in the API definitions</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="56063de4-5883-44d4-a9be-d7b43d9d14b7.xhtml" class="">Changes in the product composite&amp;#xA0;microservice</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a5f8bd43-947b-4070-963b-cf7806d3ec16.xhtml" class="">Changes in the product microservice</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="d3dd6e13-387e-4bf5-8760-fb27cff94925.xhtml" class="">Adding a circuit breaker</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="40019359-9829-415d-986d-bdc5a43f3c49.xhtml" class="">Adding dependencies to the build file</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="4413427c-fb56-432c-b080-42d9606ba06e.xhtml" class="">Adding the circuit breaker and timeout logic</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="3bc273cd-0928-4668-a749-78c32e7fcfb0.xhtml" class="">Adding fast fail fallback logic</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6685191b-6943-49b2-8f0b-29e38bb1ffd5.xhtml" class="">Adding configuration</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="345f63bb-e36e-4395-8874-52dd5d031696.xhtml" class="">Adding a retry mechanism</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="63dd6c65-ca70-42be-85b5-753df4d23b65.xhtml" class="">Adding the retry annotation</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e5e68935-8a15-4f71-a1da-b07a9dfcd413.xhtml" class="">Handling retry-specific exceptions</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="175f108b-bd8b-4c20-8f0f-6891d92a9b09.xhtml" class="">Adding configuration</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="574f1677-4cd0-4809-8bfe-9db023a02ef1.xhtml" class="">Adding automated tests</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="7b576055-9c38-488c-86f6-91eaf550a119.xhtml" class="">Trying out the circuit breaker and retry mechanism</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="62f264fe-0578-4c10-9801-4b1d88eec93d.xhtml" class="">Building and running the automated tests</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="fbcc6197-24f9-4149-b24d-4a1ef6695f50.xhtml" class="">Verifying that the circuit is closed under normal operations</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="81b7a336-096e-4751-af1b-8d3cb2bbb4ef.xhtml" class="">Forcing the&amp;#xA0;circuit breaker to open when things go wrong</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="31abc4b7-b66b-48d4-984a-733846530960.xhtml" class="">Closing the&amp;#xA0;circuit breaker again</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="5c729682-8c23-4aa9-8399-46d7ffebe39a.xhtml" class="">Trying out retries caused by random errors</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="08b0ed5c-e5b9-4f20-b70a-e209636ba2f2.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6dc80909-1e6a-46c2-aa8c-4c75136b4d44.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="14" class="chapter chapter ">
            <a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml" class="chapter chapter ">Understanding Distributed Tracing</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="b2d399bb-fb27-4e20-81a7-f7418ff26da0.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="02b60666-1899-426c-b4ed-0b01476c49d9.xhtml" class="">Introducing distributed tracing with Spring Cloud Sleuth and Zipkin</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="40af2310-acc3-46be-9fc3-c0fe1a775b41.xhtml" class="">Adding distributed tracing to the source code</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="0dc8a4ec-4d4f-401b-b390-2b70ceff9276.xhtml" class="">Adding dependencies to build files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="c4f16490-fddf-4c56-8693-4df017b9c90f.xhtml" class="">Adding configuration for Spring Cloud Sleuth and Zipkin</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e3e40713-5ed6-4a75-a60b-25b7f29757f3.xhtml" class="">Adding Zipkin to the Docker Compose files</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="9df24f00-5878-42e7-846b-97ed0797a30d.xhtml" class="">Trying out&amp;#xA0;distributed tracing</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="002f212e-9f17-4c5a-b01e-65288e6fbbaf.xhtml" class="">Starting&amp;#xA0;up the system landscape with RabbitMQ as the queue manager</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="46d1180f-0b3f-4614-8d14-c288596ee986.xhtml" class="">Sending a successful API request</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="27efcec5-a4da-4dd5-980d-2b194b4d5a32.xhtml" class="">Sending&amp;#xA0;an unsuccessful API request</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a593ae77-d96d-4a4a-8639-4ec04f092b07.xhtml" class="">Sending an API request that triggers asynchronous processing</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="8ace3ae1-d548-4c3c-8ad0-a1c1d5469b91.xhtml" class="">Monitoring trace information passed to Zipkin in RabbitMQ</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="187967c0-900a-41de-9b32-9e5dc50e429b.xhtml" class="">Using Kafka as a message broker</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="7ac1f95a-af66-4547-80ae-a94f201394f0.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="7c27ed75-00c5-4dfc-8ffd-5439f86ab1e7.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="3" class="chapter section ">
            <a href="d8d4dec4-c708-4bf7-bff8-a904cdca97ab.xhtml" class="chapter section ">Section 3: Developing Lightweight Microservices Using Kubernetes</a>
    
    
</li>
<li style="list-style-type:decimal" value="15" class="chapter chapter ">
            <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml" class="chapter chapter ">Introduction to Kubernetes</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="1c2c4da6-d8f9-4aea-b846-3d20df4f5c58.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9247e8c6-dc56-43f3-8891-bf50ed7b73a2.xhtml" class="">Introducing Kubernetes concepts</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="84b8c3cf-740c-42b3-bdc0-ee217b387f1d.xhtml" class="">Introducing Kubernetes&amp;#xA0;API objects</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="be9770ff-3311-4c73-a4b8-7f3ae9bb34b9.xhtml" class="">Introducing Kubernetes runtime components</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a129919c-5e78-42a1-95e2-71ebe7aeef47.xhtml" class="">Creating a&amp;#xA0;Kubernetes cluster using Minikube</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="7a1788f2-be15-442c-9bd3-b44a1425802a.xhtml" class="">Working with Minikube profiles</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="bdf2d112-1864-4f30-9e3d-070c6e1ff6c2.xhtml" class="">Working with Kubernetes CLI, kubectl</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="097d0e1e-5dcd-4d7f-99a2-5bba1488660c.xhtml" class="">Working with kubectl contexts</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="cec69ddf-bad3-4b6d-9e62-9a178cb18b6b.xhtml" class="">Creating a&amp;#xA0;Kubernetes cluster</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="f7f1b248-3f18-46e6-9ef9-cc4f2c8ba243.xhtml" class="">Trying out a sample deployment</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="46de86a1-7333-401b-90e8-a2c2327a3573.xhtml" class="">Managing a&amp;#xA0;Kubernetes cluster</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="55a7a86c-72df-4381-95dd-c6ba3a755308.xhtml" class="">Hibernating and resuming a Kubernetes cluster</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="8231970d-f7b7-433e-8c90-524b85d2bcb9.xhtml" class="">Terminating a Kubernetes cluster</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="d7eaa41b-5955-44d2-a9b1-cacecd000562.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="da15427c-58f4-45ea-8da2-acb12eab75a9.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="16" class="chapter chapter ">
            <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml" class="chapter chapter ">Deploying Our Microservices to Kubernetes</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="6a4db379-97d5-4cfe-b95d-861da5080a01.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="59615fe4-885f-4744-ad1c-35c3ade124e7.xhtml" class="">Replacing Netflix Eureka with Kubernetes services</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e5c2cb4e-3a6d-4713-9c9e-be9dc60c6fc9.xhtml" class="">Introducing Kustomize</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="4ad62a34-fed4-406a-ba9e-ee3bab8071fd.xhtml" class="">Setting up common definitions in the base folder</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="84b9e4a6-c560-4f1f-8f6f-360850ec7363.xhtml" class="">Deploying to Kubernetes for development and test</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="708f6213-798c-4bd7-b58f-b38611e24a65.xhtml" class="">Building Docker images</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9b7f05c9-f3ff-4a24-a2ff-3be588764f3a.xhtml" class="">Deploying to Kubernetes</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6dfcc1e0-bde9-4c8e-9311-49945d2519d9.xhtml" class="">Changes in the test script for use with Kubernetes</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="02855bfd-0c36-4720-a912-aef03828277a.xhtml" class="">Reaching the internal actuator endpoint using Docker Compose</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e30e320b-0e43-4884-a75d-848dc9130abb.xhtml" class="">Reaching the internal actuator endpoint using&amp;#xA0;Kubernetes</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="013ee3c6-8dc3-411e-bb22-4b4cac57863e.xhtml" class="">Choosing between Docker Compose and Kubernetes</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="a468091a-7e9f-479b-83c9-a1ec3b7178fb.xhtml" class="">Testing the deployment</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="0513ae0b-654c-4901-987e-410786d4a825.xhtml" class="">Deploying to Kubernetes for&amp;#xA0;staging&amp;#xA0;and production</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="9864b001-42db-4628-8c21-1ad852a370c7.xhtml" class="">Changes in the source code</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a84f30fb-75ca-4fe2-b82c-1a5424542c0e.xhtml" class="">Deploying to Kubernetes</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="5c93dbb3-fec0-47d8-bfb0-dc7a3ad6cb78.xhtml" class="">Performing a rolling upgrade</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="18306d59-74a0-44a6-8f1e-28949ab45e8b.xhtml" class="">Preparing the rolling upgrade</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="7d03527e-13df-48f4-9139-43a64bb86e79.xhtml" class="">Upgrading the product service from v1 to v2</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="855785f7-83a7-4d3d-8ed6-ad32fbe647fe.xhtml" class="">Rolling back a failed deployment</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="c53fb691-e63d-4884-8ac9-2f401182f292.xhtml" class="">Cleaning up</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="c997e575-8fdf-4764-a6f2-b0e77cde5cbd.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="6b52f131-c515-4366-a773-65ee61bdd234.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="17" class="chapter chapter ">
            <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml" class="chapter chapter ">Implementing Kubernetes Features as an Alternative</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="08ac664b-6071-46eb-b3dd-a530f5b521b5.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a61f1a70-971d-45ed-90a9-86849a26ebf7.xhtml" class="">Replacing the Spring Cloud Config Server</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="dbce8bb3-581a-47d2-b432-4057d9d9a9de.xhtml" class="">Changes in the source code to replace the Spring Cloud Config Server</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="39fbae19-2549-41b6-a133-98a2f92d1be2.xhtml" class="">Replacing the Spring Cloud Gateway</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="d12c1193-2e78-4261-aa4e-8207ccea2d6b.xhtml" class="">Changes in the source code for Spring Cloud Gateway</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="3f7c948d-b482-4dc6-86e2-45dd4a46c0d9.xhtml" class="">Testing with Kubernetes ConfigMaps, secrets, and ingress resource</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="22681c14-0791-4314-98e0-358f5c0e5eed.xhtml" class="">Walking through the deploy script</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="365f6260-843b-43b6-aa82-e0c8a2e4cf5d.xhtml" class="">Running commands for deploying and testing</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="d3d9e66c-4479-4fbc-b8de-dba87677f386.xhtml" class="">Automating the provision of certificates&amp;#xA0;</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="7a6bf152-df18-46d4-a604-547bc5bb6b18.xhtml" class="">Deploying the Cert Manager and defining Let's Encrypt issuers</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="56c44a39-af3f-4c14-b57e-4b4e6e979e38.xhtml" class="">Creating an HTTP tunnel using ngrok</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="385de63e-5c3f-467c-bfe4-cad9d755aa90.xhtml" class="">Provisioning certificates with the Cert Manager and Let's Encrypt</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="47bb8796-666f-4697-aef6-b01f236e218c.xhtml" class="">Using Let's Encrypt's staging environment</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="92806efe-3863-47e6-8300-5248dc7d00fc.xhtml" class="">Using Let's Encrypt's production environment</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="605bcb1c-59d3-425f-81cc-fd45867a2816.xhtml" class="">Cleaning up</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="21f5f59c-280a-491c-8aea-e4f070604bcb.xhtml" class="">Verifying that microservices work without Kubernetes</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="0a406a86-65d6-4551-84c3-1f6ef4037741.xhtml" class="">Changes in the source code for Docker Compose</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="4ef1c77b-7fd8-40de-996d-f9dcd4ed7ffa.xhtml" class="">Testing with Docker Compose</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="e88812f4-0f4b-444b-b4df-60f651b456ff.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1d6619eb-fabf-49d5-b97e-9501efd84c57.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="18" class="chapter chapter ">
            <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml" class="chapter chapter ">Using a Service Mesh to Improve Observability and Management</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="783d910c-7630-4944-adaa-cbe0ab89b6fb.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a31a04ab-ef1b-4eca-939f-db0d61acbc7c.xhtml" class="">Introduction to service mesh using Istio</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f250a203-3b95-4124-bfa4-f59782ec08ec.xhtml" class="">Injecting Istio proxies into existing microservices</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="75da7c84-e783-4a43-ab53-29bee96a9cff.xhtml" class="">Introducing Istio API objects</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="7b761e53-6631-4d51-9253-f6af47c6b4d7.xhtml" class="">Introducing runtime components in Istio&amp;#xA0;</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="093462ab-7d92-4c29-82e0-9488a600f82e.xhtml" class="">Changes in the microservice landscape&amp;#xA0;</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="fa9b55e9-03e1-400d-bb2d-c4fe8e12200f.xhtml" class="">Kubernetes Ingress resources are replaced with Istio Ingress Gateway as an edge server</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="5e6650dd-f201-4d1c-ab08-b34021295fae.xhtml" class="">Simplifying the system landscape and replacing Zipkin with Jaeger</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="02d5c9e8-cda0-444c-ab50-08763ac10891.xhtml" class="">Deploying Istio in a Kubernetes cluster</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="c56b536b-6857-4114-803e-a9049f70608d.xhtml" class="">Setting up access to Istio services</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="6e630387-ba15-451b-8fad-feae72b159ae.xhtml" class="">An added bonus from using the minikube tunnel command</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="cef1bc89-a721-4659-9596-deaf5dba7f68.xhtml" class="">Creating the service mesh</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="0137c27b-980f-4d45-be20-d1c02e39e0dd.xhtml" class="">Source code changes</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="d3de2889-3028-453d-bd73-d4e8889550fd.xhtml" class="">Updating the deployment scripts to inject Istio proxies</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="4e76de92-0e2f-4b83-b22f-614649f2afcf.xhtml" class="">Changing the file structure of the Kubernetes definition files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="33af9b40-9171-45ee-9855-e4033388540b.xhtml" class="">Adding Kubernetes definition files for Istio</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="3e457b65-1998-4882-bd70-3e06b7ec166d.xhtml" class="">Running commands to create the service mesh</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="e9bbf125-ef63-4730-8067-ba44931bd467.xhtml" class="">Observing the service mesh</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="05c4559f-0016-4e4a-80f6-3404e8a4d9ac.xhtml" class="">Securing a service mesh</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="dc86f92d-b8f3-4ee8-b67b-5cc30541d4b9.xhtml" class="">Protecting external endpoints with HTTPS and certificates</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="331e315c-94fe-49c9-b461-feabd78c18ff.xhtml" class="">Authenticating external requests using OAuth 2.0/OIDC access tokens</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ff4afb81-3a5b-4768-8d71-27e6fd496e44.xhtml" class="">Protecting internal communication using mutual authentication (mTLS)</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="99df1832-f4e2-4958-9731-3894858b9f0f.xhtml" class="">Ensuring that a service mesh is resilient</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="bcafe685-bacb-4621-868c-0b013f4fdc2a.xhtml" class="">Testing resilience by injecting faults</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9eb24086-767d-414e-86e4-2c2d05ab2387.xhtml" class="">Testing resilience by injecting delays</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="65b8969b-c2fc-470d-9617-1e9c93918134.xhtml" class="">Performing zero-downtime deployments</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="8d1e53ed-5b32-4f57-9999-1fcb6894022f.xhtml" class="">Source code changes</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="be2dd9cd-131b-4a5d-83b3-52ab81d39fb3.xhtml" class="">Service and deployment objects for concurrent versions of microservices</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="91132127-846c-412b-9793-ce636cbb6871.xhtml" class="">Added Kubernetes definition files for Istio</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="5f87d24b-d74e-4a09-b82f-56de83f6dee2.xhtml" class="">Deploying v1 and v2 versions of the microservices with routing to the v1 version</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="703626da-ee73-4fab-9f98-4002a4fcced5.xhtml" class="">Verifying that all traffic initially goes to the v1&amp;#xA0;version of the microservices</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="0fca386f-a374-4902-8dca-84abcf7a1817.xhtml" class="">Running canary tests</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="f258ef99-d40f-47c2-ba1b-959e6ea6f161.xhtml" class="">Running blue/green tests</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="ba34fe2c-3f6c-4a71-8ed6-5fa16a3cf122.xhtml" class="">A short introduction to the kubectl patch command</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="7ea8ce39-8152-457b-9eb3-2ab80ad06ef4.xhtml" class="">Performing the blue/green deployment</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="65d389b0-23fb-4b9c-80c2-ad69e12f4f2a.xhtml" class="">Running tests with Docker Compose</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ffb8e3f1-5b75-4f68-9d33-6d78733b1049.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="1b5b926e-69dc-48d9-beab-adb5fd099e02.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="19" class="chapter chapter ">
            <a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml" class="chapter chapter ">Centralized Logging with the EFK Stack</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="1efa8ea9-af1b-452f-82be-b0739f462017.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="03726c62-4bca-4fe9-be2f-5deb13477774.xhtml" class="">Configuring Fluentd</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="95f2a0dd-e783-4319-88fe-281561cdeef5.xhtml" class="">Introducing Fluentd</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="16a230b1-323b-4b8e-9379-bfd721d9ef1c.xhtml" class="">Configuring Fluentd</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="e5a4a1e5-f777-4129-971c-1663791f94fc.xhtml" class="">Deploying the EFK stack on Kubernetes</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="040ec579-6cce-4a5a-b05c-e39222a16b29.xhtml" class="">Building and deploying our microservices</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="62a85076-726b-47aa-9ae5-994a88a66b37.xhtml" class="">Deploying Elasticsearch and Kibana</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="f092bee3-049d-43d6-8770-07e24dfd585a.xhtml" class="">A walkthrough of the definition files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b2877289-13e4-4a26-86ab-1c3a4d49189c.xhtml" class="">Running the deploy commands</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="b7593bfb-f14e-4550-9e0b-8c07524add5f.xhtml" class="">Deploying Fluentd</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="754915d4-991a-4a70-b5bb-a05de59c5551.xhtml" class="">A walkthrough of the definition files</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="bb51b996-e1e9-4f8a-96a0-c7bef8ef2312.xhtml" class="">Running the deploy commands</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="71684000-d92d-4c92-b47a-745128a9318a.xhtml" class="">Trying out the EFK stack</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="1262e643-cd2c-490f-9e54-e624d843d0e3.xhtml" class="">Initializing Kibana</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="59e292de-72cb-4911-b7a9-e64810c18522.xhtml" class="">Analyzing the log records</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e18b7a99-c0a1-4b5a-a019-459c9263d458.xhtml" class="">Discovering the log records from microservices</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="e5911900-22cb-44a8-9c7f-1981568aeb0e.xhtml" class="">Performing root cause analyses</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="d5ebdac7-8012-495b-a706-aa1e8d3eb58e.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="4eaba061-0a46-46c4-b29a-65b048ab271a.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:decimal" value="20" class="chapter chapter ">
            <a href="5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml" class="chapter chapter ">Monitoring Microservices</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="c8983d81-c2fc-480a-9019-6e7f6dfe613e.xhtml" class="">Technical requirements</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a8d09d04-037d-4045-814f-cd4ed858ad63.xhtml" class="">Introduction to performance monitoring using Prometheus and Grafana</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="3cbb781a-8165-40c5-8c3f-e47151673d6e.xhtml" class="">Changes in source code for collecting application metrics</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="215aa6b9-a148-4899-b981-5e4bf709a639.xhtml" class="">Building and deploying the microservices</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="f6c14bd9-fdaa-4223-abe9-662a75023c9d.xhtml" class="">Monitoring microservices using Grafana dashboards</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="25c07c76-3332-469d-831f-8acdc42985cd.xhtml" class="">Installing a local mail server for tests</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ea6fa308-169d-4b77-ab41-928097f35adc.xhtml" class="">Starting up the load test</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="d2d72361-2524-4444-a05c-5bc5ac57f6ee.xhtml" class="">Using Kiali's built-in Grafana dashboards</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ffd9fda8-567b-4ea9-987a-80f2e93e383e.xhtml" class="">Importing existing Grafana dashboards</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="9dcd57a1-e979-46ca-b03f-a80a972a19fe.xhtml" class="">Developing your own Grafana dashboards</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="e5d9c14d-a2e8-4547-9f60-66b49d596d5a.xhtml" class="">Examining Prometheus metrics</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="ec691a69-9cc9-485d-a3b6-0b1d48a9579e.xhtml" class="">Creating the dashboard</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="986f7604-e9ed-479f-88f2-2bbc12cf7404.xhtml" class="">Creating an empty dashboard</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="34bb9241-093c-4ea3-a451-6ab50875d94f.xhtml" class="">Creating a new panel for the circuit breaker metric</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="8219cb4f-80e9-419f-aaa3-a3bc57ebf120.xhtml" class="">Creating a new panel for the retry&amp;#xA0;metric</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="d776f81c-532c-4ae3-a580-5386677f3566.xhtml" class="">Arranging the panels</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="48ef7f45-7d30-46b5-a8dd-58547ca5e08b.xhtml" class="">Trying out the new dashboard</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="281d08f5-a2b4-4801-a710-c31018a9b5fb.xhtml" class="">Testing the circuit breaker metrics</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="d3905f48-0593-4436-bac7-7b9b83598c0d.xhtml" class="">Testing the retry metrics</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="d9ccff3e-2e2d-465f-8159-7b18e02dfd6a.xhtml" class="">Setting up alarms in Grafana</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="e9ef662a-84fb-464e-9e08-740d08806a5d.xhtml" class="">Setting up a mail-based notification channel</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="b248fdbc-594e-418c-ab70-84e657d85a59.xhtml" class="">Setting up an alarm on the&amp;#xA0;circuit breaker</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="438680b4-8dca-4f66-91ab-1d0ad8b9db2e.xhtml" class="">Trying out the&amp;#xA0;circuit breaker&amp;#xA0;alarm</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="">
            <a href="d7d99637-f3e0-425e-83a9-2592ef5affd3.xhtml" class="">Summary</a>
    
    
</li>
<li style="list-style-type:none" class="">
            <a href="a7a4be82-bf28-45d9-9623-017e66c193ea.xhtml" class="">Questions</a>
    
    
</li>
</ol>
</li>
<li style="list-style-type:none" class="back-matter miscellaneous ">
            <a href="e027ca6b-57aa-4f96-b918-c3408b298b8f.xhtml" class="back-matter miscellaneous ">Other Books You May Enjoy</a>
    
    <ol><li style="list-style-type:none" class="">
            <a href="662ed14c-8c2b-45ec-9957-5a83d79dbecc.xhtml" class="">Leave a review - let other readers know what you think</a>
    
    
</li>
</ol>
</li>
</ol>
                </nav>
            </article>

            <footer style="margin-top: 5em;"></footer>
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Preface</h1>
                </header>
            
            <article>
                
<p class="mce-root">This book is about building production-ready microservices using Spring Boot and Spring Cloud. Five years ago, when I began to explore microservices, I was looking for a book like this.</p>
<p class="mce-root">This book has been developed after I learned about, and mastered, open source software used for developing, testing, deploying, and managing landscapes of cooperating microservices.</p>
<p class="mce-root">This book primarily covers Spring Boot, Spring Cloud, Docker, Kubernetes, Istio, the EFK stack, Prometheus, and Grafana. Each of these open source tools works great by itself, but it can be challenging to understand how to use them together in an advantageous way. In some areas, they complement each other, but in other areas they overlap, and it is not obvious which one to choose for a particular situation.</p>
<p class="mce-root">This is a hands-on book that describes <span>step by step how</span><span> to use these open source tools together. This is the book I was looking for five years ago when I started to learn about microservices, but with updated versions of the open source tools it covers.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Who this book is for</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>This </span><span>book is for Java and Spring developers and architects who want to learn how to break up their existing monoliths </span>into microservices and deploy them either on-premises or in the cloud, using Kubernetes as a container orchestrator and Istio as a service mesh. No familiarity with microservices architecture is required to get started with this book.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">What this book covers</h1>
                </header>
            
            <article>
                
<p><a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em><span>, will help you understand the basic premise of the book, microservices, along with the essential concepts and design patterns that go along with it.</span></p>
<p class="mce-root"></p>
<p><a href="7d969006-ea94-4bbb-858d-30dce8177a2c.xhtml">Chapter 2</a>, <em>Introduction to Spring Boot,</em> will get you introduced to Spring Boot and the other open source projects that will be used in the first part of the book: Spring WebFlux for developing RESTful APIs, SpringFox for producing OpenAPI- or Swagger-based documentation for the APIs, Spring Data for storing data in SQL and NoSQL databases, Spring Cloud Stream for message-based microservices, and Docker to run the microservices as containers.</p>
<p class="mce-root"><a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml">Chapter 3</a>, <em>Creating a Set of Cooperating Microservices,</em> will teach you how to create a set of cooperating microservices from scratch. You will use Spring Initializr to create skeleton projects based on Spring Framework 5.1 and Spring Boot 2.1. The idea is to create three core services (that will handle their own resources) and one composite service that uses the three core services to aggregate a composite result. Toward the end of the chapter, you will learn how to add very basic RESTful APIs based on Spring WebFlux. In the next chapter, more and more functionality will be added to these microservices.</p>
<p><a href="ce37c0f1-ac54-4258-8cb6-7c6f50f26172.xhtml">Chapter 4</a>, <em>Deploying Our Microservices Using Docker</em><span>, will teach you how to deploy microservices using Docker. You will learn how to add Dockerfiles and docker-compose files in order to start up the whole microservice landscape with a single command. Then, you will learn how to use multiple Spring profiles to handle configurations with and without Docker. </span></p>
<p><a href="ba24a656-10a1-4a3e-879e-6589621ef125.xhtml">Chapter 5</a>, <em>Adding an API Description Using OpenAPI/Swagger</em><span>, will get you up to speed with documenting the APIs exposed by a microservice using OpenAPI/Swagger. You will use the SpringFox framework to annotate the services to create OpenAPI- or Swagger-based API documentation on the fly. The key highlight will be how the APIs can be tested in a web browser using SpringFox Swagger UI.</span></p>
<p><a href="6c495e97-f473-4cb1-a404-11fd938f5478.xhtml">Chapter 6</a>, <em>Adding Persistence</em><span>,&nbsp;</span><span>will show you how to add persistence to the data of the microservice. You will use Spring Data to set up and access data in a MongoDB document database for two of the core microservices and access data in a MySQL relational database using the <strong>Java Persistence API</strong> (<strong>JPA</strong>) for the remaining microservice.</span></p>
<p><a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml">Chapter 7</a>, <em>Developing Reactive Microservices</em>, will teach you why and when a reactive approach is of importance and how to develop end-to-end reactive services. You will learn how to develop and test both non-blocking synchronous RESTful APIs and asynchronous event-driven services. You will also learn how to use the reactive non-blocking driver for MongoDB and use conventional blocking code for MySQL.</p>
<p><a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml">Chapter 8</a>, <em>Introduction to Spring Cloud</em><span>, will introduce you to Spring Cloud and the components of Spring Cloud that will be used in this book.</span></p>
<p class="mce-root"></p>
<p><a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml">Chapter 9</a>, <em>Adding Service Discovery Using Netflix Eureka and Ribbon</em><span>, will show you how to use Netflix Eureka and Ribbon in Spring Cloud to add service discovery capabilities. This will be achieved by adding a Netflix Eureka-based service discovery server to the system landscape. You will then configure the microservices to use Netflix Ribbon to find other microservices. You will understand how microservices are registered automatically and how traffic through Netflix Ribbon is automatically load balanced to new instances when they become available.</span></p>
<p><a href="a3383211-405d-4319-b142-ddb8cf3674fd.xhtml">Chapter 10</a>, <em>Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server</em>, will guide you through how to hide the microservices behind an edge server using Spring Cloud Gateway and only expose selected APIs to external consumers. You will also learn how to hide the internal complexity of the microservices from external consumers. This will be achieved by adding a Spring Cloud Gateway-based edge server to the system landscape and configuring it to only expose the public APIs. </p>
<p><a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml">Chapter 11</a>, <em>Securing Access to APIs</em>, will explain how to protect exposed APIs using OAuth 2.0 and OpenID Connect. You will learn how to add an OAuth 2.0 authorization server based on Spring Security to the system landscape, and how to configure the edge server and the composite service to require valid access tokens issued by that authorization server. You will learn how to expose the authorization server through the edge server and secure its communication with external consumers using HTTPS. Finally, you will learn how to replace the internal <span>OAuth 2.0 authorization server with an external OpenID Connect provider from Auth0.</span></p>
<p><a href="a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml">Chapter 12</a>,&nbsp;<em>Centralized Configuration</em>, will deal with how to collect the configuration files from all the microservices in one central repository and use the configuration server to distribute the configuration to the microservices at runtime. You will also learn how to add a Spring Cloud Config Server to the system landscape and configure all microservices to use the Spring Config Server to get its configuration. </p>
<p><a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em>, will explain how to use the capabilities of Resilience4j to prevent, for example, the "chain of failure" <span>anti-pattern</span>. You will learn how to add a retry mechanism and a circuit breaker to the composite service, and how to configure the circuit breaker to <em>fast fail</em> when the circuit is open, and how to utilize a fallback method to create a best-effort response.</p>
<p><a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml">Chapter 14</a>, <em>Understanding Distributed Tracing</em>, will show you how to use Zipkin to collect and visualize tracing information. You will also use Spring Cloud Sleuth to add trace IDs to requests so that request chains between cooperating microservices can be visualized.</p>
<p><a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>,&nbsp;<em> Introduction to Kubernetes</em>, will explain the core concepts of Kubernetes and how to perform a sample deployment. You will also learn how to set up Kubernetes locally for development and testing purposes using Minkube.</p>
<p class="mce-root"></p>
<p><a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em>, will show how to deploy microservices on Kubernetes. You will also learn how to use Kustomize to configure the deployment in Kubernetes for different runtime environments, such as test and production environments. Finally, you will learn how to replace Netflix Eureka with the built-in support in Kubernetes for service discovery, based on Kubernetes services objects and the kube-proxy runtime component.</p>
<p><a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 17</a>, <em>Implementing Kubernetes Features as an Alternative</em>, will explain how to use Kubernetes features as an alternative to the Spring Cloud services introduced in the previous chapters. Y<span>ou will learn why and how to replace Spring Cloud Config Server with Kubernetes secrets and config maps. You will also learn why and how to replace Spring Cloud Gateway with Kubernetes ingress objects and how to add the Cert Manager to automatically provision and rotate certificates from Let's Encrypt's for HTTPS endpoints.</span></p>
<p><span><a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em></span><span>, will introduce the concept of a service mesh and will explain how to use Istio to implement a service mesh in runtime using Kubernetes. You will learn how to use a service mesh to further improve the resilience, security, traffic management, and observability of the microservice landscape.</span></p>
<p><a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml">Chapter 19</a>,&nbsp;<em>Centralized Logging with the EFK Stack</em>, will explain how to use Elasticsearch, Fluentd, and Kibana (the EFK stack) to collect, store, and visualize log streams from microservices. You will learn how to deploy the EFK stack in Minikube and how to use it to analyze collected log records and find log output from all microservices involved in the processing of a request that spans several microservices. You will also learn how to perform root cause analysis using the EFK stack.</p>
<p><a href="5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml">Chapter 20</a>,<em> Monitoring Microservices</em>, will show you how to monitor the microservices deployed in Kubernetes using Prometheus and Grafana. You will learn how to use <span>both </span>existing dashboards in Granfana to monitor different types of metrics, and you will also learn how to create your own dashboards. Finally, you will learn how to create alerts in Grafana that will be used to send emails with alerts when configured thresholds are passed for selected metrics.</p>
<p><em>Assessments, </em><span>is uploaded on the GitHub repository </span>containing the answers to the questions asked in the respective chapters.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">To get the most out of this book</h1>
                </header>
            
            <article>
                
<p>A good understanding of Java 8, along with a basic knowledge of Spring Framework, is required. A general understanding of the challenges of distributed systems, in addition to some experience of running your own code in production, will also be beneficial.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Download the example code files</h1>
                </header>
            
            <article>
                
<p>You can download the example code files for this book from your account at <a href="http://www.packt.com" target="_blank">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="https://www.packtpub.com/support" target="_blank">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register at <a href="http://www.packt.com" target="_blank">www.packt.com</a>.</li>
<li>Select the <span class="packt_screen">Support</span> tab.</li>
<li>Click on <span class="packt_screen">Code Downloads</span>.</li>
<li>Enter the name of the book in the <span class="packt_screen">Search</span> box and follow the onscreen instructions.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR/7-Zip for Windows</li>
<li>Zipeg/iZip/UnRarX for Mac</li>
<li>7-Zip/PeaZip for Linux</li>
</ul>
<p><span>The code bundle for the book is also hosted on GitHub at</span><span>&nbsp;<a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</a></span><span>.&nbsp;</span><span>In case there's an update to the code, it will be updated on the existing GitHub repository.</span></p>
<p><span>We also have other code bundles from our rich catalog of books and videos available at</span><span>&nbsp;</span><strong><span class="Object"><a href="https://github.com/PacktPublishing/" target="_blank">https://github.com/PacktPublishing/</a></span></strong><span>. Check them out!</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Download the color images</h1>
                </header>
            
            <article>
                
<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://static.packt-cdn.com/downloads/9781789613476_ColorImages.pdf">https://static.packt-cdn.com/downloads/9781789613476_ColorImages.pdf</a>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Code in Action</h1>
                </header>
            
            <article>
                
<p>To see the code being executed, please visit the following: <a href="http://bit.ly/2kn7mSp">http://bit.ly/2kn7mSp</a>.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Conventions used</h1>
                </header>
            
            <article>
                
<p>There are a number of text conventions used throughout this book.</p>
<p><kbd>CodeInText</kbd>: <span>Indicates c</span>ode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. <span>Here is an example:</span>&nbsp;"<span>To use the local filesystem, the config server needs to be launched with the Spring profile, </span><kbd>native</kbd><span>, enabled</span>"</p>
<p>A block of code is set as follows:</p>
<pre>management.endpoint.health.show-details: "ALWAYS"<br>management.endpoints.web.exposure.include: "*"<br><br>logging.level.root: info</pre>
<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre>   backend:<br>    serviceName: auth-server<br>    <strong>servicePort: 80</strong><br> - path: /product-composite</pre>
<p>Any command-line input or output is written as follows:</p>
<pre><strong>brew install kubectl</strong></pre>
<p><strong>Bold</strong>: Indicates a new term, an important word, or w<span>ords that you see on screen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "As seen in the preceding screenshot Chrome reports: <span class="packt_screen">This certificate is valid</span>!</span><span>"</span></p>
<div class="packt_infobox">Warnings or important notes appear like this.</div>
<div class="packt_tip">Tips and tricks appear like this.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Get in touch</h1>
                </header>
            
            <article>
                
<p>Feedback from our readers is always welcome.</p>
<p class="mce-root"><strong>General feedback</strong>: If you have questions about any aspect of this book, <span>mention the book title in the subject of your message and</span> email us at <kbd><span>customercare@packtpub.com</span></kbd>.</p>
<p><strong>Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="https://www.packtpub.com/support/errata" target="_blank">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p><strong>Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd>copyright@packt.com</kbd> with a link to the material.</p>
<p class="mce-root"><strong>If you are interested in becoming an author</strong>: If there is a topic that you have expertise in, and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" target="_blank">authors.packtpub.com</a>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Reviews</h1>
                </header>
            
            <article>
                
<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p>For more information about Packt, please visit <a href="http://www.packt.com/" target="_blank">packt.com</a>.<a href="https://www.packtpub.com/" target="_blank"></a></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Section 1: Getting Started with Microservice Development Using Spring Boot</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, you will learn how to use some of the most important features of Spring Boot to develop microservices.</p>
<p>This section includes the following chapters:</p>
<ul>
<li><a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices </em></li>
<li><a href="7d969006-ea94-4bbb-858d-30dce8177a2c.xhtml"><span>Chapter </span>2</a>, <em>Introduction to Spring Boot</em></li>
<li><a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml"><span>Chapter </span>3</a>, <em>Creating a Set of Cooperating Microservices</em></li>
<li><a href="ce37c0f1-ac54-4258-8cb6-7c6f50f26172.xhtml"><span>Chapter </span>4</a>, <em>Deploying Our Microservices Using Docker </em></li>
<li><a href="ba24a656-10a1-4a3e-879e-6589621ef125.xhtml"><span>Chapter </span>5</a>, <em>Adding API Description Using OpenAPI/Swagger</em></li>
<li><a href="6c495e97-f473-4cb1-a404-11fd938f5478.xhtml"><span>Chapter </span>6</a>, <em>Adding Persistence</em></li>
<li><a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml"></a><a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml"></a><a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml"><span>Chapter </span>7</a>, <em>Developing Reactive Microservices</em></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to Microservices</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>This book does not blindly praise microservices. Instead, it's about how we can use their benefits while being able to handle the challenges of building scalable, resilient, and manageable microservices.</span></p>
<p>As an introduction to this book, the following topics will be covered in this chapter:</p>
<ul class="ol1">
<li class="li1">How I learned about microservices and what experience I have of their <span>benefits</span> and <span>challenges</span></li>
<li class="li1">What is a microservice-based architecture?</li>
<li class="li1">Challenges with microservices</li>
<li class="li1">Design patterns for handling challenges</li>
<li class="li1">Software enablers that can help us handle these challenges</li>
<li class="li1">Other important considerations that aren't covered in this book</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>No installations are required for this chapter. However, you may be interested in taking a look at the C4 model conventions, <a href="https://c4model.com">https://c4model.com</a>, since the illustrations in this chapter are inspired by the C4 model.</p>
<p><span>This chapter does not contain any source code.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">My way into microservices</h1>
                </header>
            
            <article>
                
<p>When I first learned about the concept of microservices back in 2014, I realized that I had been developing microservices (well, kind of) for a number of years without knowing it was microservices I was dealing with. I was involved in a project that started in 2009 where we developed a platform based on a set of separated features. The platform was delivered to a number of customers that deployed it on-premise. To make it easy for the customers to pick and choose what features they wanted to use from the platform, each feature was developed as an <strong>autonomous software component</strong>; that is, it had its own persistent data and only communicated with other components using well-defined APIs.</p>
<p>Since I can't discuss specific features in this project's platform, I have generalized the names of the components, which are labeled from <strong>Component A</strong> to <strong>Component F</strong>. The composition of the platform into a set of <span>compon</span>ents is illustrated as follows<span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a28adfee-5ac7-4006-8bc9-ca0bd3ce0688.png" style="width:27.58em;height:10.08em;" width="1144" height="418" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a28adfee-5ac7-4006-8bc9-ca0bd3ce0688.png"></p>
<p class="mce-root">Each component is developed using Java and the Spring Framework, and is packaged as a WAR file and deployed as a web app in a Java EE web container, for example, Apache Tomcat. Depending on the customer's specific requirements, the platform can be deployed on single or multiple servers. A two-node deployment may look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e608abe5-c594-4079-8523-6f56e88ae3b3.png" style="width:37.17em;height:17.33em;" width="1628" height="756" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e608abe5-c594-4079-8523-6f56e88ae3b3.png"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Challenges with&nbsp;autonomous software components</h1>
                </header>
            
            <article>
                
<p>We also learned that&nbsp;decomposing the platform introduced a number of new challenges that we were not exposed (at least not to the same degree) when developing more traditional, monolithic applications:</p>
<ul>
<li>Adding new instances to a component required manually configuring load balancers and manually&nbsp;setting up new nodes.&nbsp;This work was both&nbsp;<span>time-consuming and&nbsp;</span>error-prone.</li>
<li class="mce-root">The platform was initially prone to errors in the other systems it was communicating with. If a system stopped responding to requests that were sent from the platform in a timely fashion, the platform quickly ran out of crucial resources, for example, OS threads, specifically when exposed to a large number of concurrent requests. This caused components in the platform to hang or even crash. Since most of the communication in the platform is based on synchronous communication, one component crashing can lead to cascading failures; that is, clients of the crashing components could also crash after a while. This is known as a <strong>chain of failures</strong>.</li>
<li class="mce-root"><span>Keeping the configuration consistent and up to date in all the instances of the components quickly became a problem, causing a lot of manual and repetitive work. This led to quality problems from time to time.</span></li>
<li class="mce-root"><span>Monitoring the state of the platform in terms of latency issues&nbsp;and hardware usage (for example, usage of CPU, memory, disks, and the network) was more complicated compared to monitoring a single instance of a monolithic application.</span></li>
<li class="mce-root"><span>Collecting log files from a number of distributed components and correlating related log events from the components was also difficult but feasible since the number of components was fixed and known in advance.</span></li>
</ul>
<p class="mce-root">Over time, we addressed most of the challenges that were mentioned in the preceding list with a mix of in-house-developed tools and well-documented instructions for handling these challenges manually. The scale of the operation was, in general, at a level where manual procedures for releasing new versions of the components and handling runtime issues were acceptable, even though they were not desirable.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Enter microservices</h1>
                </header>
            
            <article>
                
<p class="mce-root">Learning about microservice-based architectures in 2014 made me realize that other projects had also been struggling with similar challenges (partly&nbsp;<span>for&nbsp;</span>other reasons than the ones I described earlier, for example, the large cloud service providers meeting web-scale requirements).<span>&nbsp;Many&nbsp;microservice pioneers had published details of lessons they'd learned.&nbsp;</span>It was very interesting to learn from these lessons.&nbsp;</p>
<p>Many of the pioneers initially developed monolithic applications that made them very successful from a business perspective. But over time, these&nbsp;<span>monolithic applications</span> became more and more difficult to maintain and evolve. They&nbsp;also became challenging to scale beyond the capabilities of the largest machines available (also known as <strong>vertical scaling</strong>). Eventually, the pioneers started to find ways to split monolithic applications into smaller components that could be released and scaled independently of each other. Scaling small components can be done horizontally, that is, deploying a component on a number of smaller servers and placing a load balancer in front of it. If done in the cloud, the scaling capability is potentially endless&nbsp;<span>â</span> it is just a matter of how many virtual servers you bring in (given that your component can scale out on a huge number of instances, but more on that later on).</p>
<p>In 2014, I also learned about a&nbsp;<span>number of new open source projects that delivered tools and frameworks that simplified the development of microservices and could be used to handle the challenges that come with a microservice-based architecture. Some of these are as follows:</span></p>
<ul>
<li class="mce-root">Pivotal released&nbsp;<strong>Spring Cloud</strong>, which wraps parts of the&nbsp;<strong>Netflix OSS</strong>&nbsp;in order to provide capabilities such as dynamic service discovery, configuration management, distributed tracing, circuit breaking, and more.</li>
<li class="mce-root">I also learned about <strong>Docker</strong><span><span><span>&nbsp;and the container revolution, which is great for minimizing the gap between development and production. Being able to package a component not only as a deployable runtime</span> artifact<span>&nbsp;(for example, a Java,&nbsp;<kbd>war</kbd> or,&nbsp;<kbd>jar</kbd> file) but as a complete image ready to be launched as a container (for example, an isolated process) on a server running Docker was a great step forward for development and testing.</span></span></span></li>
<li class="mce-root"><span><span>A container engine, such as Docker, is not enough to be able to use containers in a production environment. Something is needed that, for example, can ensure that all the containers are up and running and that they can scale out containers on a number of servers, thereby providing high availability and/or increased compute resources. These types of product&nbsp;became known as <strong>container orchestrators</strong><em>.</em>&nbsp;A number of products have evolved over the last few years, such as Apache Mesos, Docker in Swarm mode, Amazon ECS, HashiCorp Nomad, and <strong>Kubernetes</strong>. Kubernetes was initially developed by Google. When Google released v1.0, they also donated Kubernetes to CNCF (<a href="https://www.cncf.io/">https</a><a href="https://www.cncf.io/">://www.cncf.io/</a>). During 2018, Kubernetes became kind of a de facto standard, available both pre-packaged for on-premise use and available as a service from most major cloud providers.</span></span></li>
<li class="mce-root">I have recently started to learn about the concept of a <strong>service mesh</strong> and how a service mesh can complement a container orchestrator to further offload microservices from responsibilities to make them manageable and resilient.<span><strong>&nbsp;</strong></span><span><br></span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Enter microservices</h1>
                </header>
            
            <article>
                
<p class="mce-root">Learning about microservice-based architectures in 2014 made me realize that other projects had also been struggling with similar challenges (partly <span>for </span>other reasons than the ones I described earlier, for example, the large cloud service providers meeting web-scale requirements).<span> Many microservice pioneers had published details of lessons they'd learned. </span>It was very interesting to learn from these lessons. </p>
<p>Many of the pioneers initially developed monolithic applications that made them very successful from a business perspective. But over time, these <span>monolithic applications</span> became more and more difficult to maintain and evolve. They also became challenging to scale beyond the capabilities of the largest machines available (also known as <strong>vertical scaling</strong>). Eventually, the pioneers started to find ways to split monolithic applications into smaller components that could be released and scaled independently of each other. Scaling small components can be done horizontally, that is, deploying a component on a number of smaller servers and placing a load balancer in front of it. If done in the cloud, the scaling capability is potentially endless <span>â</span> it is just a matter of how many virtual servers you bring in (given that your component can scale out on a huge number of instances, but more on that later on).</p>
<p>In 2014, I also learned about a <span>number of new open source projects that delivered tools and frameworks that simplified the development of microservices and could be used to handle the challenges that come with a microservice-based architecture. Some of these are as follows:</span></p>
<ul>
<li class="mce-root">Pivotal released <strong>Spring Cloud</strong>, which wraps parts of the <strong>Netflix OSS</strong> in order to provide capabilities such as dynamic service discovery, configuration management, distributed tracing, circuit breaking, and more.</li>
<li class="mce-root">I also learned about <strong>Docker</strong><span><span><span> and the container revolution, which is great for minimizing the gap between development and production. Being able to package a component not only as a deployable runtime</span> artifact<span> (for example, a Java, <kbd>war</kbd> or, <kbd>jar</kbd> file) but as a complete image ready to be launched as a container (for example, an isolated process) on a server running Docker was a great step forward for development and testing.</span></span></span></li>
<li class="mce-root"><span><span>A container engine, such as Docker, is not enough to be able to use containers in a production environment. Something is needed that, for example, can ensure that all the containers are up and running and that they can scale out containers on a number of servers, thereby providing high availability and/or increased compute resources. These types of product became known as <strong>container orchestrators</strong><em>.</em> A number of products have evolved over the last few years, such as Apache Mesos, Docker in Swarm mode, Amazon ECS, HashiCorp Nomad, and <strong>Kubernetes</strong>. Kubernetes was initially developed by Google. When Google released v1.0, they also donated Kubernetes to CNCF (<a href="https://www.cncf.io/">https</a><a href="https://www.cncf.io/">://www.cncf.io/</a>). During 2018, Kubernetes became kind of a de facto standard, available both pre-packaged for on-premise use and available as a service from most major cloud providers.</span></span></li>
<li class="mce-root">I have recently started to learn about the concept of a <strong>service mesh</strong> and how a service mesh can complement a container orchestrator to further offload microservices from responsibilities to make them manageable and resilient.<span><strong>&nbsp;</strong></span><span><br></span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">A sample microservice landscape</h1>
                </header>
            
            <article>
                
<p class="mce-root">Since this book can't cover all aspects of the technologies I just mentioned, I will focus on the parts that have proven to be useful in customer projects I have been involved in since 2014. I will describe how they can be used together to create cooperating microservices that are manageable, scalable, and resilient.</p>
<p class="mce-root">Each chapter in this book will address a specific concern. To demonstrate how things fit together, I will use a small set of cooperating microservices that we will evolve throughout this book:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bd28199a-e796-4dd9-aaf1-ae322a726f93.png" style="width:31.00em;height:11.00em;" width="1184" height="418" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bd28199a-e796-4dd9-aaf1-ae322a726f93.png"></p>
<p class="mce-root">Now that we know the how and what of microservices, let's start to look into how a microservice can be defined.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Challenges with microservices</h1>
                </header>
            
            <article>
                
<p><span>In the&nbsp;</span><em>Challenges with&nbsp;<span>autonomous software components</span></em><span>&nbsp;section, w</span>e have already seen some of the challenges that autonomous software components can bring (and they all apply to microservices as well) as follows:</p>
<ul>
<li>Many small components that&nbsp;use synchronous communication can cause <em>a</em>&nbsp;<em>chain of failure</em><span>&nbsp;problem, especially under high load.</span></li>
<li>Keeping the configuration up to date for&nbsp;<span>m</span><span>any small components can be challenging.</span></li>
<li>It's hard to track a request that's being processed and involves many components, for example, when performing root cause analysis, where each component stores log events locally.</li>
<li>Analyzing the usage of hardware resources on a component level can be challenging as well.</li>
<li><span>Manual&nbsp;</span><span>configuration and management</span><span>&nbsp;of m</span>any small components can become costly and error-prone.</li>
</ul>
<p class="mce-root"></p>
<p>Another downside (but not always obvious initially) of<span>&nbsp;decomposing an application into a group of autonomous components</span> is that they form a distributed system. Distributed systems are known to be, by their nature, very hard to deal with.&nbsp;<span>This has been known for many years (but in many cases neglected until proven differently). My favorite quote to establish this fact is from&nbsp;Peter Deutsch who, back in 1994,&nbsp;</span><span>stated the following:</span></p>
<div>
<div class="packt_infobox">
<p><strong><em>T</em>he 8 fallacies of distributed computing</strong><em>:&nbsp;<span>Esse</span><span>ntially everyone, when they first build a distributed application, makes the following eight assumptions. All prove to be false in the long run and all cause&nbsp;</span><span>big</span><span>&nbsp;trouble and&nbsp;</span><span>painful</span><span>&nbsp;learning experiences:</span></em></p>
<ol>
<li><em><span>The network is reliable</span></em></li>
<li><em><span>Latency is zero</span></em></li>
<li><em><span>Bandwidth is infinite</span></em></li>
<li><em><span>The network is secure</span></em></li>
<li><em><span>Topology doesn't change</span></em></li>
<li><em><span>There is one administrator</span></em></li>
<li><em><span>Transport cost is zero</span></em></li>
<li><em><span>The network is homogeneous</span></em></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><span><em>--&nbsp;Peter Deutsch, 1994</em></span></div>
<p><span><strong>Note:</strong> The eighth fallacy was actually added by James Gosling at a later date. For more details, please go to</span>&nbsp;<a href="https://www.rgoarchitects.com/Files/fallacies.pdf">https://www.rgoarchitects.com/Files/fallacies.pdf</a><span>.<br></span></p>
</div>
</div>
<p>In general, building microservices-based on these false assumptions leads to solutions that are prone to both temporary network glitches and problems that occur in other microservice instances. When the number of microservices in a system landscape increases, the likelihood of problems also goes up. A good rule of thumb is to design your microservice architecture based on the assumption that there is always something going wrong in the system landscape.&nbsp;The microservice architecture needs to be designed to handle this, in terms of detecting problems and restarting failed components but also on the client-side so that requests are not sent to failed microservice instances. When problems are corrected, requests to the previously failing microservice should be resumed; that is, microservice clients need to be resilient. All of these need, of course, to be fully automated. With a large number of microservices, it is not feasible for operators to handle this manually!</p>
<p>The scope of this is large, but we will limit ourselves for now and move on to study design patterns for&nbsp;microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Challenges with microservices</h1>
                </header>
            
            <article>
                
<p><span>In the </span><em>Challenges with <span>autonomous software components</span></em><span> section, w</span>e have already seen some of the challenges that autonomous software components can bring (and they all apply to microservices as well) as follows:</p>
<ul>
<li>Many small components that use synchronous communication can cause <em>a</em>&nbsp;<em>chain of failure</em><span> problem, especially under high load.</span></li>
<li>Keeping the configuration up to date for <span>m</span><span>any small components can be challenging.</span></li>
<li>It's hard to track a request that's being processed and involves many components, for example, when performing root cause analysis, where each component stores log events locally.</li>
<li>Analyzing the usage of hardware resources on a component level can be challenging as well.</li>
<li><span>Manual </span><span>configuration and management</span><span> of m</span>any small components can become costly and error-prone.</li>
</ul>
<p class="mce-root"></p>
<p>Another downside (but not always obvious initially) of<span> decomposing an application into a group of autonomous components</span> is that they form a distributed system. Distributed systems are known to be, by their nature, very hard to deal with. <span>This has been known for many years (but in many cases neglected until proven differently). My favorite quote to establish this fact is from Peter Deutsch who, back in 1994, </span><span>stated the following:</span></p>
<div>
<div class="packt_infobox">
<p><strong><em>T</em>he 8 fallacies of distributed computing</strong><em>:&nbsp;<span>Esse</span><span>ntially everyone, when they first build a distributed application, makes the following eight assumptions. All prove to be false in the long run and all cause </span><span>big</span><span> trouble and </span><span>painful</span><span> learning experiences:</span></em></p>
<ol>
<li><em><span>The network is reliable</span></em></li>
<li><em><span>Latency is zero</span></em></li>
<li><em><span>Bandwidth is infinite</span></em></li>
<li><em><span>The network is secure</span></em></li>
<li><em><span>Topology doesn't change</span></em></li>
<li><em><span>There is one administrator</span></em></li>
<li><em><span>Transport cost is zero</span></em></li>
<li><em><span>The network is homogeneous</span></em></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><span><em>-- Peter Deutsch, 1994</em></span></div>
<p><span><strong>Note:</strong> The eighth fallacy was actually added by James Gosling at a later date. For more details, please go to</span>&nbsp;<a href="https://www.rgoarchitects.com/Files/fallacies.pdf">https://www.rgoarchitects.com/Files/fallacies.pdf</a><span>.<br></span></p>
</div>
</div>
<p>In general, building microservices-based on these false assumptions leads to solutions that are prone to both temporary network glitches and problems that occur in other microservice instances. When the number of microservices in a system landscape increases, the likelihood of problems also goes up. A good rule of thumb is to design your microservice architecture based on the assumption that there is always something going wrong in the system landscape. The microservice architecture needs to be designed to handle this, in terms of detecting problems and restarting failed components but also on the client-side so that requests are not sent to failed microservice instances. When problems are corrected, requests to the previously failing microservice should be resumed; that is, microservice clients need to be resilient. All of these need, of course, to be fully automated. With a large number of microservices, it is not feasible for operators to handle this manually!</p>
<p>The scope of this is large, but we will limit ourselves for now and move on to study design patterns for microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Service discovery</h1>
                </header>
            
            <article>
                
<p>The <strong>service</strong> <strong>discovery</strong> pattern has the following problem, solution, and solution requirements.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Service discovery</h1>
                </header>
            
            <article>
                
<p>The <strong>service</strong> <strong>discovery</strong> pattern has the following problem, solution, and solution requirements.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problem</h1>
                </header>
            
            <article>
                
<p>How can clients find microservices and their instances?</p>
<p>Microservices instances are typically assigned dynamically allocated IP addresses when they start up, for example, when running in containers. This makes it difficult for a client to make a request to a microservice that, for example, exposes a REST API over HTTP. Consider the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ae0b2dda-3e2d-4028-b48b-cff12017468e.png" style="width:24.00em;height:20.58em;" width="901" height="769" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ae0b2dda-3e2d-4028-b48b-cff12017468e.png"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p>Add a new component <span>â</span> a <strong>service discovery</strong> service <span>â</span> to the system landscape, which keeps track of currently available <span>microservices and the IP addresses of its instances.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Edge server</h1>
                </header>
            
            <article>
                
<p><span>The edge server pattern has the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Edge server</h1>
                </header>
            
            <article>
                
<p><span>The edge server pattern has the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p>Add a new component, an&nbsp;<strong>Edge&nbsp;Server</strong>, to the system landscape that&nbsp;all incoming requests will<span>&nbsp;go through:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ae4561e4-4945-4613-9385-0f1bf1719981.png" style="width:30.25em;height:21.00em;" width="1239" height="859" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ae4561e4-4945-4613-9385-0f1bf1719981.png"></p>
<p>Implementation notes: An edge server typically behaves like a reverse proxy and can be integrated with a discovery service to provide dynamic load balancing capabilities.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution requirements</h1>
                </header>
            
            <article>
                
<p><span>Some solution requirements are as follows:</span></p>
<ul>
<li>Hide internal services that should not be exposed outside&nbsp;their context; that is, only route requests to microservices that are configured to allow external requests.&nbsp;</li>
<li>Expose external services and protect them from malicious requests; that is, use standard protocols and best practices such as OAuth, OIDC, JWT tokens, and API keys to ensure that the clients are&nbsp;trustworthy.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution requirements</h1>
                </header>
            
            <article>
                
<p><span>Some solution requirements are as follows:</span></p>
<ul>
<li>Hide internal services that should not be exposed outside their context; that is, only route requests to microservices that are configured to allow external requests. </li>
<li>Expose external services and protect them from malicious requests; that is, use standard protocols and best practices such as OAuth, OIDC, JWT tokens, and API keys to ensure that the clients are trustworthy.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Reactive microservice</h1>
                </header>
            
            <article>
                
<p><span>The reactive microservice pattern has the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p class="mce-root">Use non-blocking I/O to ensure that no threads are allocated while waiting for processing to occur in another service, that is, a database or another microservice.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p class="mce-root">Use non-blocking I/O to ensure that no threads are allocated while waiting for processing to occur in another service, that is, a database or another microservice.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Central configuration</h1>
                </header>
            
            <article>
                
<p><span>The&nbsp;central configuration pattern has the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problem</h1>
                </header>
            
            <article>
                
<p>An application is, traditionally, deployed together with its configuration, for example, a set of environment variables and/or files containing configuration information. Given a system landscape based on a microservice architecture, that is, with a large number of deployed microservice instances, some queries arise:</p>
<ul>
<li>How do I get a complete picture of the configuration that is in place for all the running microservice instances?</li>
<li>How do I update the configuration and make sure that all the affected&nbsp;<span>microservice instances&nbsp;</span>are updated correctly?</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problem</h1>
                </header>
            
            <article>
                
<p>An application is, traditionally, deployed together with its configuration, for example, a set of environment variables and/or files containing configuration information. Given a system landscape based on a microservice architecture, that is, with a large number of deployed microservice instances, some queries arise:</p>
<ul>
<li>How do I get a complete picture of the configuration that is in place for all the running microservice instances?</li>
<li>How do I update the configuration and make sure that all the affected <span>microservice instances </span>are updated correctly?</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p>A<span>dd a new component, a <strong>c</strong></span><strong>onfiguration</strong><span> server, to the system landscape to store the configuration of all the microservices. </span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Centralized log analysis</h1>
                </header>
            
            <article>
                
<p><span>Centralized log analysis has the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Centralized log analysis</h1>
                </header>
            
            <article>
                
<p><span>Centralized log analysis has the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p class="mce-root">Add a new component that can manage <strong>centralized logging</strong>&nbsp;and is capable of the following:</p>
<ul>
<li class="mce-root">Detecting new microservice instances and collecting log events from them</li>
<li class="mce-root">Interpreting and storing log events in a structured and searchable way in a central database</li>
<li class="mce-root">Providing&nbsp;<span>APIs and&nbsp;</span>graphical tools for querying and analyzing log events</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p class="mce-root">Add a new component that can manage <strong>centralized logging</strong> and is capable of the following:</p>
<ul>
<li class="mce-root">Detecting new microservice instances and collecting log events from them</li>
<li class="mce-root">Interpreting and storing log events in a structured and searchable way in a central database</li>
<li class="mce-root">Providing <span>APIs and </span>graphical tools for querying and analyzing log events</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problem</h1>
                </header>
            
            <article>
                
<p>It must be possible to track requests and messages that flow between microservices while processing an external call to the system landscape.</p>
<p><span>Some examples of fault scenarios are as follows:</span></p>
<ul>
<li><span>If end users start to file support cases regarding a specific failure, how can we identify the microservice that caused the problem, that is, the root cause?</span></li>
<li><span>If one support case mentions problems related to a specific entity, for example, a specific order number, how can we find log messages related to processing this specific order â for example, log messages from all microservices that were involved in processing this specific order?</span></li>
</ul>
<p><span>The following diagram depicts this:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a7e6182c-b5f4-4344-8e98-3e9f62aef421.png" style="width:24.67em;height:20.42em;" width="1250" height="1031" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a7e6182c-b5f4-4344-8e98-3e9f62aef421.png"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problem</h1>
                </header>
            
            <article>
                
<p>It must be possible to track requests and messages that flow between microservices while processing an external call to the system landscape.</p>
<p><span>Some examples of fault scenarios are as follows:</span></p>
<ul>
<li><span>If end users start to file support cases regarding a specific failure, how can we identify the microservice that caused the problem, that is, the root cause?</span></li>
<li><span>If one support case mentions problems related to a specific entity, for example, a specific order number, how can we find log messages related to processing this specific order â for example, log messages from all microservices that were involved in processing this specific order?</span></li>
</ul>
<p><span>The following diagram depicts this:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a7e6182c-b5f4-4344-8e98-3e9f62aef421.png" style="width:24.67em;height:20.42em;" width="1250" height="1031" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a7e6182c-b5f4-4344-8e98-3e9f62aef421.png"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p>To track the processing between cooperating microservices, we need to ensure that all related requests and messages are marked with a common correlation ID and that the correlation ID is part of all log events. Based on a <span>correlation ID, we can use the centralized logging service to find all related log events. If one of the log events also includes information about a business-related identifier, for example, the ID of a customer, product, order, and so on, we can find all related log events for that business identifier using the correlation ID.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution requirements</h1>
                </header>
            
            <article>
                
<p><span>The solution requirements are as follows:</span></p>
<ul>
<li>Assign unique correlation IDs to all incoming or new requests and events in a well-known place, such as a header with a recognized name.</li>
<li>When a microservice makes an outgoing request or sends a message, it must add the <span>correlation ID to the request and message.</span></li>
<li>All log events must include the correlation ID in a predefined format so that the centralized logging service can extract the correlation ID from the log event and make it searchable.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Circuit Breaker</h1>
                </header>
            
            <article>
                
<p><span>The Circuit Breaker pattern will have the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problem</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span><span>A system landscape </span></span>of microservices that uses synchronous intercommunication can be exposed to a <em>chain of failure</em>. If one microservice stops responding, its clients might get into problems as well and stop responding to requests from their clients. The problem can propagate recursively throughout a system landscape and take out major parts of it.</p>
<div class="mce-root packt_infobox"><span><span>This is especially common in cases where synchronous requests are executed using blocking I/O, that is, blocking a thread from the underlying operating system while a request is being processed. Combined with a large number of concurrent requests and a service that starts to respond unexpectedly</span> <span>slowly<span>, thread pools can quickly become drained, causing the caller to hang and/or</span> crash.<span> This failure</span></span></span> can spread unpleasantly fast <span>to the caller's caller, and so on.</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p>Add a Circuit Breaker that prevents new outgoing requests from a caller if it detects a problem with the service it calls.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Control loop</h1>
                </header>
            
            <article>
                
<p><span>The control loop&nbsp;pattern will have the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Control loop</h1>
                </header>
            
            <article>
                
<p><span>The control loop pattern will have the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problem</h1>
                </header>
            
            <article>
                
<p>In a system landscape with a large number of microservice instances spread out over a number of servers, it is very difficult to manually detect and correct problems such as crashed or hung microservice instances.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p>Add a new component, a <strong>control loop</strong>, to the system landscape; this constantly observes the actual state of the system landscape; compares it with the desired state, as specified by the operators; and, if required, takes action. For example, if the two states differ, it needs to make the actual state equal to the desired state:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4716aa50-5154-4e6a-b6d2-32ae7728d640.png" width="1878" height="649" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4716aa50-5154-4e6a-b6d2-32ae7728d640.png"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution requirements</h1>
                </header>
            
            <article>
                
<p><span>Implementation notes: In the world of containers, a <em>container orchestrator</em> such as Kubernetes is typically used to implement this pattern. We will learn more about Kubernetes in <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>, <em>Introduction to Kubernetes</em>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Centralized monitoring and alarms</h1>
                </header>
            
            <article>
                
<p><span>For this pattern, we will have the following problem, solution, and solution requirements.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problem</h1>
                </header>
            
            <article>
                
<p>If observed response times and/or the usage of hardware resources <span>become unacceptably high, it can be very hard to discover the root cause of the problem. For example, we need to be able to analyze hardware resource consumption per microservice.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article>
                
<p>To curb this, we add a new component, a <strong>monitor service</strong>, to <span>the system landscape, which is capable of collecting metrics about hardware resource usage for each microservice instance level.</span>&nbsp;</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Solution requirements </h1>
                </header>
            
            <article>
                
<p><span>The solution requirements are as follows:</span></p>
<ul>
<li>It must be able to collect metrics from all the servers that are used by the system landscape, which includes auto-scaling servers.</li>
<li>It must be able to detect new microservice instances as they are launched on the available servers and start to collect metrics from them.</li>
<li>It must be able to provide <span>APIs and </span><span>graphical tools for querying and analyzing the collected metrics. </span></li>
</ul>
<p><span>The following screenshot shows Grafana, which visualizes metrics from Prometheus, a monitoring tool that we will lo</span>ok at later in this book:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c19497ae-0184-4788-baed-9bc5a5f11993.png" style="width:44.08em;height:29.75em;" width="1950" height="1318" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c19497ae-0184-4788-baed-9bc5a5f11993.png"></p>
<p>That was an extensive list! I am sure these design patterns helped you understand the challenges with microservices better. Next, we will move on to understand software enablers.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Software enablers</h1>
                </header>
            
            <article>
                
<p>As we've already mentioned, we have a number of very good open-source tools that can help us both meet our expectations of microservices and, most importantly, handle the new challenges that come with <span>them:</span></p>
<ul>
<li>Spring Boot</li>
<li>Spring Cloud/Netflix OSS</li>
<li style="font-weight: 400">Docker</li>
<li style="font-weight: 400"><span>Kubernetes</span></li>
<li style="font-weight: 400">&nbsp;<span>Istio (a service mesh)</span></li>
</ul>
<p>The following table maps the design patterns we will need to handle these challenges, along with the corresponding open-source tool that implements the design pattern:</p>
<table style="border-collapse: collapse;width: 90%" border="1">
<tbody>
<tr>
<td><strong>Design Pattern</strong></td>
<td><strong>Spring Boot</strong></td>
<td><strong>Spring Cloud</strong></td>
<td><strong>Kubernetes</strong></td>
<td><strong>Istio</strong></td>
</tr>
<tr>
<td style="width: 184px">
<p><strong>Service discovery</strong></p>
</td>
<td></td>
<td>Netflix Eureka and Netflix Ribbon</td>
<td>Kubernetes <kbd>kube-proxy</kbd> and service resources</td>
<td></td>
</tr>
<tr>
<td style="width: 184px">
<p><strong>Edge server</strong></p>
</td>
<td></td>
<td>Spring Cloud and Spring Security OAuth</td>
<td>Kubernetes Ingress controller</td>
<td>Istio ingress gateway</td>
</tr>
<tr>
<td style="width: 184px">
<p><strong>Reactive microservices</strong></p>
</td>
<td>Spring Reactor and Spring WebFlux</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="width: 184px">
<p><strong>Central configuration</strong></p>
</td>
<td></td>
<td>Spring Config Server</td>
<td>Kubernetes <kbd>ConfigMaps</kbd> and Secrets</td>
<td></td>
</tr>
<tr>
<td style="width: 184px">
<p><strong>Centralized log analysis</strong></p>
</td>
<td></td>
<td></td>
<td>
<p class="mce-root">Elasticsearch, Fluentd, and Kibana<br>
<strong>Note</strong>: Actually not part of Kubernetes<br>
but can easily be deployed and configured together with Kubernetes</p>
</td>
<td></td>
</tr>
<tr>
<td>
<p><strong>Distributed tracing</strong></p>
</td>
<td></td>
<td>Spring Cloud Sleuth and Zipkin</td>
<td></td>
<td>Jaeger</td>
</tr>
<tr>
<td style="width: 184px">
<p><strong>Circuit Breaker</strong></p>
</td>
<td></td>
<td>Resilience4j</td>
<td></td>
<td>Outlier detection</td>
</tr>
<tr>
<td style="width: 184px">
<p><strong>Control loop</strong></p>
</td>
<td></td>
<td></td>
<td><span>Kubernetes controller manager</span></td>
<td></td>
</tr>
<tr>
<td><strong><span>Centralized monitoring and alarms</span></strong></td>
<td></td>
<td></td>
<td>Grafana and Prometheus<br>
<strong>Note:</strong><span> Actually not part of Kubernetes</span><br>
<span>but can </span><span>easily be deployed and configured  together with Kubernetes</span></td>
<td>Kiali, <span>Grafana, and Prometheus</span></td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>Please note that Spring Cloud, Kubernetes, and Istio can be used to implement some design patterns, such as service discovery, e<span>dge server, and c</span><span>entral configuratio</span><span>n. We will discuss the pros and cons of using these alternatives later in this book.</span></p>
<p>Now, let's look at some other important things that we need to take into consideration.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Other important considerations</h1>
                </header>
            
            <article>
                
<p>To be successful implementing a microservice architecture, there are a number of related areas to consider as well. I will not cover these areas in this book; instead, I'll just briefly mention them here as follows:</p>
<ul>
<li><strong>Importance of Dev/Ops</strong>:<strong>&nbsp;</strong><span>One of the benefits of a microservice architecture is that it enables shorter delivery times and, in extreme cases allows the <em>continuous delivery</em> of new versions. To be able to deliver that fast, you need to establish an organization where dev and ops work together under the mantra <em>you built it, you run it</em>. This means that developers are no longer allowed to simply pass new versions of the software over to the operations team. Instead, the dev and ops organizations need to work much more closely together, organized into teams that have full responsibility for the end-to-end life cycle of one microservice (or a group of related microservices). Besides the organizational part of <kbd>dev</kbd>/<kbd>ops</kbd>, the teams also need to automate the delivery chain, that is, the steps for building, testing, packaging, and deploying the microservices to the various deployment environments. This is known as setting up a <em>delivery pipeline</em>.</span></li>
<li><strong>Organizational aspects and <span>Conway's law</span></strong>: Another interesting aspect of how a microservice architecture might affect the organization is <span><em>Conway's law</em>, which states the following:</span></li>
</ul>
<div style="padding-left: 60px" class="packt_quote">"Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure."<br>
<br>
<span>-- Melvyn Conway, 1967</span></div>
<p style="padding-left: 60px">This means that the traditional approach of organizing IT teams for large applications based on their technology expertise (for example, UX, business logic, and databases-teams) will lead to a big three-tier application <span>â t</span>ypically a big monolithic application with a separately deployable unit for the UI, one for processing the business logic, and one for the big database. To successfully deliver an application based on a microservice architecture, the organization needs to be changed into teams that work with one or a group of related microservices. The team must have the skills that are required for those microservices, for example, languages and frameworks for the business logic and database technologies for persisting its data.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<ul>
<li><strong>Decomposing a monolithic application into microservices: </strong>One of the most difficult and expensive decisions is how to <span>decompose a monolithic application into a set of cooperating microservices. If this is done in the wrong way, you will end up with problems such as the following:<br></span>
<ul>
<li><strong>Slow delivery</strong>: Changes in the business requirements will affect too many of the microservices, resulting in extra work.</li>
<li><strong>Slow performance</strong>: To be able to perform a specific business function, a lot of requests have to be passed between various microservices, resulting in long response times.</li>
<li><strong>Inconsistent data</strong>: Since related data is separated into different microservices, inconsistencies can appear over time in data that's managed by different microservices.</li>
</ul>
</li>
</ul>
<p style="padding-left: 90px">A good approach to finding proper boundaries for microservices is to apply <strong>Domain-Driven Design</strong> and its <strong>Bounded Context</strong> concept. According to Eric Evans,<span>&nbsp;a <em>Bounded Context</em> is "</span><em>A description of a boundary (typically a subsystem, or the work of a particular team) within which a particular model is defined and applicable." </em><span>This means that the microservice defined by a Bounded Context will have a well-defined model of its own data. </span></p>
<ul>
<li><strong>Importance of API design: </strong>If a group of microservices expose a common, externally available API, it is important that the API is easy to understand and consumes the following:
<ul>
<li>If the same concept is used in multiple APIs, it should have the same description in terms of the naming and data types used.</li>
<li>It is of great importance that APIs are allowed to evolve in a controlled manner. This typically requires applying a proper versioning schema for the APIs, for example, <a href="https://semver.org/">https://semver.org/</a>, and having the capability of handling multiple major versions of an API over a specific period of time, allowing clients of the API to migrate to new major versions at their own pace.</li>
</ul>
</li>
<li><strong>Migration paths from on-premise to the cloud</strong>:<strong>&nbsp;</strong>Many companies today run their workload on-premise, but are searching for ways to move parts of their workload to the cloud. Since most cloud providers today offer Kubernetes as a Service, an appealing migration approach can be to first move the workload into Kubernetes <span>on-premise </span>(as microservices or not) and then redeploy it on a <em>Kubernetes as a Service</em> offering provided by a preferred cloud provider. </li>
<li><strong>Good design principles for microservices, the 12-factor app</strong>:<strong>&nbsp;</strong>Th<span>e 12-factor app (</span><a href="https://12factor.net/">https://12factor.net</a>) is a set of <span>design principles for building software that can be deployed in the cloud. Most of these design principles are applicable to building microservices independently of where and how they will be deployed, that is, in the cloud or on-premise. </span>Some of these principles will be covered in this book, such as <span>c</span><span>onfig, processes, and logs,</span><span> but not all.</span></li>
</ul>
<p>That's it for the first chapter! I hope this gave you a good basic idea of microservices and helped you understand the large scale topics that will be covered in this book.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this introductory chapter, </span>I described my own way into microservices and delved into a bit of their history. We defined what a microservice is, that is, a kind of autonomous distributed component with some specific requirements. We also went through the good and challenging aspects of a microservice-based architecture.</p>
<p>To handle these challenges, we defined a set of design patterns and briefly mapped the capabilities of open source products such as Spring Boot, Spring Cloud, and Kubernetes to them.</p>
<p>You're eager to develop your first microservice now, right? In the next chapter, we will be introduced to Spring Boot and complementary open source tools that we will use to develop our first microservices</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to Spring Boot</h1>
                </header>
            
            <article>
                
<p class="p1">In this chapter, we will be introduced to how to build a set of cooperating microservices using Spring Boot,<span> focusing on how to develop functionality </span>that delivers business value<span>.</span> The challenges that we pointed out in the previous chapter will be considered only to some degree, but they will be addressed to their full extent in later chapters.</p>
<p>We will develop <span>microservices that contain b</span>usiness logic based on plain Spring Beans and REST APIs using Spring WebFlux, the Swagger/OpenAPI-based documentation of the REST APIs, and SpringFox and data persistence, while using Spring Data to store data in both SQL and NoSQL databases</p>
<p>Since <span>Spring Boot v2.0 was released in March 2018, it has become much easier to develop</span> reactive microservices (refer to <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml" target="_blank">Chapter 1</a>,&nbsp;<span><em>Introduction to Microservices</em>, the <em>Reactive mi</em></span><span><em>croservices</em> section for more information</span><span>)</span>. Therefore, we will also cover how to create <span>reactive microservices in this chapter, including both non-blocking synchronous REST APIs and message-based asynchronous services. We will use Spring WebFlux to develop non-blocking synchronous REST APIs and Spring Cloud Stream to develop message-based asynchronous services.</span></p>
<p>Finally, we will use Docker to run our microservices as containers. This will allow us to start and stop our microservice landscape, including database servers and a message broker, with a single command. </p>
<p>That's a lot of technologies and frameworks, so let's go through each of them briefly to see what they are about! </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Learning about Spring Boot</li>
<li>Beginning with Spring WebFlux</li>
<li>Exploring SpringFox</li>
<li>Understanding Spring Data</li>
<li>Understanding Spring Cloud Stream</li>
<li>Learning about Docker</li>
</ul>
<div class="packt_infobox">More details about each product will be provided in upcoming chapters.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>This chapter does not contain any source code that can be downloaded, nor does it require any tools to be installed.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Convention over configuration and fat JAR files</h1>
                </header>
            
            <article>
                
<p>Spring Boot targets the fast development of production-ready Spring applications by being strongly opinionated about how to set up both core modules from the Spring Framework and third-party products, such as libraries that are used for logging or connecting to a database. Spring Boot does that by applying a number of conventions by default, minimizing the need for configuration. Whenever required, each convention can be overridden by writing some configuration, case by case. This design pattern is known as <strong>convention over configuration</strong>&nbsp;and&nbsp;<span>minimizes</span>&nbsp;the need for initial configuration.&nbsp;</p>
<div class="packt_infobox"><span>Configuration, when required, is in my opinion written best using Java and annotations. The good old XML-based configuration files can still be used, although they are significantly smaller than before Spring Boot was introduced.</span>&nbsp;</div>
<p><span>Added to the usage of <em>c</em></span><em>onvention over configuration</em><span>, Spring Boot also favors a runtime model based on a standalone</span> JAR&nbsp;<span>file, also known as a fat</span> JAR <span>file. Before Spring Boot, the&nbsp;most common way to run a Spring application was to deploy it as a</span> WAR <span>file&nbsp;on a Java EE web server, such as Apache Tomcat. WAR file deployment is still supported by Spring Boot.</span></p>
<div class="packt_infobox">A&nbsp;fat JAR file contains not only the classes and resource files of the application itself, but also all the&nbsp;<kbd>.jar</kbd>&nbsp;files the application depends on. This means that the fat JAR file is the only <span>JAR f</span>ile required to run the application; that is, we only need to transfer one JAR file to an environment where we want to run the application instead of transferring the application's JAR file along with all the <span>JAR&nbsp;</span>files the application depends on.</div>
<p>Starting a fat JAR requires no separately installed&nbsp;<span>Java EE web server, such as Apache Tomcat. Instead, it can be started with a simple command such as&nbsp;<kbd>java -jar app.jar</kbd>, making it a perfect choice for running in a Docker container! If the Spring Boot application uses HTTP, for example, to expose a REST API, it will contain an embedded web server.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Code examples for setting up a Spring Boot application</h1>
                </header>
            
            <article>
                
<p>To better understand what this means, let's look at some source code examples.</p>
<div class="packt_infobox"><span>We will only look at some small fragments of code here to point out the main features. For a fully working example, you'll have to wait until the next chapter!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The magic&nbsp;@SpringBootApplication annotation</h1>
                </header>
            
            <article>
                
<p>The&nbsp;<span>convention-based&nbsp;</span>autoconfiguration mechanism can be initiated by annotating the application class, that is, the class that contains the static <kbd>main</kbd> method,&nbsp; with the&nbsp;<span><kbd>@SpringBootApplication</kbd>&nbsp;annotation. The following code shows this:</span></p>
<pre><span>@SpringBootApplication<br></span><span>public class </span>MyApplication {<br><br>  <span>public static void </span>main(String[] args) {<br>    SpringApplication.<span>run</span>(MyApplication.<span>class</span>, args);<br>  }<br>}</pre>
<p>The following functionality will be provided by this annotation:</p>
<ul>
<li>It enables component scanning, that is, looking for Spring components and configuration classes in the package of the application class and all its sub-packages.</li>
<li>The application class itself becomes a configuration class.</li>
<li>It enables autoconfiguration, where&nbsp;Spring Boot looks for JAR files in the classpath that it can configure automatically. If you, for example, have Tomcat in the classpath, Spring Boot will automatically configure Tomcat as an embedded web server.&nbsp;</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The magic @SpringBootApplication annotation</h1>
                </header>
            
            <article>
                
<p>The <span>convention-based </span>autoconfiguration mechanism can be initiated by annotating the application class, that is, the class that contains the static <kbd>main</kbd> method,  with the <span><kbd>@SpringBootApplication</kbd> annotation. The following code shows this:</span></p>
<pre><span>@SpringBootApplication<br></span><span>public class </span>MyApplication {<br><br>  <span>public static void </span>main(String[] args) {<br>    SpringApplication.<span>run</span>(MyApplication.<span>class</span>, args);<br>  }<br>}</pre>
<p>The following functionality will be provided by this annotation:</p>
<ul>
<li>It enables component scanning, that is, looking for Spring components and configuration classes in the package of the application class and all its sub-packages.</li>
<li>The application class itself becomes a configuration class.</li>
<li>It enables autoconfiguration, where Spring Boot looks for JAR files in the classpath that it can configure automatically. If you, for example, have Tomcat in the classpath, Spring Boot will automatically configure Tomcat as an embedded web server. </li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Java-based configuration</h1>
                </header>
            
            <article>
                
<p>If we want to override&nbsp;<span>Spring Boot's</span>&nbsp;default configuration or if we want to add our own configuration, we can simply annotate a class with&nbsp;<kbd>@Configuration</kbd>&nbsp;and it will be picked up by the component scanning mechanism we described previously.</p>
<p>If we, for example,&nbsp;<span>want to set up a filter in the processing of HTTP requests (handled by Spring WebFlux, which is described as follows) that writes a log message at the beginning and at the end of the processing of the request, we can configure a log-filter, as follows:</span></p>
<pre>@<span>Configuration</span><br><span>public class </span>SubscriberApplication {<br><br><span>  @Bean<br></span><span>  public </span>Filter logFilter() {<br><span>    </span>CommonsRequestLoggingFilter filter = <span>new <br>        </span>CommonsRequestLoggingFilter();<br>    filter.setIncludeQueryString(<span>true</span>);<br>    filter.setIncludePayload(<span>true</span>);<br>    filter.setMaxPayloadLength(<span>5120</span>);<br>    <span>return </span>filter;<br>  }</pre>
<div class="packt_infobox">We can also place the configuration directly in the application class since the&nbsp;<span><kbd>@SpringBootApplication</kbd> annotation</span>&nbsp;implies the <kbd>@Configuration</kbd>&nbsp;annotation.</div>
<p class="mce-root">Now that we have learned about Spring Boot, let's talk about Spring WebFlux.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Java-based configuration</h1>
                </header>
            
            <article>
                
<p>If we want to override <span>Spring Boot's</span> default configuration or if we want to add our own configuration, we can simply annotate a class with <kbd>@Configuration</kbd> and it will be picked up by the component scanning mechanism we described previously.</p>
<p>If we, for example, <span>want to set up a filter in the processing of HTTP requests (handled by Spring WebFlux, which is described as follows) that writes a log message at the beginning and at the end of the processing of the request, we can configure a log-filter, as follows:</span></p>
<pre>@<span>Configuration</span><br><span>public class </span>SubscriberApplication {<br><br><span>  @Bean<br></span><span>  public </span>Filter logFilter() {<br><span>    </span>CommonsRequestLoggingFilter filter = <span>new <br>        </span>CommonsRequestLoggingFilter();<br>    filter.setIncludeQueryString(<span>true</span>);<br>    filter.setIncludePayload(<span>true</span>);<br>    filter.setMaxPayloadLength(<span>5120</span>);<br>    <span>return </span>filter;<br>  }</pre>
<div class="packt_infobox">We can also place the configuration directly in the application class since the <span><kbd>@SpringBootApplication</kbd> annotation</span> implies the <kbd>@Configuration</kbd> annotation.</div>
<p class="mce-root">Now that we have learned about Spring Boot, let's talk about Spring WebFlux.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Beginning with Spring WebFlux</h1>
                </header>
            
            <article>
                
<p>Spring Boot 2.0 is based on Spring Framework 5.0, which came with built-in support for developing reactive applications. Spring Framework uses <strong>Project Reactor </strong>as the base implementation of its reactive support, and also comes with a new web framework, Spring WebFlux, which supports the development of reactive, that is, non-blocking, HTTP clients and services.</p>
<p>Spring WebFlux supports two different programming models:</p>
<ul>
<li>An annotation-based imperative style, similar to the already existing web framework, Spring Web MVC, but with support for reactive services</li>
<li>A new function-oriented model based on routers and handlers</li>
</ul>
<p>In this book, we will use the <span>annotation-based imperative style to demonstrate how easy it is to move REST services from Spring Web MVC to Spring WebFlux and then start to refactor the services so that they become fully reactive.</span></p>
<p class="mce-root"><span>Spring WebFlux also provides a fully r</span>eactive HTTP client, <kbd>WebClient</kbd>, as a complement to the existing <kbd>RestTemplate</kbd> client.</p>
<p><span>Spr</span><span>ing WebFlux supports running on a servlet container (it requires Servlet v3.1 or higher), but also supports reactive non-servlet-based embedded web servers such as Netty (<a href="https://netty.io/">https://netty.io/</a>).</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Starter dependencies</h1>
                </header>
            
            <article>
                
<p>In this book, we will use Gradle as our build tool, so the <span>Spring WebFlux&nbsp;starter dependency will be added to the <kbd>build.gradle</kbd> file. It looks like this:</span></p>
<pre>implementation(<span>'org.springframework.boot:spring-boot-starter-webflux'</span>)</pre>
<div class="packt_infobox">You might be wondering why we don't specify a version number.<br>
We will talk about that when we look at a complete example in &nbsp;<a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml">Chapter 3</a>, <em>Creating a Set of Cooperating Microservices</em>!</div>
<p>When the microservice is started up, Spring Boot will detect <span>Spring WebFlux on the classpath and configure it, as well as other things that are used to start up an embedded web server. Netty is used by default, which&nbsp;</span>we can see from the log output:</p>
<pre><strong>2018-09-30 15:23:43.592 INFO 17429 --- [ main] o.s.b.web.embedded.netty.NettyWebServer : Netty started on port(s): 8080</strong></pre>
<p>If we want to switch from Netty to Tomcat as our embedded web server, we can override the default configuration by excluding Netty from the starter dependency and add the starter dependency for Tomcat:</p>
<pre>implementation(<span>'org.springframework.boot:spring-boot-starter-webflux'</span>) <br>{<br> exclude <span>group</span>: <span>'org.springframework.boot'</span>, <span>module</span>: <span>'spring-boot-<br> starter-reactor-netty'<br></span>}<br>implementation(<span>'org.springframework.boot:spring-boot-starter-tomcat'</span>)</pre>
<p class="mce-root">After restarting the microservice, we can see that Spring Boot picked Tomcat instead:</p>
<pre>2018-09-30 18:23:44.182 INFO 17648 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http)</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Property files</h1>
                </header>
            
            <article>
                
<p>As you can see from the preceding examples, the web server is started up using port <kbd>8080</kbd>. If you want to change the port, you can override the default value using a property file.&nbsp;<span>Spring Boot application property files can either be a <kbd>.properties</kbd> file or a YAML file. B</span>y default, they are named<span>&nbsp;</span><kbd>application.properties</kbd><span>&nbsp;and</span>&nbsp;<span><kbd>application.yml</kbd>, respectively.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>In this book, we will use YAML files so that the HTTP port used by the embedded web server can be changed to <kbd>7001</kbd>. By doing this, we can avoid port collisions with other microservices running on the same server. To do this, add the following line to the <kbd>application.yml</kbd> file:</span></p>
<pre><span>server.port</span>: <span>7001</span></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Property files</h1>
                </header>
            
            <article>
                
<p>As you can see from the preceding examples, the web server is started up using port <kbd>8080</kbd>. If you want to change the port, you can override the default value using a property file. <span>Spring Boot application property files can either be a <kbd>.properties</kbd> file or a YAML file. B</span>y default, they are named<span>&nbsp;</span><kbd>application.properties</kbd><span> and</span>&nbsp;<span><kbd>application.yml</kbd>, respectively.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>In this book, we will use YAML files so that the HTTP port used by the embedded web server can be changed to <kbd>7001</kbd>. By doing this, we can avoid port collisions with other microservices running on the same server. To do this, add the following line to the <kbd>application.yml</kbd> file:</span></p>
<pre><span>server.port</span>: <span>7001</span></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Exploring SpringFox</h1>
                </header>
            
            <article>
                
<p>One very important aspect of developing APIs, for example, RESTful services, is how to document them so that they are easy to use. When it comes to RESTful services, Swagger is one of the most widely used ways of documenting&nbsp;<span>RESTful services. Many leading API gateways have native support for exposing the documentation of&nbsp;RESTful services using Swagger.&nbsp;</span></p>
<p>In 2015, SmartBear Software donated the Swagger specification to the Linux Foundation under the OpenAPI Initiative and created the OpenAPI Specification. The name Swagger is still used for the tooling provided by SmartBear Software.</p>
<p>SpringFox is an open-source project, separate from the Spring Framework, that can create Swagger-based API documentation at runtime. It does so by examining the application at startup, for example,&nbsp;<span>inspecting <kbd>WebFlux</kbd> and Swagger-based&nbsp;annotations.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>We will look at full source code examples in upcoming chapters, but for now the following screenshot of this sample API documentation will do:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9c5f6fb1-6de7-4c81-996c-8c69ede17c43.png" width="1566" height="1630" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9c5f6fb1-6de7-4c81-996c-8c69ede17c43.png"></p>
<div class="packt_infobox">Note the big <span class="packt_screen">Execute</span> button, which can be used to actually try out the API, not just read its documentation!</div>
<p>SpringFox helped us understand how microservices delved into Spring Framework. Now, let's move on to Spring Data.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Understanding Spring Data</h1>
                </header>
            
            <article>
                
<p>Spring Data comes with a common programming model for persisting data in various types of database engine, ranging from traditional relational databases (SQL databases) to various types of NoSQL database engine, such as document databases (for example, MongoDB), key-value databases (<span>for example,</span>&nbsp;Redis), and graph databases (<span>for example,&nbsp;</span>Neo4J).</p>
<p>The Spring Data project is divided into several subprojects and in this book we will use Spring Data&nbsp;<span>subprojects</span> for MongoDB and JPA that have been mapped to a MySQL database.</p>
<div class="packt_infobox"><span><strong>JPA</strong> stands for <strong>Java Persistence API</strong> and is a Java specification about how to handle relational data. Please go to&nbsp;<a href="https://jcp.org/aboutJava/communityprocess/mrel/jsr338/index.html">https://jcp.org/aboutJava/communityprocess/mrel/jsr338/index.html</a> for the latest specification, which is JPA 2.2 at the time of writing.</span></div>
<p>The two core concepts of the programming model in Spring Data are entities and repositories. <span>Entities and repositories generalize how data is stored and accessed from the various types of database. They provide a common abstraction but still support adding database-specific behavior to the entities and repositories.&nbsp;These two core&nbsp;concepts are briefly explained together with some illustrative code examples as we proceed through this chapter. Remember that more&nbsp;details will be provided in the upcoming chapters!</span></p>
<div class="packt_infobox">Even though Spring Data provides a common programming model for different types of database, this doesn't mean that you will be able to write portable source code, for example, switching the database technology from a SQL database to a NoSQL database, without changes needing to be made to the source code!</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Understanding Spring Data</h1>
                </header>
            
            <article>
                
<p>Spring Data comes with a common programming model for persisting data in various types of database engine, ranging from traditional relational databases (SQL databases) to various types of NoSQL database engine, such as document databases (for example, MongoDB), key-value databases (<span>for example,</span> Redis), and graph databases (<span>for example, </span>Neo4J).</p>
<p>The Spring Data project is divided into several subprojects and in this book we will use Spring Data <span>subprojects</span> for MongoDB and JPA that have been mapped to a MySQL database.</p>
<div class="packt_infobox"><span><strong>JPA</strong> stands for <strong>Java Persistence API</strong> and is a Java specification about how to handle relational data. Please go to <a href="https://jcp.org/aboutJava/communityprocess/mrel/jsr338/index.html">https://jcp.org/aboutJava/communityprocess/mrel/jsr338/index.html</a> for the latest specification, which is JPA 2.2 at the time of writing.</span></div>
<p>The two core concepts of the programming model in Spring Data are entities and repositories. <span>Entities and repositories generalize how data is stored and accessed from the various types of database. They provide a common abstraction but still support adding database-specific behavior to the entities and repositories. These two core concepts are briefly explained together with some illustrative code examples as we proceed through this chapter. Remember that more details will be provided in the upcoming chapters!</span></p>
<div class="packt_infobox">Even though Spring Data provides a common programming model for different types of database, this doesn't mean that you will be able to write portable source code, for example, switching the database technology from a SQL database to a NoSQL database, without changes needing to be made to the source code!</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Repositories</h1>
                </header>
            
            <article>
                
<p>Repositories are used&nbsp;to store and access data from different types of database. In its most basic form, a repository can be declared as a Java interface, and Spring Data will generate its implementation on the fly using&nbsp;<span>opinionated convention</span>s. These conventions can be overridden and/or complemented by additional configuration and, if required, some Java code. Spring Data also comes with&nbsp;some base Java interfaces, for example, <kbd>CrudRepository</kbd>, to make the definition of a repository even simpler. The base interface,&nbsp;<span><kbd>CrudRepository</kbd>, provides us with standard methods for create, read, update, and delete operations.</span></p>
<p>To specify a repository for handling the JPA entity,&nbsp;<kbd>ReviewEntity</kbd>, we only need to declare the following:</p>
<pre><span>import </span>org.springframework.data.repository.CrudRepository;<br><br><span>public interface </span>ReviewRepository <span>extends </span>CrudRepository&lt;ReviewEntity, ReviewEntityPK&gt; {<br>    Collection&lt;ReviewEntity&gt; findByProductId(<span>int </span>productId);<br>}</pre>
<p>In this example we use a class,&nbsp;<kbd>ReviewEntityPK</kbd>, to describe a composite primary key. It looks as follows:</p>
<pre><span>public class </span>ReviewEntityPK <span>implements </span>Serializable {<br>    <span>public int </span><span>productId</span>;<br>    <span>public int </span><span>reviewId</span>;<br>}</pre>
<p>We have also added an extra method,&nbsp;<kbd>findByProductId</kbd>, which&nbsp;allows us to look up <kbd>Review</kbd> entities based on&nbsp;<kbd>productId</kbd>&nbsp;<span>â&nbsp;</span>a field that is part of the primary key. The naming of the method follows a naming convention defined by Spring Data that allows Spring Data to generate the implementation of this method on the fly as well.</p>
<p>If we want to use the repository, we can simply inject it and then start to use it, for example:</p>
<pre><span>private final </span>ReviewRepository <span>repository</span>;<br><br><span>@Autowired<br></span><span>public </span>ReviewService(ReviewRepository repository) {<br> <span>this</span>.<span>repository </span>= repository;<br>}<br><br><span>public void someMethod</span>() {<br><span>  repository</span>.save(entity);<br><span>  repository</span>.delete(entity);<br><span>  repository</span>.findByProductId(productId);</pre>
<p>Also added to the&nbsp;<span><kbd>CrudRepository</kbd> interface, Spring Data also provides a reactive base interface,&nbsp;</span><kbd>ReactiveCrudRepository</kbd>, which enables reactive repositories. The methods in this interface do not return objects or collections of objects; instead, they return&nbsp;<kbd>Mono</kbd> and <kbd>Flux</kbd> objects.&nbsp;<span>&nbsp;</span><kbd>Mono</kbd><span>&nbsp;and&nbsp;</span><kbd>Flux</kbd><span>&nbsp;objects are, as we will see in later chapters,&nbsp;</span><span>reactive streams that are capable of returning either <kbd>0</kbd>..<kbd>1</kbd> or <kbd>0</kbd>..<kbd>m</kbd> entities as they become available on the stream. The reactive-based interface can only be used by Spring Data subprojects that support reactive database drivers; that is, they are based on non-blocking I/O. The Spring Data MongoDB subproject supports reactive repositories, while Spring Data JPA does not.</span></p>
<p>Specifying a reactive repository for handling the MongoDB entity, <kbd>RecommendationEntity</kbd>, as described previously, might look something like the following:</p>
<pre><span>import </span>org.springframework.data.repository.reactive.ReactiveCrudRepository;<br><span>import </span>reactor.core.publisher.Flux;<br><br><span>public interface </span>RecommendationRepository <span>extends </span>ReactiveCrudRepository&lt;RecommendationEntity, String&gt; {<br>    Flux&lt;RecommendationEntity&gt; findByProductId(<span>int </span>productId);<br>}</pre>
<p>This concludes the section on Spring Data. Now let's see what the Spring Cloud Stream is about.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Understanding Spring Cloud Stream</h1>
                </header>
            
            <article>
                
<p>We will not focus on Spring Cloud in this chapter; we will do that from <a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml">Chapter 9</a>,&nbsp;<em>Adding Service Discovery Using Netflix Eureka and Ribbon</em>&nbsp;to <a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml">Chapter 14</a>, <em>Understanding Distributed Tracing</em>.&nbsp;However, we will bring in one of the modules that's part of Spring Cloud: Spring Cloud Stream.&nbsp;<span>Spring Cloud Stream</span> provides a streaming abstraction over messaging, based on the publish-and-subscribe integration pattern.&nbsp;<span>Spring Cloud Stream currently comes with</span>&nbsp;support for Apache Kafka and RabbitMQ out of the box. <span>A number of separate projects exist that provide</span> <span>integration with other popular messaging systems. See</span> <a href="https://github.com/spring-cloud?q=binder">https://github.com/spring-cloud?q=binder</a> for more details.</p>
<p>The core concepts in Spring Cloud Stream are as follows:</p>
<ul>
<li><strong>Message:</strong>&nbsp;A data structure that's used to describe data sent to and received from a messaging system.</li>
<li><strong>Publisher:</strong>&nbsp;Sends messages to the&nbsp;<span>messaging system.</span></li>
<li><strong>Subscriber</strong>: Receives&nbsp;<span>messages from the&nbsp;</span><span>messaging system.</span>&nbsp;</li>
<li><strong>Channel:</strong>&nbsp;Used to communicate with the&nbsp;<span>messaging system. Publishers use output channels and subscribers use input channels.</span></li>
<li><strong>Binder:</strong>&nbsp;A<span>&nbsp;binder&nbsp;</span><span>provides the actual integration with a specific messaging system, similar to what a JDBC driver does for a specific type of database.&nbsp;</span></li>
</ul>
<p>The actual messaging system to be used is determined at runtime, depending on what is found on the classpath. Spring Cloud Stream comes with&nbsp;<span>opinionated convention</span><span>s</span> on how to handle messaging.&nbsp;<span>These conventions can be overridden by specifying a configuration for messaging features such as&nbsp;</span>consumer groups, partitioning, persistence, durability, and error handling, such as retries and dead letter queue handling.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Understanding Spring Cloud Stream</h1>
                </header>
            
            <article>
                
<p>We will not focus on Spring Cloud in this chapter; we will do that from <a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml">Chapter 9</a>,&nbsp;<em>Adding Service Discovery Using Netflix Eureka and Ribbon</em> to <a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml">Chapter 14</a>, <em>Understanding Distributed Tracing</em>. However, we will bring in one of the modules that's part of Spring Cloud: Spring Cloud Stream. <span>Spring Cloud Stream</span> provides a streaming abstraction over messaging, based on the publish-and-subscribe integration pattern. <span>Spring Cloud Stream currently comes with</span> support for Apache Kafka and RabbitMQ out of the box. <span>A number of separate projects exist that provide</span> <span>integration with other popular messaging systems. See</span> <a href="https://github.com/spring-cloud?q=binder">https://github.com/spring-cloud?q=binder</a> for more details.</p>
<p>The core concepts in Spring Cloud Stream are as follows:</p>
<ul>
<li><strong>Message:</strong> A data structure that's used to describe data sent to and received from a messaging system.</li>
<li><strong>Publisher:</strong> Sends messages to the <span>messaging system.</span></li>
<li><strong>Subscriber</strong>: Receives <span>messages from the </span><span>messaging system.</span>&nbsp;</li>
<li><strong>Channel:</strong> Used to communicate with the <span>messaging system. Publishers use output channels and subscribers use input channels.</span></li>
<li><strong>Binder:</strong>&nbsp;A<span> binder </span><span>provides the actual integration with a specific messaging system, similar to what a JDBC driver does for a specific type of database. </span></li>
</ul>
<p>The actual messaging system to be used is determined at runtime, depending on what is found on the classpath. Spring Cloud Stream comes with <span>opinionated convention</span><span>s</span> on how to handle messaging. <span>These conventions can be overridden by specifying a configuration for messaging features such as </span>consumer groups, partitioning, persistence, durability, and error handling, such as retries and dead letter queue handling.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Code examples for sending and receiving messages with Spring Cloud Stream</h1>
                </header>
            
            <article>
                
<p>To better understand how all this fits together, let's look at some source code examples.</p>
<p>Let's assume that we have a simple message class such as the following (constructors, getters, and setters have been left out for improved readability):</p>
<pre><span>public class My</span>Message {<br>  <span>private </span>String <span>attribute1 </span>= <span>null</span>;<br>  <span>private </span>String <span>attribute2</span><span> </span>= <span>null</span>;</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>Spring Cloud Stream comes with default input and output channels, <span><kbd>Sink</kbd> and </span><kbd>Source</kbd>, so we don't need to create our own to get started. To publish a message, we can use the following source code:</p>
<pre><span>import </span>org.springframework.cloud.stream.messaging.Source;<br><br><span>@EnableBinding</span>(Source.<span>class</span>)<br><span>public class </span>MyPublisher {<br><br> <span>@Autowired </span><span>private </span>Source <span>mysource</span>;<br><br> <span>public </span>String processMessage(MyMessage message) {<span><br></span><span>   </span><span>mysource</span>.output().send(MessageBuilder.<span>withPayload</span>(message).build());</pre>
<p>To receive messages, we can use the following code:</p>
<pre><span>import </span>org.springframework.cloud.stream.messaging.Sink;<br><span><br>@EnableBinding</span>(Sink.<span>class</span>)<br><span>public class My</span>Subscriber {<br><br> <span>@StreamListener</span>(target = Sink.<span>INPUT</span>)<br> <span>public void </span>receive(MyMessage message) {<br> <span>LOG</span>.info(<span>"Received: {}"</span>,message);</pre>
<p>To bind to RabbitMQ, we will use a dedicated starter dependency in the build file, <kbd>build.gradle</kbd>:</p>
<pre>implementation(<span>'org.springframework.cloud:spring-cloud-starter-stream-rabbit'</span>)</pre>
<p>For the subscriber to receive messages from the publisher, we need to configure the input<span> and output channel </span>to use the same destination. If we use YAML to describe our configuration, it might look like the following for the publisher:</p>
<pre><span>spring.cloud.stream</span>:<br>  <span>default.contentType</span>: application/json<br>  <span>bindings.output.destination</span>: mydestination</pre>
<p>The configuration for the subscriber is as follows:</p>
<pre><span>spring.cloud.stream</span>:<br>  <span>default.contentType</span>: application/json<br>  <span>bindings.input.</span><span>destination</span>: mydestination</pre>
<div class="packt_infobox">We use <kbd>default.contentType</kbd> to specify that we prefer messages to be serialized in JSON format. </div>
<p>Now that we understand the various Spring APIs, let's understand a concept relatively newer, Docker, in the next section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have been introduced to Spring Boot and complementary open source tools that can be used to build cooperating microservices.</p>
<p>Spring Boot is used to simplify the development of Spring-based, production-ready applications. It is strongly opinionated in terms of how to set up both core modules from the Spring Framework and third-party products.</p>
<p class="mce-root">Spring WebFlux is a new module in the Spring family and is used to develop reactive, that is, non-blocking, REST services. It runs on both lightweight web servers such as Netty and on any Servlet 3.1+ compatible web server. It also supports the programming model from the older Spring MVC module;<span>&nbsp;</span>it is easy to move REST services written for Spring MVC to Spring WebFlux without fully rewriting the code.</p>
<p class="mce-root">SpringFox can be used to create Swagger and OpenAPI-based documentation regarding REST services. It creates the documentation on the fly at runtime by inspecting the annotations for the REST services&nbsp;<span>â</span> both the Spring annotations and some Swagger specific annotations&nbsp;<span>â</span> if used.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root">Spring Data provides an elegant abstraction for accessing and manipulating persistent data using entities and repositories. The programming model is similar, but isn't portable between different types of database, for example, relational, document, key-value, and graph databases.</p>
<p class="mce-root">Spring Cloud Stream provides a streaming abstraction over messaging, based on the publish and subscribe integration pattern. Spring Cloud Stream comes with out of the box support for Apache Kafka and RabbitMQ but can be extended to support other messaging brokers using custom binders.</p>
<p class="mce-root">Docker makes the concept of containers as a lightweight alternative to virtual machines easy to use. Based on Linux Namespaces and Control Groups, containers provide isolation similar to what traditional virtual machines provide, but with a significantly lower overhead in terms of CPU and memory usage. Docker is a very good tool for development and testing but in most cases requires a container orchestrator such as Kubernetes to be used in a production environment.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have been introduced to Spring Boot and complementary open source tools that can be used to build cooperating microservices.</p>
<p>Spring Boot is used to simplify the development of Spring-based, production-ready applications. It is strongly opinionated in terms of how to set up both core modules from the Spring Framework and third-party products.</p>
<p class="mce-root">Spring WebFlux is a new module in the Spring family and is used to develop reactive, that is, non-blocking, REST services. It runs on both lightweight web servers such as Netty and on any Servlet 3.1+ compatible web server. It also supports the programming model from the older Spring MVC module;<span>&nbsp;</span>it is easy to move REST services written for Spring MVC to Spring WebFlux without fully rewriting the code.</p>
<p class="mce-root">SpringFox can be used to create Swagger and OpenAPI-based documentation regarding REST services. It creates the documentation on the fly at runtime by inspecting the annotations for the REST services <span>â</span> both the Spring annotations and some Swagger specific annotations <span>â</span> if used.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root">Spring Data provides an elegant abstraction for accessing and manipulating persistent data using entities and repositories. The programming model is similar, but isn't portable between different types of database, for example, relational, document, key-value, and graph databases.</p>
<p class="mce-root">Spring Cloud Stream provides a streaming abstraction over messaging, based on the publish and subscribe integration pattern. Spring Cloud Stream comes with out of the box support for Apache Kafka and RabbitMQ but can be extended to support other messaging brokers using custom binders.</p>
<p class="mce-root">Docker makes the concept of containers as a lightweight alternative to virtual machines easy to use. Based on Linux Namespaces and Control Groups, containers provide isolation similar to what traditional virtual machines provide, but with a significantly lower overhead in terms of CPU and memory usage. Docker is a very good tool for development and testing but in most cases requires a container orchestrator such as Kubernetes to be used in a production environment.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li class="mce-root">What is the purpose of the <kbd>@SpringBootApplication</kbd> annotation?</li>
<li>What are the main differences between the older Spring component for developing REST services, Spring Web MVC, and the new Spring WebFlux?</li>
<li>How does SpringFox help a developer document REST APIs?</li>
<li>What is the function of a repository in Spring Data and what is the simplest possible implementation of a repository?</li>
<li>What is the purpose of a binder in Spring Cloud Stream?</li>
<li>What is the purpose of Docker Compose?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All of the commands that are described in this book are run on a MacBook Pro using macOS Mojave but should be straightforward to modify so that they can be run on another platform such as Linux or Windows.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Tool installation</h1>
                </header>
            
            <article>
                
<p>To be able to execute the commands that are used in this chapter, you need to have the following tools installed on your computer:</p>
<ul>
<li><strong>Git:&nbsp;</strong>Can be downloaded and installed from&nbsp;<a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a>.</li>
<li><strong>Java:&nbsp;</strong><span>Can be downloaded and installed from&nbsp;<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html">https://www.oracle.com/technetwork/java/javase/downloads/index.html</a>.<br></span></li>
<li><kbd>curl</kbd>: This command-line tool for testing HTTP-based APIs c<span>an be downloaded and installed from&nbsp;<a href="https://curl.haxx.se/download.html">https://curl.haxx.se/download.html</a>.</span></li>
<li><kbd>jq</kbd>: This command-line JSON processor c<span>an be downloaded and installed from&nbsp;<a href="https://stedolan.github.io/jq/download/">https://stedolan.github.io/jq/download/</a>.</span></li>
<li><strong>Spring Boot CLI</strong>: This&nbsp;<span>command-line tool for Spring Boot applications can be downloaded and installed from&nbsp;<a href="https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started-installing-spring-boot.html#getting-started-installing-the-cli">https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started-installing-spring-boot.html#getting-started-installing-the-cli</a>.<br></span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Tool installation</h1>
                </header>
            
            <article>
                
<p>To be able to execute the commands that are used in this chapter, you need to have the following tools installed on your computer:</p>
<ul>
<li><strong>Git: </strong>Can be downloaded and installed from <a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a>.</li>
<li><strong>Java: </strong><span>Can be downloaded and installed from <a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html">https://www.oracle.com/technetwork/java/javase/downloads/index.html</a>.<br></span></li>
<li><kbd>curl</kbd>: This command-line tool for testing HTTP-based APIs c<span>an be downloaded and installed from <a href="https://curl.haxx.se/download.html">https://curl.haxx.se/download.html</a>.</span></li>
<li><kbd>jq</kbd>: This command-line JSON processor c<span>an be downloaded and installed from <a href="https://stedolan.github.io/jq/download/">https://stedolan.github.io/jq/download/</a>.</span></li>
<li><strong>Spring Boot CLI</strong>: This <span>command-line tool for Spring Boot applications can be downloaded and installed from <a href="https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started-installing-spring-boot.html#getting-started-installing-the-cli">https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started-installing-spring-boot.html#getting-started-installing-the-cli</a>.<br></span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Homebrew to install Java, curl, jq, and the Spring Boot CLI</h1>
                </header>
            
            <article>
                
<p>On a macOS, <kbd>curl</kbd> is already preinstalled and <kbd>git</kbd> was installed as part of the installation of<span>&nbsp;Homebrew. T</span>he remaining tools can be installed on a macOS using Homebrew with the following command:</p>
<pre><strong>brew tap pivotal/tap <span>&amp;&amp; \<br></span><span>brew cask install java &amp;&amp; \</span><span><br></span><span>brew install jq </span><span>&amp;&amp; \</span><span><br></span>brew install springboot</strong></pre>
<p>The installation of these tools can be verified by the following commands:</p>
<pre class="p1"><strong><span class="s1">git --version<br></span><span class="s1">java -version<br></span><span class="s1">curl --version<br></span><span class="s1">jq --version<br></span><span class="s1">spring --version</span><span class="s1"> </span></strong></pre>
<p>These commands will return something like the following (some extra irrelevant output was removed):</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf7decb8-42ed-41c9-a9fe-6143e7d1c0e5.png" style="width:20.58em;height:8.33em;" width="695" height="280" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf7decb8-42ed-41c9-a9fe-6143e7d1c0e5.png"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Homebrew to install Java, curl, jq, and the Spring Boot CLI</h1>
                </header>
            
            <article>
                
<p>On a macOS, <kbd>curl</kbd> is already preinstalled and <kbd>git</kbd> was installed as part of the installation of<span> Homebrew. T</span>he remaining tools can be installed on a macOS using Homebrew with the following command:</p>
<pre><strong>brew tap pivotal/tap <span>&amp;&amp; \<br></span><span>brew cask install java &amp;&amp; \</span><span><br></span><span>brew install jq </span><span>&amp;&amp; \</span><span><br></span>brew install springboot</strong></pre>
<p>The installation of these tools can be verified by the following commands:</p>
<pre class="p1"><strong><span class="s1">git --version<br></span><span class="s1">java -version<br></span><span class="s1">curl --version<br></span><span class="s1">jq --version<br></span><span class="s1">spring --version</span><span class="s1"> </span></strong></pre>
<p>These commands will return something like the following (some extra irrelevant output was removed):</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf7decb8-42ed-41c9-a9fe-6143e7d1c0e5.png" style="width:20.58em;height:8.33em;" width="695" height="280" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf7decb8-42ed-41c9-a9fe-6143e7d1c0e5.png"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using an IDE</h1>
                </header>
            
            <article>
                
<p>I recommend that you work with your Java code using an IDE that supports the development of Spring Boot applications such as <span>Spring Tool Suite or IntelliJ</span> <span>IDEA</span> <span>Ultimate Edition</span><span>. See the <em>Testing APIs manually</em> section to learn how to use the Spring Boot Dashboard. However, you don't need an IDE to be able to follow the instructions in this book.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing the microservice landscape</h1>
                </header>
            
            <article>
                
<p>In <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>,&nbsp;<em>Introduction to Microservices</em>, we were briefly introduced to the microservice-based system landscape that we will use throughout&nbsp;this book:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf74f5f6-c0f7-471c-8eae-a2566ecee996.png" style="width:35.75em;height:12.67em;" width="1184" height="418" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf74f5f6-c0f7-471c-8eae-a2566ecee996.png"></p>
<p>It consists of three core microservices, the <strong>Product</strong>, <strong>Review</strong>, and <strong>Recommendation</strong> services, all of which deal with one type of resource, and a composite microservice called the <strong>Product Composite</strong> service, which aggregates information from the three core services.&nbsp;</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing the microservice landscape</h1>
                </header>
            
            <article>
                
<p>In <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>,&nbsp;<em>Introduction to Microservices</em>, we were briefly introduced to the microservice-based system landscape that we will use throughout this book:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf74f5f6-c0f7-471c-8eae-a2566ecee996.png" style="width:35.75em;height:12.67em;" width="1184" height="418" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf74f5f6-c0f7-471c-8eae-a2566ecee996.png"></p>
<p>It consists of three core microservices, the <strong>Product</strong>, <strong>Review</strong>, and <strong>Recommendation</strong> services, all of which deal with one type of resource, and a composite microservice called the <strong>Product Composite</strong> service, which aggregates information from the three core services. </p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Product service</h1>
                </header>
            
            <article>
                
<p>The <kbd>product</kbd> service manages product information and describes each product with the following attributes:</p>
<ul>
<li><span>Product ID</span></li>
<li><span>Name</span></li>
<li><span>Weight</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Review service</h1>
                </header>
            
            <article>
                
<p>The <kbd>review</kbd> service manages product reviews&nbsp;<span>and stores the following information about each review:</span></p>
<ul>
<li><span>Product ID</span></li>
<li><span>Review ID</span></li>
<li><span>Author</span></li>
<li><span>Subject</span></li>
<li><span>Content</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Recommendation service</h1>
                </header>
            
            <article>
                
<p>The <kbd>recommendation</kbd> service manages product recommendations&nbsp;<span>and stores the following information about each recommendation:</span></p>
<ul>
<li><span>Product ID</span></li>
<li><span>Recommendation ID</span></li>
<li><span>Author</span></li>
<li><span>Rate</span></li>
<li><span>Content</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Product composite service</h1>
                </header>
            
            <article>
                
<p><span>The product composite service aggregates information from the three core services and presents information about a product as follows:</span></p>
<ul>
<li>Product information, as described in the <kbd>product</kbd> service</li>
<li>A list of product reviews for the specified product, as described in the <kbd>review</kbd> service</li>
<li>A list of product recommendations for the specified product, as described in the <kbd>recommendation</kbd> service</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Infrastructure-related information</h1>
                </header>
            
            <article>
                
<p><span>Once we start to run our microservices as containers that are managed by the infrastructure (first Docker and later on Kubernetes), it will be of interest to track which container actually responded to our requests. To simplify this tracking, we have also added a <kbd>serviceAddress</kbd>&nbsp;attribute to all our responses,&nbsp;formatted as&nbsp;<kbd>hostname/ip-address:port</kbd>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Infrastructure-related information</h1>
                </header>
            
            <article>
                
<p><span>Once we start to run our microservices as containers that are managed by the infrastructure (first Docker and later on Kubernetes), it will be of interest to track which container actually responded to our requests. To simplify this tracking, we have also added a <kbd>serviceAddress</kbd> attribute to all our responses, formatted as <kbd>hostname/ip-address:port</kbd>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Temporarily replacing a discovery service</h1>
                </header>
            
            <article>
                
<p><span>Since, at this stage, we don't have any service discovery mechanism in place, we will use hardcoded port numbers for each microservice. </span><span>We will use the following ports:</span></p>
<ul>
<li>Product composite service: <kbd>7000</kbd></li>
<li>Product service: <kbd>7001</kbd></li>
<li>Review service: <kbd>7002</kbd></li>
<li>Recommendation service: <kbd>7003</kbd></li>
</ul>
<div class="packt_infobox">We will get rid of the hardcoded ports later when we start using Docker and Kubernetes!</div>
<p>In this section, we have been introduced to the microservices we are going to create and the information that they will handle. In the next section, we will use Spring Initializr to create skeleton code for the microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Generating skeleton microservices</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now it's time to see how we can create projects for our microservices. The final result for this topic can be found in the <kbd>$BOOK_HOME/Chapter03/1-spring-init</kbd><strong>&nbsp;</strong>folder. To simplify setting up the projects, we will use Spring Initializr to generate a skeleton project for each microservice. A skeleton project contains the necessary files for building the project, along with an empty <kbd>main</kbd> class and test class for the microservice. After that<span>, we will see how we can build all our microservices with one command using </span><span>multi-project builds</span> <span>in the build tool that we will use, Gradle</span><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Spring Initializr to generate skeleton code</h1>
                </header>
            
            <article>
                
<p>To get started with developing our microservices, we will use a tool called <strong>Spring Initializr</strong> to generate skeleton code for us. It can either be invoked from a web browser using the <a href="https://start.spring.io/">https://start.spring.io/</a> URL or by a command-line tool, <kbd>spring init</kbd>. To make it easier to reproduce the creation of the microservices, we will use the <span>command-line tool.</span></p>
<p>For each microservice, we will create a Spring Boot project which does the following:</p>
<ul>
<li>Uses Gradle as a build tool</li>
<li>Generates code for Java 8 </li>
<li>Packages the project as a fat JAR file</li>
<li>Brings in dependencies for the <kbd>Actuator</kbd> and <kbd>WebFlux</kbd> Spring modules</li>
<li>Is based on Spring Boot v2.1.0 RC1 (which depends on Spring Framework v5.1.1)</li>
</ul>
<div class="packt_infobox">Spring Boot Actuator enables a number of valuable endpoints for management and monitoring. We will see them in action later on. Spring WebFlux will be used here to create our RESTful APIs.</div>
<p>To create skeleton code for our microservices, we need to run the following command for <kbd>product-service</kbd>:</p>
<pre>spring init \<br>--boot-version=2.1.0.RC1 \<br>--build=gradle \<br>--java-version=1.8 \<br>--packaging=jar \<br>--name=product-service \<br>--package-name=se.magnus.microservices.core.product \<br>--groupId=se.magnus.microservices.core.product \<br>--dependencies=actuator,webflux \<br>--version=1.0.0-SNAPSHOT \<br>product-service</pre>
<div class="packt_infobox"><span>If you want to learn more about the <kbd>spring init</kbd> CLI, you can run the </span><kbd>spring help init</kbd><span> command. To see what dependencies you can add, run the </span><kbd>spring init --list</kbd> command.</div>
<p>If you want to create the four projects on your own instead of using the source code in this book's GitHub repository, try out <kbd>$BOOK_HOME/Chapter03/1-spring-init<span><span>/create-projects.bash</span></span></kbd>, as follows:</p>
<div>
<div>
<pre><strong>mkdir </strong><strong>some-temp-folder<br></strong><strong>cd some-temp-folder</strong><br><strong>$BOOK_HOME/Chapter03/1-spring-init/create-projects.bash</strong></pre>
<p>After creating our four projects using <kbd><span>create-projects.bash</span></kbd>, we will have the following file structure: </p>
<pre class="p1"><span class="s1">microservices/<br></span><span class="s1">âââ product-composite-service<br></span><span class="s1">âââ product-service<br></span><span class="s1">âââ recommendation-service<br></span><span class="s1">âââ review-service</span></pre></div>
<p>For each project, we can list the created files. Let's do this for the <kbd>product-service</kbd> project:</p>
</div>
<pre><strong>find microservices/product-service -type f</strong></pre>
<p class="CDPAlignLeft CDPAlign">We will receive the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/21a0ab5c-8c4c-4898-9cab-598833bd976f.png" width="1758" height="444" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/21a0ab5c-8c4c-4898-9cab-598833bd976f.png"></p>
<p>Spring Initializr created a number of files for Gradle, including a <kbd>.gitignore</kbd> file and three Spring Boot files:</p>
<ul>
<li class="mce-root"><kbd>ProductServiceApplication.java</kbd><span>, our main application class</span></li>
<li class="mce-root"><kbd>application.properties</kbd><span>, an empty property file</span></li>
<li class="mce-root"><kbd>ProductServiceApplicationTests.java</kbd><span>, a test class that's been configured to run tests on our Spring Boot application using JUnit</span></li>
</ul>
<p>The <kbd>main</kbd> application class, <span><kbd>ProductServiceApplication.java</kbd>,&nbsp;</span>looks as we'd expect based on the previous chapter:</p>
<div>
<pre><span>package </span>se.magnus.microservices.core.product;<br><br><span>@SpringBootApplication<br></span><span>public class </span>ProductServiceApplication {<br>   <span>public static void </span>main(String[] args) {<br>      SpringApplication.<span>run</span>(ProductServiceApplication.<span>class</span>, args);<br>   }<br>}</pre></div>
<div>
<div>
<p>The test class looks as follows:</p>
</div>
</div>
<div>
<pre><span>package </span>se.magnus.microservices.core.product;<br><br><span>@RunWith</span>(SpringRunner.<span>class</span>)<br><span>@SpringBootTest<br></span><span>public class </span>ProductServiceApplicationTests {<br>   <span>@Test<br></span><span>   </span><span>public void </span>contextLoads() {<br>   }<br>}</pre></div>
<div>
<p>The <kbd>@RunWith(SpringRunner.class)</kbd> and <kbd>@SpringBootTest</kbd> annotations will initialize our application in the same way as <span><kbd>@SpringBootApplication</kbd> does </span>when running the application; that is, the Spring application context will be set up <span>before the tests are executed </span>using component scanning and auto-configuration, as described in the previous chapter.<span><br></span></p>
<div>
<p>Let's also look at the most important Gradle file, <kbd>build.gradle</kbd>. The content of this file describes how to build the project, for example, compile, test, and package the source code. The Gradle file starts by setting up the conditions for the rest of the build file by declaring the <kbd>buildscript</kbd> element and listing what plugins to apply:</p>
</div>
</div>
<div>
<pre>buildscript {<br>  ext {<br>    springBootVersion = <span>'</span><span class="s1">2.1.0.RC1</span><span>'<br></span><span>  </span>}<br>  repositories {<br>    mavenCentral()<br>    maven { url <span>"https://repo.spring.io/snapshot" </span>}<br>    maven { url <span>"https://repo.spring.io/milestone" </span>}<br>  }<br>  dependencies {<br>    classpath(<span>"org.springframework.boot:spring-boot-gradle-<br>    plugin:</span>${springBootVersion}<span>"</span>)<br>  }<br>}<br><br>apply <span>plugin</span>: <span>'java'<br></span>apply <span>plugin</span>: <span>'eclipse'<br></span>apply <span>plugin</span>: <span>'org.springframework.boot'<br></span>apply <span>plugin</span>: <span>'io.spring.dependency-management'</span></pre></div>
<p>Let's explain the preceding source code in more detail:</p>
<ul>
<li>The Spring Boot version is set to what we specified when we ran the <kbd>spring init</kbd> command, <kbd>2.1.0.RC1</kbd>.</li>
<li>A number of Gradle plugins are declared. The most important ones are the <kbd>org.springframework.boot</kbd> and <kbd>io.spring.dependency-management</kbd> plugins, which together ensure that Gradle will build a fat JAR file and that we don't need to specify any explicit version numbers on our Spring Boot starter dependencies. Instead, they are implied by the <kbd>springBootVersion</kbd> property.</li>
<li>Plugins are fetched from the central Maven repository and from Spring's snapshot and milestone repositories since we have specified a release candidate of Spring Boot, v2.1.0 RC1, and not a version that's been released and is available in the central Maven repository.</li>
</ul>
<p>In the rest of the build file, we basically declare a group name and version for our project, Java version, and its dependencies:</p>
<div>
<pre>group = <span>'se.magnus.microservices.core.product'<br></span>version = <span>'1.0.0-SNAPSHOT'<br></span>sourceCompatibility = <span>1.8<br></span><span><br></span>repositories {<br>  mavenCentral()<br>  maven { url <span>"https://repo.spring.io/snapshot" </span>}<br>  maven { url <span>"https://repo.spring.io/milestone" </span>}<br>}<br><br>dependencies {<br>  implementation(<span>'org.springframework.boot:spring-boot-starter-<br>  actuator'</span>)<br>  implementation(<span>'org.springframework.boot:spring-boot-starter-<br>  webflux'</span>)<br>  testImplementation(<span>'org.springframework.boot:spring-boot-starter-<br>  test'</span>)<br>  testImplementation(<span>'io.projectreactor:reactor-test'</span>)<br>}</pre></div>
<p><span>Let's explain the preceding source code in more detail as follows:</span></p>
<ul>
<li>Dependencies are, as with the preceding plugins, fetched from the central Maven repository and from Spring's snapshot and milestone repositories.</li>
<li>Dependencies are set up as specified in the <kbd>Actuator</kbd> and <kbd>WebFlux</kbd> modules, along with a couple of useful test dependencies.</li>
</ul>
<div>
<div>
<p><span>We can build each microservice separately with the following command:</span></p>
<pre><strong><span>cd microservices/product-composite-service; ./gradlew build; cd -; \<br></span><span>cd </span><span>microservices/product-service</span><span>;           ./gradlew build; cd -; \<br></span><span>cd </span><span>microservices/recommendation-service</span><span>;    ./gradlew build; cd -; \<br></span></strong><span><strong>cd microservices/review-service;            ./gradlew build; cd -;</strong> </span></pre></div>
</div>
<div class="packt_infobox">Note how we use the <kbd>gradlew</kbd> executables that are created by Spring Initializr; that is, we don't need to have Gradle installed!<br>
<br>
The first time we run a command with <kbd>gradlew</kbd>, it will download Gradle automatically. The Gradle version that's used is determined by the <kbd>distributionUrl</kbd> property in the <kbd>gradle/wrapper/gradle-wrapper.properties</kbd> file.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up multi-project builds in Gradle</h1>
                </header>
            
            <article>
                
<p>To make it a bit simpler to build all the microservices with one command, we can set up a multi-project build in Gradle. The steps are as follows:</p>
<ol>
<li>First, we create the <kbd>settings.gradle</kbd> file, which describes what projects that Gradle should build:</li>
</ol>
<pre style="padding-left: 60px"><strong>cat &lt;&lt;EOF &gt; settings.gradle</strong><br><strong>include ':microservices:product-service'</strong><br><strong>include ':microservices:review-service'</strong><br><strong>include ':microservices:recommendation-service'</strong><br><strong>include ':microservices:product-composite-service'</strong><br><strong>EOF</strong></pre>
<ol start="2">
<li>Next, we copy the Gradle executable files that were generated from one of the projects so that we can reuse them for the multi-project builds:</li>
</ol>
<pre style="padding-left: 60px"><strong>cp -r microservices/product-service/gradle .</strong><br><strong>cp microservices/product-service/gradlew .</strong><br><strong>cp microservices/product-service/gradlew.bat .</strong><br><strong>cp microservices/product-service/.gitignore .</strong></pre>
<ol start="3">
<li>We no longer need the generated Gradle executable files in each project, so we can remove them with the following commands:</li>
</ol>
<pre style="padding-left: 60px" class="p1"><strong><span class="s1">find microservices -depth -name "gradle" -exec rm -rfv "{}" \; <br></span></strong><span class="s1"><strong>find microservices -depth -name "gradlew*" -exec rm -fv "{}" \;</strong> </span></pre>
<p style="padding-left: 60px"><span>The result should be similar to the code you can find in</span> <span>the</span> <span>folder </span><kbd>$BOOK_HOME/Chapter03/1-spring-init</kbd><strong>.</strong></p>
<ol start="4">
<li>Now, we can build all the microservices with one command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>./gradlew build</span></strong></pre>
<p style="padding-left: 60px">If you haven't run the preceding commands, you can simply go to the book's source code and build it from there:</p>
<pre style="padding-left: 60px"><strong><span class="s1">cd $BOOK_HOME/Chapter03/1-spring-init<br><br></span><span>./gradlew build</span></strong></pre>
<ol start="5">
<li class="CDPAlignLeft CDPAlign">This should result in the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/efd0c892-cbca-4962-8248-b17da6fe8cd0.png" style="width:28.75em;height:15.83em;" width="598" height="328" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/efd0c892-cbca-4962-8248-b17da6fe8cd0.png"></p>
<p>With skeleton projects for the microservices created using Spring Initializr and successfully built using Gradle, we are ready to add some code to the microservices in the next section.</p>
<div class="packt_infobox">From a DevOps perspective, a multi-project setup might not be preferred. Instead, setting up a separate build pipeline for each microservice project would probably be preferred. However, for the purposes of this book, we will use the <span>multi-project setup to make it easier to build and deploy the whole system landscape with a single command.</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding RESTful APIs</h1>
                </header>
            
            <article>
                
<p>Now that we have projects set up for our microservices, let's add some RESTful APIs to our three core microservices!</p>
<p><span>The final result of this and the remaining topics in this chapter can be found in the</span> <kbd>$BOOK_HOME/Chapter03/2-basic-rest-services</kbd><strong>&nbsp;</strong>folder.</p>
<p>First, we will add two projects (<kbd>api</kbd> and <kbd>util</kbd>) that will contain code that is shared by the microservice projects, and then we will implement the RESTful APIs.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding an API and a util project</h1>
                </header>
            
            <article>
                
<p>To add an API, we need to do the following:</p>
<ol>
<li>First, we will set up a separate Gradle project where we can place our API definitions. We will use Java interfaces in order to describe our RESTful APIs and model classes to describe the data that the API uses in its requests and responses. Describing a RESTful API in a Java interface instead of directly in the Java class is, to me, a good way of separating the API definition from its implementation. We will further extend this pattern later in this book when we add more API information in the Java interfaces to be exposed in the Swagger/OpenAPI definition. See <a href="ba24a656-10a1-4a3e-879e-6589621ef125.xhtml">Chapter 5</a>, <em>Adding an API Description Using OpenAPI/Swagger</em>, for more information.</li>
</ol>
<div class="packt_infobox">Describing RESTful APIs in Java interfaces wasn't fully supported until Spring Framework v5.1.0. See <a href="https://jira.spring.io/browse/SPR-11055">https://jira.spring.io/browse/SPR-11055</a> for details.<br>
It is debatable whether it is good practice to store API definitions for a group of microservices in a common API module. To me, it is a good choice for microservices that are part of the same delivery organization, that is, whose releases are governed by one and the same organization <span>(compare to a <em>Bounded Context</em> in <em>Domain-Driven Design</em>, where our microservices are placed in one and the same bounded context).</span></div>
<ol start="2">
<li>Next, we will create a <kbd>util</kbd> project that can hold some helper classes that are shared by our microservices, for example, for handling errors in a uniform way.</li>
</ol>
<div class="packt_infobox">Again, from a DevOps perspective, it would be preferable to build all the projects in their own build pipeline and have version-controlled dependencies for the <kbd>api</kbd> and <kbd>util</kbd> projects in the microservice projects; that is, so that each microservice can choose what versions of the <kbd>api</kbd> and <kbd>util</kbd> projects to use. But to keep the build and deployment steps simple in the context of this book, we will make the <kbd>api</kbd> and <kbd>util</kbd> projects part of the multi-project build.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The api project</h1>
                </header>
            
            <article>
                
<p>The <kbd>api</kbd> project will be packaged as a library; that is, it won't have its own <kbd>main</kbd> application class. Unfortunately, Spring Initializr <span>doesn't </span>support the creation of library projects. Instead, a library project has to be created manually from scratch. The source code for the API project is available at <kbd>$BOOK_HOME/Chapter03/2-basic-rest-services/api</kbd><strong>.</strong></p>
<p>The structure of a library project is the same as for an application project, except that we no longer have the <kbd>main</kbd> application class, as well as some minor differences in the <kbd>build.gradle</kbd> file. The Gradle <kbd>org.springframework.boot</kbd> and <kbd>io.spring.dependency-management</kbd> plugins are replaced with a <kbd>dependencyManagement</kbd> section:</p>
<pre>plugins {<br>   id <span>"io.spring.dependency-management" </span>version <span>"1.0.5.RELEASE"<br></span>}<br><br>dependencyManagement {<br>  imports { mavenBom(<span>"org.springframework.boot:spring-boot-<br>  dependencies:</span>${springBootVersion}<span>"</span>) }<br>}</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p>This allows us to retain Spring Boot <span>dependency management while we are replacing the </span>construction of a fat JAR in the build step with the creation of normal JAR files; that is, they only contain the library project's own class and property files.</p>
<p>The Java files in the <kbd>api</kbd> project for our three core microservices are as follows:</p>
<pre><span>$BOOK_HOME/Chapter03/2-basic-rest-services/</span>api/src/main/java/se/magnus/api/core<br>âââ product<br>â   âââ Product.java<br>â   âââ ProductService.java<br>âââ recommendation<br>â   âââ Recommendation.java<br>â   âââ RecommendationService.java<br>âââ review<br>    âââ Review.java<br>    âââ ReviewService.java</pre>
<p>The structure of the Java classes looks very similar for the three <span>core microservices, so we will only go through the source code for</span> the <kbd>product</kbd> service<span>.</span></p>
<p>First, we will look at the <kbd>ProductService.java</kbd> Java interface, as shown in the following code:</p>
<pre><span>package </span>se.magnus.api.core.product;<br><span><br>public interface </span>ProductService {<br><span>    </span><span>@GetMapping</span>(<br>        value    = <span>"/product/{productId}"</span>,<br>        produces = <span>"application/json"</span>)<br>     Product getProduct(<span>@PathVariable </span><span>int </span>productId);<br>}</pre>
<p>Let's explain the preceding source code in more detail:</p>
<ul>
<li><span>The <kbd>product</kbd> service only exposes one API method, </span><kbd>getProduct()</kbd><span> (we will extend the API later in this book).</span></li>
<li><span>To map the method to an HTTP <kbd>GET</kbd> request, we use the </span><span><kbd>@GetMapping</kbd> Spring annotation, where we specify what URL path the method will be mapped to (</span><kbd><span>/product/{productId}</span></kbd><span><span>) and what format the response will be, in this case, JSON.</span></span></li>
<li><span>The <kbd>{productId}</kbd> part of the path maps to a <kbd>path</kbd> variable named <kbd>productId</kbd>.</span></li>
<li><span>The</span> <span><kbd>productId</kbd> method parameter is annotated with </span><span><kbd>@PathVariable</kbd>, which will map the value that's passed in the HTTP request to the parameter. For example, an HTTP <kbd>GET</kbd> request to <kbd>/product/123</kbd> will result in the <kbd>getProduct()</kbd> method being called with the</span> <span><kbd>productId</kbd> parameter set to</span> <kbd>123</kbd><span>.</span></li>
</ul>
<p>The method returns a <kbd>Product</kbd> object, a plain POJO-based model class with the member variables corresponding to attributes for <kbd>Product</kbd>, as described at the start of this chapter. <kbd><span>Product.java</span></kbd> looks as follows (with constructors and getter methods excluded):</p>
<pre><span>public class </span>Product {<br> <span>private final int </span><span>productId</span>;<br> <span>private final </span>String <span>name</span>;<br> <span>private final int </span><span>weight</span>;<br> <span>private final </span>String <span>serviceAddress</span>;<br>}</pre>
<div class="packt_infobox"><span>This type of POJO class is also known as a <strong>Data Transfer Object</strong> (<strong>DTO</strong>) as it is used to transfer data between the API implementation and the caller of the API. When we get to <a href="6c495e97-f473-4cb1-a404-11fd938f5478.xhtml">Chapter 6</a>, <em>Adding Persistence</em>, we will look at another type of POJO that can be used to describe how data is stored in the databases, also known as entity objects.</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The util project</h1>
                </header>
            
            <article>
                
<p>The <kbd>util</kbd> project will be packaged as a library in the same way as the <kbd>api</kbd> project. The source code for the <kbd>util</kbd> project is available at <kbd>$BOOK_HOME/Chapter03/2-basic-rest-services/util</kbd>. The project contains the following Java files:</p>
<ul>
<li>The <kbd>InvalidInputException</kbd> and <kbd>NotFoundException</kbd> exception classes</li>
<li>The <kbd>GlobalControllerExceptionHandler</kbd>, <kbd>HttpErrorInfo</kbd>, and <kbd>ServiceUtil</kbd> utility classes</li>
</ul>
<p>Except for the code in <kbd>ServiceUtil.java</kbd>, these classes are reusable utility classes that we can use to map Java exceptions to proper HTTP status codes, as described in the <em>Adding error handling</em> section. The main purpose of <span><kbd>ServiceUtil.java</kbd> is to find out the hostname, IP address, and port used by the microservice. The class exposes a method,</span> <kbd>getServiceAddress()</kbd>, that can be used by the microservices to find their <span>hostname, IP</span> address, and port.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Implementing our API</h1>
                </header>
            
            <article>
                
<p>Now we can start to implement our APIs in the core microservices!</p>
<p><span>The implementation looks very similar for the three </span><span>core microservices, so we will only go through the source code for the <kbd>product</kbd> service. You can find the other files in</span> <span><kbd>$BOOK_HOME/Chapter03/2-basic-rest-services/microservices</kbd>. Let's see how we go about this:</span></p>
<ol>
<li>We need to add the <kbd>api</kbd> and <kbd>util</kbd> projects as dependencies in our <kbd><span>build.</span>gradle</kbd> file, that is, <kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services/</span>microservices/product-service/build.gradle</kbd>:</li>
</ol>
<pre style="padding-left: 60px">dependencies {<br>   implementation project(<span>':api'</span>)<br>   implementation project(<span>':util'</span>)</pre>
<ol start="2">
<li>To enable Spring Boot's autoconfiguration feature to detect Spring beans in the <kbd>api</kbd> and <kbd>util</kbd> projects, we also need to add a <span><kbd>@ComponentScan</kbd> annotation to the <kbd>main</kbd> application class, which includes the packages of the <kbd>api</kbd> and <kbd>util</kbd> projects:</span></li>
</ol>
<pre style="padding-left: 60px"><span>@SpringBootApplication<br></span><span>@ComponentScan</span>(<span>"se.magnus"</span>)<br><span>public class </span>ProductServiceApplication {</pre>
<ol start="3">
<li>Next, we create our service implementation file, <kbd>ProductServiceImpl.java</kbd>, in order to implement the Java interface, <kbd>ProductService</kbd>, from the <kbd>api</kbd> project and annotate the class with <span><kbd>@RestController</kbd> so that Spring will call the methods in this class according to the mappings specified in the <kbd>Interface</kbd> class:</span></li>
</ol>
<pre style="padding-left: 60px"><span>package </span>se.magnus.microservices.core.product.services;<br><br><span>@RestController<br></span><span>public class </span>ProductServiceImpl <span>implements </span>ProductService {<br>}</pre>
<ol start="4">
<li>To be able to u<span>se the <kbd>ServiceUtil</kbd> class from the <kbd>util</kbd> project, we will inject it into the constructor, as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>private final </span>ServiceUtil <span>serviceUtil</span>;<br><br><span>@Autowired<br></span><span>public </span>ProductServiceImpl(ServiceUtil serviceUtil) {<br>    <span>this</span>.<span>serviceUtil </span>= serviceUtil;<br>}</pre>
<ol start="5">
<li>Now, we can implement the API by overriding the <kbd>getProduct()</kbd> method from the interface in the <kbd>api</kbd> project:</li>
</ol>
<pre style="padding-left: 60px"><span>@Override<br></span><span>public </span>Product getProduct(<span>int </span>productId) {<br> <span>return new </span>Product(productId, <span>"name-" </span>+ productId, <span>123</span>, <br><span> serviceUtil</span>.getServiceAddress());<br>}</pre>
<p style="padding-left: 60px">Since we aren't currently using a database, we simply return a hardcoded response based on the input of <kbd>productId</kbd>, along with the service address supplied by the <kbd>ServiceUtil</kbd> class.</p>
<p style="padding-left: 60px"><span>For the final result, including logging and error handling, see</span> <kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services</span>/microservices/product-service/src/main/java/se/magnus/microservices/core/product/services/ProductServiceImpl.java</kbd><span>.</span></p>
<ol start="6">
<li>Finally, we also need to set up some runtime properties <span>â&nbsp;</span>what port to use and the desired level of logging. This is added to the <kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services</span>/microservices/product-service/src/main/resources/application.yml</kbd> property file:</li>
</ol>
<pre style="padding-left: 60px"><span>server.port</span>: <span>7001<br></span><span><br></span><span>logging</span>:<br>  <span>level</span>:<br>    <span>root</span>: INFO<br>    <span>se.magnus.microservices</span>: DEBUG</pre>
<ol start="7">
<li>We can try out the <kbd>product</kbd> service on its own. Build and start the microservice with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd <span>$BOOK_HOME/Chapter03/2-basic-rest-services</span></strong><br><strong>./gradlew build</strong><br><strong>java -jar microservices/product-service/build/libs/*.jar &amp;</strong></pre>
<ol start="8">
<li>Wait until the following is printed in the Terminal:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img style="font-size: 1em;text-align: center;color: #333333;width:33.17em;height:10.92em;" src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/41540310-c226-4100-ae87-35de5d722d65.png" width="782" height="257" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/41540310-c226-4100-ae87-35de5d722d65.png"></p>
<ol start="9">
<li>Make a test call to the <kbd>product</kbd> service:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>curl http://localhost:7001/product/123</strong></pre>
<ol start="10">
<li>It should respond with something similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/99d66466-5aec-4506-b7ea-c12540ad6ce5.png" width="1065" height="96" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/99d66466-5aec-4506-b7ea-c12540ad6ce5.png"></p>
<ol start="11">
<li>Finally, stop the <kbd>product</kbd> service:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kill $(jobs -p)</strong></pre>
<p>We have now built, run, and tested our first single microservice. In the next section, we will implement the composite microservice that will use the three core microservices that we've created so far. </p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">API classes</h1>
                </header>
            
            <article>
                
<p>In this section, we will take a look at the classes that describes the API of the composite component. They can be found in&nbsp;<kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services/</span>api</kbd>. The following are the API classes:</p>
<pre><span>$BOOK_HOME/Chapter03/2-basic-rest-services/</span>api<br>âââ src/main/java/se/magnus/api/composite<br>    âââ product<br>        âââ ProductAggregate.java<br>        âââ ProductCompositeService.java<br>        âââ RecommendationSummary.java<br>        âââ ReviewSummary.java<br>        âââ ServiceAddresses.java</pre>
<p>The Java interface class,&nbsp;<kbd>ProductCompositeService.java</kbd>, follows the same pattern that's used by the core services and looks as follows:</p>
<pre><span>package </span>se.magnus.api.composite.product;<br><br><span>public interface </span>ProductCompositeService {<br><span>    </span><span>@GetMapping</span>(<br>        value    = <span>"/product-composite/{productId}"</span>,<br>        produces = <span>"application/json"</span>)<br>    ProductAggregate getProduct(<span>@PathVariable </span><span>int </span>productId);<br>}</pre>
<p>The model class,&nbsp;<kbd>ProductAggregate.java</kbd>, is a bit more complex than the core models since it contains fields for lists of recommendations and reviews:</p>
<pre><span>package </span>se.magnus.api.composite.product;<br><br><span>public class </span>ProductAggregate {<br>    <span>private final int </span><span>productId</span>;<br>    <span>private final </span>String <span>name</span>;<br>    <span>private final int </span><span>weight</span>;<br>    <span>private final </span>List&lt;RecommendationSummary&gt; <span>recommendations</span>;<br>    <span>private final </span>List&lt;ReviewSummary&gt; <span>reviews</span>;<br>    <span>private final </span>ServiceAddresses <span>serviceAddresses</span>;</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Properties</h1>
                </header>
            
            <article>
                
<p>To avoid hardcoding the address information for the core services into the source code of the composite microservice,&nbsp;<span>the latter uses a&nbsp;</span>property file where information on how to find the core services is stored. The property file can be found in&nbsp;<kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services</span>/microservices/product-composite-service/src/main/resources/application.yml</kbd> and looks as follows:</p>
<pre><span>server.port</span>: <span>7000<br></span><span><br></span><span>app</span>:<br>  <span>product-service</span>:<br>    <span>host</span>: localhost<br>    <span>port</span>: 7001<br>  <span>recommendation-service</span>:<br>    <span>host</span>: localhost<br>    <span>port</span>: 7002<br>  <span>review-service</span>:<br>    <span>host</span>: localhost<br>    <span>port</span>: 7003</pre>
<p><span>This configuration will, as already noted, be replaced by a service discovery mechanism later on in this book.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Properties</h1>
                </header>
            
            <article>
                
<p>To avoid hardcoding the address information for the core services into the source code of the composite microservice, <span>the latter uses a </span>property file where information on how to find the core services is stored. The property file can be found in <kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services</span>/microservices/product-composite-service/src/main/resources/application.yml</kbd> and looks as follows:</p>
<pre><span>server.port</span>: <span>7000<br></span><span><br></span><span>app</span>:<br>  <span>product-service</span>:<br>    <span>host</span>: localhost<br>    <span>port</span>: 7001<br>  <span>recommendation-service</span>:<br>    <span>host</span>: localhost<br>    <span>port</span>: 7002<br>  <span>review-service</span>:<br>    <span>host</span>: localhost<br>    <span>port</span>: 7003</pre>
<p><span>This configuration will, as already noted, be replaced by a service discovery mechanism later on in this book.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Integration component</h1>
                </header>
            
            <article>
                
<p>Let's look at the integration component, <kbd>ProductCompositeIntegration.java</kbd>. It is declared as a Spring Bean using the <span><kbd>@Component</kbd> annotation </span>and implements the three core services' API interfaces:</p>
<pre><span>package </span>se.magnus.microservices.composite.product.services;<br><br><span>@Component<br></span><span>public class </span>ProductCompositeIntegration <span>implements </span>ProductService, RecommendationService, ReviewService {</pre>
<p>The integration component uses a helper class <span>in</span> <span>Spring Framework</span>, <kbd>RestTemplate.java</kbd>, to perform the actual HTTP requests to the core microservices. Before we can inject it into the <span>integration component, we need to configure it. We do that in the <kbd>main</kbd> application class,</span> <kbd>ProductCompositeServiceApplication.java</kbd>, as follows:</p>
<pre><span>@Bean<br></span>RestTemplate restTemplate() {<br>   <span>return new </span>RestTemplate();<br>}</pre>
<p><kbd>RestTemplate</kbd> is highly configurable, but we leave it with its default values for now.</p>
<p>We can now inject <kbd>RestTemplate</kbd>, along with a JSON mapper that's used for error handling and the configuration values that we set up in the property file in the constructor of the integration component. Let's see how this is done:</p>
<ol>
<li>The configuration values we use to set up the URLs for the three core services are injected into the constructor as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>private final </span>RestTemplate <span>restTemplate</span>;<br><span>private final </span>ObjectMapper <span>mapper</span>;<br><br><span>private final </span>String <span>productServiceUrl</span>;<br><span>private final </span>String <span>recommendationServiceUrl</span>;<br><span>private final </span>String <span>reviewServiceUrl</span>;<br><br><span>@Autowired<br></span><span>public </span>ProductCompositeIntegration(<br>  RestTemplate restTemplate,<br>  ObjectMapper mapper,<br><br>  <span>@Value</span>(<span>"${app.product-service.host}"</span>) String productServiceHost,<br>  <span>@Value</span>(<span>"${app.product-service.port}"</span>) <span>int </span>productServicePort,<br><br>  <span>@Value</span>(<span>"${app.recommendation-service.host}"</span>) String <br>  recommendationServiceHost,<br>  <span>@Value</span>(<span>"${app.recommendation-service.port}"</span>) <span>int <br>  </span>recommendationServicePort,<br><br>  <span>@Value</span>(<span>"${app.review-service.host}"</span>) String reviewServiceHost,<br>  <span>@Value</span>(<span>"${app.review-service.port}"</span>) <span>int </span>reviewServicePort<br>)</pre>
<p style="padding-left: 60px">The body of the constructor builds the URLs based on the injected values, as follows:</p>
<pre style="padding-left: 60px">{<br>  <span>this</span>.<span>restTemplate </span>= restTemplate;<br>  <span>this</span>.<span>mapper </span>= mapper;<br><br>  <span>productServiceUrl </span>= <span>"http://" </span>+ productServiceHost + <span>":" </span>+ <br>  productServicePort + <span>"/product/"</span>;<br>  <span>recommendationServiceUrl </span>= <span>"http://" </span>+ recommendationServiceHost<br>  + <span>":" </span>+ recommendationServicePort + <span>"/recommendation?<br>  productId="</span>; <span>reviewServiceUrl </span>= <span>"http://" </span>+ reviewServiceHost + <br><span>  ":" </span>+ reviewServicePort + <span>"/review?productId="</span>;<br>}</pre>
<ol start="2">
<li>Finally, the integration component implements the API methods for the three core services by using <kbd>RestTemplate</kbd> to make the actual outgoing calls:</li>
</ol>
<pre style="padding-left: 60px"><span>public </span>Product getProduct(<span>int </span>productId) {<br> String url = <span>productServiceUrl </span>+ productId;<br> Product product = <span>restTemplate</span>.getForObject(url, Product.<span>class</span>);<br> <span>return </span>product;<br>}<br><br><span>public </span>List&lt;Recommendation&gt; getRecommendations(<span>int </span>productId) {<br>    String url = <span>recommendationServiceUrl </span>+ productId;<br>    List&lt;Recommendation&gt; recommendations = <br>    <span>restTemplate</span>.exchange(url, <span>GET</span>, <span>null</span>, <span>new <br>    </span>ParameterizedTypeReference&lt;List&lt;Recommendation&gt;&gt;() <br>    {}).getBody();<br>    <span>return </span>recommendations;<br>}<br><br><span>public </span>List&lt;Review&gt; getReviews(<span>int </span>productId) {<br>    String url = <span>reviewServiceUrl </span>+ productId;<br>    List&lt;Review&gt; reviews = <span>restTemplate</span>.exchange(url, <span>GET</span>, <span>null</span>,<br>    <span>new </span>ParameterizedTypeReference&lt;List&lt;Review&gt;&gt;() {}).getBody();<br>    <span>return </span>reviews;<br>}</pre>
<p>Let's explain the preceding source code in more detail:</p>
<ul>
<li>For the <kbd>getProduct()</kbd> implementation, the <kbd>getForObject()</kbd> method can be used in <kbd>RestTemplate</kbd>.&nbsp;<span>The expected response is a<span>&nbsp;</span><kbd>Product</kbd><span>&nbsp;</span>object, and it can be expressed in the call to </span><kbd>getForObject()</kbd> by specifying the <kbd>Product.class</kbd><span> class </span>that <kbd>RestTemplate</kbd> will map the JSON response to.<span>&nbsp;</span></li>
<li>For the calls to <kbd>getRecommendations()</kbd> and <kbd>getReviews()</kbd>, a more advanced method, <kbd>exchange()</kbd>, has to be used. The reason for this is the automatic mapping from a JSON response to a model class that <kbd>RestTemplate</kbd> performs.</li>
<li>The <kbd>getRecommendations()</kbd><span> and </span><kbd>getReviews()</kbd> methods expect a generic list in the responses, that is, <kbd>List&lt;Recommendation&gt;</kbd> <span>and </span><kbd>List&lt;Review&gt;</kbd><span>. S</span><span>ince generics don't hold any type of information at runtime, we can't specify that the methods expect a generic list in their responses. Instead, we can use a helper class from the Spring Framework, </span><kbd>ParameterizedTypeReference</kbd><span>, that is designed to resolve this problem by holding the type information at runtime. This means that <kbd>RestTemplate</kbd> can figure out what class to map the JSON responses to. To utilize this helper class, we have to use the more involved </span><span><kbd>exchange()</kbd> method instead of the simpler <kbd>getForObject()</kbd> method on <kbd>RestTemplate</kbd></span>.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Composite API implementation </h1>
                </header>
            
            <article>
                
<p>Finally, <span>we will look at the last piece of the implementation of the composite microservice: the </span><kbd>ProductCompositeServiceImpl.java</kbd>. implementation class. Let's go through it step-by-step:</p>
<ol>
<li>In the same way that we did for the core services, the composite service implements its API interface, <kbd>ProductCompositeService</kbd>, and is annotated with <span><kbd>@RestController</kbd></span> to mark it as a REST service:</li>
</ol>
<pre style="padding-left: 60px"><span>package </span>se.magnus.microservices.composite.product.services;<br><br><span>@RestController<br></span><span>public class </span>ProductCompositeServiceImpl <span>implements </span>ProductCompositeService {</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<ol start="2">
<li>The implementation class requires the <kbd>ServiceUtil</kbd> bean and its own integration component, so they are injected in its constructor:</li>
</ol>
<pre style="padding-left: 60px"><span>private final </span>ServiceUtil <span>serviceUtil</span>;<br><span>private  </span>ProductCompositeIntegration <span>integration</span>;<br><br><span>@Autowired<br></span><span>public </span>ProductCompositeServiceImpl(ServiceUtil serviceUtil, ProductCompositeIntegration integration) {<br>    <span>this</span>.<span>serviceUtil </span>= serviceUtil;<br>    <span>this</span>.<span>integration </span>= integration;<br>}</pre>
<ol start="3">
<li>Finally, the API method is implemented as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>@Override<br></span><span>public </span>ProductAggregate getProduct(<span>int </span>productId) {<br>    Product product = <span>integration</span>.getProduct(productId);<br>    List&lt;Recommendation&gt; recommendations = <br>    <span>integration</span>.getRecommendations(productId);<br>    List&lt;Review&gt; reviews = <span>integration</span>.getReviews(productId);<br>    <span>return </span>createProductAggregate(product, recommendations,<br>    reviews, <span>serviceUtil</span>.getServiceAddress());<br>}</pre>
<p>The integration component is used to call the three core services, and a helper method, <kbd>createProductAggregate()</kbd>, is used to create a response object of the <kbd>ProductAggregate</kbd> type based on the responses from the calls to the integration component.</p>
<p>The implementation of the helper method, <span><kbd>createProductAggregate()</kbd>, is quite lengthy and not very important and so has been omitted from this chapter; however, it can be found in this book's source code.</span></p>
<p>The full implementation of both the integration component and the composite service can be found in the <kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services</span>/microservices/product-composite-service/src/main/java/se/magnus/microservices/composite/product/services</kbd> folder.</p>
<p>That completes the implementation of the composite microservice from a functional point of view. In the next section, we will see how we can add source code so that we can handle errors.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding error handling</h1>
                </header>
            
            <article>
                
<p><span>Handling errors in a</span> structured and well thought-out way is essential in a microservice landscape where a large number of microservices communicate with each other using synchronous APIs, for example, using HTTP and JSON. It is also important to separate protocol-specific handling of errors, such as HTTP status codes, from the business logic.</p>
<div class="packt_infobox">It could be argued that a separate layer for the business logic should be added when implementing of the microservices. This should ensure that business logic is separated from the protocol-specific code, making it easier both to test and reuse. To avoid unnecessary complexity in the examples provided in this book, we have left out a separate layer for business logic, that is, the <span>microservices implement their business logic dire</span><span>ctly in the <kbd>@RestController</kbd> components.</span></div>
<p>I have created a set of Java exceptions <span>in the</span> <span><kbd>util</kbd></span> <span>project</span> that are used by both the API implementations and the API clients, initially <kbd><span>InvalidInputException</span></kbd> and <span><kbd>NotFoundException</kbd>. See <kbd>$BOOK_HOME/Chapter03/2-basic-rest-services/util/src/main/java/se/magnus/util/exceptions</kbd></span> <span>for details.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The global REST controller exception handler</h1>
                </header>
            
            <article>
                
<p>To separate <span>protocol-specific error handling from the business logic in the REST controllers, that is, the API implementations, I have created a utility class,</span> <kbd>GlobalControllerExceptionHandler.java</kbd>, in the <kbd>util</kbd> project that's annotated as <span><kbd>@RestControllerAdvice</kbd>.</span></p>
<p><span>For each Java exception that the API implementations throws, the utility class has an exception handler method that maps the Java exception to a proper HTTP response, that is, with a proper HTTP status and HTTP response body.</span></p>
<p>For example, if an API implementation class throws <kbd>InvalidInputException</kbd>, the utility class will map it to an HTTP response with the status code set to <kbd>422</kbd> (<span><kbd>UNPROCESSABLE_ENTITY</kbd>). The following code shows this:</span></p>
<pre><span>@ResponseStatus</span>(<span>UNPROCESSABLE_ENTITY</span>)<br><span>@ExceptionHandler</span>(InvalidInputException.<span>class</span>)<br><span>public </span><span>@ResponseBody </span>HttpErrorInfo handleInvalidInputException(ServerHttpRequest request, Exception ex) {<br>    <span>return </span>createHttpErrorInfo(<span>UNPROCESSABLE_ENTITY</span>, request, ex);<br>}</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p>In the same way, <kbd>NotFoundException</kbd> is mapped to a <kbd>404</kbd> (<span><kbd>NOT_FOUND</kbd>) HTTP status code.</span></p>
<p>Whenever a REST controller throws any of these exceptions, Spring will use the utility class to create an HTTP response.</p>
<div class="packt_infobox"><span>Note that Spring itself returns the HTTP status code</span> <span><kbd>400</kbd> ( <kbd>BAD_REQUEST</kbd>) when it detects an invalid request, for example, if the request contains a non-numeric product ID (<kbd>productId</kbd> is specified as an integer in the API declaration).</span></div>
<p>For the full source code of the utility class, see <kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services</span><span>/</span>util/src/main/java/se/magnus/util/http/GlobalControllerExceptionHandler.java</kbd>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Error-handling in API implementations</h1>
                </header>
            
            <article>
                
<p>API implementations use the exceptions in the <kbd>util</kbd> project to signal errors. They will be reported back to the REST client as HTTPS status codes indicating what went wrong. For example, the <kbd>Product</kbd> microservice implementation class, <kbd>ProductServiceImpl.java</kbd>, uses the <kbd>InvalidInputException</kbd> exception to return an error that indicates invalid input, as well as the <kbd>NotFoundException</kbd> exception to tell us that the product that was asked for does not exist. The code looks as follows:</p>
<pre>if <span>(productId &lt; </span><span>1</span><span>) </span><span>throw new </span><span>InvalidInputException(</span><span>"Invalid productId: <br>    " </span><span>+ productId);<br></span><span>if </span>(productId == <span>13</span>) <span>throw new </span>NotFoundException(<span>"No product found for <br>    productId: " </span>+ productId);</pre>
<div class="packt_infobox"><span>Since we currently aren't using a </span><span>databas</span><span>e,</span><span> we have to simulate when to throw </span><kbd>NotFoundException</kbd>.</div>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Error-handling in the API client</h1>
                </header>
            
            <article>
                
<p>The API client, that is, the integration component of the <kbd>Composite</kbd> microservice, does the reverse; that is, it maps the <kbd>422</kbd> <span>(</span><span><kbd>UNPROCESSABLE_ENTITY</kbd>) HTTP status code to</span> <kbd>InvalidInputException</kbd> <span>and the <kbd>404</kbd> (<kbd>NOT_FOUND</kbd>) HTTP status code to </span><span><kbd>NotFoundException</kbd>. See the </span><kbd>getProduct()</kbd> method in <kbd>ProductCompositeIntegration.java</kbd> for the implementation of this error handling logic. The source code looks as follows:</p>
<pre><span>catch </span>(HttpClientErrorException ex) {<br><br>    <span>switch </span>(ex.getStatusCode()) {<br><br>    <span>case </span><span>NOT_FOUND</span>:<br>        <span>throw new </span>NotFoundException(getErrorMessage(ex));<br><br>    <span>case </span><span>UNPROCESSABLE_ENTITY </span>:<br>        <span>throw new </span>InvalidInputException(getErrorMessage(ex));<br><br>    <span>default</span>:<br>        <span>LOG</span>.warn(<span>"Got a unexpected HTTP error: {}, will rethrow it"</span>, <br>        ex.getStatusCode());<br>        <span>LOG</span>.warn(<span>"Error body: {}"</span>, ex.getResponseBodyAsString());<br>        <span>throw </span>ex;<br>    }<br>}</pre>
<p>The error handling for <kbd>getRecommendations()</kbd> and <kbd>getReviews()</kbd> in the integration component is a bit more relaxed <span>â classed as best-effort</span>, meaning that, if it succeeds in getting product information but fails to get either recommendations or reviews, it is still considered to be okay. However, a warning is written to the log.</p>
<p>For more details, see <kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services/microservices/</span>product-composite-service/src/main/java/se/magnus/microservices/composite/product/services/ProductCompositeIntegration.java</kbd><span>.</span></p>
<p>That completes the implementation of both the code and composite microservices. In the next section, we will test the microservices and the API that they expose.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing APIs manually</h1>
                </header>
            
            <article>
                
<p>That concludes the implementation of our microservices. Let's try them out by performing the following steps:</p>
<ol>
<li>Build and start them up as background processes.</li>
<li>Use <kbd>curl</kbd> to call the composite API.</li>
<li>Stop them.</li>
</ol>
<p>First, build and start-up each microservice<span> as a background process, as follows:</span></p>
<pre><strong><span class="s1">cd $BOOK_HOME/Chapter03/2-basic-rest-services/<br><br></span><span>./gradlew build</span></strong></pre>
<p class="mce-root">Once the build completes, we can launch our microservices as background processes to the Terminal process with the following code:</p>
<pre class="mce-root"><strong><span>java -jar microservices/product-composite-service/build/libs/*.jar &amp;</span></strong><br><strong><span>java -jar microservices/product-service/build/libs/*.jar &amp;</span></strong><br><strong><span>java -jar microservices/recommendation-service/build/libs/*.jar &amp;</span></strong><br><strong><span>java -jar microservices/review-service/build/libs/*.jar &amp;</span></strong></pre>
<p>A lot of log messages will be written to the Terminal, but after a few seconds, things will calm down and we will find the following messages written to the log:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7437d6fb-99f1-4fa9-957b-be6257567694.png" style="width:24.83em;height:6.00em;" width="491" height="118" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7437d6fb-99f1-4fa9-957b-be6257567694.png"></p>
<p><span>This means that they all are ready to receive requests. Try this out with the following code:</span></p>
<pre><strong>curl http://localhost:7000/product-composite/1</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>After some log output, we will get a JSON response that looks something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4cf8525d-ea4f-4c59-99f9-a076421e41dd.png" style="width:36.58em;height:13.17em;" width="728" height="261" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4cf8525d-ea4f-4c59-99f9-a076421e41dd.png"></p>
<p>To get the JSON response pretty-printed, you can use the <kbd>jq</kbd> tool: </p>
<pre class="mce-root"><strong>curl http://localhost:7000/product-composite/1 -s | jq</strong> .</pre>
<p>This results in the following output (some details have been replaced by <kbd>...</kbd>&nbsp;<span>for increased readability)</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/771fc141-de97-432e-af16-a3f1d0f03b70.png" style="width:26.50em;height:10.50em;" width="599" height="236" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/771fc141-de97-432e-af16-a3f1d0f03b70.png"></p>
<p><span>If you want to, you can also try out the following commands to verify that the error handling works as expected:</span></p>
<pre><span><br># Verify that a 404 (Not Found) error is returned for a non-existing productId (13)<br></span><strong><span>curl http://localhost:7000/product-composite/13 -i<br></span></strong><span><br></span><span># Verify that no recommendations are returned for productId 113<br></span><strong><span>curl http://localhost:7000/product-<span class="annotator-hl">composite</span>/113 -s </span><span>| jq .</span><span><br></span></strong><br><span># Verify that no reviews are returned for productId 213<br></span><strong><span>curl http://localhost:7000/product-composite/213 -s | jq .<br></span></strong><br><span># Verify that a 422 (Unprocessable Entity) error is returned for a productId that is out of range (-1)<br></span><strong><span>curl http://localhost:7000/product-composite/-1 -i<br></span></strong><span><br></span><span># Verify that a 400 (Bad Request) error is returned for a productId that is not a number, i.e. invalid format<br></span><strong><span>curl http://localhost:7000/product-composite/invalidProductId -i<br></span></strong></pre>
<p>Finally, you can shut down the microservices with the following command:</p>
<pre><strong><span>kill $(jobs -p)</span></strong></pre>
<p class="mce-root">If you are using either Spring Tool Suite or IntelliJ<span> IDEA </span>Ultimate Edition as your IDE, you can use their Spring Boot Dashboard to start and stop your microservices with one click.</p>
<p class="mce-root">The following screenshot shows the use of Spring Tool Suite:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/5017f9b2-a124-4c5a-95fa-cc52247a15e6.png" width="1950" height="743" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/5017f9b2-a124-4c5a-95fa-cc52247a15e6.png"></p>
<p class="mce-root">The following screenshot shows the use of IntelliJ IDEA Ultimate Edition:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/871261b3-dbde-4c1d-afde-61ca65c7bdca.png" width="1955" height="567" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/871261b3-dbde-4c1d-afde-61ca65c7bdca.png"></p>
<p class="mce-root"></p>
<p>In this section, we have learned how to manually start, test, and stop the system landscape of cooperating microservices. These types of test are time-consuming, so they clearly need to be automated. In the next two sections, we will take our first steps toward learning how to automate testing, testing both a single microservice in isolation and a whole system landscape of cooperating microservices. Throughout this book, we will improve how we test our microservices. </p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Preventing slow lookup of the localhost hostname</h1>
                </header>
            
            <article>
                
<p>With effect from macOS Sierra, looking up the hostname that's used by the localhost in a Java program on a macOS can take a very long time, that is, 5 seconds, making tests very slow. The problem seems to be fixed when using macOS Mojave, but if you are using an older version of macOS, this can <span>easily be </span>fixed.</p>
<p>First, you need to verify whether the problem affects you by downloading a small tool from GitHub and running it:</p>
<pre><strong>git clone https://github.com/thoeni/inetTester.git</strong><br><strong><span class="s1">java -jar </span>inetTester/<span class="s1">bin/inetTester.jar</span></strong></pre>
<p>Let's say the program responds with something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c41f046a-ac2c-4f77-a856-2131fef48987.png" style="width:35.67em;height:6.33em;" width="1346" height="239" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c41f046a-ac2c-4f77-a856-2131fef48987.png"></p>
<p>If you have a response time of 5 seconds, then you have a problem!</p>
<p>The solution is to edit the<span>&nbsp;</span><span><kbd>/etc/hosts</kbd> file and </span>add your local hostname, which is <span class="s1"><kbd>Magnuss-Mac.local</kbd> in the preceding example, </span>after<span>&nbsp;</span><kbd>localhost</kbd>; for example:</p>
<pre class="p1"><span class="s1">127.0.0.1<span class="Apple-tab-span"> </span>localhost Magnuss-Mac.local<br></span><span class="s1">::1 <span class="Apple-converted-space">      </span>localhost Magnuss-Mac.local</span></pre>
<p>Rerun the test. It should respond with a response time of a<span> few milliseconds</span>,<span>&nbsp;</span>for example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d1ba5f11-22f1-4ce4-a3fd-03e8486b5e13.png" style="width:36.42em;height:6.67em;" width="1283" height="235" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d1ba5f11-22f1-4ce4-a3fd-03e8486b5e13.png"></p>
<p>Now lets see how to add automated tests in isolation for microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding automated microservice tests in isolation</h1>
                </header>
            
            <article>
                
<p>Before we wrap up the implementation, we also need to write some automated tests.</p>
<p>We don't have much business logic to test at this time, so we don't need to write any unit tests. Instead, we will focus on testing the APIs that our microservices expose; that is, we will start them up in integration tests with their embedded web server and then use a test client to perform HTTP requests and validate the responses. With Spring WebFlux came a new test client, <kbd>WebTestClient</kbd>, that provides a fluent API for making a request and then applying assertions on its result.</p>
<p>The following is an example where we test the composite product API by doing the following:</p>
<ul>
<li>Sending in <kbd>productId</kbd> for an existing product and asserting that we get back 200 as an HTTP response code and a JSON response that contains the requested <kbd>productId</kbd> along with one recommendation and one review</li>
<li>Sending in a missing <kbd>productId</kbd> and asserting that we get back 404 <span>as an HTTP response code and a JSON response that contains relevant error information</span></li>
</ul>
<p><span>The implementation for these two tests is shown in the following code:</span></p>
<pre><span>@Autowired<br></span><span>private </span>WebTestClient <span>client</span>;<br><br><span>@Test<br></span><span>public void </span>getProductById() {<br><span>  client</span>.get()<br>    .uri(<span>"/product-composite/" </span>+ <span>PRODUCT_ID_OK</span>)<br>    .accept(<span>APPLICATION_JSON_UTF8</span>)<br>    .exchange()<br>    .expectStatus().isOk()<br>    .expectHeader().contentType(<span>APPLICATION_JSON_UTF8</span>)<br>    .expectBody()<br>    .jsonPath(<span>"$.productId"</span>).isEqualTo(<span>PRODUCT_ID_OK</span>)<br>    .jsonPath(<span>"$.recommendations.length()"</span>).isEqualTo(<span>1</span>)<br>    .jsonPath(<span>"$.reviews.length()"</span>).isEqualTo(<span>1</span>);<br>}</pre>
<p>Let's explain the preceding source code in more detail:</p>
<ul>
<li>The test uses the fluent <kbd>WebTestClient</kbd> API <span><span>to set up the URL to call</span></span> <kbd>"/product-composite/" <span>+</span> <span>PRODUCT_ID_OK</span></kbd><span> and specify the accepted response format, JSON.</span></li>
<li>After executing the request using the <kbd>exchange()</kbd> method, the test verifies that the response status is OK (200) and that the response format actually is JSON (as requested).</li>
<li>Finally, the test inspects the response body and verifies that it contains the expected information in terms of <kbd>productId</kbd> and the number of recommendations and reviews.</li>
</ul>
<p>The second test looks as follows:</p>
<pre style="padding-left: 60px"><br><span>@Test<br></span><span>public void </span>getProductNotFound() {<br><span>  client</span>.get()<br>    .uri(<span>"/product-composite/" </span>+ <span>PRODUCT_ID_NOT_FOUND</span>)<br>    .accept(<span>APPLICATION_JSON_UTF8</span>)<br>    .exchange()<br>    .expectStatus().isNotFound()<br>    .expectHeader().contentType(<span>APPLICATION_JSON_UTF8</span>)<br>    .expectBody()<br>    .jsonPath(<span>"$.path"</span>).isEqualTo(<span>"/product-composite/" </span>+ <br>     <span>PRODUCT_ID_NOT_FOUND</span>)<br>    .jsonPath(<span>"$.message"</span>).isEqualTo(<span>"NOT FOUND: " </span>+ <br>     <span>PRODUCT_ID_NOT_FOUND</span>);<br>}</pre>
<p><span>Let's explain the preceding source code in more detail:</span></p>
<ul>
<li>This negative test is very similar to the preceding test in terms of its structure; the main difference is that it verifies that it got an error status code back, Not Found (404), and that the response body contains the expected error message.</li>
</ul>
<p>To test the composite product API in isolation, we need to mock its dependencies, that is, the requests to the other three microservices that were performed by the integration component, <kbd>ProductCompositeIntegration</kbd>. We use Mockito to do this, as follows:</p>
<pre><span>private static final int </span><span>PRODUCT_ID_OK </span>= <span>1</span>;<br><span>private static final int </span><span>PRODUCT_ID_NOT_FOUND </span>= <span>2</span>;<br><span>private static final int </span><span>PRODUCT_ID_INVALID </span>= <span>3</span>;<br><br><span>@MockBean<br></span><span>private </span>ProductCompositeIntegration <span>compositeIntegration</span>;<br><br><span>@Before<br></span><span>public void </span>setUp() {<br><br>  <span>when</span>(<span>compositeIntegration</span>.getProduct(<span>PRODUCT_ID_OK</span>)).<br>    thenReturn(<span>new </span>Product(<span>PRODUCT_ID_OK</span>, <span>"name"</span>, <span>1</span>, <span>"mock-address"</span>));<br>  <span>when</span>(<span>compositeIntegration</span>.getRecommendations(<span>PRODUCT_ID_OK</span>)).<br>    thenReturn(<span>singletonList</span>(<span>new </span>Recommendation(<span>PRODUCT_ID_OK</span>, <span>1</span>, <br>    <span>"author"</span>, <span>1</span>, <span>"content"</span>, <span>"mock address"</span>)));<br>     <span>when</span>(<span>compositeIntegration</span>.getReviews(<span>PRODUCT_ID_OK</span>)).<br>    thenReturn(<span>singletonList</span>(<span>new </span>Review(<span>PRODUCT_ID_OK</span>, <span>1</span>, <span>"author"</span>, <br>    <span>"subject"</span>, <span>"content"</span>, <span>"mock address"</span>)));<br><br>  <span>when</span>(<span>compositeIntegration</span>.getProduct(<span>PRODUCT_ID_NOT_FOUND</span>)).<br>    thenThrow(<span>new </span>NotFoundException(<span>"NOT FOUND: " </span>+ <br>    <span>PRODUCT_ID_NOT_FOUND</span>));<br><br>  <span>when</span>(<span>compositeIntegration</span>.getProduct(<span>PRODUCT_ID_INVALID</span>)).<br>    thenThrow(<span>new </span>InvalidInputException(<span>"INVALID: " </span>+ <br>    <span>PRODUCT_ID_INVALID</span>));<br>}</pre>
<p>Let's explain the preceding source code in more detail:</p>
<ul>
<li>First, we declare three constants that are used in the test class: <kbd>PRODUCT_ID_OK</kbd>, <kbd>PRODUCT_ID_NOT_FOUND</kbd>, and <kbd>PRODUCT_ID_INVALID</kbd>.</li>
<li>If the <kbd>getProduct()</kbd>, <kbd>getRecommendations()</kbd>, and <kbd>getReviews()</kbd> methods are called on the integration component, and <kbd>productId</kbd> is set to <kbd>PRODUCT_ID_OK</kbd>, the mock will return a normal response.</li>
<li>If the <span><span><kbd>getProduct()</kbd> method is called with <kbd>productId</kbd> set to <kbd>PRODUCT_ID_NOT_FOUND</kbd>, the mock will throw </span></span><kbd>NotFoundException</kbd>.</li>
<li>If the <span><span><span><kbd>getProduct()</kbd> method is called with <kbd>productId</kbd> set to <kbd>PRODUCT_ID_INVALID</kbd>, the mock will throw <kbd>InvalidInputException</kbd>.<br></span></span></span></li>
</ul>
<p>The full source code for the automated integration tests on the composite product API can be found in <kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services/microservices</span>/product-composite-service/src/test/java/se/magnus/microservices/composite/product/ProductCompositeServiceApplicationTests.java</kbd>.</p>
<p><span>The automated integration tests on the API exposed by the three core microservices are similar, but simpler since they don't need to mock anything! The source code for the tests can be found in each microservice's <kbd>test</kbd> folder.</span></p>
<p>The tests are run automatically by Gradle when performing a build:</p>
<pre><strong>./gradlew build</strong></pre>
<p>You can, however, specify that you only want to run the tests (and not the rest of the build):</p>
<pre><strong>./gradlew test</strong></pre>
<p>This was an introduction to how to write automated tests for microservices in isolation. In the next section, we will learn how to write tests that automatically test a microservice landscape. In this chapter, these tests will only be semi-automated. In upcoming chapters, the tests will be fully automated, a significant improvement.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding semi-automated tests of a microservice landscape</h1>
                </header>
            
            <article>
                
<p>Being able to automatically test each microservice in isolation is, of course, very useful, but insufficient!</p>
<p>We need a way to automatically test all of our microservices to ensure that they deliver what we expect!</p>
<p>For this reason, I have written a simple bash script that can perform calls to a RESTful API using <kbd>curl</kbd> and verify its return code and parts of its JSON response using <kbd>jq</kbd>. The script contains two helper functions, <span><kbd>assertCurl()</kbd> and </span><span><kbd>assertEqual()</kbd>, to make the test code compact and easier to read.</span></p>
<p>For example, making a normal request and expecting 200 as the status code, as well as <span>asserting that we get back a JSON response that returns the requested <kbd>productId</kbd> along with three recommendations and three reviews, looks like the following:</span></p>
<pre><span># Verify that a normal request works, expect three recommendations and three reviews<br></span><span>assertCurl </span><span>200 </span><span>"curl http://$HOST:${PORT}/product-composite/1 -s"<br></span><span>assertEqual </span><span>1 </span><span>$</span>(<span>echo </span>$RESPONSE | jq .productId)<br><span>assertEqual </span><span>3 </span><span>$</span>(<span>echo </span>$RESPONSE | jq <span>".recommendations | length"</span>)<br><span>assertEqual </span><span>3 </span><span>$</span>(<span>echo </span>$RESPONSE | jq <span>".reviews | length"</span>)<br><br></pre>
<p class="mce-root">Verifying that we get <span><kbd>404 (Not Found)</kbd> back as an HTTP response code (when we try to look up a product that doesn't exist) looks as follows:</span></p>
<pre><span># Verify that a 404 (Not Found) error is returned for a non-existing productId (13)<br></span><span>assertCurl </span><span>404 </span><span>"curl http://$HOST:${PORT}/product-composite/13 -s"</span><span> </span></pre>
<p><span>The test script implements the manual tests that were described in the</span> <em>Testing APIs manually</em><span> section and can be found in </span><kbd><span>$BOOK_HOME/Chapter03/2-basic-rest-services/test-em-all.bash</span></kbd><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the test script</h1>
                </header>
            
            <article>
                
<p>To try out the test script, perform the following steps:</p>
<ol>
<li>First, start the microservices, as we did previously:</li>
</ol>
<pre style="padding-left: 60px"><strong><span class="s1">cd $BOOK_HOME/Chapter03/2-basic-rest-services<br></span><span>java -jar microservices/product-composite-service/build/libs/*.jar<br>&amp; </span></strong><strong><span>java -jar microservices/product-service/build/libs/*.jar &amp;</span></strong><br><strong><span>java -jar microservices/recommendation-service/build/libs/*.jar &amp;</span></strong><br><strong><span>java -jar microservices/review-service/build/libs/*.jar &amp;</span></strong></pre>
<ol start="2">
<li>Once they've all started up, run the test script:</li>
</ol>
<pre style="padding-left: 60px"><strong>./test-em-all.bash</strong></pre>
<ol start="3">
<li>Expect the output to look similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/34c2f31c-8138-4f03-9414-cfb4c0779f29.png" width="927" height="532" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/34c2f31c-8138-4f03-9414-cfb4c0779f29.png"></p>
<ol start="4">
<li>Wrap this up by shutting down the microservices with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>kill $(jobs -p)</span></strong></pre>
<p>In this section, we have taken the first steps toward automating testing a system landscape of cooperating microservices, all of which will be improved in upcoming chapters.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We have now built our first few microservices using Spring Boot. After being introduced to the microservice landscape, which we will use throughout this book, we learned how to use Spring Initializr to create skeleton projects for each microservice.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>Next, we learned how to add APIs using Spring WebFlux for the three core services and implemented a composite service that uses the three core services APIs to create an aggregated view of the information in them. The composite service uses the <kbd>RestTemplate</kbd> class in Spring Framework to perform HTTP requests to APIs that are exposed by the core services. After adding logic for error handling in the services, we ran some manual tests on the microservice landscape.</p>
<p>We wrapped this chapter up by learning how to add tests for microservices in isolation and when they work together as a system landscape. To provide controlled isolation for the composite service, we mocked its dependencies to the core services using Mockito. Testing the whole system landscape is performed by a bash script that uses <kbd>curl</kbd> to perform calls to the API of the composite service.</p>
<p>With these skills in place, we are ready to take the next step, entering<span> the world of Docker and containers, in the next chapter! Among other things, we will learn how to use Docker to fully automate testing of </span>a system landscape of cooperating microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the command that lists available dependencies when you create a new Spring Boot project using the <kbd>spring init</kbd> Spring Initializr CLI tool?</li>
<li>How can you set up Gradle to build multiple related projects with one command?</li>
<li>What are the <kbd>@PathVariable</kbd> and <kbd>@RequestParam</kbd> annotations used for?</li>
<li>How can you separate protocol-specific error handling from the business logic in an API implementation class?</li>
<li>What is Mockito used for?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying Our Microservices Using Docker</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this chapter, we will start using Docker and put our microservices into containers!</span></p>
<p><span>By the end of this chapter, we will have run fully automated tests of our microservice landscape that s</span>tart all our microservices as Docker containers, requiring no other infrastructure than a Docker engine. We will have also run a number of tests to verify that the <span>microservices work together as expected and finally s</span>hut down all the microservices, leaving no traces of the tests we executed.</p>
<p>Being able to test a number of cooperating microservices in this way is very useful. As developers, we can verify that it works on our local developer machines. We can also run exactly the same tests in a build server to automatically verify that changes to the source code won't break the tests at a system level. Additionally, we don't need to have a dedicated infrastructure allocated to run these types of tests. <span>In the upcoming chapters, we will see how we</span><span> can add databases and queue managers to our test landscape, all of which will run as Docker containers. </span></p>
<div class="packt_infobox">This does not, however, replace the need for automated unit and integrations tests, which test individual microservices in isolation. They are as important as ever. <br>
<br>
<span>For production usage, as we mentioned earlier in this book, we need a container orchestrator such as Kubernetes. We will go back to container orchestrators and Kubernetes later in this book. </span></div>
<p><span>The following topics will be covered in this chapter:</span></p>
<ul>
<li>Introduction to Docker.</li>
<li>Docker and Java. Java hasn't been very friendly to containers historically, but that changed with Java 10. Let's see how <span>Docker and Java</span> fit together on this topic!</li>
<li>Using Docker with one microservice.</li>
<li>Managing a landscape of microservices using Docker Compose.</li>
<li>Testing them all together automatically.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to Docker</h1>
                </header>
            
            <article>
                
<p>As we already mentioned in <a href="7d969006-ea94-4bbb-858d-30dce8177a2c.xhtml">Chapter 2</a>, <em>Introduction to Spring Boot</em>, Docker made the concept of containers as a lightweight alternative to virtual machines very popular in 2013. Containers are actually processed in a Linux host that uses Linux namespaces to provide isolation between containers of global system resources, such as users, processes, filesystems, and networking. <strong>Linux Control Groups</strong> (also knows as&nbsp;<strong>cgroups</strong>) are used to limit the amount of CPU and memory that a container is allowed to consume. Compared to a virtual machine that uses a hypervisor to run a complete copy of an operating system in each virtual machine, the overhead in a container is a fraction of the overhead in a virtual machine. This leads to much faster startup times and significantly lower overhead in terms of CPU and memory usage. The isolation that's provided for a container, however, is not considered to be as secure as the isolation that's provided for a virtual machine. With the release of Windows Server 2016 and Windows 10 Pro (<span>1607 Anniversary Update)</span>, Microsoft supports the usage of Docker on Windows as well. Take a look at the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/caee1a35-71e9-4b6c-953f-b00a2d6eaca3.png" width="1898" height="671" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/caee1a35-71e9-4b6c-953f-b00a2d6eaca3.png"></p>
<p><span>The preceding diagram illustrates the difference between the resource usage of virtual machines and contai</span><span>ners, visualizing that the same type of server can run significantly more containers than virtual machines.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running our first Docker commands</h1>
                </header>
            
            <article>
                
<p>Let's try to start a container by launching an Ubuntu server in one using Docker's <kbd>run</kbd> command:</p>
<div>
<pre><strong>docker run -it --rm ubuntu</strong></pre>
<p>With the preceding command, we ask Docker to create a container that runs Ubuntu, based on the latest version that's available of the official Docker image for Ubuntu. The <kbd>-it</kbd> option is used so that we can interact with the container using Terminal, and the <kbd>--rm</kbd> option tells Docker to remove the container once we exit the Terminal session; otherwise, the container will remain in the Docker engine with an <kbd>Exited</kbd> state.</p>
<p>The first time we use a Docker image that we haven't built ourselves, Docker will download it from a Docker registry, which is Docker Hub by default (<a href="https://hub.docker.com">https://hub.docker.com</a>). This will take some time, but for subsequent usage of that Docker image, the container will start in just a few seconds!</p>
<p>Once the Docker image has been downloaded and the container has been started up, the Ubuntu server should respond with a prompt such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/39044aaf-927e-4210-9731-1a3f10fedd10.png" style="width:15.50em;height:3.92em;" width="290" height="73" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/39044aaf-927e-4210-9731-1a3f10fedd10.png"></p>
<p><span>We can try out the container by asking what version of Ubuntu it runs:</span></p>
</div>
<div>
<pre><strong>cat /etc/os-release | grep 'VERSION='</strong></pre>
<p>It should respond with something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/60a69639-0969-4d0f-ab2c-f8a2582d51eb.png" style="width:31.17em;height:5.00em;" width="591" height="95" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/60a69639-0969-4d0f-ab2c-f8a2582d51eb.png"></p>
<p>We can leave the container with an&nbsp;<kbd>exit</kbd> command and verify&nbsp;that the Ubuntu container no longer exits with the <kbd>docker ps -a</kbd> command.&nbsp;<span>We need to use the&nbsp;</span><kbd>-a</kbd><span>&nbsp;option to see stopped containers; otherwise, only running containers are displayed.</span></p>
<p>If you favor CentOS over Ubuntu, feel free to try the same with the <span><kbd>docker run --rm -it centos</kbd>&nbsp;command. Once the CoreOS server has started running in its container you can, for example, ask what version of CoreOS that it runs with the&nbsp;</span><span><kbd>cat /etc/redhat-release</kbd>&nbsp;command. It should respond with something like the following:&nbsp;</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0bc608f2-03e6-40ec-a0b4-d343d9eff2b3.png" style="width:23.42em;height:5.92em;" width="475" height="120" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0bc608f2-03e6-40ec-a0b4-d343d9eff2b3.png"></p>
<p>Leave the container with the <kbd>exit</kbd> command to remove it.</p>
</div>
<div class="packt_tip">
<p>If, at some point, you find that you have a lot of unwanted containers in the Docker engine and you want to get a clean sheet, that is, get rid of them all, you can run the following command:</p>
<pre><strong>docker rm -f $(docker ps -aq)</strong></pre>
<p>The <kbd>docker rm -f</kbd><span>&nbsp;command&nbsp;</span>stops and removes the containers whose container IDs are specified to the command. The <kbd>docker ps -aq</kbd><span>&nbsp;command&nbsp;</span>lists the container IDs of all the running and stopped containers in the Docker engine. The<span>&nbsp;</span><kbd>-q</kbd><span>&nbsp;</span>option reduces the output from the<span>&nbsp;</span><kbd>docker ps</kbd><span>&nbsp;</span>command so that it only lists the container IDs.</p>
</div>
<p>After understanding what Docker is, next we can move on to understand the problems which we might face while running Java in Docker.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running our first Docker commands</h1>
                </header>
            
            <article>
                
<p>Let's try to start a container by launching an Ubuntu server in one using Docker's <kbd>run</kbd> command:</p>
<div>
<pre><strong>docker run -it --rm ubuntu</strong></pre>
<p>With the preceding command, we ask Docker to create a container that runs Ubuntu, based on the latest version that's available of the official Docker image for Ubuntu. The <kbd>-it</kbd> option is used so that we can interact with the container using Terminal, and the <kbd>--rm</kbd> option tells Docker to remove the container once we exit the Terminal session; otherwise, the container will remain in the Docker engine with an <kbd>Exited</kbd> state.</p>
<p>The first time we use a Docker image that we haven't built ourselves, Docker will download it from a Docker registry, which is Docker Hub by default (<a href="https://hub.docker.com">https://hub.docker.com</a>). This will take some time, but for subsequent usage of that Docker image, the container will start in just a few seconds!</p>
<p>Once the Docker image has been downloaded and the container has been started up, the Ubuntu server should respond with a prompt such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/39044aaf-927e-4210-9731-1a3f10fedd10.png" style="width:15.50em;height:3.92em;" width="290" height="73" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/39044aaf-927e-4210-9731-1a3f10fedd10.png"></p>
<p><span>We can try out the container by asking what version of Ubuntu it runs:</span></p>
</div>
<div>
<pre><strong>cat /etc/os-release | grep 'VERSION='</strong></pre>
<p>It should respond with something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/60a69639-0969-4d0f-ab2c-f8a2582d51eb.png" style="width:31.17em;height:5.00em;" width="591" height="95" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/60a69639-0969-4d0f-ab2c-f8a2582d51eb.png"></p>
<p>We can leave the container with an <kbd>exit</kbd> command and verify that the Ubuntu container no longer exits with the <kbd>docker ps -a</kbd> command. <span>We need to use the </span><kbd>-a</kbd><span> option to see stopped containers; otherwise, only running containers are displayed.</span></p>
<p>If you favor CentOS over Ubuntu, feel free to try the same with the <span><kbd>docker run --rm -it centos</kbd> command. Once the CoreOS server has started running in its container you can, for example, ask what version of CoreOS that it runs with the </span><span><kbd>cat /etc/redhat-release</kbd> command. It should respond with something like the following: </span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0bc608f2-03e6-40ec-a0b4-d343d9eff2b3.png" style="width:23.42em;height:5.92em;" width="475" height="120" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0bc608f2-03e6-40ec-a0b4-d343d9eff2b3.png"></p>
<p>Leave the container with the <kbd>exit</kbd> command to remove it.</p>
</div>
<div class="packt_tip">
<p>If, at some point, you find that you have a lot of unwanted containers in the Docker engine and you want to get a clean sheet, that is, get rid of them all, you can run the following command:</p>
<pre><strong>docker rm -f $(docker ps -aq)</strong></pre>
<p>The <kbd>docker rm -f</kbd><span> command </span>stops and removes the containers whose container IDs are specified to the command. The <kbd>docker ps -aq</kbd><span> command </span>lists the container IDs of all the running and stopped containers in the Docker engine. The<span>&nbsp;</span><kbd>-q</kbd><span>&nbsp;</span>option reduces the output from the<span>&nbsp;</span><kbd>docker ps</kbd><span>&nbsp;</span>command so that it only lists the container IDs.</p>
</div>
<p>After understanding what Docker is, next we can move on to understand the problems which we might face while running Java in Docker.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Challenges with running Java in Docker</h1>
                </header>
            
            <article>
                
<p>When it comes to Java, over the past few years, there have been a number of attempts to get Java working in Docker in a good way. Currently, the official Docker image for Java is based on OpenJDK: <a href="https://hub.docker.com/_/openjdk/">https://hub.docker.com/_/openjdk/</a>. We will use Java SE 12 with the Docker tag <span><kbd>openjdk:12.0.2</kbd></span>, that is, Java SE v12.0.2. </p>
<p>Java has historically not been very good at honoring the quotas specified for a Docker container using Linux cgroups; it has simply ignored these settings. So, instead of allocating memory inside the JVM in relation to the memory available in the container, Java allocated memory as if it had access to all the memory in the Docker host, which obviously isn't good! In the same way, Java allocated CPU-related resources such as thread pools in relation to the total number of available CPU cores in the Docker host instead of the number of CPU cores that were made available for the container JVM was running in. In Java SE 9, some initial support was provided, which was also back-ported to later versions of Java SE 8. In Java 10, however, much-improved support for CPU and memory constraints was put in place.</p>
<p>Let's try it out!</p>
<p>First, we will try out Java commands locally,<span>&nbsp;</span>without Docker, since that tells us how much memory and the number of CPU cores that the JVM sees. Next, we will try the commands in Docker using Java SE 12 to verify that it honors the constraints we set on the Docker container it runs in. Finally, we will also try out a Java SE 9 container and see how it fails to honor the constraints and what problems it can result in.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Java in Docker</h1>
                </header>
            
            <article>
                
<p>Let's look at how Java SE 12 responds to limits we set on a container it runs in!</p>
<p>Since I'm using Docker for macOS, I'm actually running the Docker engine on a virtual machine on my MacBook Pro as the Docker host. I have configured Docker for <span>macOS</span> so that it allows the Docker host to use all 12 cores in my <span>macOS</span> but only use up to 16 GB of memory. All in all, the Docker host has 12 cores and 16 GB of memory.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Java in Docker</h1>
                </header>
            
            <article>
                
<p>Let's look at how Java SE 12 responds to limits we set on a container it runs in!</p>
<p>Since I'm using Docker for macOS, I'm actually running the Docker engine on a virtual machine on my MacBook Pro as the Docker host. I have configured Docker for <span>macOS</span> so that it allows the Docker host to use all 12 cores in my <span>macOS</span> but only use up to 16 GB of memory. All in all, the Docker host has 12 cores and 16 GB of memory.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">CPU</h1>
                </header>
            
            <article>
                
<p>Let's start by applying no constraints, that is, the same test that we did without Docker:</p>
<pre><strong><span>echo 'Runtime.getRuntime().availableProcessors()' | docker run --rm -i </span><span>openjdk:12.0.2</span><span> </span><span>jshell -q</span></strong></pre>
<div class="packt_infobox"><span>This command will send the <kbd>Runtime.getRuntime().availableProcessors()</kbd> string to the Docker container that will process the string using <kbd>jshell</kbd>.</span></div>
<p>It will respond with the same result, that is, <kbd><ccc title="$1" class="ccc--converted" style="font-size: inherit; display: inline; color: inherit;">â¬0.84</ccc> ==&gt; 12</kbd> in my case. Let's move on and restrict the Docker container to only be allowed to use three CPU cores using the <kbd>--cpus 3</kbd> Docker option and ask the JVM about how many available processors it sees:</p>
<pre><strong>echo 'Runtime.getRuntime().availableProcessors()' | docker run --rm -i --cpus 3</strong><strong><span> openjdk:12.0.2</span><span> </span></strong><strong>jshell -q</strong></pre>
<p>The JVM now responds with <kbd><ccc title="$1" class="ccc--converted" style="font-size: inherit; display: inline; color: inherit;">â¬0.84</ccc> ==&gt; 3</kbd>, that is, Java SE 12 honors the settings in the container and will, therefore, be able to configure CPU-related resources such as thread pools correctly!</p>
<p>Let's also try to specify a relative share of the available CPUs instead of an exact number of CPUs. 1,024 shares correspond to one core by default, so if we want to limit the container to two cores, we set the <kbd>--cpu-shares</kbd> Docker option to 2,048, like so:</p>
<pre><strong>echo 'Runtime.getRuntime().availableProcessors()' | docker run --rm -i --cpu-shares 2048 openjdk:12.0.2 jshell -q</strong></pre>
<p>The JVM will respond with <kbd><ccc title="$1" class="ccc--converted" style="font-size: inherit; display: inline; color: inherit;">â¬0.84</ccc> ==&gt; 2</kbd>, that is, Java SE 12 honors the relative <kbd>share</kbd> option as well!</p>
<div class="packt_infobox"><span>While the </span><kbd>--cpus</kbd><span> option is a hard constraint, the <kbd>--cpu-shares</kbd> option only applies when the Docker host is under high load. This means that a container can consume more CPU than what the</span><span>&nbsp;<kbd>share</kbd> option indicates</span><span> whether CPU capacity is available.</span></div>
<p>Let's try out limiting the amount of memory next.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Problems with Docker and Java SE 9 (or older)</h1>
                </header>
            
            <article>
                
<p>First, try out limiting a Java SE 9 JVM to three CPU cores using <kbd>openjdk:9-jdk</kbd>&nbsp;image.</p>
<p>Java 9 fails to obey the three-CPU limit:</p>
<pre><strong>echo 'Runtime.getRuntime().availableProcessors()' | docker run --rm -i --cpus 3 openjdk:9-jdk jshell -q</strong></pre>
<p>It responds with&nbsp;<kbd>$1 ==&gt; 12</kbd> on my machine, that is, it ignores the limitation of three CPU cores.</p>
<p>We will see the same result, that is,&nbsp;<kbd><span>$1 ==&gt; 12</span></kbd>, if we try out the&nbsp;<kbd>--cpu-shares</kbd> option:</p>
<pre><strong>echo 'Runtime.getRuntime().availableProcessors()' | docker run --rm -i --cpu-shares 2048 openjdk:9-jdk jshell -q</strong></pre>
<p>Now, let's try to limit the memory to 1 GB:</p>
<pre class="mce-root"><strong>docker run -it --rm -m=1024M openjdk:9-jdk java -XX:+PrintFlagsFinal -version | grep MaxHeapSize</strong></pre>
<p>As expected, Java SE 9 does not honor the memory constraint that we set in Docker; that is, it reports a max heap size of&nbsp;4,202,692,608 bytes = <em>4 GB <span>â&nbsp;</span>4 * 1024^3</em> bytes. Here, Java 9 calculated the available memory when given the memory in the Docker host, not in the actual container!</p>
<p>So, what happens if we repeat the memory allocation tests that we did for Java SE 12?</p>
<p>Let's try out the first test, that is, allocating a 100 MB array:</p>
<pre><strong>echo '<span>new byte[100_000_000]' | </span>docker run -i --rm -m=1024M openjdk:9-jdk jshell -q</strong></pre>
<p>The command responds with&nbsp;<kbd>$1 ==&gt; byte[100000000] { 0, 0, 0, ...</kbd>, so that worked fine!</p>
<p>Now, let's move on to the really interesting test: what if we allocate a byte array of 500 MB that doesn't fit in the memory that was allocated to the container by Docker?</p>
<pre><strong>echo '<span>new byte[500_000_000]' | </span>docker run -i --rm -m=1024M openjdk:9-jdk jshell -q</strong></pre>
<p>From a Java perspective, this should work. Since Java thinks the total memory is 16 GB, it has set the max heap size to 4 GB, so it&nbsp;<span>happily&nbsp;</span>starts to allocate 500 MB for the byte array. But after a while, the total size of the JVM exceeds 1 GB and Docker will kill the container with no mercy, resulting in a confusing exception such as&nbsp;<kbd>State engine terminated</kbd>. We basically have no clue what went wrong, even though we can guess that we ran out of memory.</p>
<p>So, to summarize, if you plan to do any serious work with Docker and Java, ensure that you use Java SE 10 or later!</p>
<div class="packt_infobox">To be fair to Java SE 9, it should be mentioned that Java SE 9 contains some initial support for cgroups. If you specify the Java options&nbsp;<kbd>-XX:+UnlockExperimentalVMOptions</kbd>&nbsp;and&nbsp;<kbd>-XX:+UseCGroupMemoryLimitForHeap</kbd>, it will honor parts of the cgroup constraints, but not all&nbsp;of them, and it should be noted that this is only e<span>xperimental. Due to this, it should be avoided in production environments. Simply use Java SE 10 or later in Docker!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Docker with one microservice</h1>
                </header>
            
            <article>
                
<p>Now that we understand how Java works, we can start using Docker with one of our microservices. Before we can run our microservice as a Docker container, we need to package it in a Docker image. To build a Docker image, we need a Dockerfile, so we will start with that. Next, we need a Docker-specific configuration for our microservice. Since a microservice that runs in a container is isolated from other microservices, for example, has its own IP address, hostname, and ports, it needs a different configuration compared to when it's running on the same host with other microservices. For example, since the other<span>&nbsp;</span>microservices<span>&nbsp;</span>no longer run on the same host, no port conflicts will occur. When running in Docker, we can use the default port <kbd>8080</kbd> for all our microservices without any risk of port conflicts. On the other hand, if we need to talk to the other microservices, we can no longer use localhost like we could when we ran them on the same host. The source code in the microservices will not be affected by running the microservices in containers, only their configuration.</p>
<p>To handle the different configurations that are required when running locally without Docker and when running the microservices as Docker containers, we will use Spring profiles. Since <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml" target="_blank">Chapter 3</a>,&nbsp;<em>Creating a Set of Cooperating Microservices</em>, we have been using the default Spring profile for running locally without Docker, so we will create a Spring profile named <kbd>docker</kbd> for when we run our microservices as containers in Docker.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Docker with one microservice</h1>
                </header>
            
            <article>
                
<p>Now that we understand how Java works, we can start using Docker with one of our microservices. Before we can run our microservice as a Docker container, we need to package it in a Docker image. To build a Docker image, we need a Dockerfile, so we will start with that. Next, we need a Docker-specific configuration for our microservice. Since a microservice that runs in a container is isolated from other microservices, for example, has its own IP address, hostname, and ports, it needs a different configuration compared to when it's running on the same host with other microservices. For example, since the other<span>&nbsp;</span>microservices<span>&nbsp;</span>no longer run on the same host, no port conflicts will occur. When running in Docker, we can use the default port <kbd>8080</kbd> for all our microservices without any risk of port conflicts. On the other hand, if we need to talk to the other microservices, we can no longer use localhost like we could when we ran them on the same host. The source code in the microservices will not be affected by running the microservices in containers, only their configuration.</p>
<p>To handle the different configurations that are required when running locally without Docker and when running the microservices as Docker containers, we will use Spring profiles. Since <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml" target="_blank">Chapter 3</a>,&nbsp;<em>Creating a Set of Cooperating Microservices</em>, we have been using the default Spring profile for running locally without Docker, so we will create a Spring profile named <kbd>docker</kbd> for when we run our microservices as containers in Docker.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building a Docker image</h1>
                </header>
            
            <article>
                
<p>To build the Docker image, we need to build our deployment artifact, that is, the fat-file, for&nbsp;<kbd>product-service</kbd>:</p>
<pre><strong><span>cd $BOOK_HOME/</span><span>Chapter04</span><span><br></span><span>./gradlew :microservices:product-service:build</span></strong></pre>
<div class="packt_infobox">Since we only want to build&nbsp;<kbd>product-service</kbd> and the projects it depends on, <kbd>api</kbd> and <kbd>util</kbd>, we don't use the normal <kbd>build</kbd> command, which builds all the microservices, but a variant that tells Gradle to only build&nbsp;<span><span><kbd>product-service</kbd>:&nbsp;</span></span><span><kbd><span>:microservices:product-service:build</span></kbd></span><span>.</span></div>
<p>We can find the <kbd>fat-jar</kbd> file in the&nbsp;<span>Gradle build library,&nbsp;</span><kbd>build/libs</kbd><span>. For example, the</span>&nbsp;<span><kbd>ls -l microservices/product-service/build/libs</kbd>&nbsp;command will report something like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2af54aa7-9e16-41b1-964a-021bda81ebd5.png" style="width:41.33em;height:4.75em;" width="862" height="98" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2af54aa7-9e16-41b1-964a-021bda81ebd5.png"></p>
<div class="packt_infobox">As you can see, the JAR file is close to 20 MB in size&nbsp;<span>â</span> no wonder they are called <kbd>fat-jar</kbd> files!<br>
<br>
If you are curious about its actual content, you can view it by using the&nbsp;<kbd>unzip -l microservices/product-service/build/libs/product-service-1.0.0-SNAPSHOT.jar</kbd> command.</div>
<p class="mce-root"></p>
<p>Next, we will build the Docker image and name it&nbsp;<kbd><span>product-service</span></kbd>, as follows:</p>
<pre><strong><span>cd </span><span>microservices/product-service<br></span>docker build -t product-service .</strong></pre>
<p><span>Docker will use the Dockerfile in the current directory to build the Docker image. The image will be tagged with the name <kbd>product-service</kbd>&nbsp;and stored locally inside the Docker engine.</span></p>
<p>Verify that we got a Docker image, as expected, by using the following command:</p>
<pre><strong>docker images | grep product-service</strong></pre>
<p>The expected output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/81cd92d2-7f8a-42a7-8e75-bfd20c69a1a6.png" style="width:28.58em;height:4.58em;" width="605" height="97" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/81cd92d2-7f8a-42a7-8e75-bfd20c69a1a6.png"></p>
<p>So now that we have built the image, lets see how we can start the service.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Starting up the service</h1>
                </header>
            
            <article>
                
<p>Let's start up the <kbd>product</kbd> microservice as a container by using the following command:</p>
<pre><strong>docker run --rm -p8080:8080 -e "SPRING_PROFILES_ACTIVE=docker" product-service</strong></pre>
<p>This is what we can infer from the preceding code:</p>
<ol>
<li><kbd>docker run</kbd>: The Docker run command will start the container and display log output in Terminal. Terminal will be locked as long as the container runs.</li>
<li>We have seen the <kbd>--rm</kbd> option already; it will tell Docker to clean up the container once we stop the execution from Terminal using <em>Ctrl + C</em>.</li>
<li>The&nbsp;<kbd>-p8080:8080</kbd>&nbsp;option maps port <kbd>8080</kbd> in the container to port <kbd>8080</kbd> in the Docker host, which makes it possible to call it from the outside. In the case of Docker for macOS, which runs Docker in a local Linux virtual machine, the port will also be port-forwarded to macOS, which is made available on localhost. We can only have one container mapping to a specific port in the Docker host!</li>
</ol>
<ol start="4">
<li>With the <kbd>-e</kbd> option, we can specify environment variables for the container, which in this case is&nbsp;<kbd>SPRING_PROFILES_ACTIVE=docker</kbd>. The&nbsp;<span><kbd>SPRING_PROFILES_ACTIVE</kbd>&nbsp;environment variable is used to tell Spring what profile to use. In our case, we want Spring to use the <kbd>docker</kbd> profile.</span></li>
<li>Finally, we have&nbsp;<kbd>product-service</kbd>, which is the name of the Docker image that Docker will use to start the container.</li>
</ol>
<p>The expected output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6418f725-7182-4415-a87c-283962b36cad.png" width="1490" height="604" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6418f725-7182-4415-a87c-283962b36cad.png"></p>
<p>This is what we infer from the preceding output:</p>
<ul>
<li>The profile that's used by Spring is <kbd>docker</kbd>. Look for <kbd>The following profiles are active: docker</kbd> in the output to verify this.</li>
<li>The port that's allocated by the container is <kbd>8080</kbd>. Look for&nbsp;<kbd>Netty started on port(s): 8080</kbd>&nbsp;in the output to verify this.</li>
<li><span>The microservice is ready to accept requests once the log message</span> <kbd>Started ProductServiceApplication</kbd>&nbsp;has been written!&nbsp;</li>
</ul>
<p>Try out the following code in another Terminal window:</p>
<pre><strong>curl localhost:8080/product/3</strong></pre>
<p>Note that we can use port <kbd>8080</kbd> on localhost, as explained previously!</p>
<p>The following is the expected output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/da91fca5-5223-4ded-b277-26ef0ea17866.png" width="932" height="102" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/da91fca5-5223-4ded-b277-26ef0ea17866.png"></p>
<p>This is similar to the output we received from the previous chapter, but with one major difference; we have the content of <kbd>"service Address":"aebb42b32fef/172.17.0.2:8080"</kbd>, the port is <kbd>8080</kbd>, as expected, and the IP address,&nbsp;<span><kbd>172.17.0.2</kbd>,&nbsp;</span>is an IP address that's been allocated to the container from an internal network in Docker <span>â bu</span>t where did the hostname,&nbsp;<span><kbd>aebb42b32fef</kbd>, c</span>ome from?</p>
<p>Ask Docker for all the running containers:</p>
<pre><strong>docker ps</strong></pre>
<p>We will see something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ab9eb9ee-8dc6-4e0c-bcd2-14269175f84c.png" width="1446" height="123" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ab9eb9ee-8dc6-4e0c-bcd2-14269175f84c.png"></p>
<p>As we can see from the preceding output that, the hostname is equivalent to the ID of the container, which is good to know if you want to understand what container actually responded to your request!</p>
<p>Wrap this up by stopping the container in Terminal with the&nbsp;<em>Ctrl + C</em>&nbsp;command. With this done, we can now move on to running the container detached while being detached.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Starting up the service</h1>
                </header>
            
            <article>
                
<p>Let's start up the <kbd>product</kbd> microservice as a container by using the following command:</p>
<pre><strong>docker run --rm -p8080:8080 -e "SPRING_PROFILES_ACTIVE=docker" product-service</strong></pre>
<p>This is what we can infer from the preceding code:</p>
<ol>
<li><kbd>docker run</kbd>: The Docker run command will start the container and display log output in Terminal. Terminal will be locked as long as the container runs.</li>
<li>We have seen the <kbd>--rm</kbd> option already; it will tell Docker to clean up the container once we stop the execution from Terminal using <em>Ctrl + C</em>.</li>
<li>The <kbd>-p8080:8080</kbd> option maps port <kbd>8080</kbd> in the container to port <kbd>8080</kbd> in the Docker host, which makes it possible to call it from the outside. In the case of Docker for macOS, which runs Docker in a local Linux virtual machine, the port will also be port-forwarded to macOS, which is made available on localhost. We can only have one container mapping to a specific port in the Docker host!</li>
</ol>
<ol start="4">
<li>With the <kbd>-e</kbd> option, we can specify environment variables for the container, which in this case is <kbd>SPRING_PROFILES_ACTIVE=docker</kbd>. The <span><kbd>SPRING_PROFILES_ACTIVE</kbd> environment variable is used to tell Spring what profile to use. In our case, we want Spring to use the <kbd>docker</kbd> profile.</span></li>
<li>Finally, we have <kbd>product-service</kbd>, which is the name of the Docker image that Docker will use to start the container.</li>
</ol>
<p>The expected output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6418f725-7182-4415-a87c-283962b36cad.png" width="1490" height="604" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6418f725-7182-4415-a87c-283962b36cad.png"></p>
<p>This is what we infer from the preceding output:</p>
<ul>
<li>The profile that's used by Spring is <kbd>docker</kbd>. Look for <kbd>The following profiles are active: docker</kbd> in the output to verify this.</li>
<li>The port that's allocated by the container is <kbd>8080</kbd>. Look for <kbd>Netty started on port(s): 8080</kbd> in the output to verify this.</li>
<li><span>The microservice is ready to accept requests once the log message</span> <kbd>Started ProductServiceApplication</kbd> has been written! </li>
</ul>
<p>Try out the following code in another Terminal window:</p>
<pre><strong>curl localhost:8080/product/3</strong></pre>
<p>Note that we can use port <kbd>8080</kbd> on localhost, as explained previously!</p>
<p>The following is the expected output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/da91fca5-5223-4ded-b277-26ef0ea17866.png" width="932" height="102" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/da91fca5-5223-4ded-b277-26ef0ea17866.png"></p>
<p>This is similar to the output we received from the previous chapter, but with one major difference; we have the content of <kbd>"service Address":"aebb42b32fef/172.17.0.2:8080"</kbd>, the port is <kbd>8080</kbd>, as expected, and the IP address, <span><kbd>172.17.0.2</kbd>,&nbsp;</span>is an IP address that's been allocated to the container from an internal network in Docker <span>â bu</span>t where did the hostname, <span><kbd>aebb42b32fef</kbd>, c</span>ome from?</p>
<p>Ask Docker for all the running containers:</p>
<pre><strong>docker ps</strong></pre>
<p>We will see something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ab9eb9ee-8dc6-4e0c-bcd2-14269175f84c.png" width="1446" height="123" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ab9eb9ee-8dc6-4e0c-bcd2-14269175f84c.png"></p>
<p>As we can see from the preceding output that, the hostname is equivalent to the ID of the container, which is good to know if you want to understand what container actually responded to your request!</p>
<p>Wrap this up by stopping the container in Terminal with the <em>Ctrl + C</em> command. With this done, we can now move on to running the container detached while being detached.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running the container detached</h1>
                </header>
            
            <article>
                
<p>Okay, that was great, but what if we don't want to hang the Terminal windows from where we started the container?</p>
<p>It's time to start the container as detached, that is, running the container without locking Terminal!</p>
<p>We can do this by adding the <kbd>-d</kbd> option and at the same time giving it a name using the <kbd>--name</kbd> option. The <kbd>--rm</kbd> option is no longer required since we will stop and remove the container explicitly when we are done with it:</p>
<pre><strong>docker run -d -p8080:8080 -e "SPRING_PROFILES_ACTIVE=docker" --name my-prd-srv product-service</strong></pre>
<p>If we run the <kbd>docker ps</kbd> command again, we will see our new container, called <kbd>my-prd-srv</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a683c0b3-0d98-4e01-afad-b521ee76cd42.png" width="1408" height="121" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a683c0b3-0d98-4e01-afad-b521ee76cd42.png"></p>
<p>But how do we get the log output from our container?</p>
<p>Meet the Docker <kbd>logs</kbd> command:</p>
<pre class="mce-root"><strong>docker logs my-prd-srv -f</strong></pre>
<p class="mce-root">The <kbd>-f</kbd> option tells the command to follow the log output, that is, not end the command when all the current log output has been written to Terminal, but also wait for more output. If you expect a lot of old log messages that you don't want to see, you can also add the <kbd>--tail 0</kbd> option so that you only see new log messages. Alternatively, you can use the <kbd>--since</kbd> option and use either an absolute timestamp or a relative time, for example, <kbd>--since 5m</kbd>, to see log messages that are at most five minutes old.</p>
<p>Try this out with a new <kbd>curl</kbd> request. You should see that a new log message has been written to the log output in Terminal!</p>
<p>Wrap this up by stopping and removing the container:</p>
<pre><strong>docker rm -f my-prd-srv</strong></pre>
<p>The <kbd>-f</kbd> option forces Docker to remove the container, even if it is running. Docker will automatically stop the container before it removes it.</p>
<p>Now that we know how to use Docker with a microservice, we can now see how to manage a microservices landscape with the help of Docker Compose and see the changes in it.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Managing a landscape of microservices using Docker Compose</h1>
                </header>
            
            <article>
                
<p><span>We've already seen how we can</span> run a single microservice as a Docker container, but what about managing a whole system landscape of microservices?</p>
<p>As we mentioned earlier, this is the purpose of <kbd>docker-compose</kbd>. By using single commands, we can build, start, log, and stop a group of cooperating microservices running as Docker containers!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the source code</h1>
                </header>
            
            <article>
                
<p><span>To be able to use Docker Compose, we need to create a configuration file, </span><kbd>docker-compose.yml</kbd><span>, that describes the microservices</span> <span>Docker Compose will manage for us. We also need to set up Dockerfiles for the remaining microservices and add a Docker-specific Spring profile to each of them.</span></p>
<p>All four microservices have their own Dockerfile, but they all look the same as the preceding one. You can find them here:</p>
<ul>
<li><kbd><span>$BOOK_HOME/</span><span>Chapter04/microservices/product-service/Dockerfile</span></kbd></li>
<li><kbd><span>$BOOK_HOME/</span><span>Chapter04/microservices/</span>recommendation-service/Dockerfile</kbd></li>
<li><kbd><span>$BOOK_HOME/</span><span>Chapter04/microservices/</span>review-service/Dockerfile</kbd></li>
<li><kbd><span>$BOOK_HOME/</span><span>Chapter04/microservices/</span>product-composite-service/Dockerfile</kbd></li>
</ul>
<p>When it comes to the Spring profiles, the three core services, <kbd>product</kbd>, <kbd>recommendation</kbd>, and <kbd>review-service</kbd>, have the same <kbd>docker</kbd> profile, which only specifies that the default port <kbd>8080</kbd> should be used when running as a container.</p>
<p>For <kbd>product-composite-service</kbd>, things are a bit more complicated since it needs to know where to find the core services. When we ran all the services on localhost, it was configured to use localhost and individual port numbers, <kbd>7001</kbd>-<kbd>7003</kbd>, for each core service. When running in Docker, each service will have its own hostname but will be accessible on the same port number, <kbd>8080</kbd>. Here, the <kbd>docker</kbd> profile for <span><kbd>product-composite-service</kbd> looks as follows:</span></p>
<pre>---<br><span>spring.profiles</span>: docker<br><br><span>server.port</span>: <span>8080<br></span><span><br></span><span>app</span>:<br>  <span>product-service</span>:<br>    <span>host</span>: product<br>    <span>port</span>: 8080<br>  <span>recommendation-service</span>:<br>    <span>host</span>: recommendation<br>    <span>port</span>: 8080<br>  <span>review-service</span>:<br>    <span>host</span>: review<br>    <span>port</span>: 8080</pre>
<p>See <kbd>$BOOK_HOME/Chapter04/microservices/product-composite-service/src/main/resources/application.yml</kbd> for details.</p>
<p>Where did the hostnames, products, recommendations, and reviews come from?</p>
<p>These are specified in the <span><kbd>docker-compose.yml</kbd> file, which is located in the <kbd>$BOOK_HOME/Chapter04</kbd> folder. It looks like this:</span></p>
<pre><span>version</span>: <span>'2.1'<br></span><span><br></span><span>services</span>:<br>  <span>product</span>:<br>    <span>build</span>: microservices/product-service<br>    mem_limit: 350m<br>    <span>environment</span>:<br>      - SPRING_PROFILES_ACTIVE=docker<br><br>  <span>recommendation</span>:<br>    <span>build</span>: microservices/recommendation-service<br>    mem_limit: 350m<br>    <span>environment</span>:<br>      - SPRING_PROFILES_ACTIVE=docker<br><br>  <span>review</span>:<br>    <span>build</span>: microservices/review-service<br>    mem_limit: 350m<br>    <span>environment</span>:<br>      - SPRING_PROFILES_ACTIVE=docker<br><br>  <span>product-composite</span>:<br>    <span>build</span>: microservices/product-composite-service<br>    mem_limit: 350m<br>    <span>ports</span>:<br>      - <span>"8080:8080"<br></span><span>    </span><span>environment</span>:<br>      - SPRING_PROFILES_ACTIVE=docker</pre>
<p>For each microservice, we specify the following:</p>
<ul>
<li>The name of the microservice. This will also be the hostname of the container in the internal Docker network.</li>
<li>A build directive that specifies where to find the Dockerfile that was used to build the Docker image.</li>
<li>A memory limit of 350 MB. This ensures that all containers in this and the upcoming chapters will fit in the 6 GB of memory that we allocated to the Docker engine in the <em>Technical requirements</em> section.</li>
<li>The environment variables that will be set up for the container. In our case, we used these to specify what Spring profile to use.</li>
</ul>
<p>For the <kbd>product-composite</kbd> service, we will also specify port mappings, that is, we will expose its <span>port to the outside of Docker. The other</span> microservices will not be accessible from the outside. Next, we will see how to start up a microservice landscape.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Starting up the microservice landscape</h1>
                </header>
            
            <article>
                
<p>With all the necessary code changes in place, we can build our Docker images, start up the microservice landscape, and run some tests to verify that it works as expected. For this, we need to do the following:</p>
<ol>
<li>First, we build our deployment artifacts with Gradle and then the Docker images with Docker Compose:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>cd $BOOK_HOME/</span><span>Chapter04<br></span><span>./gradlew build<br></span><span>docker-compose build</span></strong></pre>
<ol start="2">
<li>Then, we need to verify that we can see our Docker images, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker images | grep chapter04</strong></pre>
<ol start="3">
<li>We should see the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/75213781-b3ea-4f66-84b9-62727717d4c5.png" style="width:41.33em;height:9.08em;" width="747" height="164" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/75213781-b3ea-4f66-84b9-62727717d4c5.png"></p>
<ol start="4">
<li>Start up the microservices landscape with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>docker-compose up -d</span></strong></pre>
<div class="packt_infobox">The <kbd>-d</kbd> option means the same as for Docker, as described previously.</div>
<p>We can follow the startup by monitoring the output that's written to each container log with the following command:</p>
<pre><strong><span>docker-compose </span><span>logs -f</span></strong></pre>
<div class="packt_infobox">The <kbd>docker compose logs</kbd> command supports the same <span><kbd>-f</kbd> and <kbd>--tail</kbd>&nbsp;</span>options as <kbd>docker logs</kbd>, as described earlier.<br>
<br>
The Docker Compose <kbd>logs</kbd> command also supports restricting the log output to a group of containers. Simply add the names of the containers you want to see the log output of after the <kbd>logs</kbd> command. For example, t<span>o only see log output from the <kbd>product</kbd> and <kbd>review</kbd> service, use</span> <kbd>docker-compose logs -f product review</kbd>.</div>
<p>When all four microservices have reported that they have started up, we are ready to try out the microservices landscape. Look for the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/760e3b57-8782-48c5-90ef-7ec483ef7f0e.png" width="943" height="351" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/760e3b57-8782-48c5-90ef-7ec483ef7f0e.png"></p>
<p>&nbsp;</p>
<div class="packt_infobox">Note that each log message is prefixed with the name of the container that produces the output!</div>
<p>Now, we are ready to run some tests to verify that this works as expected. <span>The only change we need to make when calling the composite service in Docker from when we ran it directly on the localhost, as we did in the previous chapter, is the port number. We now use port <kbd>8080</kbd>:</span></p>
<pre><strong>curl localhost:8080/product-composite/123 -s | jq .</strong></pre>
<p>We will get the same type of response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ea65b614-de1e-426a-83dc-b750d6cef6be.png" style="width:27.42em;height:11.83em;" width="549" height="236" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ea65b614-de1e-426a-83dc-b750d6cef6be.png"></p>
<p>However, there's one big difference <span>â</span> the hostnames and ports reported by <kbd>serviceAddresses</kbd> in the response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/60a1a060-0f7d-4bb9-9088-89a1c926ae2b.png" style="width:26.00em;height:9.67em;" width="441" height="164" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/60a1a060-0f7d-4bb9-9088-89a1c926ae2b.png"></p>
<p>Here, we can see the hostnames and IP addresses that have been allocated to each of the Docker containers.</p>
<p>We're done; now only one step is left:</p>
<pre><strong><span>docker-compose</span><span> </span><span>down<br></span></strong></pre>
<p><span>The preceding command will shut down the microservices landscape.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing them all <span class="annotator-hl">together </span>automatically</h1>
                </header>
            
            <article>
                
<p>Docker Compose is really helpful when it comes to manually managing a group of microservices! In this section, we will take this one step further and integrate Docker Compose into our test script, <kbd>test-em-all.bash</kbd>. The test script will automatically start up the microservice landscape, run all the required tests to verify that the <span>microservice landscape works as expected, and finally tear it down, leaving no traces behind.</span></p>
<p>The test script can be found at <kbd><span>$BOOK_HOME/Chapter04/</span>test-em-all.bash</kbd>.</p>
<p>Before the test script runs the test suite, it will check for the presence of a <kbd>start</kbd> argument in the invocation of the test script. If found, it will restart the containers with the following code:</p>
<pre><span>if [[ </span>$@ <span>== </span>*<span>"start"</span>*<span> ]]<br></span><span>then<br></span><span>    </span><span>echo </span><span>"Restarting the test environment..."<br></span><span>    </span><span>echo </span><span>"$ docker-compose down"<br></span><span>    </span>docker-compose down<br>    <span>echo </span><span>"$ docker-compose up -d"<br></span><span>    </span>docker-compose up -d<br><span>fi<br></span></pre>
<p>After that, the test script will wait for the <kbd>product-composite</kbd> service to respond with OK:</p>
<pre><span>waitForService </span>http://$HOST:<span>$</span>{PORT}/product-composite/1</pre>
<p>The <span><kbd>waitForService</kbd> bash function can be implemented like so:</span></p>
<pre><span>function </span><span>testUrl</span>() {<br>    url=$@<br>    <span>if </span>curl $url -ks -f -o /dev/null<br>    <span>then<br></span><span>          </span><span>echo </span><span>"Ok"<br></span><span>          </span><span>return </span><span>0<br></span><span>    </span><span>else<br></span><span>          </span><span>echo </span>-n <span>"not yet"<br></span><span>          </span><span>return </span><span>1<br></span><span>    </span><span>fi</span>;<br>}<br><br><span>function </span><span>waitForService</span>() {<br>    url=$@<br>    <span>echo </span>-n <span>"Wait for: $url... "<br></span><span>    </span>n=<span>0<br></span><span>    </span><span>until </span><span>testUrl </span>$url<br>    <span>do<br></span><span>        n=$((n + 1))</span><span><br></span><span>        if [[ </span>$n <span>== </span><span>10</span>0<span> ]]<br></span><span>        then<br></span><span>            </span><span>echo </span><span>" Give up"<br></span><span>            </span><span>exit </span><span>1<br></span><span>        </span><span>else<br></span><span>            </span>sleep <span>6<br></span><span>            </span><span>echo </span>-n <span>", retry #$n "<br></span><span>        </span><span>fi<br></span><span>    done<br></span>}</pre>
<p>Next, all the tests are executed like they were previously. Afterward, they will tear down the landscape if it finds the <kbd>stop</kbd> argument in the invocation of the test scripts:</p>
<pre><span>if [[ </span>$@ <span>== </span>*<span>"stop"</span>*<span> ]]<br></span><span>then<br></span><span>    </span><span>echo </span><span>"We are done, stopping the test environment..."<br></span><span>    </span><span>echo </span><span>"$ docker-compose down"<br></span><span>    </span>docker-compose down<br><span>fi</span></pre>
<div class="packt_infobox">Note that the test script will not tear down the landscape if some tests fail; it will simply stop, leaving the landscape up for error analysis!</div>
<p>The test script has also changed the default port from <kbd>7000</kbd>, which we used when we ran the microservices without Docker, to <kbd>8080</kbd>, which is used by our Docker containers.</p>
<p>Let's try it out! To start the landscape, run the tests and tear it down afterward, like so:</p>
<pre><strong>./test-em-all.bash start stop</strong></pre>
<p>The following is some sample output from a test run (with output from the specific tests that were deleted):</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/16df5488-1e1d-45e3-850e-fad5f66cdbaf.png" width="957" height="696" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/16df5488-1e1d-45e3-850e-fad5f66cdbaf.png"></p>
<p>After testing these, we can now move on to see how to troubleshoot tests that fail.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Troubleshooting a test run</h1>
                </header>
            
            <article>
                
<p>If the tests that were running <kbd>./test-em-all.bash start stop</kbd> fail, following these steps can help you identify the problem and resume the tests once the problem has been fixed:</p>
<ol>
<li>First, check the status of the running microservices with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>docker-compose ps</span></strong></pre>
<ol start="2">
<li>If all the <span>microservices are up and running and healthy, you will receive the following output:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8cdb0618-6760-4126-a3f5-abb60989b739.png" width="1695" height="459" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8cdb0618-6760-4126-a3f5-abb60989b739.png"></p>
<ol start="3">
<li class="mce-root"><span>If any of the </span>microservices <span>do not have a status of </span><kbd>Up</kbd><span>, check its log output for any errors by using the</span> <kbd>docker-compose logs</kbd><span> command. For example, you would use the following code if you wanted to check the log output for the <kbd>product</kbd> service:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>docker-compose logs product</strong></pre>
<ol start="4">
<li class="mce-root"><span>If errors in the log output indicate that Docker is running out of disk space, parts of it can be reclaimed with the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>docker system prune -f --volumes</strong></pre>
<ol start="5">
<li>If required, you can restart a failed <span>microservice</span> with the <span><kbd>docker-compose up -d --scale</kbd> command. </span>For example, you would use the following code if you wanted to restart the <kbd>product</kbd> service:<span><span><br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong><span>docker-compose up -d --scale product=0<br>docker-compose up -d --scale product=1</span></strong></pre>
<ol start="6">
<li>If a <span>microservice</span> is missing, for example, due to a crash, you start it up with the <span><span><kbd>docker-compose up -d --scale</kbd> command. For example, you would use the following code for the <kbd>product</kbd> service:<br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong><span>docker-compose up -d --scale product=1</span></strong></pre>
<ol start="7">
<li>Once all the <span>microservices</span> are up and running and healthy, run the test script again, but without starting the microservices:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>./test-em-all.bash</span></strong></pre>
<p style="padding-left: 60px">The tests should run fine!</p>
<div class="packt_tip">Finally, a tip about a combined command that builds runtime artifacts and Docker images from source and then runs all the tests in Docker:<br>
<br>
<span><kbd>./gradlew clean build &amp;&amp; docker-compose build &amp;&amp; ./test-em-all.bash start stop</kbd><br>
<br>
This is perfect if you want to check that everything works before you push new code to your Git repository or as part of a build pipeline in your build server!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the major differences between a virtual machine and a Docker container?</li>
<li>What is the purpose of namespaces and cgroups in Docker?</li>
<li>What happens with a Java application that doesn't honor the max memory settings in a container and allocates more memory than it is allowed to?</li>
<li>How can we make a Spring-based application run as a Docker container without requiring modifications of its source code?</li>
<li>Why will the following Docker Compose code snippet not work?</li>
</ol>
<pre style="padding-left: 60px"><span>  review</span>:<br>    <span>build</span>: microservices/review-service<br>    <span>ports</span>:<br>      - <span>"8080:8080"<br></span><span>    environment</span>:<br>      - SPRING_PROFILES_ACTIVE=docker<br><br>  <span>product-composite</span>:<br>    <span>build</span>: microservices/product-composite-service<br>    <span>ports</span>:<br>      - <span>"8080:8080"<br></span><span>    </span><span>environment</span>:<br>      - SPRING_PROFILES_ACTIVE=docker</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the major differences between a virtual machine and a Docker container?</li>
<li>What is the purpose of namespaces and cgroups in Docker?</li>
<li>What happens with a Java application that doesn't honor the max memory settings in a container and allocates more memory than it is allowed to?</li>
<li>How can we make a Spring-based application run as a Docker container without requiring modifications of its source code?</li>
<li>Why will the following Docker Compose code snippet not work?</li>
</ol>
<pre style="padding-left: 60px"><span>  review</span>:<br>    <span>build</span>: microservices/review-service<br>    <span>ports</span>:<br>      - <span>"8080:8080"<br></span><span>    environment</span>:<br>      - SPRING_PROFILES_ACTIVE=docker<br><br>  <span>product-composite</span>:<br>    <span>build</span>: microservices/product-composite-service<br>    <span>ports</span>:<br>      - <span>"8080:8080"<br></span><span>    </span><span>environment</span>:<br>      - SPRING_PROFILES_ACTIVE=docker</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All of the commands that are described in this book<span>&nbsp;are run on a MacBook Pro using macOS Mojave, but should be straightforward to modify if you want to run them on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed before you can work through this chapter.</p>
<p>The source code for this chapter&nbsp;can be found i<span>n this book's GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter05">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter05</a>.</p>
<p>To be able to run the commands that are described in this book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter05</span></strong></pre>
<p>The Java source code in this book has been written for Java 8 and tested&nbsp;on Java 12.&nbsp;This chapter uses Spring Boot 2.1.0 (and Spring 5.1.2), which is the latest available version of Spring Boot at the time of writing this chapter.</p>
<p>The code examples in this chapter all come from the source code in&nbsp;<span><kbd>$BOOK_HOME/Chapter05</kbd>&nbsp;but have been, in many cases, edited to remove irrelevant parts of the source code, such as comments, imports, and log statements.</span></p>
<p>If you want to view the changes that were applied to the source code in this chapter,&nbsp;that is, see&nbsp;<span>what it took to create Swagger-based API documentation using SpringFox,&nbsp;</span>you can compare it with the source code for <a href="ce37c0f1-ac54-4258-8cb6-7c6f50f26172.xhtml">Chapter 4</a>, <em>Deploying Our Microservices Using Docker</em>.<span>&nbsp;You can use your favorite&nbsp;</span><kbd>diff</kbd><span>&nbsp;tool and compare the two folders, that is,&nbsp;</span><kbd>$BOOK_HOME/Chapter04</kbd><span>&nbsp;and&nbsp;</span><kbd>$BOOK_HOME/Chapter05</kbd><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All of the commands that are described in this book<span> are run on a MacBook Pro using macOS Mojave, but should be straightforward to modify if you want to run them on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed before you can work through this chapter.</p>
<p>The source code for this chapter can be found i<span>n this book's GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter05">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter05</a>.</p>
<p>To be able to run the commands that are described in this book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter05</span></strong></pre>
<p>The Java source code in this book has been written for Java 8 and tested on Java 12. This chapter uses Spring Boot 2.1.0 (and Spring 5.1.2), which is the latest available version of Spring Boot at the time of writing this chapter.</p>
<p>The code examples in this chapter all come from the source code in <span><kbd>$BOOK_HOME/Chapter05</kbd> but have been, in many cases, edited to remove irrelevant parts of the source code, such as comments, imports, and log statements.</span></p>
<p>If you want to view the changes that were applied to the source code in this chapter, that is, see <span>what it took to create Swagger-based API documentation using SpringFox, </span>you can compare it with the source code for <a href="ce37c0f1-ac54-4258-8cb6-7c6f50f26172.xhtml">Chapter 4</a>, <em>Deploying Our Microservices Using Docker</em>.<span> You can use your favorite </span><kbd>diff</kbd><span> tool and compare the two folders, that is, </span><kbd>$BOOK_HOME/Chapter04</kbd><span> and </span><kbd>$BOOK_HOME/Chapter05</kbd><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the source code</h1>
                </header>
            
            <article>
                
<p>To add Swagger-based documentation about the external API that's exposed by the <span><kbd>product-composite-service</kbd>&nbsp;microservice, we need to change the source code in&nbsp;two modules:</span></p>
<ul>
<li><kbd><span>product-composite-service</span>s</kbd>: Here, we will set up a SpringFox configuration in the Java application class,&nbsp;<kbd>ProductCompositeServiceApplication</kbd>, and describe general information about the API.&nbsp;</li>
<li><kbd>api</kbd>: Here, we will add Swagger annotations to the Java interface,&nbsp;<kbd>ProductCompositeService</kbd>, describing each RESTful service. At this stage, we only have one RESTful service,&nbsp;<kbd>/product-composite/{productId}</kbd>, which is used for&nbsp;<span>requesting composite information regarding a specific product.</span></li>
</ul>
<p>The actual texts that are used to describe the API operation will be placed in the default property file,&nbsp;<kbd>application.yml</kbd>.</p>
<p>Before we can start using SpringFox, we need to add it as a dependency in the Gradle build files. So, let's start with that!</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the source code</h1>
                </header>
            
            <article>
                
<p>To add Swagger-based documentation about the external API that's exposed by the <span><kbd>product-composite-service</kbd> microservice, we need to change the source code in two modules:</span></p>
<ul>
<li><kbd><span>product-composite-service</span>s</kbd>: Here, we will set up a SpringFox configuration in the Java application class, <kbd>ProductCompositeServiceApplication</kbd>, and describe general information about the API. </li>
<li><kbd>api</kbd>: Here, we will add Swagger annotations to the Java interface, <kbd>ProductCompositeService</kbd>, describing each RESTful service. At this stage, we only have one RESTful service, <kbd>/product-composite/{productId}</kbd>, which is used for <span>requesting composite information regarding a specific product.</span></li>
</ul>
<p>The actual texts that are used to describe the API operation will be placed in the default property file, <kbd>application.yml</kbd>.</p>
<p>Before we can start using SpringFox, we need to add it as a dependency in the Gradle build files. So, let's start with that!</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding dependencies to the Gradle build files</h1>
                </header>
            
            <article>
                
<p>As we mentioned previously, we will use a snapshot version of SpringFox V3. The SpringFox product is divided into a number of modules. The modules that we need to specify dependencies are as follows:</p>
<ul>
<li><kbd>springfox-swagger2</kbd>, so that we can create Swagger 2-based documentation</li>
<li><span><kbd>springfox-spring-webflux</kbd>, so that we can support the use of Spring WebFlux-based RESTful operations</span></li>
<li><kbd>springfox-swagger-ui</kbd>, so that we can embed a Swagger viewer in our microservice</li>
</ul>
<p><span>We can add these to the Gradle build file, <kbd>build.gradle</kbd>, for the <kbd>product-composite-service</kbd> module as follows:</span></p>
<pre>implementation(<span>'io.springfox:springfox-swagger2:3.0.0-SNAPSHOT'</span>)<br>implementation(<span>'io.springfox:springfox-swagger-ui:3.0.0-SNAPSHOT'</span>)<br>implementation(<span>'io.springfox:springfox-spring-webflux:3.0.0-SNAPSHOT'</span>)</pre>
<p><span>The <kbd>api</kbd> project only needs one dependency for the <kbd>springfox-swagger2</kbd> module, and so only the following dependency needs to be added to its <kbd>build.gradle</kbd> file:</span></p>
<pre>implementation(<span>'io.springfox:springfox-swagger2:3.0.0-SNAPSHOT'</span>)</pre>
<p>The SpringFox project publishes snapshot builds in the Maven repository (<a href="http://oss.jfrog.org/artifactory/oss-snapshot-local/">http://oss.jfrog.org/artifactory/oss-snapshot-local/</a>), so we need to add that as well:</p>
<pre>repositories {<br>   mavenCentral()<br>   maven { url <span>'http://oss.jfrog.org/artifactory/oss-snapshot-local/' </span>}<br>}</pre>
<p>To be able to build the core modules, that is, <kbd>product-service</kbd>,&nbsp;<kbd>recommendation-service</kbd>, and <kbd>review-service</kbd>, we need to add the Maven repository to their Gradle build files, <kbd>build.gradle</kbd>, as well.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding configuration and general API documentation to Product Composite Service Application</h1>
                </header>
            
            <article>
                
<p>&nbsp;</p>
<p>To enable SpringFox in the <kbd>product-composite-service</kbd><span> microservice, we have to add a configuration. To keep the source code compact, we will add it directly to the <kbd>ProductCompositeServiceApplication</kbd> application class.</span></p>
<div class="packt_infobox"><span>If you prefer, you can place the configuration of SpringFox in a separate Spring configuration class.</span></div>
<p>&nbsp;</p>
<p>First, we need to add the <kbd>@EnableSwagger2WebFlux</kbd> annotation in order to get SpringFox to generate Swagger V2 documentation for our RESTful services, which is implemented using Spring WebFlux. Next, we need to define a Spring Bean that returns a SpringFox <kbd>Docket</kbd> bean, which is used to configure <span>SpringFox.</span></p>
<p><span>The source code that we will be adding to <kbd>$BOOK_HOME/Chapter05/microservices/product-composite-service/src/main/java/se/magnus/microservices/composite/product/ProductCompositeServiceApplication.java</kbd> looks as follows:</span></p>
<div>
<pre><span>@EnableSwagger2WebFlux</span><br><span>public class </span>ProductCompositeServiceApplication {<br><br><span>   </span><span>@Bean<br></span><span>   </span><span>public </span>Docket apiDocumentation() {<br>      <span>return new </span>Docket(<span>SWAGGER_2</span>)<br>         .select()<br>         .apis(<span>basePackage</span>(<span>"se.magnus.microservices.composite.product"</span>))<br>         .paths(PathSelectors.<span>any</span>())<br>         .build()<br>            .globalResponseMessage(<span>GET</span>, <span>emptyList</span>())<br>            .apiInfo(<span>new </span>ApiInfo(<br>                   <span>apiTitle</span>,<br>                   <span>apiDescription</span>,<br>                   <span>apiVersion</span>,<br>                   <span>apiTermsOfServiceUrl</span>,<br>                   <span>new </span>Contact(<span>apiContactName</span>, <span>apiContactUrl</span>, <br>                    <span>apiContactEmail</span>),<br>                   <span>apiLicense</span>,<br>                   <span>apiLicenseUrl</span>,<br>                   <span>emptyList</span>()<br>                                  ));<br>    }</pre></div>
<p>From the preceding code, we can understand the following:</p>
<ul>
<li>The <kbd>@EnableSwagger2WebFlux</kbd> annotation is the starting point for initiating SpringFox.</li>
<li>The <kbd>Docket</kbd> bean is initiated to create Swagger V2 documentation.</li>
<li>Using the <kbd>apis()</kbd> and <kbd>paths()</kbd> methods, we can specify where SpringFox shall look for API documentation.</li>
<li>Using the <kbd>globalResponseMessage()</kbd> method, we ask SpringFox not to add any default HTTP response codes to the API documentation, such as <kbd>401</kbd> and <kbd>403</kbd>, which we don't currently use.</li>
<li>The <kbd>api*</kbd><span> variables that are used to configure the <kbd>Docket</kbd> bean with general information about the API are initialized from the property file using Spring</span> <kbd>@Value</kbd>&nbsp;<span>annotations. These are as follows:</span></li>
</ul>
<pre>    <span>@Value</span>(<span>"${api.common.version}"</span>)           String <span>apiVersion</span>;<br>    <span>@Value</span>(<span>"${api.common.title}"</span>)             String <span>apiTitle</span>;<br>    <span>@Value</span>(<span>"${api.common.description}"</span>)       String <span>apiDescription</span>;<br>    <span>@Value</span>(<span>"${api.common.termsOfServiceUrl}"</span>) String <br>                                              <span>apiTermsOfServiceUrl</span>;<br>    <span>@Value</span>(<span>"${api.common.license}"</span>)           String <span>apiLicense</span>;<br>    <span>@Value</span>(<span>"${api.common.licenseUrl}"</span>)        String <span>apiLicenseUrl</span>;<br>    <span>@Value</span>(<span>"${api.common.contact.name}"</span>)      String <span>apiContactName</span>;<br>    <span>@Value</span>(<span>"${api.common.contact.url}"</span>)       String <span>apiContactUrl</span>;<br>    <span>@Value</span>(<span>"${api.common.contact.email}"</span>)     String <span>apiContactEmail</span>;</pre>
<p>After adding a configuration and API documentation, we can now proceed to understand how to add an API specific documentation to ProductCompositeService.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding API-specific documentation to ProductCompositeService</h1>
                </header>
            
            <article>
                
<p>To document the actual API, <kbd><span>ProductCompositeService</span></kbd>, and its RESTful operations, we will add an <kbd>@Api</kbd><span> annotation to the Java interface declaration so that we can provide a general description of the API. For each RESTful operation in the API, we will add an <kbd>@ApiOperation</kbd> annotation, along with <kbd>@ApiResponse</kbd> annotations on the corresponding Java method, to describe the operation and its expected error responses.</span></p>
<p class="mce-root"></p>
<p><span>SpringFox will inspect the <kbd>@GetMapping</kbd> Spring annotation </span><span>to understand what input argument the operation takes and what the response will look like if a successful response is produced.</span></p>
<p>In the following example, we have extracted the actual text from the <kbd>@ApiOperation</kbd><span> annotation</span> to a property file. The annotation contains <span>property placeholders that SpringFox will use to look up the actual text from the property files at runtime.</span></p>
<p>The documentation of the API on the resource level appears as follows:</p>
<pre><span>@Api</span>(description = <span>"REST API for composite product information."</span>)<br><span>public interface </span>ProductCompositeService {</pre>
<p>The single API operation is documented as follows:</p>
<pre><span>    </span><span>@ApiOperation</span>(<br>        value = <span>"${api.product-composite.get-composite-<br>         product.description}"</span>,<br>        notes = <span>"${api.product-composite.get-composite-product.notes}"</span>)<br>    <span>@ApiResponses</span>(value = {<br>        <span>@ApiResponse</span>(code = <span>400</span>, message = <span>"Bad Request, invalid format <br>        of the request. See response message for more information."</span>),<br>        <span>@ApiResponse</span>(code = <span>404</span>, message = <span>"Not found, the specified id <br>         does not exist."</span>),<br>        <span>@ApiResponse</span>(code = <span>422</span>, message = <span>"Unprocessable entity, input <br>         parameters caused the processing to fails. See response <br>         message for more information."</span>)<br>    })<br>    <span>@GetMapping</span>(<br>        value    = <span>"/product-composite/{productId}"</span>,<br>        produces = <span>"application/json"</span>)<br>    ProductAggregate getProduct(<span>@PathVariable </span><span>int </span>productId);</pre>
<p>For the values specified in the <span><kbd>@ApiOperation</kbd> Swagger annotation, </span>we can use property placeholders directly, without using Spring <kbd>@Value</kbd> annotations. For the description of the expected <kbd>ApiResponses</kbd>, that is, the expected error codes, SpringFox currently does not support the use of property placeholders, so in this case, the actual text describing each error code is placed directly in the Java source code.</p>
<p><span>For details, see </span><span><kbd>$BOOK_HOME/Chapter05/api/src/main/java/se/magnus/api/composite/product/ProductCompositeService.java</kbd>.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building and starting the microservice landscape</h1>
                </header>
            
            <article>
                
<p><span>Before we can try out the Swagger documentation, we need to build and start the microservice landscape!</span></p>
<p>This can be done with the following commands:</p>
<pre><strong><span>cd $BOOK_HOME/</span><span>Chapter05</span></strong><br><strong>./gradlew build &amp;&amp; docker-compose build <span>&amp;&amp; docker-compose up -d</span></strong></pre>
<p>You may run into an error message regarding port <kbd>8080</kbd> already being allocated. This will look as follows:</p>
<pre>ERROR: for product-composite Cannot start service product-composite: driver failed programming external connectivity on endpoint chapter05_product-composite_1 (0138d46f2a3055ed1b90b3b3daca92330919a1e7fec20351728633222db5e737): Bind for 0.0.0.0:8080 failed: port is already allocated</pre>
<p>If this is the case, you might have forgotten to bring down the&nbsp;<span>microservice landscape from the previous chapter. To find out the names of the executing containers, run the following command:</span></p>
<pre><strong> docker ps --format {{.Names}}</strong></pre>
<p>A sample response when&nbsp;<span>a&nbsp;</span><span>microservice landscape from the previous chapter is still running is as follows:</span></p>
<pre><strong>chapter05_review_1</strong><br><strong>chapter05_product_1</strong><br><strong>chapter05_recommendation_1</strong><br><strong>chapter04_review_1</strong><br><strong>chapter04_product-composite_1</strong><br><strong>chapter04_product_1</strong><br><strong>chapter04_recommendation_1</strong></pre>
<p><span>If you find containers from other chapters in the output from the command, for example, from <a href="ce37c0f1-ac54-4258-8cb6-7c6f50f26172.xhtml">Chapter 4</a>,&nbsp;<em>Deploying Our Microservices Using Docker</em>, as in the preceding example, you need to jump over to that chapter and bring down the containers for that chapter:</span></p>
<pre><strong>cd ../Chapter04</strong><br><strong>docker-compose down</strong></pre>
<p>Now, you can bring up the missing container for this chapter:</p>
<pre><strong>cd ../Chapter05</strong><br><strong><span>docker-compose up -d</span></strong></pre>
<p>Note that only the missing container, <kbd>product-composite</kbd>, is started by the command since the other ones were already started successfully:</p>
<pre><strong>Starting chapter05_product-composite_1 ... done</strong></pre>
<p>To wait for the microservice landscape to startup and verify that it works, you can run the following command:</p>
<pre><strong>./test-em-all.bash<br></strong></pre>
<p>The successful start-up of this microservice helps us understand its landscape better and also aids in understanding the Swagger documentation which&nbsp; we are about to study in the next section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building and starting the microservice landscape</h1>
                </header>
            
            <article>
                
<p><span>Before we can try out the Swagger documentation, we need to build and start the microservice landscape!</span></p>
<p>This can be done with the following commands:</p>
<pre><strong><span>cd $BOOK_HOME/</span><span>Chapter05</span></strong><br><strong>./gradlew build &amp;&amp; docker-compose build <span>&amp;&amp; docker-compose up -d</span></strong></pre>
<p>You may run into an error message regarding port <kbd>8080</kbd> already being allocated. This will look as follows:</p>
<pre>ERROR: for product-composite Cannot start service product-composite: driver failed programming external connectivity on endpoint chapter05_product-composite_1 (0138d46f2a3055ed1b90b3b3daca92330919a1e7fec20351728633222db5e737): Bind for 0.0.0.0:8080 failed: port is already allocated</pre>
<p>If this is the case, you might have forgotten to bring down the <span>microservice landscape from the previous chapter. To find out the names of the executing containers, run the following command:</span></p>
<pre><strong> docker ps --format {{.Names}}</strong></pre>
<p>A sample response when <span>a&nbsp;</span><span>microservice landscape from the previous chapter is still running is as follows:</span></p>
<pre><strong>chapter05_review_1</strong><br><strong>chapter05_product_1</strong><br><strong>chapter05_recommendation_1</strong><br><strong>chapter04_review_1</strong><br><strong>chapter04_product-composite_1</strong><br><strong>chapter04_product_1</strong><br><strong>chapter04_recommendation_1</strong></pre>
<p><span>If you find containers from other chapters in the output from the command, for example, from <a href="ce37c0f1-ac54-4258-8cb6-7c6f50f26172.xhtml">Chapter 4</a>,&nbsp;<em>Deploying Our Microservices Using Docker</em>, as in the preceding example, you need to jump over to that chapter and bring down the containers for that chapter:</span></p>
<pre><strong>cd ../Chapter04</strong><br><strong>docker-compose down</strong></pre>
<p>Now, you can bring up the missing container for this chapter:</p>
<pre><strong>cd ../Chapter05</strong><br><strong><span>docker-compose up -d</span></strong></pre>
<p>Note that only the missing container, <kbd>product-composite</kbd>, is started by the command since the other ones were already started successfully:</p>
<pre><strong>Starting chapter05_product-composite_1 ... done</strong></pre>
<p>To wait for the microservice landscape to startup and verify that it works, you can run the following command:</p>
<pre><strong>./test-em-all.bash<br></strong></pre>
<p>The successful start-up of this microservice helps us understand its landscape better and also aids in understanding the Swagger documentation which  we are about to study in the next section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the Swagger documentation</h1>
                </header>
            
            <article>
                
<p><span>To browse the Swagger documentation, we will use the embedded Swagger viewer. If we open the</span>&nbsp;<kbd>http://localhost:8080/swagger-ui.html</kbd> URL <span>in a web browser, we will see a web page that looks something like the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/eb92fdb1-5db9-4a87-92d4-7c71147b6608.png" style="width:42.92em;height:24.58em;" width="1950" height="1113" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/eb92fdb1-5db9-4a87-92d4-7c71147b6608.png"></p>
<p>Here, we can find the following:</p>
<ul>
<li>The general information we specified in the SpringFox <kbd>Docket</kbd> bean and <span>a link to the actual Swagger document, </span><kbd>http://localhost:8080/v2/api-docs</kbd></li>
<li>A list of API resources; in our case, the <kbd>product-composite-service</kbd> API</li>
<li>At the bottom of the page, there is a section where we can inspect the models that are used in the API</li>
</ul>
<p>This is how it works:</p>
<ol>
<li>Click on the<span>&nbsp;</span><kbd>product-composite-service</kbd><span> API resource to expand it. You will get a list of operations that are available on the resource.</span></li>
<li><span>You will only see one operation, </span><span class="packt_screen">/product-composite/{productId}</span><span>. Click on it to expand it. You will see the documentation of the operation that we specified in the</span>&nbsp;<kbd>ProductCompositeService</kbd><span><span> Java interface:</span></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8cfce8b6-7453-449a-a9ef-2e09eaa283fa.png" style="width:44.58em;height:43.67em;" width="1984" height="1943" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8cfce8b6-7453-449a-a9ef-2e09eaa283fa.png"></p>
<p><span>Here, we can see the following:</span></p>
<ul>
<li><span>T</span>he one-line description of the operation.</li>
<li>A section with details regarding the operation, including the input parameters it supports. Note how the markdown syntax from the <kbd>notes</kbd> field in the <span><kbd>@ApiOperation</kbd> annotation </span>has been nicely rendered!</li>
</ul>
<p>If you scroll down the web page, you will also find documentation regarding the expected responses, both a normal <span class="packt_screen">200</span> response and the various 4xx error responses we defined, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/552a2eb1-0907-4982-9bea-1c27987a129f.png" style="width:53.92em;height:50.08em;" width="1912" height="1773" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/552a2eb1-0907-4982-9bea-1c27987a129f.png"></p>
<p>If we scroll back up to the parameter description, we will find the <span class="packt_screen">Try it out!</span> button. If we click on that, we can fill in actual parameter values and send a request to the API by clicking on the <span class="packt_screen">Execute</span> button. For example, if we put in <span class="packt_screen">productId</span> <kbd>123</kbd>, we will get the following response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/25e34f7c-852a-407f-a9ad-c7963c7808bb.png" width="1944" height="1937" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/25e34f7c-852a-407f-a9ad-c7963c7808bb.png"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root">We will get an expected <span class="packt_screen">200</span> (OK) as the response code and a JSON structure in the response body that we already are familiar with!</p>
<p>If we enter an incorrect input, such as <kbd>-1</kbd>, we will get a proper error code as the response code and a corresponding JSON-based error description in the response body:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3361c3ba-305c-4950-8454-afea6db7d2e4.png" width="1961" height="1756" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3361c3ba-305c-4950-8454-afea6db7d2e4.png"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>If you want to try out calling the API without using the Swagger UI, you can copy the corresponding <kbd>curl</kbd> command from the response section and run it in a Terminal window! Look at the following by way of an example:</p>
<pre><strong>curl -X GET "http://localhost:8080/product-composite/123" -H "accept: application/json"</strong></pre>
<p>Great, isn't it?</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Good documenting of an API is essential for its acceptance, and Swagger<span> is </span><span>one of the most commonly used </span><span>specifications when it comes to documenting </span><span>RESTful services. SpringFox is an opensource project that makes it possible to create Swagger-based API documentation on the fly at runtime by inspecting Spring WebFlux and Swagger annotations. Textual descriptions of an API can be extracted from the annotations in the Java source code and be placed in a property file for ease of editing. SpringFox can be configured to bring in an embedded Swagger viewer to a microservice, which makes it very easy to read about APIs that have been exposed by the microservice and also try them out from the viewer. </span></p>
<p><span>Now, what about bringing some life to our microservices by adding persistence, that is, the capability to save data in a database? To do this, we need to add some more APIs so that we can create and delete the information that's handled by the microservices. Head over to the next chapter to find out more!</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding Persistence</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to persist data that a microservice is using. As already mentioned <span>in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml" target="_blank">Chapter 2</a>, <em>Introduction to Spring Boot</em></span><span>, we will use the Spring Data project to persist data to MongoDB and MySQL databases. The</span> <kbd>project</kbd> <span>and</span> <kbd>recommendation</kbd> <span>microservices will use Spring Data for MongoDB and the</span> <kbd>review</kbd><span> microservice will use Spring Data for the <strong>JPA</strong> (short for the <strong>Java Persistence API</strong>) to access a MySQL database. We will add operations to the RESTful APIs to be able to create and delete data in the databases. The existing APIs for reading data will be updated to access the databases. We will run the databases as Docker containers, managed by Docker Compose, that is, in the same way as we run our microservices.</span></p>
<p><span>The following topics will be covered in this chapter:</span></p>
<ul>
<li>Adding a persistence layer to the core microservices</li>
<li>Writing automated tests that focus on <span>persistence</span></li>
<li>Using the persistence layer in the service<span>&nbsp;</span><span>layer</span></li>
<li>Extending the composite service API</li>
<li>Adding databases to the Docker Compose landscape</li>
<li>Manual testing of the new APIs and the <span>persistence layer</span></li>
<li>Updating the <span>automated tests of the </span><span><span>microservice landscape</span></span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All commands described in this book<span>&nbsp;are run on a MacBook Pro using macOS Mojave but should be straightforward to modify to run on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p class="mce-root"></p>
<p>To access the databases manually, we will use the CLI tools provided in the Docker images used to run the databases. We will, however, expose the standard ports used for each database in Docker Composeâ<kbd>3306</kbd> for MySQL and <kbd>27017</kbd> for MongoDB. This will enable you to use your local favorite database tools for accessing the databases in the same way as if they were running locally on your computer.&nbsp;</p>
<p>The source code for this chapter&nbsp;can be found&nbsp;<span>on GitHub</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter06">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter06</a>.</p>
<p>To be able to run the commands as described in the book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Following are some sample commands:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter06</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Boot 2.1.0 (and Spring 5.1.2)âthe latest available version of Spring Boot at the time of writing this chapter.</p>
<p>The source code contains the following Gradle projects:</p>
<ul>
<li><kbd>api</kbd></li>
<li><kbd>util</kbd></li>
<li><kbd>microservices/product-service</kbd></li>
<li><kbd>microservices/review-service</kbd></li>
<li><kbd>microservices/recommendation-service</kbd></li>
<li><kbd>microservices/product-composite-service</kbd></li>
</ul>
<p>The code examples in this chapter all come from source code in&nbsp;<span><kbd>$BOOK_HOME/Chapter06</kbd>&nbsp;but are, in many cases, edited to remove non-relevant parts of the source code, such as comments and import and log statements.</span></p>
<p>If you want to see the changes applied to the source code in <a href="6c495e97-f473-4cb1-a404-11fd938f5478.xhtml" target="_blank">Chapter 6</a>,<em>&nbsp;Adding Persistence</em>, which sees&nbsp;<span>what it took to add persistence to the microservices using Spring Data,&nbsp;</span>you can compare it with the source code for <a href="ba24a656-10a1-4a3e-879e-6589621ef125.xhtml" target="_blank"></a><a href="ba24a656-10a1-4a3e-879e-6589621ef125.xhtml" target="_blank">Chapter 5</a>, <em>Adding API Description Using OpenAPI/Swagger</em>.<span>&nbsp;You can use your favorite&nbsp;</span>diff&nbsp;<span>tool and compare the two folders,&nbsp;</span><kbd>$BOOK_HOME/Chapter05</kbd><span>&nbsp;and&nbsp;</span><kbd>$BOOK_HOME/Chapter06</kbd><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">But first, let's see where we are heading</h1>
                </header>
            
            <article>
                
<p><span>By the end of&nbsp;</span><span>t</span><span>his chapter,&nbsp;we will have layers inside our microservices that look like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/148fd8fc-30e4-4039-986a-554a4237bb7c.png" style="width:37.83em;height:30.83em;" width="1285" height="1046" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/148fd8fc-30e4-4039-986a-554a4237bb7c.png"></p>
<p>The <strong>Protocol layer</strong>&nbsp;is very thin, only consisting of&nbsp;<kbd>RestController</kbd> annotations and the common <kbd>GlobalControllerExceptionHandler</kbd>. The main functionality of each microservice resides in the service layers. The <kbd>product-composite</kbd> service contains an integration layer to communicate with the three core microservices.&nbsp;The core&nbsp;<span>microservices will all have a <strong>Persistence layer</strong> used for communicating with their databases.</span></p>
<p><span>We will be able to s</span><span>ee data stored in MongoDB with a c</span><span>ommand like the following:</span></p>
<pre><strong>docker-compose exec mongodb mongo product-db --quiet --eval "db.products.find()"</strong></pre>
<p>The result of the command should look like&nbsp;the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/396be685-3e82-4fc9-b871-3b6e72c7f6d8.png" style="width:42.00em;height:6.58em;" width="1523" height="238" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/396be685-3e82-4fc9-b871-3b6e72c7f6d8.png"></p>
<p>Regarding data stored in MySQL, we will be able to see it with a command like this:</p>
<pre><strong>docker-compose exec mysql mysql -uuser -p review-db -e "select * from reviews"</strong></pre>
<p><span>The result of the command should look as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/05445cec-486e-426e-8b16-b611e61649e5.png" style="width:24.25em;height:8.08em;" width="1123" height="374" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/05445cec-486e-426e-8b16-b611e61649e5.png"></p>
<div class="packt_infobox"><strong>Note:</strong> The output from the <kbd>mongo</kbd> and <kbd>mysql</kbd> commands have been shortened for improved readability.</div>
<p>Let's see how to go about this.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">But first, let's see where we are heading</h1>
                </header>
            
            <article>
                
<p><span>By the end of </span><span>t</span><span>his chapter, we will have layers inside our microservices that look like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/148fd8fc-30e4-4039-986a-554a4237bb7c.png" style="width:37.83em;height:30.83em;" width="1285" height="1046" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/148fd8fc-30e4-4039-986a-554a4237bb7c.png"></p>
<p>The <strong>Protocol layer</strong> is very thin, only consisting of <kbd>RestController</kbd> annotations and the common <kbd>GlobalControllerExceptionHandler</kbd>. The main functionality of each microservice resides in the service layers. The <kbd>product-composite</kbd> service contains an integration layer to communicate with the three core microservices. The core <span>microservices will all have a <strong>Persistence layer</strong> used for communicating with their databases.</span></p>
<p><span>We will be able to s</span><span>ee data stored in MongoDB with a c</span><span>ommand like the following:</span></p>
<pre><strong>docker-compose exec mongodb mongo product-db --quiet --eval "db.products.find()"</strong></pre>
<p>The result of the command should look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/396be685-3e82-4fc9-b871-3b6e72c7f6d8.png" style="width:42.00em;height:6.58em;" width="1523" height="238" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/396be685-3e82-4fc9-b871-3b6e72c7f6d8.png"></p>
<p>Regarding data stored in MySQL, we will be able to see it with a command like this:</p>
<pre><strong>docker-compose exec mysql mysql -uuser -p review-db -e "select * from reviews"</strong></pre>
<p><span>The result of the command should look as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/05445cec-486e-426e-8b16-b611e61649e5.png" style="width:24.25em;height:8.08em;" width="1123" height="374" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/05445cec-486e-426e-8b16-b611e61649e5.png"></p>
<div class="packt_infobox"><strong>Note:</strong> The output from the <kbd>mongo</kbd> and <kbd>mysql</kbd> commands have been shortened for improved readability.</div>
<p>Let's see how to go about this.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a persistence layer to the core microservices</h1>
                </header>
            
            <article>
                
<p>Let's start with adding a persistence layer to the core microservices. Besides using Spring Data, we will also use a Java bean mapping tool, MapStruct, that makes it easy to transform between Spring Data entity objects and the API model classes. For further details, see<span>&nbsp;<a href="http://mapstruct.org/">http://mapstruct.org/</a>.</span></p>
<p>First, we need to add dependencies to MapStruct, Spring Data, and the JDBC drivers for the databases we intend to use. After that, we can define our Spring Data entity classes and repositories. The <span>Spring Data entity classes and repositories will be placed in their own Java package, <kbd>persistence</kbd>. For example, for the product microservice, they will be placed in the Java package, </span><kbd>se.magnus.microservices.core.product.persistence</kbd>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding dependencies</h1>
                </header>
            
            <article>
                
<p>We will use MapStruct V1.3.0-Beta 2, so we start by defining a variable holding the version information in the build file for each core microservice, <kbd>build.gradle</kbd>:</p>
<pre>ext {<br> mapstructVersion = "1.3.0.Beta2"<br>}</pre>
<p>Next, we declare a dependency on <span>MapStruct</span>:</p>
<pre>implementation("org.mapstruct:mapstruct:${mapstructVersion}")</pre>
<p>Since M<span>apStruct generates the implementation of the bean mappings at compile time by processing MapStruct annotations, we need to add an </span><kbd>annotationProcessor</kbd><span> and a </span><kbd>testAnnotationProcessor</kbd>&nbsp;<span>dependency:</span></p>
<pre>iannotationProcessor "org.mapstruct:mapstruct-processor:${mapstructVersion}"<br>testAnnotationProcessor "org.mapstruct:mapstruct-processor:${mapstructVersion}"</pre>
<p>To make the compile-time generation work in popular IDEs such as <span>IntelliJ IDEA, we also need to add the following dependency:</span></p>
<pre>compileOnly "org.mapstruct:mapstruct-processor:${mapstructVersion}"</pre>
<div class="packt_infobox">If you are using <span>IntelliJ IDEA, you also need to ensure that support for annotation processing is enabled. Open</span> <span class="packt_screen">Preferences</span> <span>and navigate to</span> <span class="packt_screen">Build</span>, <span class="packt_screen">Execute</span>, <span class="packt_screen">Deployment</span>&nbsp;| <span class="packt_screen">Compiler |</span>&nbsp;<span class="packt_screen">Annotations Processors</span>. <span>Verify that the checkbox named <span class="packt_screen">Enable annotation processing</span> is selected!</span></div>
<p><span>For the </span><kbd>project</kbd><span> and </span><kbd>recommendation</kbd><span> microservices, we declare the following dependencies to Spring Data for MongoDB:</span></p>
<pre class="mce-root">implementation('org.springframework.boot:spring-boot-starter-data-mongodb')<br>testImplementation('de.flapdoodle.embed:de.flapdoodle.embed.mongo')</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p><span>The test dependency to </span><kbd>de.flapdoodle.embed.mongo</kbd> enables us to run<span> MongoDB embedded when we run JUnit-based tests.</span></p>
<p>The <kbd>review</kbd><span> microservices will use Spring Data for JPA together with MySQL as its database in runtime and it will use an embedded database, H2, during tests. Therefore, it declares the following dependencies in its build file, <kbd>build.gradle</kbd>:</span></p>
<pre class="mce-root">implementation('org.springframework.boot:spring-boot-starter-data-jpa')<br>implementation('mysql:mysql-connector-java')<br>testImplementation('com.h2database:h2')</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Storing data with entity classes</h1>
                </header>
            
            <article>
                
<p>The entity classes are similar to the corresponding API model classes in terms of what fields they containâsee the Java package, <kbd>se.magnus.api.core</kbd>, in the <kbd>api</kbd> project. We will add two fields, <kbd>id</kbd>, and <kbd>version</kbd>, in the entity classes compared to the fields in the API model classes.</p>
<p>The <kbd>id</kbd> field is used to hold the database identity of each stored entityâthe primary key when using a relational database. We will delegate the responsibility to generate unique values of the identity field to Spring Data. Depending on the database used, Spring Data can delegate this responsibility to the database engine. In either case, the application code does not need to consider how a unique database <kbd>id</kbd> value is set. <span>The <kbd>id</kbd> field is not exposed in the API, as a best practice from a security perspective</span><span>. The fields in the model classes that identify an entity will be assigned a unique index in the corresponding entity class, to ensure consistency in the database from a business perspective. </span></p>
<p>The <kbd>version</kbd> field is used to implement optimistic locking, that is, allowing Spring Data to verify that updates of an entity in the database do not overwrite a concurrent update. If the value of the version field stored in the database is higher than the value of the version field in an update request, it indicates that the update is performed on stale dataâthe information to be updated has been updated by someone else since it was read from the database. Attempts to perform updates based on stale data will be prevented by Spring Data. In the section on writing persistence tests, we will see tests that verify the optimistic locking mechanism in Spring Data prevent updates performed on stale data. Since we only implement APIs for create, read, and delete operations, we will, however, not expose the version field in the API.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>The most interesting parts of the product entity class look like this:</p>
<pre><span>@Document</span>(collection=<span>"products"</span>)<br><span>public class </span>ProductEntity {<br><br> <span>@</span><span>Id<br></span><span> </span><span>private </span>String <span>id</span>;<br><br> <span>@Version<br></span><span> </span><span>private </span>Integer <span>version</span>;<br><br> <span>@Indexed</span>(unique = <span>true</span>)<br> <span>private int </span><span>productId</span>;<br><br> <span>private </span>String <span>name</span>;<br> <span>private int </span><span>weight</span>;</pre>
<p>The following are the observations from the preceding code:</p>
<ul>
<li>The <kbd>@Document(collection="products")</kbd> annotation is used to mark the class as an entity class used for MongoDB, that is, mapped to a collection in MongoDB with the name, <kbd><span>products</span></kbd>.</li>
<li>The <kbd>@Id</kbd> and <kbd>@Version</kbd> annotations are used to mark the <kbd>id</kbd> and <kbd>version</kbd> fields to be used by Spring Data, as explained previously.</li>
<li>The <kbd>@Indexed(unique = true)</kbd> annotation is used to get a unique index created for the business key, <kbd>productId</kbd>.</li>
</ul>
<p><span>The most interesting parts of the </span><kbd>Recommendation</kbd><span> entity class look like this:</span></p>
<pre><span>@Document</span>(collection=<span>"recommendations"</span>)<br><span>@CompoundIndex</span>(name = <span>"prod-rec-id"</span>, unique = <span>true</span>, def = <span>"{'productId': 1, 'recommendationId' : 1}"</span>)<br><span>public class </span>RecommendationEntity {<br><br>    <span>@Id<br></span><span>    </span><span>private </span>String <span>id</span>;<br><br>    <span>@Version<br></span><span>    </span><span>private </span>Integer <span>version</span>;<br><br>    <span>private int </span><span>productId</span>;<br>    <span>private int </span><span>recommendationId</span>;<br>    <span>private </span>String <span>author</span>;<br>    <span>private int </span><span>rating</span>;<br>    <span>private </span>String <span>content</span>;</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p>Added to the explanations for the preceding product entity, we can see how a unique compound index is created using the <span><kbd>@CompoundIndex</kbd> annotation </span>for the compound business key based on the fields, <kbd>productId</kbd> and <kbd>recommendationId</kbd>.</p>
<p>Finally, t<span>he most interesting parts of the </span><kbd>Review</kbd><span> entity class look like this:</span></p>
<pre><span>@Entity<br></span><span>@Table</span>(name = <span>"reviews"</span>, indexes = { <span>@Index</span>(name = <span>"reviews_unique_idx"</span>, unique = <span>true</span>, columnList = <span>"productId,reviewId"</span>) })<br><span>public class </span>ReviewEntity {<br><br>    <span>@Id @GeneratedValue<br></span><span>    </span><span>private int </span><span>id</span>;<br><br>    <span>@Version<br></span><span>    </span><span>private int </span><span>version</span>;<br><br>    <span>private int </span><span>productId</span>;<br>    <span>private int </span><span>reviewId</span>;<br>    <span>private </span>String <span>author</span>;<br>    <span>private </span>String <span>subject</span>;<br>    <span>private </span>String <span>content</span>;</pre>
<p><span>The following are the observations from the preceding code:</span></p>
<ul>
<li>The <kbd>@Entity</kbd> and <kbd>@Table</kbd> annotations are used to mark the class as an entity class used for JPAâmapped to a table in a SQL database with the name, <kbd>products</kbd>.</li>
<li><span>The</span><span>&nbsp;</span><kbd>@Table</kbd><span>&nbsp;a</span><span>nnotation is also used to specify that a unique compound index shall be created for the compound business key based on the fields, <kbd>productId</kbd> and <kbd>reviewId</kbd>.</span></li>
<li>The<span>&nbsp;</span><kbd>@Id</kbd><span>&nbsp;</span>and<span>&nbsp;</span><kbd>@Version</kbd><span>&nbsp;</span>annotations are used to mark the <kbd>id</kbd> and <kbd>version</kbd> fields to be used by Spring Data as explained previously. To direct Spring Data for JPA to automatically generate unique <kbd>id</kbd> values for the <kbd>id</kbd> field, we are using the <kbd>@GeneratedValue</kbd> annotation.</li>
</ul>
<p><span>For full source code of the entity classes, see the following:</span></p>
<ul>
<li><kbd>se.magnus.microservices.core.product.persistence.ProductEntity</kbd> in the <kbd><span>product</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.recommendation.persistence.RecommendationEntity</kbd> in the <kbd><span>recommendation</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.review.persistence.ReviewEntity</kbd> in the <kbd><span>review</span></kbd> project</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Defining repositories in Spring Data</h1>
                </header>
            
            <article>
                
<p>Spring Data comes with a set of base classes for defining repositories. We will use the base classes, <kbd>CrudRepository</kbd> and <kbd>PagingAndSortingRepository</kbd>. The<span>&nbsp;</span><kbd>CrudRepository</kbd><span> base class provides standard methods for performing basic create, read, update, and delete operations on the data stored in the databases. The <kbd>PagingAndSortingRepository</kbd> base class adds support for paging and sorting to the <kbd>CrudRepository</kbd> base class.</span></p>
<p>We will use the <span><kbd>CrudRepository</kbd> class as the base class for the <kbd>Recommendation</kbd> and <kbd>Review</kbd> repositories and the </span><span><kbd>PagingAndSortingRepository</kbd> class as the base class for the <kbd>Product</kbd> repository.</span></p>
<p>We will also add a few extra query methods to our repositories for looking up entities using the business key, <kbd>productId</kbd>.</p>
<p>Spring Data supports defining extra query methods based on naming conventions for the signature of the method. For example, the <kbd>findByProductId(<span>int</span> productId)</kbd> <span>method signature </span>can be used to direct Spring Data to automatically create a query that returns entities from the underlying collection or table that has <span>the </span><kbd>productId</kbd><span> field set to the value specified in the <kbd>productId</kbd> parameter when calling the query method. For more details on how to declare extra queries, see </span><a href="https://docs.spring.io/spring-data/data-commons/docs/current/reference/html/#repositories.query-methods.query-creation">https://docs.spring.io/spring-data/data-commons/docs/current/reference/html/#repositories.query-methods.query-creation</a>.</p>
<p><span>The</span> <kbd>Product</kbd><span> repository class looks like this:</span></p>
<pre><span>public interface </span>ProductRepository <span>extends </span>PagingAndSortingRepository&lt;ProductEntity, String&gt; {<br>    Optional&lt;ProductEntity&gt; findByProductId(<span>int </span>productId);<br>}</pre>
<p>Since the <kbd>findByProductId</kbd> method might return zero or one product entity, <span>the return value is marked to be optional by wrapping it in an <kbd>Optional</kbd> object.</span></p>
<p><span>The <kbd>Recommendation</kbd></span><span> repository class looks like this:</span></p>
<pre><span>public interface </span>RecommendationRepository <span>extends </span>CrudRepository&lt;RecommendationEntity, String&gt; {<br>    List&lt;RecommendationEntity&gt; findByProductId(<span>int </span>productId);<br>}</pre>
<p>In this case, the <kbd>findByProductId</kbd> method will return zero to many recommendation entities, so the return value is defined as a list.</p>
<p>Finally, the<span>&nbsp;<kbd>Review</kbd></span><span> repository class looks like this:</span></p>
<pre><span>public interface </span>ReviewRepository <span>extends </span>CrudRepository&lt;ReviewEntity, Integer&gt; {<br><span>    @Transactional</span>(readOnly = <span>true</span>)<br>    List&lt;ReviewEntity&gt; findByProductId(<span>int </span>productId);<br>}</pre>
<p>Since SQL databases are transactional, we have to specify the default transaction typeâread-only in our caseâfor the query method, <kbd>findByProductId()</kbd>.</p>
<p>That's itâthis is all it takes to establish a persistence layer for our core microservices.</p>
<p><span>For full source code of the repository classes, see the following:</span></p>
<ul>
<li><kbd>se.magnus.microservices.core.product.persistence.<span>ProductRepository</span></kbd> in the <kbd><span>product</span></kbd><span>&nbsp;</span>project</li>
<li><kbd>se.magnus.microservices.core.recommendation.persistence.<span>RecommendationRepository</span></kbd> in the <kbd><span>recommendation</span></kbd><span>&nbsp;</span>project</li>
<li><kbd>se.magnus.microservices.core.review.persistence.<span>ReviewRepository</span></kbd> in the <kbd><span>review</span></kbd><span>&nbsp;</span>project</li>
</ul>
<p>Let's start using them by writing some persistence tests to verify that they work as intended.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Writing automated tests that focus on persistence</h1>
                </header>
            
            <article>
                
<p>When writing persistence tests, we want to start an embedded database when the tests begin and tear it down when the tests complete. However, we don't want the tests to wait for other resources to start up, for example, a web server such as Netty (which is <span>required in runtime)</span>.</p>
<p class="mce-root"></p>
<p>Spring Boot comes with two class level annotations tailored for this specific requirement: </p>
<ul>
<li><kbd>@DataMongoTest</kbd>: This starts up an embedded MongoDB database when the test starts.</li>
<li><kbd>@DataJpaTest</kbd><span>: This </span>starts up an embedded SQL database when the test starts:<br>
<ul>
<li>Since we added a test dependency in the build file for the review microservice to the H2 database, it will be used as the embedded SQL database.</li>
<li><span>By default, Spring Boot configures the tests to roll back updates to the SQL database to minimize the risk of negative side effects on other tests. In our case, this behavior will cause some of the tests to</span> fail. Therefore, automatic rollback is disabled with the class level annotation: <kbd>@Transactional(propagation = NOT_SUPPORTED)</kbd>.</li>
</ul>
</li>
</ul>
<p>The persistence tests for the three core microservices are similar to each other, so we will only go through the <span>persistence tests for the <kbd>Product</kbd> microservice. </span></p>
<p><span>The test class declares a method,</span><span>&nbsp;<kbd>setupDb()</kbd>,&nbsp;</span><span>annotated with </span><kbd>@Before</kbd><span>, which is executed before each test method. The setup method removes any entities from previous tests in the database and inserts an entity that the test methods can use as a base for their tests:</span></p>
<pre><span>@RunWith</span>(SpringRunner.<span>class</span>)<br><span>@DataMongoTest<br></span><span>public class </span>PersistenceTests {<br><br>    <span>@Autowired<br></span><span>    </span><span>private </span>ProductRepository <span>repository</span>;<br>    <span>private </span>ProductEntity <span>savedEntity</span>;<br><br>    <span>@Before<br></span><span>    </span><span>public void </span>setupDb() {<br>        <span>repository</span>.deleteAll();<br>        ProductEntity entity = <span>new </span>ProductEntity(<span>1</span>, <span>"n"</span>, <span>1</span>);<br>        <span>savedEntity </span>= <span>repository</span>.save(entity);<br>        assertEqualsProduct(entity, <span>savedEntity</span>);<br>    }</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p>Next comes the various test methods. First out is a <kbd>create</kbd> test:</p>
<pre><span>@Test<br></span><span>public void </span>create() {<br>    ProductEntity newEntity = <span>new </span>ProductEntity(<span>2</span>, <span>"n"</span>, <span>2</span>);<br>    <span>savedEntity </span>= <span>repository</span>.save(newEntity);<br><br>    ProductEntity foundEntity = <br>    <span>repository</span>.findById(newEntity.getId()).get();<br>    assertEqualsProduct(newEntity, foundEntity);<br><br>    <span>assertEquals</span>(<span>2</span>, <span>repository</span>.count());<br>}</pre>
<p>This test creates a new entity and verifies that it can be found using the <span><kbd>findByProductId()</kbd> method and wraps up with asserting that there are two entities stored in the database, the one created by the <kbd>setup</kbd> method and the one created by the test itself.</span></p>
<p>The <kbd>update</kbd> test looks like this:</p>
<pre><span>@Test<br></span><span>public void </span>update() {<br>    <span>savedEntity</span>.setName(<span>"n2"</span>);<br>    <span>repository</span>.save(<span>savedEntity</span>);<br><br>    ProductEntity foundEntity = <br>    <span>repository</span>.findById(<span>savedEntity</span>.getId()).get();<br>    <span>assertEquals</span>(<span>1</span>, (<span>long</span>)foundEntity.getVersion());<br>    <span>assertEquals</span>(<span>"n2"</span>, foundEntity.getName());<br>}</pre>
<p>This test updates the entity created by the setup method, reads it again from the database using the standard <kbd>findById()</kbd> method, and asserts that it contains expected values for some of its fields. Note that, when an entity is created, its <kbd>version</kbd> field is set to <kbd>0</kbd> by Spring Data.</p>
<p>The <kbd>delete</kbd> test looks like this:</p>
<pre><span>@Test<br></span><span>public void </span>delete() {<br>    <span>repository</span>.delete(<span>savedEntity</span>);<br>    <span>assertFalse</span>(<span>repository</span>.existsById(<span>savedEntity</span>.getId()));<br>}</pre>
<p>This test deletes the entity <span>created by the <kbd>setup</kbd> method and verifies that it no longer exists in the database.</span></p>
<p class="mce-root"></p>
<p>The <kbd>read</kbd> test looks like this:</p>
<pre><span>@Test<br></span><span>public void </span>getByProductId() {<br>    Optional&lt;ProductEntity&gt; entity = <br>    <span>repository</span>.findByProductId(<span>savedEntity</span>.getProductId());<br>    <span>assertTrue</span>(entity.isPresent());<br>    assertEqualsProduct(<span>savedEntity</span>, entity.get());<br>}</pre>
<p>This test uses the <kbd>findByProductId()</kbd> method to get the<span> entity </span><span>created by the <kbd>setup</kbd> method, verifies that it was found, and then uses the local helper method, </span><kbd>assertEqualsProduct()</kbd>, to verify that the entity returned by <span><kbd>findByProductId()</kbd> looks the same as the entity stored by the</span> setup m<span>ethod.</span></p>
<p>Next, it follows two test methods that verify alternative flowsâhandling of error conditions. First, is a test that verifies that duplicates are handled correctly:</p>
<pre>@Test(expected = DuplicateKeyException.class)<br><span>public void </span>duplicateError() {<br>    ProductEntity entity = <span>new <br>    </span>ProductEntity(<span>savedEntity</span>.getProductId(), <span>"n"</span>, <span>1</span>);<br>    <span>repository</span>.save(entity);<br>}</pre>
<p>The test tries to store an entity with the same business key as used by the entity saved by the setup method. The test will fail if the save operation succeeds or if the save fails with an exception other than the expected, <kbd>DuplicateKeyException</kbd>.</p>
<p>The other negative test is, in my opinion, the most interesting test in the test class. It is a test that verifies a correct error handling in the case of updates of stale dataâit verifies that the optimistic locking mechanism works. It looks like this:</p>
<pre><span>@Test<br></span><span>public void </span>optimisticLockError() {<br><br>    <span>// Store the saved entity in two separate entity objects<br></span><span>    </span>ProductEntity entity1 = <br>    <span>repository</span>.findById(<span>savedEntity</span>.getId()).get();<br>    ProductEntity entity2 = <br>    <span>repository</span>.findById(<span>savedEntity</span>.getId()).get();<br><br>    <span>// Update the entity using the first entity object<br></span><span>    </span>entity1.setName(<span>"n1"</span>);<br>    <span>repository</span>.save(entity1);<br><br>    <span>//  Update the entity using the second entity object.<br></span><span>    // This should fail since the second entity now holds a old version <br>    // number, that is, a Optimistic Lock Error<br></span><span>    </span><span>try </span>{<br>        entity2.setName(<span>"n2"</span>);<br>        <span>repository</span>.save(entity2);<br><br>        <span>fail</span>(<span>"Expected an OptimisticLockingFailureException"</span>);<br>    } <span>catch </span>(OptimisticLockingFailureException e) {}<br><br>    <span>// Get the updated entity from the database and verify its new <br>    // state<br></span><span>    </span>ProductEntity updatedEntity = <br>    <span>repository</span>.findById(<span>savedEntity</span>.getId()).get();<br>    <span>assertEquals</span>(<span>1</span>, (<span>int</span>)updatedEntity.getVersion());<br>    <span>assertEquals</span>(<span>"n1"</span>, updatedEntity.getName());<br>}</pre>
<p>The following is observed from the preceding code:</p>
<ol>
<li>First, the test reads the same entity twice and stores it in two different variables, <span><kbd>entity1</kbd> and <kbd>entity2</kbd></span>.</li>
<li>Next, it uses one of the variables, <kbd>entity1</kbd>, to update the entity. The update of the entity in the database will cause the version field of the entity to be increased automatically by Spring Data. The other variable, <span><kbd>entity2</kbd>,&nbsp;</span>now contains stale data, manifested by its version field that holds a lower value than the corresponding value in the database. </li>
<li>When the test tries to update the entity using the variable, <kbd><span>entity2</span></kbd>, that contains stale data, it is expected to fail by throwing an <kbd>OptimisticLockingFailureException</kbd> exception.</li>
<li>The test wraps up by asserting that the entity in the database reflects the first update, that is, contains the name <kbd>"n1"</kbd>, and that the version field has the value <kbd>1</kbd>, that is, only one update has been performed on the entity in the database.</li>
</ol>
<p>Finally, the <kbd>product</kbd> service contains a test that demonstrates the usage of built-in support for sorting and paging in Spring Data:</p>
<pre><span>@Test<br></span><span>public void </span>paging() {<br>    <span>repository</span>.deleteAll();<br>    List&lt;ProductEntity&gt; newProducts = <span>rangeClosed</span>(<span>1001</span>, <span>1010</span>)<br>        .mapToObj(i -&gt; <span>new </span>ProductEntity(i, <span>"name " </span>+ i, i))<br>        .collect(Collectors.<span>toList</span>());<br>    <span>repository</span>.saveAll(newProducts);<br><br>    Pageable nextPage = PageRequest.<span>of</span>(<span>0</span>, <span>4</span>, <span>ASC</span>, <span>"productId"</span>);<br>    nextPage = testNextPage(nextPage, <span>"[1001, 1002, 1003, 1004]"</span>, <br>    <span>true</span>);<br>    nextPage = testNextPage(nextPage, <span>"[1005, 1006, 1007, 1008]"</span>, <br>    <span>true</span>);<br>    nextPage = testNextPage(nextPage, <span>"[1009, 1010]"</span>, <span>false</span>);<br>}</pre>
<p><span>The following is observed from the preceding code:</span></p>
<ol>
<li>The test starts with removing any existing data, then inserts 10 entities with the <kbd>productId</kbd> field ranging from <kbd>1001</kbd> to <kbd>1010</kbd>.</li>
<li>Next, it creates <kbd>PageRequest</kbd>, requesting a page count of <kbd>4</kbd> entities per page and a sort order based on <kbd>ProductId</kbd> in ascending order.</li>
<li>Finally, it uses a helper method, <kbd>testNextPage</kbd>, to read the expected three pages, verifying the expected product IDs in each page and verifying that Spring Data <span>correctly </span>reports back whether more pages exist or not.</li>
</ol>
<p>The helper method <kbd>testNextPage</kbd><span> looks like this:</span></p>
<pre><span>private </span>Pageable testNextPage(Pageable nextPage, String expectedProductIds, <span>boolean </span>expectsNextPage) {<br>    Page&lt;ProductEntity&gt; productPage = <span>reposito</span><span>ry</span>.findAll(nextPage);<br>    <span>assertEquals</span>(expectedProductIds, productPage.getContent()<br>    .stream().map(p -&gt; p.getProductId()).collect(Collectors.<br><span>    toList</span>()).toString());<br>    <span>assertEquals</span>(expectsNextPage, productPage.hasNext());<br>    <span>return </span>productPage.nextPageable();<br>}</pre>
<p>The helper method uses the page request object, <kbd>nextPage</kbd>, to get the next page from the repository method, <kbd>findAll()</kbd>. Based on the result, it extracts the product IDs from the returned entities into a string and compares it to the expected list of product IDs. Finally, it returns a Boolean indicating whether more pages can be retrieved or not.</p>
<p>For full source code of the three persistence test classes, see the following:</p>
<ul>
<li><kbd>se.magnus.microservices.core.product.PersistenceTests</kbd> in the <span><kbd>product</kbd> project</span></li>
<li><kbd>se.magnus.microservices.core.recommendation.PersistenceTests</kbd> in the <kbd><span>recommendation</span></kbd><span> project</span></li>
<li><kbd>se.magnus.microservices.core.review.PersistenceTests</kbd> in the <kbd><span>review</span></kbd><span> project</span></li>
</ul>
<p class="mce-root"></p>
<p>The persistence tests in the <kbd>product</kbd> microservice can be executed using Gradle with a command like this:</p>
<pre><strong>cd <span>$BOOK_HOME/Chapter06<br></span>./gradlew microservices:product-service:test --tests PersistenceTests</strong></pre>
<p>After running the tests, it should respond with the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf9108ce-73db-41ea-ab82-0bf1eb098921.png" style="width:10.83em;height:4.08em;" width="390" height="147" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cf9108ce-73db-41ea-ab82-0bf1eb098921.png"></p>
<p>With a persistence layer in place, we can update the service layer in our core microservices to use the persistence layer.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using the persistence layer in the service layer</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will learn how to use the persistence layer in the service layer to store data and retrieve data from a database. We will go through the following steps:</span></p>
<ol>
<li class="mce-root"><span>Log the database connection URL.</span></li>
<li class="mce-root">Add new APIs.</li>
<li class="mce-root">Use the persistence layer.</li>
<li class="mce-root">Declare a Java bean mapper.</li>
<li class="mce-root">Update the service tests.</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding new APIs</h1>
                </header>
            
            <article>
                
<p>Before we can use the persistence layer for creating and deleting information in the database, we need to create the corresponding API operations in our core service APIs.</p>
<p>The API operations for creating and deleting a product entity looks like this:</p>
<pre><span>@PostMapping</span>(<br>    value    = <span>"/product"</span>,<br>    consumes = <span>"application/json"</span>,<br>    produces = <span>"application/json"</span>)<br>Product createProduct(<span>@RequestBody </span>Product body);<br><br><span>@DeleteMapping</span>(value = <span>"/product/{productId}"</span>)<br><span>void </span>deleteProduct(<span>@PathVariable </span><span>int </span>productId);</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<div class="packt_infobox">The implementation of the delete operation will be idempotent, that is, it will return the same result if called several times. This is a valuable characteristic in fault scenarios. For example, if a client experience a network timeout during a call to a delete operation, it can simply call the delete operation again without worrying about varying responses, for example,&nbsp;OK (200) in response the first time and Not Found (404) in response to consecutive calls, or any unexpected side effects. This implies that the operation should&nbsp;<span>return the status code OK (200) even though the entity no longer exists in the database.</span></div>
<p>The API operations for the&nbsp;<kbd>recommendation</kbd> and <kbd>review</kbd> entities look similar; however,&nbsp;<span>note&nbsp;</span>that, when it comes to the delete operation for <kbd>recommendation</kbd> <span>and <kbd>review</kbd> entities, it will delete all&nbsp;<kbd>recommendations</kbd> and <kbd>reviews</kbd> for the specified <kbd>productId</kbd>.</span></p>
<p>For the full source code, see the following classes in the <kbd>api</kbd> project:</p>
<ul>
<li><kbd>se.magnus.api.core.product.ProductService</kbd></li>
<li><kbd>se.magnus.api.core.recommendation.RecommendationService</kbd></li>
<li><kbd>se.magnus.api.core.review.ReviewService</kbd></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The use of the persistence layer</h1>
                </header>
            
            <article>
                
<p><span>The source code in the service layer for using the persistence layer is structured in the same way for all core microservices. Therefore, we will only go through the source code for the <kbd>Product</kbd> microservice.</span></p>
<p>First, we need to inject the repository class from the persistence layer and a Java bean mapper class into the constructor:</p>
<pre><span>private final </span>ServiceUtil <span>serviceUtil</span>;<br><span>private final </span>ProductRepository <span>repository</span>;<br><span>private final </span>ProductMapper <span>mapper</span>;<br><br><span>@Autowired<br></span><span>public </span>ProductServiceImpl(ProductRepository repository, ProductMapper mapper, ServiceUtil serviceUtil) {<br>    <span>this</span>.<span>repository </span>= repository;<br>    <span>this</span>.<span>mapper </span>= mapper;<br>    <span>this</span>.<span>serviceUtil </span>= serviceUtil;<br>}</pre>
<p>In the next section, we will see how the Java mapper class is defined.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p>Next, the <kbd>createProduct</kbd> method is implemented as follows:</p>
<pre><span>public </span>Product createProduct(Product body) {<br>    <span>try </span>{<br>        ProductEntity entity = <span>mapper</span>.apiToEntity(body);<br>        ProductEntity newEntity = <span>repository</span>.save(entity);<br>        <span>return </span><span>mapper</span>.entityToApi(newEntity);<br>    } <span>catch </span>(DuplicateKeyException dke) {<br>        <span>throw new </span>InvalidInputException(<span>"Duplicate key, Product Id: " </span>+ <br>        body.getProductId());<br>    }<br>}</pre>
<p>The <kbd>create</kbd> method used the <kbd>save</kbd> method in the repository to store a new entity. It should be noted how the mapper class is used to convert Java beans between an API model class and an entity class using the two mapper methods,&nbsp;<kbd>apiToEntity()</kbd> and <kbd>entityToApi()</kbd>. The only error we handle for the <kbd>create</kbd> method is the&nbsp;<kbd>DuplicateKeyException</kbd> exception, which we convert into an&nbsp;<kbd>InvalidInputException</kbd> exception.</p>
<p>The <kbd>getProduct</kbd> method looks like this:</p>
<pre><span>public </span>Product getProduct(<span>int </span>productId) {<br>    <span>if </span>(productId &lt; <span>1</span>) <span>throw new </span>InvalidInputException(<span>"Invalid <br>    productId: " </span>+ productId);<br>    ProductEntity entity = <span>repository</span>.findByProductId(productId)<br>        .orElseThrow(() -&gt; <span>new </span>NotFoundException(<span>"No product found for <br>         productId: " </span>+ <span>productId</span>));<br>    Product response = <span>mapper</span>.entityToApi(entity);<br>    response.setServiceAddress(<span>serviceUtil</span>.getServiceAddress());<br>    <span>return </span>response;<br>}</pre>
<p>After some basic input validation (that is, ensuring that <kbd>productId</kbd> is not negative), the <kbd>findByProductId()</kbd> method in the repository is used to find the product entity. Since the repository method returns an <kbd>Optional</kbd> product, we can use the&nbsp;<kbd>orElseThrow()</kbd> method in the <kbd>Optional</kbd> class to conveniently throw a <kbd>NotFoundException</kbd> exception if no product entity is found. Before the product information is returned, the&nbsp;<span><kbd>serviceUtil</kbd> object is used to fill in the currently used address of the microservice.</span></p>
<p>Finally, let's see the <kbd>deleteProduct</kbd> method:</p>
<pre><span>public void </span>deleteProduct(<span>int </span>productId) {<br>    <span>repository</span>.findByProductId(productId).ifPresent(e -&gt; <br>    <span>repository</span>.delete(e));<br>}</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p>The <kbd>delete</kbd> method also uses the&nbsp;<kbd>findByProductId()</kbd> method in the repository and uses the&nbsp;<kbd>ifPresent()</kbd> method in the <kbd>Optional</kbd> class to&nbsp;<span>conveniently delete the entity only if it exists. Note that the implementation is idempotent, that is, it will not report any failure if the entity is not found.</span></p>
<p>The source code for the three service implementation classes can be found at the following:</p>
<ul>
<li><kbd>se.magnus.microservices.core.product.services.ProductServiceImpl</kbd>&nbsp;in the&nbsp;<kbd><span>product</span></kbd><span>&nbsp;</span>project</li>
<li><kbd>se.magnus.microservices.core.recommendation.services.RecommendationServiceImpl</kbd>&nbsp;in the&nbsp;<kbd><span>recommendation</span></kbd><span>&nbsp;</span>project</li>
<li><kbd>se.magnus.microservices.core.review.services.ReviewServiceImpl</kbd>&nbsp;in the&nbsp;<kbd><span>review</span></kbd><span>&nbsp;</span>project</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Declaring a Java bean mapper</h1>
                </header>
            
            <article>
                
<p>So, what about the magic Java bean mapper?</p>
<p>As already mentioned, we use MapStruct to declare our mapper classes. The use of&nbsp;<span>MapStruct</span>&nbsp;is<span>&nbsp;similar in all three</span>&nbsp;core microservices, so we will only go through the&nbsp;<span>source code for the mapper object in the&nbsp;<kbd>Product</kbd>&nbsp;microservice.&nbsp;</span></p>
<p>The mapper class for the <kbd>product</kbd> service looks like this:</p>
<pre><span>@Mapper</span>(componentModel = <span>"spring"</span>)<br><span>public interface </span>ProductMapper {<br><br>    <span>@Mappings</span>({<br>        <span>@Mapping</span>(target = <span>"serviceAddress"</span>, ignore = <span>true</span>)<br>    })<br>    Product entityToApi(ProductEntity entity);<br><br>    <span>@Mappings</span>({<br>        <span>@Mapping</span>(target = <span>"id"</span>, ignore = <span>true</span>),<br>        <span>@Mapping</span>(target = <span>"version"</span>, ignore = <span>true</span>)<br>    })<br>    ProductEntity apiToEntity(Product api);<br>}</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<p><span>The following is observed from the preceding code:</span></p>
<ul>
<li>The <kbd>entityToApi()</kbd> method maps entity objects to the API model object. Since the entity class does not have a field for&nbsp;<kbd>serviceAddress</kbd>, the&nbsp;<span><kbd>entityToApi()</kbd> method</span> is annotated to ignore&nbsp;<span><kbd>serviceAddress</kbd>.</span></li>
<li>The <kbd>apiToEntity()</kbd> method maps&nbsp;<span>API model objects to&nbsp;entity objects. In the same way, the <kbd>apiToEntity()</kbd> method&nbsp;is annotated to ignore the&nbsp;<kbd>id</kbd> and <kbd>version</kbd> fields that are missing in the API model class.&nbsp;</span></li>
</ul>
<p>MapStruct does not only support mapping fields by name, but it can also be directed to map fields with different names. In the mapper class for the <kbd>Recommendation</kbd> service, the&nbsp;<kbd>rating</kbd> <span>entity field&nbsp;</span>is mapped to the API model field,&nbsp;<kbd>rate</kbd>, using the following annotations:</p>
<pre>    @Mapping(target = "rate", source="entity.rating"),<br>    Recommendation entityToApi(RecommendationEntity entity);<br><br>    @Mapping(target = "rating", source="api.rate"),<br>    RecommendationEntity apiToEntity(Recommendation api);</pre>
<p>After a successful&nbsp;<span>Gradle&nbsp;</span>build, the generated mapping implementation can be found in the <span><kbd>build/classes</kbd>&nbsp;folder, for example, the <kbd>Product</kbd> service:&nbsp;</span><kbd>$BOOK_HOME/Chapter06/microservices/product-service/build/classes/java/main/se/magnus/microservices/core/product/services/ProductMapperImpl.java</kbd>.</p>
<p>The source code for the three mapper classes can be found at the following:</p>
<ul>
<li><kbd>se.magnus.microservices.core.product.services.ProductMapper</kbd>&nbsp;in the&nbsp;<kbd><span>product</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.recommendation.services.RecommendationMapper</kbd>&nbsp;in the&nbsp;<kbd><span>recommendation</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.review.services.ReviewMapper</kbd>&nbsp;in the&nbsp;<kbd><span>review</span></kbd> project</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Declaring a Java bean mapper</h1>
                </header>
            
            <article>
                
<p>So, what about the magic Java bean mapper?</p>
<p>As already mentioned, we use MapStruct to declare our mapper classes. The use of <span>MapStruct</span> is<span> similar in all three</span> core microservices, so we will only go through the <span>source code for the mapper object in the <kbd>Product</kbd> microservice. </span></p>
<p>The mapper class for the <kbd>product</kbd> service looks like this:</p>
<pre><span>@Mapper</span>(componentModel = <span>"spring"</span>)<br><span>public interface </span>ProductMapper {<br><br>    <span>@Mappings</span>({<br>        <span>@Mapping</span>(target = <span>"serviceAddress"</span>, ignore = <span>true</span>)<br>    })<br>    Product entityToApi(ProductEntity entity);<br><br>    <span>@Mappings</span>({<br>        <span>@Mapping</span>(target = <span>"id"</span>, ignore = <span>true</span>),<br>        <span>@Mapping</span>(target = <span>"version"</span>, ignore = <span>true</span>)<br>    })<br>    ProductEntity apiToEntity(Product api);<br>}</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<p><span>The following is observed from the preceding code:</span></p>
<ul>
<li>The <kbd>entityToApi()</kbd> method maps entity objects to the API model object. Since the entity class does not have a field for <kbd>serviceAddress</kbd>, the <span><kbd>entityToApi()</kbd> method</span> is annotated to ignore <span><kbd>serviceAddress</kbd>.</span></li>
<li>The <kbd>apiToEntity()</kbd> method maps <span>API model objects to entity objects. In the same way, the <kbd>apiToEntity()</kbd> method is annotated to ignore the <kbd>id</kbd> and <kbd>version</kbd> fields that are missing in the API model class. </span></li>
</ul>
<p>MapStruct does not only support mapping fields by name, but it can also be directed to map fields with different names. In the mapper class for the <kbd>Recommendation</kbd> service, the <kbd>rating</kbd> <span>entity field </span>is mapped to the API model field, <kbd>rate</kbd>, using the following annotations:</p>
<pre>    @Mapping(target = "rate", source="entity.rating"),<br>    Recommendation entityToApi(RecommendationEntity entity);<br><br>    @Mapping(target = "rating", source="api.rate"),<br>    RecommendationEntity apiToEntity(Recommendation api);</pre>
<p>After a successful <span>Gradle </span>build, the generated mapping implementation can be found in the <span><kbd>build/classes</kbd> folder, for example, the <kbd>Product</kbd> service: </span><kbd>$BOOK_HOME/Chapter06/microservices/product-service/build/classes/java/main/se/magnus/microservices/core/product/services/ProductMapperImpl.java</kbd>.</p>
<p>The source code for the three mapper classes can be found at the following:</p>
<ul>
<li><kbd>se.magnus.microservices.core.product.services.ProductMapper</kbd> in the <kbd><span>product</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.recommendation.services.RecommendationMapper</kbd> in the <kbd><span>recommendation</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.review.services.ReviewMapper</kbd> in the <kbd><span>review</span></kbd> project</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Updating the service tests</h1>
                </header>
            
            <article>
                
<p>The tests of the APIs exposed by the core microservices have been updated since the previous chapter with tests on the create and delete API operations.</p>
<p class="mce-root"></p>
<p>The added tests are<span> similar in all three</span> core microservices, so we will only go through the <span>source code for the service tests in the <kbd>Product</kbd> microservice. </span></p>
<p>To ensure a known state for each test, a setup method, <kbd>setupDb()</kbd>, is declared and annotated with <kbd>@Before</kbd>, so it runs before each test runs. The setup method removes any previously created entities:</p>
<pre><span>@Autowired<br></span><span>private </span>ProductRepository <span>repository</span>;<br><br><span>@Before<br></span><span>public void </span>setupDb() {<br>   <span>repository</span>.deleteAll();<br>}</pre>
<p><span>The test method for the create API verifies that a</span> product <span>entity can be retrieved after it has been created and that creating another</span> product <span>entity with the same <kbd>productId</kbd> results in an expected error, </span><span><kbd>UNPROCESSABLE_ENTITY</kbd>,&nbsp;</span><span>in the response to the API request:</span></p>
<pre><span>@Test<br></span><span>public void </span>duplicateError() {<br>   <span>int </span>productId = <span>1</span>;<br>   postAndVerifyProduct(productId, <span>OK</span>);<br>   <span>assertTrue</span>(<span>repository</span>.findByProductId(productId).isPresent());<br><br>   postAndVerifyProduct(productId, <span>UNPROCESSABLE_ENTITY</span>)<br>      .jsonPath(<span>"$.path"</span>).isEqualTo(<span>"/product"</span>)<br>      .jsonPath(<span>"$.message"</span>).isEqualTo(<span>"Duplicate key, Product Id: " </span>+ <br>       productId);<br>}</pre>
<p>The test method for the delete API verifies that a product entity can be deleted and that a second delete request is idempotentâit also returns the status code OK, even though the entity no longer exists in the database:</p>
<pre><span>@Test<br></span><span>public void </span>deleteProduct() {<br>   <span>int </span>productId = <span>1</span>;<br>   postAndVerifyProduct(productId, <span>OK</span>);<br>   <span>assertTrue</span>(<span>repository</span>.findByProductId(productId).isPresent());<br><br>   deleteAndVerifyProduct(productId, <span>OK</span>);<br>   <span>assertFalse</span>(<span>repository</span>.findByProductId(productId).isPresent());<br><br>   deleteAndVerifyProduct(productId, <span>OK</span>);<br>}</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>To simplify sending the create, read, and delete requests to the API and verify the response status, three helper methods have been created: </p>
<ul>
<li><kbd>postAndVerifyProduct()</kbd></li>
<li>&nbsp;<kbd>getAndVerifyProduct()</kbd>&nbsp;</li>
<li>&nbsp;<kbd>deleteAndVerifyProduct()</kbd></li>
</ul>
<p>The <span><kbd>postAndVerifyProduct()</kbd> method looks like this:</span></p>
<pre><span>private </span>WebTestClient.BodyContentSpec postAndVerifyProduct(<span>int </span>productId, HttpStatus expectedStatus) {<br>   Product product = <span>new </span>Product(productId, <span>"Name " </span>+ productId, <br>   productId, <span>"SA"</span>);<br>   <span>return </span><span>client</span>.post()<br>      .uri(<span>"/product"</span>)<br>      .body(<span>just</span>(product), Product.<span>class</span>)<br>      .accept(<span>APPLICATION_JSON_UTF8</span>)<br>      .exchange()<br>      .expectStatus().isEqualTo(expectedStatus)<br>      .expectHeader().contentType(<span>APPLICATION_JSON_UTF8</span>)<br>      .expectBody();<br>}</pre>
<p>Added to performing the actual HTTP request and verifying its response code, the helper method also returns the body of the response for further investigations by the caller, if required. The other two helper methods for read and delete requests are similar and can be found in the source code pointed out at the beginning of this section.</p>
<p>The source code for the three service tests classes can be found at the following:</p>
<ul>
<li><kbd>se.magnus.microservices.core.product.ProductServiceApplicationTests</kbd> in the <kbd><span>product</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.recommendation.RecommendationServiceApplicationTests</kbd> in the <kbd><span>recommendation</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.review.ReviewServiceApplicationTests</kbd> in the <kbd><span>review</span></kbd> project</li>
</ul>
<p>Now, lets move on to seeing how we extend a composite service API.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding new operations in the composite service API</h1>
                </header>
            
            <article>
                
<p>The composite versions of creating and deleting entities and handling aggregated entities are similar to the create and delete operations in the core service APIs. The major difference is that they have annotations added for Swagger-based documentation.<span>&nbsp;For an explanation of the usage of the Swagger annotations, <kbd>@ApiOperation</kbd> and <kbd>@ApiResponses</kbd></span>,<span>&nbsp; refer to <a href="ba24a656-10a1-4a3e-879e-6589621ef125.xhtml">Chapter 5</a>, <em>Adding API Description Using OpenAPI/Swagger</em>, the section,&nbsp;<em>Adding API specific documentation in ProductCompositeService</em>.&nbsp;</span>The API operation for creating a composite product entity is declared as follows:</p>
<pre><span>@ApiOperation</span>(<br>    value = <span>"${api.product-composite.create-composite-<br>    product.description}"</span>,<br>    notes = <span>"${api.product-composite.create-composite-product.notes}"</span>)<br><span>@ApiResponses</span>(value = {<br>    <span>@ApiResponse</span>(code = <span>400</span>, message = <span>"Bad Request, invalid format of <br>    the request. See response message for more information."</span>),<br>    <span>@ApiResponse</span>(code = <span>422</span>, message = <span>"Unprocessable entity, input <br>    parameters caused the processing to fail. See response message for <br>    more information."</span>)<br>})<br><span>@PostMapping</span>(<br>    value    = <span>"/product-composite"</span>,<br>    consumes = <span>"application/json"</span>)<br><span>void </span>createCompositeProduct(<span>@RequestBody </span>ProductAggregate body);</pre>
<p><span>The API operation for deleting a composite</span> product <span>entity is declared as follows:</span></p>
<pre><span>@ApiOperation</span>(<br>    value = <span>"${api.product-composite.delete-composite-<br>    product.description}"</span>,<br>    notes = <span>"${api.product-composite.delete-composite-product.notes}"</span>)<br><span>@ApiResponses</span>(value = {<br>    <span>@ApiResponse</span>(code = <span>400</span>, message = <span>"Bad Request, invalid format of <br>    the request. See response message for more information."</span>),<br>    <span>@ApiResponse</span>(code = <span>422</span>, message = <span>"Unprocessable entity, input <br>    parameters caused the processing to fail. See response message for <br>    more information."</span>)<br>})<br><span>@DeleteMapping</span>(value = <span>"/product-composite/{productId}"</span>)<br><span>void </span>deleteCompositeProduct(<span>@PathVariable </span><span>int </span>productId);</pre>
<p>For the full source code, see the Java interface,&nbsp;<kbd>se.magnus.api.composite.product.ProductCompositeService</kbd>, in the <kbd>api</kbd> project.&nbsp;</p>
<p>We also need to, as before, add the descriptive text of the API documentation to the property file,&nbsp;<kbd>application.yml</kbd>:</p>
<pre><span>create-composite-product</span>:<br>  <span>description</span>: Creates a composite product<br>  <span>notes</span>: |<br>    # Normal response<br>    The composite product information posted to the API will be <br>    splitted up and stored as separate product-info, recommendation and <br>    review entities.<br><br>    # Expected error responses<br>    1. If a product with the same productId as specified in the posted <br>    information already exists, an &lt;b&gt;422 - Unprocessable Entity&lt;/b&gt; <br>    error with a "duplicate key" error message will be returned<br><br><span>delete-composite-product</span>:<br>  <span>description</span>: Deletes a product composite<br>  <span>notes</span>: |<br>    # Normal response<br>    Entities for product information, recommendations and reviews <br>    related to the specificed productId will be deleted.<br>    The implementation of the delete method is idempotent, that is, it <br>    can be called several times with the same response.<br>    This means that a delete request of a non existing product will <br>    return &lt;b&gt;200 Ok&lt;/b&gt;.</pre>
<p>For details, see the configuration file, <kbd>src/main/resources/application.yml</kbd>, in the&nbsp;<span><kbd>product-composite</kbd> project.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<p>The updated Swagger documentation will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/52f270b1-6039-4d6e-ad95-f4a9a7029063.png" width="1592" height="896" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/52f270b1-6039-4d6e-ad95-f4a9a7029063.png"></p>
<p>Later on in this chapter, we will user the Swagger UI to try out the new composite API operations.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding new operations in the composite service API</h1>
                </header>
            
            <article>
                
<p>The composite versions of creating and deleting entities and handling aggregated entities are similar to the create and delete operations in the core service APIs. The major difference is that they have annotations added for Swagger-based documentation.<span> For an explanation of the usage of the Swagger annotations, <kbd>@ApiOperation</kbd> and <kbd>@ApiResponses</kbd></span>,<span>  refer to <a href="ba24a656-10a1-4a3e-879e-6589621ef125.xhtml">Chapter 5</a>, <em>Adding API Description Using OpenAPI/Swagger</em>, the section, <em>Adding API specific documentation in ProductCompositeService</em>.&nbsp;</span>The API operation for creating a composite product entity is declared as follows:</p>
<pre><span>@ApiOperation</span>(<br>    value = <span>"${api.product-composite.create-composite-<br>    product.description}"</span>,<br>    notes = <span>"${api.product-composite.create-composite-product.notes}"</span>)<br><span>@ApiResponses</span>(value = {<br>    <span>@ApiResponse</span>(code = <span>400</span>, message = <span>"Bad Request, invalid format of <br>    the request. See response message for more information."</span>),<br>    <span>@ApiResponse</span>(code = <span>422</span>, message = <span>"Unprocessable entity, input <br>    parameters caused the processing to fail. See response message for <br>    more information."</span>)<br>})<br><span>@PostMapping</span>(<br>    value    = <span>"/product-composite"</span>,<br>    consumes = <span>"application/json"</span>)<br><span>void </span>createCompositeProduct(<span>@RequestBody </span>ProductAggregate body);</pre>
<p><span>The API operation for deleting a composite</span> product <span>entity is declared as follows:</span></p>
<pre><span>@ApiOperation</span>(<br>    value = <span>"${api.product-composite.delete-composite-<br>    product.description}"</span>,<br>    notes = <span>"${api.product-composite.delete-composite-product.notes}"</span>)<br><span>@ApiResponses</span>(value = {<br>    <span>@ApiResponse</span>(code = <span>400</span>, message = <span>"Bad Request, invalid format of <br>    the request. See response message for more information."</span>),<br>    <span>@ApiResponse</span>(code = <span>422</span>, message = <span>"Unprocessable entity, input <br>    parameters caused the processing to fail. See response message for <br>    more information."</span>)<br>})<br><span>@DeleteMapping</span>(value = <span>"/product-composite/{productId}"</span>)<br><span>void </span>deleteCompositeProduct(<span>@PathVariable </span><span>int </span>productId);</pre>
<p>For the full source code, see the Java interface, <kbd>se.magnus.api.composite.product.ProductCompositeService</kbd>, in the <kbd>api</kbd> project. </p>
<p>We also need to, as before, add the descriptive text of the API documentation to the property file, <kbd>application.yml</kbd>:</p>
<pre><span>create-composite-product</span>:<br>  <span>description</span>: Creates a composite product<br>  <span>notes</span>: |<br>    # Normal response<br>    The composite product information posted to the API will be <br>    splitted up and stored as separate product-info, recommendation and <br>    review entities.<br><br>    # Expected error responses<br>    1. If a product with the same productId as specified in the posted <br>    information already exists, an &lt;b&gt;422 - Unprocessable Entity&lt;/b&gt; <br>    error with a "duplicate key" error message will be returned<br><br><span>delete-composite-product</span>:<br>  <span>description</span>: Deletes a product composite<br>  <span>notes</span>: |<br>    # Normal response<br>    Entities for product information, recommendations and reviews <br>    related to the specificed productId will be deleted.<br>    The implementation of the delete method is idempotent, that is, it <br>    can be called several times with the same response.<br>    This means that a delete request of a non existing product will <br>    return &lt;b&gt;200 Ok&lt;/b&gt;.</pre>
<p>For details, see the configuration file, <kbd>src/main/resources/application.yml</kbd>, in the <span><kbd>product-composite</kbd> project.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<p>The updated Swagger documentation will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/52f270b1-6039-4d6e-ad95-f4a9a7029063.png" width="1592" height="896" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/52f270b1-6039-4d6e-ad95-f4a9a7029063.png"></p>
<p>Later on in this chapter, we will user the Swagger UI to try out the new composite API operations.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Implementing the new composite API operations</h1>
                </header>
            
            <article>
                
<p>Now, we can implement the composite create and delete methods!</p>
<p>The composite's create method will split up the aggregate product object into discrete objects for&nbsp;<span><kbd>product</kbd>, <kbd>recommendation</kbd>, and <kbd>review</kbd>&nbsp;</span>and call the corresponding create methods in the integration layer:</p>
<pre><span>@Override<br></span><span>public void </span>createCompositeProduct(ProductAggregate body) {<br>    <span>try </span>{<br>        Product product = <span>new </span>Product(body.getProductId(), <br>        body.getName(), body.getWeight(), <span>null</span>);<br>        <span>integration</span>.createProduct(product);<br><br>        <span>if </span>(body.getRecommendations() != <span>null</span>) {<br>            body.getRecommendations().forEach(r -&gt; {<br>                Recommendation recommendation = <span>new <br>                </span>Recommendation(<span>body</span>.getProductId(), <br>                r.getRecommendationId(), r.getAuthor(), r.getRate(), <br>                r.getContent(), <span>null</span>);<br>                <span>integration</span>.createRecommendation(recommendation);<br>            });<br>        }<br><br>        <span>if </span>(body.getReviews() != <span>null</span>) {<br>            body.getReviews().forEach(r -&gt; {<br>                Review review = <span>new </span>Review(<span>body</span>.getProductId(), <br>                r.getReviewId(), r.getAuthor(), r.getSubject(), <br>                r.getContent(), <span>null</span>);<br>                <span>integration</span>.createReview(review);<br>            });<br>        }<br>    } <span>catch </span>(RuntimeException re) {<br>        <span>LOG</span>.warn(<span>"createCompositeProduct failed"</span>, re);<br>        <span>throw </span>re;<br>    }<br>}</pre>
<p>The&nbsp;<span>composite's delete method</span>&nbsp;simply calls the three delete methods in the integration layer to delete the corresponding entities in the underlying databases:</p>
<pre><span>@Override<br></span><span>public void </span>deleteCompositeProduct(<span>int </span>productId) {<br>    <span>integration</span>.deleteProduct(productId);<br>    <span>integration</span>.deleteRecommendations(productId);<br>    <span>integration</span>.deleteReviews(productId);<br>}</pre>
<p>For full source code, see the&nbsp;<kbd>se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl</kbd>&nbsp;<span>class&nbsp;</span>in the&nbsp;<span><kbd>product-composite</kbd> project.</span></p>
<p>For happy day scenarios, this implementation will work fine, but if we consider various error scenarios this implementation will cause trouble!</p>
<p>What if, for example, one of the underlying core microservices temporarily is not available, for example, due to internal, network, or database problems?&nbsp;</p>
<p>This might result in partly created or deleted composite products. For the delete operation, this can be fixed if the requestor simply calls the composite's delete method until it succeeds. However, if the underlying problem remains for a while, the requestor will probably give up, resulting in an inconsistent state of the composite productânot acceptable in most cases!</p>
<p class="mce-root"></p>
<p>In the next chapter,&nbsp;<span><a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml" target="_blank">Chapter 7</a>,&nbsp;<em>Developing Reactive Microservices</em></span>, we will see how we can address these types of shortcomings with synchronous APIs as a RESTful API!</p>
<p>For now, let's move on with this fragile design in mind.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Updating the composite service tests</h1>
                </header>
            
            <article>
                
<p>Testing composite services, as already mentioned in <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml">Chapter 3</a>,<span>&nbsp;<em>Creating a Set of Cooperating Microservices</em> (refer to the&nbsp;</span><em>Automated tests of microservices in isolation&nbsp;</em><span>section), are&nbsp;</span>limited to using simple mock components instead of the actual core services. This restricts us from testing more complex scenarios, for example, error handling when trying to create duplicates in the underlying databases. The tests of the composite create and delete API operations are therefore relatively simple:</p>
<pre><span>@Test<br></span><span>public void </span>createCompositeProduct1() {<br>   ProductAggregate compositeProduct = <span>new </span>ProductAggregate(<span>1</span>, <span>"name"</span>, <br>   <span>1</span>, <span>null</span>, <span>null</span>, <span>null</span>);<br>   postAndVerifyProduct(compositeProduct, <span>OK</span>);<br>}<br><br><span>@Test<br></span><span>public void </span>createCompositeProduct2() {<br>    ProductAggregate compositeProduct = <span>new </span>ProductAggregate(<span>1</span>, <span>"name"</span>, <br>        <span>1</span>, <span>singletonList</span>(<span>new </span>RecommendationSummary(<span>1</span>, <span>"a"</span>, <span>1</span>, <span>"c"</span>)),<br>        <span>singletonList</span>(<span>new </span>ReviewSummary(<span>1</span>, <span>"a"</span>, <span>"s"</span>, <span>"c"</span>)), <span>null</span>);<br>    postAndVerifyProduct(compositeProduct, <span>OK</span>);<br>}<br><br><span>@Test<br></span><span>public void </span>deleteCompositeProduct() {<br>    ProductAggregate compositeProduct = <span>new </span>ProductAggregate(<span>1</span>, <span>"name"</span>, <br>        <span>1</span>,<span>singletonList</span>(<span>new </span>RecommendationSummary(<span>1</span>, <span>"a"</span>, <span>1</span>, <span>"c"</span>)),<br>        <span>singletonList</span>(<span>new </span>ReviewSummary(<span>1</span>, <span>"a"</span>, <span>"s"</span>, <span>"c"</span>)), <span>null</span>);<br>    postAndVerifyProduct(compositeProduct, <span>OK</span>);<br>    deleteAndVerifyProduct(compositeProduct.getProductId(), <span>OK</span>);<br>    deleteAndVerifyProduct(compositeProduct.getProductId(), <span>OK</span>);<br>}</pre>
<p><span>For the full source code, see the test class,&nbsp;<kbd>se.magnus.microservices.composite.product.ProductCompositeServiceApplicationTests</kbd>, in the&nbsp;<kbd>product-composite</kbd> project.</span></p>
<p>Next, we will see how to add databases to the landscape of Docker Compose.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Updating the composite service tests</h1>
                </header>
            
            <article>
                
<p>Testing composite services, as already mentioned in <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml">Chapter 3</a>,<span>&nbsp;<em>Creating a Set of Cooperating Microservices</em> (refer to the </span><em>Automated tests of microservices in isolation </em><span>section), are </span>limited to using simple mock components instead of the actual core services. This restricts us from testing more complex scenarios, for example, error handling when trying to create duplicates in the underlying databases. The tests of the composite create and delete API operations are therefore relatively simple:</p>
<pre><span>@Test<br></span><span>public void </span>createCompositeProduct1() {<br>   ProductAggregate compositeProduct = <span>new </span>ProductAggregate(<span>1</span>, <span>"name"</span>, <br>   <span>1</span>, <span>null</span>, <span>null</span>, <span>null</span>);<br>   postAndVerifyProduct(compositeProduct, <span>OK</span>);<br>}<br><br><span>@Test<br></span><span>public void </span>createCompositeProduct2() {<br>    ProductAggregate compositeProduct = <span>new </span>ProductAggregate(<span>1</span>, <span>"name"</span>, <br>        <span>1</span>, <span>singletonList</span>(<span>new </span>RecommendationSummary(<span>1</span>, <span>"a"</span>, <span>1</span>, <span>"c"</span>)),<br>        <span>singletonList</span>(<span>new </span>ReviewSummary(<span>1</span>, <span>"a"</span>, <span>"s"</span>, <span>"c"</span>)), <span>null</span>);<br>    postAndVerifyProduct(compositeProduct, <span>OK</span>);<br>}<br><br><span>@Test<br></span><span>public void </span>deleteCompositeProduct() {<br>    ProductAggregate compositeProduct = <span>new </span>ProductAggregate(<span>1</span>, <span>"name"</span>, <br>        <span>1</span>,<span>singletonList</span>(<span>new </span>RecommendationSummary(<span>1</span>, <span>"a"</span>, <span>1</span>, <span>"c"</span>)),<br>        <span>singletonList</span>(<span>new </span>ReviewSummary(<span>1</span>, <span>"a"</span>, <span>"s"</span>, <span>"c"</span>)), <span>null</span>);<br>    postAndVerifyProduct(compositeProduct, <span>OK</span>);<br>    deleteAndVerifyProduct(compositeProduct.getProductId(), <span>OK</span>);<br>    deleteAndVerifyProduct(compositeProduct.getProductId(), <span>OK</span>);<br>}</pre>
<p><span>For the full source code, see the test class, <kbd>se.magnus.microservices.composite.product.ProductCompositeServiceApplicationTests</kbd>, in the <kbd>product-composite</kbd> project.</span></p>
<p>Next, we will see how to add databases to the landscape of Docker Compose.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The Docker Compose configuration</h1>
                </header>
            
            <article>
                
<p>MongoDB and MySQL are declared as follows in the Docker Compose configuration file,&nbsp;<kbd>docker-compose.yml</kbd>:</p>
<pre><span>mongodb</span>:<br>  <span>image</span>: mongo:3.6.9<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"27017:27017"<br></span><span>  </span><span>command</span>: mongod --smallfiles<br><br><span>mysql</span>:<br>  <span>image</span>: mysql:5.7<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"3306:3306"<br></span><span>  </span><span>environment</span>:<br>    - MYSQL_ROOT_PASSWORD=rootpwd<br>    - MYSQL_DATABASE=review-db<br>    - MYSQL_USER=user<br>    - MYSQL_PASSWORD=pwd<br>  <span>healthcheck</span>:<br>    <span>test</span>: [<span>"CMD"</span>, <span>"mysqladmin" </span>,<span>"ping"</span>, <span>"-uuser"</span>, <span>"-ppwd"</span>, <span>"-h"</span>, <span>"localhost"</span>]<br>    <span>interval</span>: 10s<br>    <span>timeout</span>: 5s<br>    <span>retries</span>: 10</pre>
<p class="mce-root"></p>
<p><span>The following is observed from the preceding code:</span></p>
<ol>
<li>We will use the official Docker image for MongoDB V3.6.9 and MySQL 5.7 and forward their default ports <kbd>27017</kbd> and <kbd>3306</kbd> to the Docker host, also made available on <kbd>localhost</kbd> when using Docker for Mac.</li>
<li>For MySQL, we also declare some environment variables, defining the following:
<ul>
<li>The root password</li>
<li><span>The name of the database that will be created on image startup</span></li>
<li>A username and password for a user that is set up for the database on image startup</li>
</ul>
</li>
<li>For MySQL, we also declare a health check that Docker will run to determine the status of the MySQL database.</li>
</ol>
<p>To avoid problems with microservices that try to connect to their databases before the database is up and running,&nbsp; the <kbd>product</kbd> and&nbsp;<span><kbd>recommendation</kbd> services are declared dependent on the <kbd>mongodb</kbd> database, as follows</span>:</p>
<pre><span>product/</span><span>recommendation</span>:<br><span> depends_on</span>:<br> - mongodb</pre>
<p>This means that Docker Compose will not start up the&nbsp;<span><kbd>product</kbd> and&nbsp;</span><span><kbd>recommendation</kbd> containers until the <kbd>mongodb</kbd> container is launched.</span></p>
<p>For the same reason, the <kbd>review</kbd> service is declared dependent&nbsp;on the <kbd>mysql</kbd> database:</p>
<pre><span>review:</span><span><br>  depends_on</span>:<br>    <span>mysql</span>:<br>      <span>condition</span>: service_healthy</pre>
<p>In this case, the <kbd>review</kbd> service depends on the fact&nbsp;that the <kbd>mysql</kbd> container is not only launched, but also that the <kbd>mysql</kbd> containers health check reports are okay. The reason for this extra step is that the initialization of the <kbd>mysql</kbd> container includes setting up a database and creating a superuser for the database. This takes a few seconds and, to hold back the <kbd>review</kbd> service to startup before this is done, we direct Docker Compose to hold back the <kbd>review</kbd> container from being launched until the <kbd>mysql</kbd> container reports that it is operational through its health check.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The Docker Compose configuration</h1>
                </header>
            
            <article>
                
<p>MongoDB and MySQL are declared as follows in the Docker Compose configuration file, <kbd>docker-compose.yml</kbd>:</p>
<pre><span>mongodb</span>:<br>  <span>image</span>: mongo:3.6.9<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"27017:27017"<br></span><span>  </span><span>command</span>: mongod --smallfiles<br><br><span>mysql</span>:<br>  <span>image</span>: mysql:5.7<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"3306:3306"<br></span><span>  </span><span>environment</span>:<br>    - MYSQL_ROOT_PASSWORD=rootpwd<br>    - MYSQL_DATABASE=review-db<br>    - MYSQL_USER=user<br>    - MYSQL_PASSWORD=pwd<br>  <span>healthcheck</span>:<br>    <span>test</span>: [<span>"CMD"</span>, <span>"mysqladmin" </span>,<span>"ping"</span>, <span>"-uuser"</span>, <span>"-ppwd"</span>, <span>"-h"</span>, <span>"localhost"</span>]<br>    <span>interval</span>: 10s<br>    <span>timeout</span>: 5s<br>    <span>retries</span>: 10</pre>
<p class="mce-root"></p>
<p><span>The following is observed from the preceding code:</span></p>
<ol>
<li>We will use the official Docker image for MongoDB V3.6.9 and MySQL 5.7 and forward their default ports <kbd>27017</kbd> and <kbd>3306</kbd> to the Docker host, also made available on <kbd>localhost</kbd> when using Docker for Mac.</li>
<li>For MySQL, we also declare some environment variables, defining the following:
<ul>
<li>The root password</li>
<li><span>The name of the database that will be created on image startup</span></li>
<li>A username and password for a user that is set up for the database on image startup</li>
</ul>
</li>
<li>For MySQL, we also declare a health check that Docker will run to determine the status of the MySQL database.</li>
</ol>
<p>To avoid problems with microservices that try to connect to their databases before the database is up and running,  the <kbd>product</kbd> and <span><kbd>recommendation</kbd> services are declared dependent on the <kbd>mongodb</kbd> database, as follows</span>:</p>
<pre><span>product/</span><span>recommendation</span>:<br><span> depends_on</span>:<br> - mongodb</pre>
<p>This means that Docker Compose will not start up the <span><kbd>product</kbd> and </span><span><kbd>recommendation</kbd> containers until the <kbd>mongodb</kbd> container is launched.</span></p>
<p>For the same reason, the <kbd>review</kbd> service is declared dependent on the <kbd>mysql</kbd> database:</p>
<pre><span>review:</span><span><br>  depends_on</span>:<br>    <span>mysql</span>:<br>      <span>condition</span>: service_healthy</pre>
<p>In this case, the <kbd>review</kbd> service depends on the fact that the <kbd>mysql</kbd> container is not only launched, but also that the <kbd>mysql</kbd> containers health check reports are okay. The reason for this extra step is that the initialization of the <kbd>mysql</kbd> container includes setting up a database and creating a superuser for the database. This takes a few seconds and, to hold back the <kbd>review</kbd> service to startup before this is done, we direct Docker Compose to hold back the <kbd>review</kbd> container from being launched until the <kbd>mysql</kbd> container reports that it is operational through its health check.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Database connect configuration</h1>
                </header>
            
            <article>
                
<p>With the database in place, we now need to set up the configuration for the core microservices so they know how to connect to their databases. This is set up in each<span> core microservice's configuration file, <kbd>src/main/resources/application.yml</kbd>, in the <kbd>product</kbd>, <kbd>recommendation</kbd>, and <kbd>review</kbd> projects.</span></p>
<p>The configuration for the <kbd>product</kbd> and <kbd>recommendation</kbd> services are similar, so we will only look into the configuration of the <kbd>product</kbd> services. The following part of the configuration is of interest:</p>
<pre><span>spring.data.mongodb</span>:<br>  <span>host</span>: localhost<br>  <span>port</span>: <span>27017<br></span><span>  </span><span>database</span>: product-db<br><span><br>logging</span>:<br> <span>level</span>:<br> <span>org.springframework.data.mongodb.core.MongoTemplate</span>: DEBUG<br><br>---<br><span>spring.profiles</span>: docker<br><br><span>spring.data.mongodb.host</span>: mongodb</pre>
<p><span>The following is observed from the preceding code:</span></p>
<ol>
<li>When running without Docker using the default Spring profile, the database is expected to be reachable on <kbd>localhost:27017</kbd>.</li>
<li>Setting the log level for <kbd>MongoTemplate</kbd> to <kbd>DEBUG</kbd> will allow us to see which MongoDB statements are executed in the log.</li>
<li>When running inside Docker using the Spring profile, <kbd><span>Docker</span></kbd>, the <span><span>database is expected to be reachable on</span></span> <span><kbd>mongodb:27017</kbd>.</span></li>
</ol>
<p><span>The configuration for the <kbd>review</kbd> service, which affects how it connects to its SQL database, looks like the following:</span></p>
<pre><span>spring.jpa.hibernate.ddl-auto</span>: update<br><br><span>spring.datasource</span>:<br>  <span>url</span>: jdbc:mysql://localhost/review-db<br>  <span>username</span>: user<br>  <span>password</span>: pwd<br><br><span>spring.datasource.hikari.initializationFailTimeout</span>: 60000<br><br><span>logging</span>:<br> <span>level</span>:<br> <span>org.hibernate.SQL</span>: DEBUG<br> <span>org.hibernate.type.descriptor.sql.BasicBinder</span>: TRACE<br><br>---<br><span>spring.profiles</span>: docker<br><br><span>spring.datasource</span>:<br> <span>url</span>: jdbc:mysql://mysql/review-db</pre>
<p><span>The following is observed from the preceding code:</span></p>
<ol>
<li>By default, Hibernate will be used by Spring Data JPA as the JPA Entity Manager.</li>
<li>The <kbd>spring.jpa.hibernate.ddl-auto</kbd> <span>property </span>is used to tell Spring Data JPA to create new or update existing SQL tables during startup.<br>
<strong>Note:</strong> It is strongly recommended to set the <kbd>spring.jpa.hibernate.ddl-auto</kbd><span> property to <kbd>none</kbd> in a production environmentâthis prevents Spring Data JPA to manipulate the structure of the SQL tables.</span></li>
<li>When running without Docker, using the default Spring profile, the database is expected to be reachable on<span>&nbsp;</span><kbd>localhost</kbd> using the default port <kbd>3306</kbd>.</li>
<li>By default, HikariCP is used <span>by Spring Data JPA </span>as the<span><span> JDBC connection pool. </span></span><span><span>To minimize startup problems on computers with limited hardware resources, the</span></span> <kbd>initializationFailTimeout</kbd> <span>parameter </span>is set to 60 seconds. This means that the Spring Boot application will wait for up to 60 seconds during startup to establish a database connection.</li>
<li>The log level settings for Hibernate will cause Hibernate to print the SQL statements used and the actual values used. Please note that, when used in a production environment, writing the actual values to the log should be avoided for privacy reasons.</li>
<li>When running inside Docker using the Spring profile, <kbd><span>Docker</span></kbd>, the <span><span>database is expected to be reachable on the</span></span> <span><kbd>mysql</kbd> hostname using the default port <kbd>3306</kbd>.</span></li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Manual tests of the new APIs and the&nbsp;persistence layer</h1>
                </header>
            
            <article>
                
<p>Now, it is finally time to start everything up and test it manually using the Swagger UI.</p>
<p>Build and start the system landscape with the following command:</p>
<pre><strong><span>cd $BOOK_HOME/</span><span>Chapter06<br></span><span>./gradlew build &amp;&amp; </span><span>docker-compose build </span><span>&amp;&amp; </span><span>docker-compose up</span></strong></pre>
<p class="mce-root"></p>
<p>Open the Swagger UI in a web browser, <kbd>http://localhost:8080/swagger-ui.html</kbd>, and perform the following steps on the web page:</p>
<ol>
<li>Click on <span class="packt_screen">product-composite-service-impl</span> and the&nbsp;<span class="packt_screen">POST</span> <span>method&nbsp;</span>to expand them.</li>
<li>Click on the <span class="packt_screen">Try it out</span>&nbsp;button and go down to the body field.</li>
<li>Replace the default value, <kbd>0</kbd>,&nbsp; of the <kbd>productId</kbd> field with <kbd>123456</kbd>.</li>
<li>Scroll down to the<span>&nbsp;</span><span class="packt_screen">Execute</span><span>&nbsp;</span>button and click on it.</li>
<li>Verify that the returned response code is <kbd>200</kbd>.</li>
</ol>
<p>Following is a sample screenshot after hitting the <span class="packt_screen">Execute</span> button:&nbsp;</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/eb728f85-f6b7-42e9-89a2-41ed11559551.png" style="width:44.67em;height:34.58em;" width="1344" height="1040" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/eb728f85-f6b7-42e9-89a2-41ed11559551.png"></p>
<p class="mce-root"></p>
<p>In the log output from the <span><kbd>docker-compose up</kbd>&nbsp;command,&nbsp;we should be able to see output like the following (abbreviated&nbsp;for increased readability):</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1b2a26c1-fe45-49c0-95e9-9a8d4384351e.png" width="2074" height="246" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1b2a26c1-fe45-49c0-95e9-9a8d4384351e.png"></p>
<p>We can also use the database CLI tools to see the actual content in the different databases.</p>
<p>Look up content in the <kbd>product</kbd> service, that is, the&nbsp;<kbd>products</kbd> collection in MongoDB, with the following command:</p>
<pre><strong>docker-compose exec mongodb mongo product-db --quiet --eval "db.products.find()"</strong></pre>
<p>Expect a response like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0b57bb76-075c-4cfd-9d65-43d2760a8200.png" width="2093" height="191" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0b57bb76-075c-4cfd-9d65-43d2760a8200.png"></p>
<p><span>Look up c</span>ontent in the <kbd>recommendation</kbd> service, that is, the<span>&nbsp;</span><kbd>recommendations</kbd><span>&nbsp;</span>collection in MongoDB,<span>&nbsp;with the following command</span>:</p>
<pre><strong>docker-compose exec mongodb mongo recommendation-db --quiet --eval "db.recommendations.find()"</strong></pre>
<p><span>Expect a response like:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/12d6b674-b3ac-4819-9fc8-5423d00c17af.png" width="1923" height="240" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/12d6b674-b3ac-4819-9fc8-5423d00c17af.png">10</p>
<p>Look up content in the <kbd>review</kbd> service, that is, the<span>&nbsp;</span><kbd>reviews</kbd><span>&nbsp;</span>table in MySQL<span>,</span><span>&nbsp;with the following command</span>:</p>
<pre><strong>docker-compose exec mysql mysql -uuser -p review-db -e "select * from reviews"</strong></pre>
<p class="mce-root"></p>
<p><span>The&nbsp;</span><kbd>mysql</kbd><span>&nbsp;CLI tool will prompt you for a password; y</span><span>ou can find it in the&nbsp;<kbd>docker-compose.yml</kbd>&nbsp;file. Look for the value of the environment variable,&nbsp;</span><kbd>MYSQL_PASSWORD</kbd><span>.&nbsp;</span><span>Expect a response like&nbsp;the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b408949c-fb6d-4b76-8976-556538b135d2.png" style="width:30.58em;height:7.08em;" width="1427" height="331" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b408949c-fb6d-4b76-8976-556538b135d2.png"></p>
<p><span>Bring down the system landscape by&nbsp;i</span>nterrupting the&nbsp;<span><kbd>docker-compose up</kbd> command with&nbsp;</span><em>Ctrl + C</em>, <span>followed by the command,&nbsp;</span><span><kbd>docker-compose down</kbd>. After this, let us see how to update the automated tests in a microservice landscape.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Manual tests of the new APIs and the persistence layer</h1>
                </header>
            
            <article>
                
<p>Now, it is finally time to start everything up and test it manually using the Swagger UI.</p>
<p>Build and start the system landscape with the following command:</p>
<pre><strong><span>cd $BOOK_HOME/</span><span>Chapter06<br></span><span>./gradlew build &amp;&amp; </span><span>docker-compose build </span><span>&amp;&amp; </span><span>docker-compose up</span></strong></pre>
<p class="mce-root"></p>
<p>Open the Swagger UI in a web browser, <kbd>http://localhost:8080/swagger-ui.html</kbd>, and perform the following steps on the web page:</p>
<ol>
<li>Click on <span class="packt_screen">product-composite-service-impl</span> and the <span class="packt_screen">POST</span> <span>method </span>to expand them.</li>
<li>Click on the <span class="packt_screen">Try it out</span> button and go down to the body field.</li>
<li>Replace the default value, <kbd>0</kbd>,  of the <kbd>productId</kbd> field with <kbd>123456</kbd>.</li>
<li>Scroll down to the<span>&nbsp;</span><span class="packt_screen">Execute</span><span>&nbsp;</span>button and click on it.</li>
<li>Verify that the returned response code is <kbd>200</kbd>.</li>
</ol>
<p>Following is a sample screenshot after hitting the <span class="packt_screen">Execute</span> button: </p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/eb728f85-f6b7-42e9-89a2-41ed11559551.png" style="width:44.67em;height:34.58em;" width="1344" height="1040" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/eb728f85-f6b7-42e9-89a2-41ed11559551.png"></p>
<p class="mce-root"></p>
<p>In the log output from the <span><kbd>docker-compose up</kbd> command, we should be able to see output like the following (abbreviated for increased readability):</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1b2a26c1-fe45-49c0-95e9-9a8d4384351e.png" width="2074" height="246" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1b2a26c1-fe45-49c0-95e9-9a8d4384351e.png"></p>
<p>We can also use the database CLI tools to see the actual content in the different databases.</p>
<p>Look up content in the <kbd>product</kbd> service, that is, the <kbd>products</kbd> collection in MongoDB, with the following command:</p>
<pre><strong>docker-compose exec mongodb mongo product-db --quiet --eval "db.products.find()"</strong></pre>
<p>Expect a response like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0b57bb76-075c-4cfd-9d65-43d2760a8200.png" width="2093" height="191" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0b57bb76-075c-4cfd-9d65-43d2760a8200.png"></p>
<p><span>Look up c</span>ontent in the <kbd>recommendation</kbd> service, that is, the<span>&nbsp;</span><kbd>recommendations</kbd><span>&nbsp;</span>collection in MongoDB,<span> with the following command</span>:</p>
<pre><strong>docker-compose exec mongodb mongo recommendation-db --quiet --eval "db.recommendations.find()"</strong></pre>
<p><span>Expect a response like:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/12d6b674-b3ac-4819-9fc8-5423d00c17af.png" width="1923" height="240" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/12d6b674-b3ac-4819-9fc8-5423d00c17af.png">10</p>
<p>Look up content in the <kbd>review</kbd> service, that is, the<span>&nbsp;</span><kbd>reviews</kbd><span>&nbsp;</span>table in MySQL<span>,</span><span> with the following command</span>:</p>
<pre><strong>docker-compose exec mysql mysql -uuser -p review-db -e "select * from reviews"</strong></pre>
<p class="mce-root"></p>
<p><span>The </span><kbd>mysql</kbd><span> CLI tool will prompt you for a password; y</span><span>ou can find it in the <kbd>docker-compose.yml</kbd> file. Look for the value of the environment variable, </span><kbd>MYSQL_PASSWORD</kbd><span>.&nbsp;</span><span>Expect a response like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b408949c-fb6d-4b76-8976-556538b135d2.png" style="width:30.58em;height:7.08em;" width="1427" height="331" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b408949c-fb6d-4b76-8976-556538b135d2.png"></p>
<p><span>Bring down the system landscape by i</span>nterrupting the <span><kbd>docker-compose up</kbd> command with </span><em>Ctrl + C</em>, <span>followed by the command, </span><span><kbd>docker-compose down</kbd>. After this, let us see how to update the automated tests in a microservice landscape.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Updating the automated tests of the microservice landscape</h1>
                </header>
            
            <article>
                
<p>The automated tests of the microservice landscape, <kbd>test-em-all.bash</kbd>, needs to be updated so that they ensure that the database of each microservice has a known state before it runs the tests.</p>
<p>The script is extended with a setup function, <kbd><span>setupTestdata</span>()</kbd>, which uses the <span>composite's </span>create and delete APIs to recreate the products that the tests use into a known state.</p>
<p><span>The <kbd>setupTestdata</kbd> function looks like this:</span></p>
<pre><span>function </span><span>setupTestdata</span>() {<br><br>    body=\<br><span>    '{"productId":1,"name":"product 1","weight":1, "recommendations":[<br></span><span>        {"recommendationId":1,"author":"author <br>         1","rate":1,"content":"content 1"},<br></span><span>        {"recommendationId":2,"author":"author <br>         2","rate":2,"content":"content 2"},<br></span><span>        {"recommendationId":3,"author":"author <br>         3","rate":3,"content":"content 3"}<br></span><span>    ], "reviews":[<br></span><span>        {"reviewId":1,"author":"author 1","subject":"subject <br>         1","content":"content 1"},<br></span><span>        {"reviewId":2,"author":"author 2","subject":"subject <br>         2","content":"content 2"},<br></span><span>        {"reviewId":3,"author":"author 3","subject":"subject <br>         3","content":"content 3"}<br></span><span>    ]}'<br></span><span>    </span><span>recreateComposite </span><span>1 </span><span>"$body"<br></span><span><br></span><span>    </span>body=\<br><span>    '{"productId":113,"name":"product 113","weight":113, "reviews":[<br></span><span>    {"reviewId":1,"author":"author 1","subject":"subject <br>     1","content":"content 1"},<br></span><span>    {"reviewId":2,"author":"author 2","subject":"subject <br>     2","content":"content 2"},<br></span><span>    {"reviewId":3,"author":"author 3","subject":"subject <br>     3","content":"content 3"}<br></span><span>]}'<br></span><span>    </span><span>recreateComposite </span><span>113 </span><span>"$body"<br></span><span><br></span><span>    </span>body=\<br><span>    '{"productId":213,"name":"product 213","weight":213, <br>    "recommendations":[</span><span><br>       {"recommendationId":1,"author":"author <br>         1","rate":1,"content":"content 1"},<br></span><span>       {"recommendationId":2,"author":"author <br>        2","rate":2,"content":"content 2"},<br></span><span>       {"recommendationId":3,"author":"author <br>        3","rate":3,"content":"content 3"}<br></span><span>]}'<br></span><span>    </span><span>recreateComposite </span><span>213 </span><span>"$body"<br></span><span><br></span>}</pre>
<p>It uses a helper function, <kbd>recreateComposite()</kbd>, to perform the actual requests to the create and delete APIs:</p>
<pre><span>function </span><span>recreateComposite</span>() {<br>    <span>local </span>productId=$1<br>    <span>local </span>composite=$2<br><br>    <span>assertCurl </span><span>200 </span><span>"curl -X DELETE http://$HOST:$PORT/product-<br>    composite/${productId} -s"<br></span><span>    </span>curl -X POST http://$HOST:$PORT/product-composite -H <span>"Content-Type: <br>    application/json" </span>--data <span>"</span><span>$composite</span><span>"<br></span>}</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>The<span>&nbsp;</span><kbd>setupTestdata</kbd><span>&nbsp;</span><span>function is called directly after the </span><span><kbd>waitForService</kbd> function:</span></p>
<pre><strong><span>waitForService </span>curl -X DELETE http://$HOST:$PORT/product-composite/1</strong>3<br><br><span>setupTestdata</span></pre>
<p><span>The main purpose of the <kbd>waitForService</kbd> function is to verify that all microservices are up and running. In the previous chapter, the get API on the composite product service was used. In this chapter, the delete API is used instead. When using the get API, only the product core microservice is called if the entity is not found; the recommendation and <kbd>review</kbd> services will not be called to verify that they are up and running. The call to the delete API will also ensure that the <em>Not Found</em>-test on <kbd>productId 13</kbd> will succeed. </span><span>Later on in this book, we will see how we can define specific APIs for checking the health state of a microservice landscape.</span></p>
<p>Execute the updated test script with the following command:</p>
<pre><strong><span>cd $BOOK_HOME/</span><span>Chapter06</span><span><br>./test-em-all.bash start stop</span></strong></pre>
<p>The execution should end by writing a log message like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bcd02c04-4291-440a-96e4-0660e2792679.png" style="width:19.25em;height:4.25em;" width="657" height="145" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bcd02c04-4291-440a-96e4-0660e2792679.png"></p>
<p>This ends updating of the automated tests of the microservice landscape.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how we can use Spring Data to add a persistence layer to the core microservices. We used the core concepts of Spring Data, repositories and entities, to store data in both MongoDB and MySQL using a programming model that is similar, even though not fully portable. We have also seen how Spring Boot's annotations, <span><kbd>@DataMongoTest</kbd> and </span><span><kbd>@DataJpaTest</kbd>, can be used to conveniently set up tests targeted for persistence; this is where an embedded database is started automatically before the test runs, but no other infrastructure that the microservice will need in runtime, for example, a web server such as Netty, is started up. This results in persistence tests that are easy to set up and that start with minimum overhead.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>We have also seen how the persistence layer can be used by the service layer and how we can add APIs for creating and deleting entities, both core and composite entities. </span></p>
<p>Finally, we learned how convenient it is to start up databases such as MongoDB and MySQL in runtime using Docker Compose and how to use the new create and delete APIs to set up test data before running automated tests of the microservice-based system landscape.</p>
<p>However, one major concern was identified in this chapter. Updating (creating or deleting) a composite entityâan entity whose parts are stored in a number of microservices<span>â</span>using synchronous APIs can lead to inconsistencies, if not all involved microservices are updated successfully. This is, in general, not acceptable. This leads us into the next chapter, where we will look into why and how to build reactive microservices, that is, microservices that are scalable and robust.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Spring Data, a common programming model based on entities and repositories, can be used for different types of database engines. From the source code examples in this chapter, what are the most important differences in the persistence code for MySQL and MongoDB?</li>
<li>What is required to implement optimistic locking using Spring Data?</li>
<li>What is MapStruct used for?</li>
<li>What does it mean that an operation is idempotent and why is that useful?</li>
<li>How can we access the data that is stored in the MySQL and MongoDB databases <span>without using the API</span>?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Developing Reactive Microservices</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to develop reactive microservices, that is, how to develop non-blocking synchronous REST APIs and asynchronous event-driven services <span>using Spring</span>.<span> We will also learn about how to choose between these two alternatives. Finally, we will see how to create and run manual and automated tests of a reactive microservice landscape.</span></p>
<p><span>As already described</span> i<span>n the <em>Reactive microservices</em> section in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em>,</span> <span>the foundations for reactive systems is that they are </span>message-driven<span>âthey use asynchronous communication. This enables them to be </span>elastic<span>, that is, scalable and </span>resilient<span>, meaning that they will be tolerant to failures. Elasticity and resilience together will enable a reactive system to be </span>responsive; they will be able to respond in a timely fashion.  </p>
<p><span>The following topics will be covered in this chapter:</span></p>
<ul>
<li><span>Choosing between non-blocking synchronous APIs and event-driven asynchronous services</span></li>
<li><span>Developing non-blocking synchronous REST APIs using Spring</span></li>
<li>Developing event-driven asynchronous services</li>
<li>Running manual tests of the reactive microservice landscape</li>
<li>Running automated tests of the reactive microservice landscape</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Choosing between non-blocking synchronous APIs and event-driven asynchronous services</h1>
                </header>
            
            <article>
                
<p><span>When developing reactive microservices, i</span>t is not always obvious when to use non-blocking synchronous APIs and when to use event-driven asynchronous services. In general, to make a microservice robust and scalable, it is important to make it as autonomous as possible, for example, minimizing its runtime dependencies. This is also known as <strong>loose</strong> <strong>coupling</strong>.&nbsp;Therefore, asynchronous message passing&nbsp;of events, is preferable over synchronous APIs. This is because the microservice will only depend on access to the messaging system at runtime instead of being dependent on synchronous access to a number of other microservices.</p>
<p>There are, however, a number of cases where&nbsp;<span>non-blocking synchronous APIs&nbsp;could be favorable to use, for example:</span></p>
<ul>
<li>For read operations where an end user is waiting for a response</li>
<li>Where the client platforms are more suitable for consuming synchronous APIs, for example, mobile apps or SPA web applications</li>
<li>Where the clients will connect to the service from other organizationsâwhere it might be hard to agree over a common messaging system to use across&nbsp;<span>organizations</span></li>
</ul>
<p>For the system landscape used in this book, we will use the following:</p>
<ul>
<li>The create, read, and delete services exposed by the product composite microservice will be based on<span>&nbsp;synchronous APIs. The composite microservice&nbsp;</span>is assumed to have clients on both web and mobile platforms, as well as clients coming from other organizations rather than the ones that operate the&nbsp;<span><span>system landscape. Therefore, synchronous APIs seem like a natural match.</span></span></li>
<li>The read services provided by the core microservices will also be developed as&nbsp;<span>non-blocking synchronous APIs since there is an end user waiting for their responses.</span></li>
<li>The create and delete services&nbsp;<span>provided by the core microservices will&nbsp;be developed as&nbsp;event-driven asynchronous services. The synchronous APIs provided by the composite microservices to create and delete aggregated product information will simply publish, create, and delete events on the topics that the core services listen on and then return with a 200 (OK) response.</span></li>
</ul>
<p>This is illustrated by the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a8f1c5bd-e29b-4f4d-a527-a0f86fd863b1.png" style="width:57.58em;height:16.67em;" width="1793" height="519" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a8f1c5bd-e29b-4f4d-a527-a0f86fd863b1.png"></p>
<p><span>First, let's learn how we can develop non-blocking synchronous REST APIs, and thereafter, we will look at how to develop event-driven asynchronous services.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Developing non-blocking synchronous REST APIs using Spring</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to develop non-blocking versions of the read APIs. The composite service will make reactive, that is, non-blocking, calls in parallel to the three core services. When the composite service has received responses from the core services, it will create a composite response and send it back to the caller. This is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/676c4d79-653b-438c-b8ff-7ffe0477644d.png" style="width:39.42em;height:17.25em;" width="1299" height="568" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/676c4d79-653b-438c-b8ff-7ffe0477644d.png"></p>
<p>We will cover the following:</p>
<ul>
<li>An introduction to Spring Reactor</li>
<li>Non-blocking persistence using Spring Data for MongoDB</li>
<li>Non-blocking REST APIs in the core services, including how to handle blocking code for the JPA-based persistence layer</li>
<li>Non-blocking REST APIs in the composite service</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Developing non-blocking synchronous REST APIs using Spring</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to develop non-blocking versions of the read APIs. The composite service will make reactive, that is, non-blocking, calls in parallel to the three core services. When the composite service has received responses from the core services, it will create a composite response and send it back to the caller. This is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/676c4d79-653b-438c-b8ff-7ffe0477644d.png" style="width:39.42em;height:17.25em;" width="1299" height="568" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/676c4d79-653b-438c-b8ff-7ffe0477644d.png"></p>
<p>We will cover the following:</p>
<ul>
<li>An introduction to Spring Reactor</li>
<li>Non-blocking persistence using Spring Data for MongoDB</li>
<li>Non-blocking REST APIs in the core services, including how to handle blocking code for the JPA-based persistence layer</li>
<li>Non-blocking REST APIs in the composite service</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">An introduction to Spring Reactor</h1>
                </header>
            
            <article>
                
<p><span>As we mentioned</span> i<span>n the <em>Beginning with Spring WebFlux</em> section in <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml">Chapter 2</a>, <em>Introduction to Spring Boot</em>,</span> <span>the reactive support in Spring 5 is based on </span><strong>Project Reactor</strong> <span>(</span><a href="https://projectreactor.io">https://projectreactor.io</a><span>). </span> <span>Project Reactor is based on the</span> <span><em>Reactive Streams specification</em> (</span><a href="http://www.reactive-streams.org">http://www.reactive-streams.org</a><span>)</span><span>, a standard for building reactive applications. Spring Reactor is fundamental and it is what Spring</span> WebFlux<span>, Spring</span> WebClient, <span>and Spring Data rely on to provide their reactive and non-blocking features.</span></p>
<p>The programming model is based on processing streams of data, and the core data types in Project Reactor are <kbd>Flux</kbd> and <kbd>Mono</kbd>. A <kbd>Flux</kbd> object is used to process a stream of <em>0</em>...<em>n</em> elements and a <kbd>Mono</kbd> object is used to process <em>0</em>...<em>1</em> elements. We will see numerous examples of its usage in this chapter. As a short introduction, let's look at the following test:</p>
<pre><span>@Test<br></span><span>public void </span>TestFlux() {<br><br>    List&lt;Integer&gt; list = <span>new </span>ArrayList&lt;&gt;();<br><br>    Flux.<span>just</span>(<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>)<br>        .filter(n -&gt; n % <span>2 </span>== <span>0</span>)<br>        .map(n -&gt; n * <span>2</span>)<br>        .log()<br>        .subscribe(n -&gt; <span>list</span>.add(n));<br><br>    <span>assertThat</span>(list).containsExactly(<span>4</span>, <span>8</span>);<br>}</pre>
<p>Here is an explanation of the preceding source code:</p>
<ol>
<li>We initiate the stream with the integers <kbd>1</kbd>, <kbd>2</kbd>, <kbd>3</kbd>, and <kbd>4</kbd>.</li>
<li>Next, we <kbd>filter</kbd> out the odd numbersâwe only allow even numbers to proceed through the streamâin this test, these are <kbd>2</kbd> and <kbd>4</kbd>.</li>
<li>Next, we transform (or <kbd>map</kbd>) the values in the stream by multiplying them by <kbd>2</kbd>, that is, to <kbd>4</kbd> and <kbd>8</kbd>.</li>
<li>Then, we <kbd>log</kbd> the data that flows through the stream after the <kbd>map</kbd> operation.</li>
<li>So far, we have only declared the processing of a stream. To actually get the stream processed, we need someone to subscribe to it. The final call to the <kbd>subscribe</kbd> method will register a subscriber and the subscriber will apply the lambda function specified in the call to the <kbd>subscribe</kbd> method on each element it gets from the stream. Thereafter, it will add them to the <kbd>list</kbd> element.</li>
<li>Finally, we can assert that <kbd>list</kbd> after the processing of the stream contains the expected resultâthe integers <kbd>4</kbd> and <kbd>8</kbd>.</li>
</ol>
<p>The log output will look like the following code:</p>
<pre><strong>20:01:45.714 [main] INFO reactor.Flux.MapFuseable.1 - | onSubscribe([Fuseable] FluxMapFuseable.MapFuseableSubscriber)</strong><br><strong>20:01:45.716 [main] INFO reactor.Flux.MapFuseable.1 - | request(unbounded)</strong><br><strong>20:01:45.716 [main] INFO reactor.Flux.MapFuseable.1 - | onNext(4)</strong><br><strong>20:01:45.717 [main] INFO reactor.Flux.MapFuseable.1 - | onNext(8)</strong><br><strong>20:01:45.717 [main] INFO reactor.Flux.MapFuseable.1 - | onComplete()</strong></pre>
<p><span>Here is an explanation of the preceding source code:</span></p>
<ol>
<li>The processing of the stream is started by a subscriber that subscribes to the stream and requests its content.</li>
<li>Next, the integers <kbd>4</kbd> and <kbd>8</kbd> pass through the <kbd>log</kbd> operation.</li>
<li>The processing concludes with a call to the <kbd>onComplete</kbd> method on the subscriber, notifying it that the stream has come to an end.</li>
</ol>
<p><span>For the full source code, see the <kbd>se.magnus.util.reactor.ReactorTests</kbd> test class in the <kbd>util</kbd> project.</span></p>
<div class="packt_infobox">Normally, we don't initiate the processing of the stream. Instead, we will only define how it shall be processed, and it will be the responsibility of an infrastructure component, such as Spring WebFlux, to initiate the processing, for example, as a response to an incoming HTTP request. An exception to this rule of thumb is the case where blocking code needs a response from the reactive stream. In these cases, the blocking code can call the <kbd>block()</kbd> method on the <kbd>Flux</kbd> or <kbd>Mono</kbd> object to get the response from the <span><kbd>Flux</kbd> or <kbd>Mono</kbd> object in a blocking way.</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Non-blocking persistence using Spring Data for MongoDB</h1>
                </header>
            
            <article>
                
<div>
<div>
<p><span>Making the MongoDB-based repositories for the <kbd>product</kbd> and <kbd>recommendation</kbd> services reactive is very simple:</span></p>
<ul>
<li><span>Change to the</span>&nbsp;<span><kbd>ReactiveCrudRepository</kbd> base class for the repositories </span></li>
<li>Change the custom finder methods to return a <kbd>Mono</kbd> or <kbd>Flux</kbd> object</li>
</ul>
<p><span>The</span>&nbsp;<span><kbd>ProductRepository</kbd></span>&nbsp;<span>and </span><span><kbd>RecommendationRepository</kbd></span><span> look like the following after the change:<br></span></p>
</div>
</div>
<pre>public interface ProductRepository extends ReactiveCrudRepository&lt;ProductEntity, String&gt; {<br>    Mono&lt;ProductEntity&gt; findByProductId(int productId);<br>}<br><br>public interface RecommendationRepository extends ReactiveCrudRepository&lt;RecommendationEntity, String&gt; {<br>    Flux&lt;RecommendationEntity&gt; findByProductId(int productId);<br>}</pre>
<div class="packt_infobox">
<p><span>No changes are applied to the persistence code for the <kbd>review</kbd> service, it will remain blocking using the JPA repository!</span></p>
</div>
<p class="mce-root"></p>
<p>For the <span>full source code, take a look at the following classes:</span></p>
<ul>
<li><kbd><span>se.magnus.microservices.core.product.persistence.ProductRepository</span></kbd> in the <kbd>product</kbd>&nbsp;<span>project</span>.</li>
<li><span><kbd>se.magnus.microservices.core.recommendation.persistence.RecommendationRepository</kbd> in the <kbd>recommendation</kbd> project.</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the test code</h1>
                </header>
            
            <article>
                
<p>When it comes to testing the persistence layer, we have to make some changes. Since our persistence methods now return a <kbd>Mono</kbd> or <kbd>Flux</kbd> object, the test methods have to wait for the response to be available in the returned reactive objects. The test methods can either use an explicit call to the <kbd>block()</kbd> method on the <kbd>Mono</kbd>/<kbd>Flux</kbd> object to wait until a response is available or use the <kbd>StepVerifier</kbd>&nbsp;<span>helper class </span>from Project Reactor to declare a verifiable sequence of asynchronous events.</p>
<p class="mce-root"><span>The following example shows how to change the test code to work for the reactive version of the repository:</span></p>
<pre>ProductEntity foundEntity = repository.findById(newEntity.getId()).get();<br>assertEqualsProduct(newEntity, foundEntity);</pre>
<p>We can use the <kbd>block()</kbd> method on the <kbd>Mono</kbd> object returned by the <kbd>repository.findById()</kbd> <span>method </span>and keep the imperative programming style, as shown here:</p>
<pre>ProductEntity foundEntity = repository.findById(newEntity.getId()).block();<br>assertEqualsProduct(newEntity, foundEntity);</pre>
<p>Alternatively, we can use the <kbd>StepVerifier</kbd> class to set up a sequence of processing steps that both execute the repository find operation and also verifies the result. The sequence is initialized by the final call to the <kbd>verifyComplete()</kbd> method like so:</p>
<pre>StepVerifier.create(repository.findById(newEntity.getId()))<br>    .expectNextMatches(foundEntity -&gt; areProductEqual(newEntity, <br>     foundEntity))<br>    .verifyComplete();</pre>
<p>For examples of using the <kbd>StepVerifier</kbd> class to write tests, see the <kbd>se.magnus.microservices.core.product.PersistenceTests</kbd>&nbsp;<span>test class </span>in the <kbd>product</kbd> project.</p>
<p>For corresponding examples of using the <kbd>block()</kbd> method to write tests, see the <kbd>se.magnus.microservice.core.recommendation.PersistenceTests</kbd> test class in the <span><kbd>recommendation</kbd> project.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the APIs</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>To make the APIs of the core services reactive, we need to update their methods so that they return either a <kbd>Mono</kbd> or <kbd>Flux</kbd> object.</span></p>
<p class="mce-root">For example,&nbsp;<kbd>getProduct()</kbd> in the <kbd>product</kbd> service now returns&nbsp;<kbd>Mono&lt;Product&gt;</kbd> instead of a <kbd>Product</kbd> object:</p>
<pre>Mono&lt;Product&gt; getProduct(@PathVariable int productId);</pre>
<p class="mce-root">For the&nbsp;<span>full source code, take a look at the following classes in the <kbd>api</kbd> project:</span></p>
<ul>
<li><span><kbd>se.magnus.api.core.product.ProductService</kbd>&nbsp;</span></li>
<li><kbd><span>se.magnus.api.core.recommendation.RecommendationService</span></kbd></li>
<li><kbd><span>se.magnus.api.core.review.ReviewService</span></kbd></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the APIs</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>To make the APIs of the core services reactive, we need to update their methods so that they return either a <kbd>Mono</kbd> or <kbd>Flux</kbd> object.</span></p>
<p class="mce-root">For example, <kbd>getProduct()</kbd> in the <kbd>product</kbd> service now returns <kbd>Mono&lt;Product&gt;</kbd> instead of a <kbd>Product</kbd> object:</p>
<pre>Mono&lt;Product&gt; getProduct(@PathVariable int productId);</pre>
<p class="mce-root">For the <span>full source code, take a look at the following classes in the <kbd>api</kbd> project:</span></p>
<ul>
<li><span><kbd>se.magnus.api.core.product.ProductService</kbd>&nbsp;</span></li>
<li><kbd><span>se.magnus.api.core.recommendation.RecommendationService</span></kbd></li>
<li><kbd><span>se.magnus.api.core.review.ReviewService</span></kbd></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the service implementations</h1>
                </header>
            
            <article>
                
<p class="mce-root">For the implementations of the services in the <kbd>product</kbd> and <kbd>recommendation</kbd> services that use a reactive persistence layer, we can use the fluent API in Project Reactor. For example, the implementation of the <kbd>getProduct()</kbd> method looks like the following code:</p>
<pre>public Mono&lt;Product&gt; getProduct(int productId) {<br><br>    if (productId &lt; 1) throw new InvalidInputException("Invalid <br>        productId: " + productId);<br><br>    return repository.findByProductId(productId)<br>        .switchIfEmpty(error(new NotFoundException("No product found <br>         for productId: " + productId)))<br>        .log()<br>        .map(e -&gt; mapper.entityToApi(e))<br>        .map(e -&gt; {e.setServiceAddress(serviceUtil.getServiceAddress()); return e;});<br>} </pre>
<p class="mce-root"><span>Here is an explanation of the preceding source code:</span></p>
<ol>
<li class="mce-root">The method will return a <kbd>Mono</kbd> object; the processing here is declared, not triggered. It is triggered by the web framework, <kbd>WebFlux</kbd>, once it receives a request to this service!</li>
<li class="mce-root">A product will be retrieved using its <kbd>productId</kbd> from the underlying database using the <kbd>findByProductId()</kbd> method in the persistence repository.</li>
<li class="mce-root">If no product is found for the given <kbd>productId</kbd>,&nbsp;<kbd>NotFoundException</kbd> will be thrown.</li>
<li class="mce-root">The <kbd>log</kbd> method will produce log output.</li>
<li class="mce-root">The <kbd>mapper.entityToApi()</kbd><span> method </span>will be called to transform the returned entity from the persistence layer to an API model object.</li>
<li class="mce-root">The final <kbd>map</kbd> method will set the DNS name and IP address of the microservices that processed the request in the <kbd>serviceAddress</kbd> field of the model object.</li>
</ol>
<p class="mce-root">Some sample log output for successful processing is as follows:</p>
<pre><strong><span>2019-02-06 10:09:47.006 INFO 62314 --- [ctor-http-nio-2] reactor.Mono.SwitchIfEmpty.1 : onSubscribe(FluxSwitchIfEmpty.SwitchIfEmptySubscriber)</span></strong><br><strong><span>2019-02-06 10:09:47.007 INFO 62314 --- [ctor-http-nio-2] reactor.Mono.SwitchIfEmpty.1 : request(unbounded)</span></strong><br><strong><span>2019-02-06 10:09:47.034 INFO 62314 --- [ntLoopGroup-2-2] reactor.Mono.SwitchIfEmpty.1 : onNext(ProductEntity: 1)</span></strong><br><strong><span>2019-02-06 10:09:47.048 INFO 62314 --- [ntLoopGroup-2-2] reactor.Mono.SwitchIfEmpty.1 : onComplete()</span></strong></pre>
<p class="mce-root">The following is a sample of failed processing (throwing a not found exception):</p>
<pre><strong>2019-02-06 10:09:52.643 INFO 62314 --- [ctor-http-nio-3] reactor.Mono.SwitchIfEmpty.2 : onSubscribe(FluxSwitchIfEmpty.SwitchIfEmptySubscriber)</strong><br><strong>2019-02-06 10:09:52.643 INFO 62314 --- [ctor-http-nio-3] reactor.Mono.SwitchIfEmpty.2 : request(unbounded)</strong><br><strong>2019-02-06 10:09:52.648 ERROR 62314 --- [ntLoopGroup-2-2] reactor.Mono.SwitchIfEmpty.2 : onError(se.magnus.util.exceptions.NotFoundException: No product found for productId: 2)</strong><br><strong>2019-02-06 10:09:52.654 ERROR 62314 --- [ntLoopGroup-2-2] reactor.Mono.SwitchIfEmpty.2 : </strong><br><br><strong>se.magnus.util.exceptions.NotFoundException: No product found for productId: 2</strong><br><strong>  at se.magnus.microservices.core.product.services.ProductServiceImpl.getProduct(ProductServiceImpl.java:58) ~[classes/:na]</strong><br><strong>    ...</strong></pre>
<p class="mce-root">For the full source code, see the following classes:</p>
<ul>
<li><kbd><span>se.magnus.microservices.core.product.services.ProductServiceImpl</span></kbd> in the <span><kbd>product</kbd> project</span></li>
<li><kbd><span>se.magnus.microservices.core.recommendation.services.RecommendationServiceImpl</span></kbd> in the <span><kbd>recommendation</kbd> project</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the test code</h1>
                </header>
            
            <article>
                
<p>The test code for service implementations has been changed in the same way as the tests for the persistence layer we described previously. To handle the asynchronous behavior of the reactive return types, <kbd>Mono</kbd> and <kbd>Flux</kbd>, the tests use a mix of calling the <kbd>block()</kbd> method and using the <kbd>StepVerifier</kbd><span> helper class.</span></p>
<p><span>For the full source code, see the following test classes:</span></p>
<ul>
<li><kbd>se.magnus.microservices.core.product.ProductServiceApplicationTests</kbd><span> in the </span><span><kbd>product</kbd> project</span></li>
<li><kbd>se.magnus.microservices.core.recommendation.RecommendationServiceApplicationTests</kbd><span> in the </span><span><kbd>recommendation</kbd> project</span></li>
</ul>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Dealing with blocking code</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the case of the <kbd>review</kbd> service, which uses JPA to access its data in a relational database, we don't have support for a non-blocking programming model. Instead, we can run the blocking code using <kbd>Scheduler</kbd>, which is capable of running the blocking code on a thread from a dedicated thread pool with a limited number of threads. Using a thread pool for the blocking code avoids draining the available threads in the microservice (avoids affecting the non-blocking processing in the microservice).</p>
<p class="mce-root">Let's see how this process works, as laid out in the following steps:</p>
<ol>
<li class="mce-root">Firstly, we configure the thread pool in the <kbd>main</kbd>&nbsp;<kbd>ReviewServiceApplication</kbd> class, as follows:</li>
</ol>
<pre style="padding-left: 60px">@Autowired<br>public ReviewServiceApplication (<br>    @Value("${spring.datasource.maximum-pool-size:10}") Integer <br>    connectionPoolSize<br>) {<br>    this.connectionPoolSize = connectionPoolSize;<br>}<br><br>@Bean<br>public Scheduler jdbcScheduler() {<br>    LOG.info("Creates a jdbcScheduler with connectionPoolSize = " + <br>    connectionPoolSize);<br>    return Schedulers.fromExecutor(Executors.newFixedThreadPool<br>    (connectionPoolSize));<br>}</pre>
<div class="packt_infobox"><span>We can configure the size of the thread pool using the</span> <kbd>spring.datasource.maximum-pool-size</kbd><span> parameter. If it is not set, it will default to 10 threads. </span>For the full source code, see the <kbd>se.magnus.microservices.core.review.ReviewServiceApplication</kbd> class in the <span><kbd>review</kbd> project.</span></div>
<ol start="2">
<li class="mce-root">Next, we inject the scheduler into the <kbd>review</kbd> service implementation class, as shown here:</li>
</ol>
<pre style="padding-left: 60px">@RestController<br>public class ReviewServiceImpl implements ReviewService {<br><br>    private final Scheduler scheduler;<br><br>    @Autowired<br>    public ReviewServiceImpl(Scheduler scheduler, ...) {<br>        this.scheduler = scheduler;<br>    }</pre>
<ol start="3">
<li class="mce-root">Finally, we use the thread pool in the reactive implementation of the <kbd>getReviews()</kbd> method, like so:</li>
</ol>
<pre style="padding-left: 60px">@Override<br>public Flux&lt;Review&gt; getReviews(int productId) {<br><br>    if (productId &lt; 1) throw new InvalidInputException("Invalid <br>        productId: " + productId);<br><br>    return asyncFlux(getByProductId(productId)).log();<br>}<br><br>protected List&lt;Review&gt; getByProductId(int productId) {<br><br>    List&lt;ReviewEntity&gt; entityList = <br>    repository.findByProductId(productId);<br>    List&lt;Review&gt; list = mapper.entityListToApiList(entityList);<br>    list.forEach(e -&gt; <br>            e.setServiceAddress(serviceUtil.getServiceAddress()));<br><br>    LOG.debug("getReviews: response size: {}", list.size());<br><br>    return list;<br>}<br><br>private &lt;T&gt; Flux&lt;T&gt; asyncFlux(Iterable&lt;T&gt; iterable) {<br>    return Flux.fromIterable(iterable).publishOn(scheduler);<br>}</pre>
<p class="mce-root">Here is an explanation of the preceding code:</p>
<ul>
<li class="mce-root">The blocking code is placed in the <kbd>getByProductId()</kbd> method</li>
<li class="mce-root">The <kbd>getReviews()</kbd><span> method </span>uses the <kbd>asyncFlux()</kbd> <span>method </span>to run the blocking code in a thread from the thread pool</li>
</ul>
<p>For the full source code, see the <kbd>se.magnus.microservices.core.review.services.ReviewServiceImpl</kbd><span> class in the </span><span><kbd>review</kbd> project.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Non-blocking REST APIs in the composite services</h1>
                </header>
            
            <article>
                
<p>To make our REST API in the composite service non-blocking, we need to do the following:</p>
<ul>
<li>Change the APIs so that they only return reactive datatypes</li>
<li>Change the integration layer so it uses a non-blocking HTTP client</li>
<li>Change the service implementation so it calls the core services APIs in parallel and non-blocking</li>
<li>Change our tests so that they can test the reactive service</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the integration layer</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the&nbsp;<kbd>ProductCompositeIntegration</kbd>&nbsp;integration class, we have replaced the <kbd>RestTemplate</kbd><span>&nbsp;blocking HTTP client</span><span>&nbsp;</span>with the&nbsp;<kbd>WebClient</kbd>&nbsp;<span>non-blocking HTTP client</span>&nbsp;that comes with Spring 5.</p>
<p class="mce-root">A builder for the <kbd>WebClient</kbd> is auto-injected in to the constructor. If customization is required, for example, in setting up common headers or filters, it can be done in the constructor. For the available configuration options, see <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html#webflux-client-builder">https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html#webflux-client-builder</a>. Please have a look at the following steps:</p>
<ol>
<li class="mce-root">Here, we simply build the <kbd>WebClient</kbd> instance that we will use in our integration class,&nbsp;<span>without any configuration:</span></li>
</ol>
<pre style="padding-left: 60px">public class ProductCompositeIntegration implements ProductService, RecommendationService, ReviewService {<br><br>    private final WebClient webClient;<br><br>    @Autowired<br>    public ProductCompositeIntegration(<br>        WebClient.Builder webClient, ...<br>    ) {<br>        this.webClient = webClient.build();<br>    }</pre>
<ol start="2">
<li class="mce-root">Next, we use the <kbd>webClient</kbd>&nbsp;instance to make our non-blocking requests for calling the <kbd>product</kbd> service:</li>
</ol>
<pre style="padding-left: 60px">@Override<br>public Mono&lt;Product&gt; getProduct(int productId) {<br>    String url = productServiceUrl + "/product/" + productId;<br><br>    return webClient.get().uri(url).retrieve().bodyToMono(Product.class).log().onErrorMap(WebClientResponseException.class, ex -&gt; handleException(ex));<br>}</pre>
<p class="mce-root">If the API call to the <kbd>product</kbd> service fails, the whole request will fail. The <kbd>WebClient&nbsp;onErrorMap()</kbd>&nbsp;<span>method&nbsp;</span>will call our <kbd>handleException(ex)</kbd>&nbsp;<span>method, which</span>&nbsp;maps the exceptions thrown previously by the HTTP layer to our own exceptions, for example,&nbsp;<kbd>NotFoundException</kbd>&nbsp;and <kbd>InvalidInputException</kbd>.</p>
<p class="mce-root">However, if calls to the&nbsp;<span><kbd>product</kbd> service succeed but the call to either the&nbsp;</span>recommendation or review API fails, we don't want to let the whole request fail. Instead, we want to return as much information that is available, back to the caller. Therefore, instead of propagating an exception in these cases, we will instead return an empty list of recommendations or reviews using the <kbd>WebClient&nbsp;onErrorResume(error -&gt; empty())</kbd><span>&nbsp;method.</span> For this, consider the following code:</p>
<pre>@Override<br>public Flux&lt;Recommendation&gt; getRecommendations(int productId) {<br><br>    String url = recommendationServiceUrl + "/recommendation?<br>    productId=" + productId;<br><br>    // Return an empty result if something goes wrong to make it <br>    // possible for the composite service to return partial responses<br>    return webClient.get().uri(url).retrieve().bodyToFlux(Recommendation.class).log().onErrorResume(error -&gt; empty());<br>}</pre>
<p>For the full source code, see the&nbsp;<kbd>se.magnus.microservices.composite.product.services.ProductCompositeIntegration</kbd>&nbsp;<span>class</span>&nbsp;in the&nbsp;<span><kbd>product-composite</kbd> project.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the integration layer</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the <kbd>ProductCompositeIntegration</kbd> integration class, we have replaced the <kbd>RestTemplate</kbd><span> blocking HTTP client</span><span>&nbsp;</span>with the <kbd>WebClient</kbd>&nbsp;<span>non-blocking HTTP client</span> that comes with Spring 5.</p>
<p class="mce-root">A builder for the <kbd>WebClient</kbd> is auto-injected in to the constructor. If customization is required, for example, in setting up common headers or filters, it can be done in the constructor. For the available configuration options, see <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html#webflux-client-builder">https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html#webflux-client-builder</a>. Please have a look at the following steps:</p>
<ol>
<li class="mce-root">Here, we simply build the <kbd>WebClient</kbd> instance that we will use in our integration class, <span>without any configuration:</span></li>
</ol>
<pre style="padding-left: 60px">public class ProductCompositeIntegration implements ProductService, RecommendationService, ReviewService {<br><br>    private final WebClient webClient;<br><br>    @Autowired<br>    public ProductCompositeIntegration(<br>        WebClient.Builder webClient, ...<br>    ) {<br>        this.webClient = webClient.build();<br>    }</pre>
<ol start="2">
<li class="mce-root">Next, we use the <kbd>webClient</kbd> instance to make our non-blocking requests for calling the <kbd>product</kbd> service:</li>
</ol>
<pre style="padding-left: 60px">@Override<br>public Mono&lt;Product&gt; getProduct(int productId) {<br>    String url = productServiceUrl + "/product/" + productId;<br><br>    return webClient.get().uri(url).retrieve().bodyToMono(Product.class).log().onErrorMap(WebClientResponseException.class, ex -&gt; handleException(ex));<br>}</pre>
<p class="mce-root">If the API call to the <kbd>product</kbd> service fails, the whole request will fail. The <kbd>WebClient onErrorMap()</kbd>&nbsp;<span>method </span>will call our <kbd>handleException(ex)</kbd>&nbsp;<span>method, which</span> maps the exceptions thrown previously by the HTTP layer to our own exceptions, for example, <kbd>NotFoundException</kbd> and <kbd>InvalidInputException</kbd>.</p>
<p class="mce-root">However, if calls to the <span><kbd>product</kbd> service succeed but the call to either the </span>recommendation or review API fails, we don't want to let the whole request fail. Instead, we want to return as much information that is available, back to the caller. Therefore, instead of propagating an exception in these cases, we will instead return an empty list of recommendations or reviews using the <kbd>WebClient onErrorResume(error -&gt; empty())</kbd><span> method.</span> For this, consider the following code:</p>
<pre>@Override<br>public Flux&lt;Recommendation&gt; getRecommendations(int productId) {<br><br>    String url = recommendationServiceUrl + "/recommendation?<br>    productId=" + productId;<br><br>    // Return an empty result if something goes wrong to make it <br>    // possible for the composite service to return partial responses<br>    return webClient.get().uri(url).retrieve().bodyToFlux(Recommendation.class).log().onErrorResume(error -&gt; empty());<br>}</pre>
<p>For the full source code, see the <kbd>se.magnus.microservices.composite.product.services.ProductCompositeIntegration</kbd>&nbsp;<span>class</span> in the <span><kbd>product-composite</kbd> project.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the service implementation</h1>
                </header>
            
            <article>
                
<p class="mce-root">To be able to call the three APIs in parallel, the service implementation uses the static <kbd>zip()</kbd>&nbsp;<span> method </span>on the <kbd>Mono</kbd> class. The <kbd>zip</kbd> method is capable of handling a number of parallel requests and zipping them together once they all are complete. The code looks like this:</p>
<pre>@Override<br>public Mono&lt;ProductAggregate&gt; getCompositeProduct(int productId) {<br>    return Mono.zip(<br>        values -&gt; createProductAggregate((Product) values[0], <br>        (List&lt;Recommendation&gt;) values[1], (List&lt;Review&gt;) values[2], <br>        serviceUtil.getServiceAddress()),<br>        integration.getProduct(productId),<br>        integration.getRecommendations(productId).collectList(),<br>        integration.getReviews(productId).collectList())<br>        .doOnError(ex -&gt; LOG.warn("getCompositeProduct failed: {}", <br>         ex.toString()))<br>        .log();<br>}</pre>
<p class="mce-root">Here is an explanation of the preceding source code:</p>
<ol>
<li class="mce-root">The first parameter of the <kbd>zip</kbd> method is a lambda function that will receive the responses in an array. The actual aggregation of the responses from the three API calls is handled by the same helper method <span>as before, </span><kbd>createProductAggregate</kbd>, without any changes.</li>
<li class="mce-root">The parameters after the lambda function are a list of the requests that the <kbd>zip</kbd> method will call in parallel, one <kbd>Mono</kbd> object per request. In our case, we send in three <kbd>Mono</kbd> objects that were created by the methods in the integration class, one for each request that's sent to each core microservice.</li>
</ol>
<p>For the full source code, see the <kbd>se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl</kbd> class in the <span><kbd>product-composite</kbd> project.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the test code</h1>
                </header>
            
            <article>
                
<p class="mce-root">The only change that's required in the test classes is to update the setup of the mock of the integration class so that the <kbd>Mono</kbd> and <kbd>Flux</kbd> objects are returned using the  <kbd>Mono.just()</kbd>&nbsp;<span>helper methods </span>and <kbd>Flux.fromIterable()</kbd>, as shown in the following code:</p>
<pre>public class ProductCompositeServiceApplicationTests {<br><br>    @Before<br>    public void setUp() {<br><br>        when(compositeIntegration.getProduct(PRODUCT_ID_OK)).<br>            thenReturn(just(new Product(PRODUCT_ID_OK, "name", 1, <br>             "mock-address")));<br><br>        when(compositeIntegration.getRecommendations(PRODUCT_ID_OK)).<br>            thenReturn(Flux.fromIterable(singletonList(new <br>             Recommendation(PRODUCT_ID_OK, 1, "author", 1, "content", <br>             "mock address"))));<br><br>        when(compositeIntegration.getReviews(PRODUCT_ID_OK)).<br>            thenReturn(Flux.fromIterable(singletonList(new <br>             Review(PRODUCT_ID_OK, 1, "author", "subject", "content", <br>             "mock address"))));</pre>
<p class="mce-root"><span>For the full source code, see the</span>&nbsp;<kbd>se.magnus.microservices.composite.product.ProductCompositeServiceApplicationTests</kbd> test class in the <span><kbd>product-composite</kbd> project.</span></p>
<p>Now that we have developed non-blocking REST APIs with Spring, it is time to develop an event-driven synchronous service.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Developing event-driven asynchronous services</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to develop event-driven and asynchronous versions of the create and delete services. The composite service will publish <span>create and delete </span>events on each core service topic and then return a OK response back to the caller without waiting for processing to take place in the core services. This is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6a55381b-4a64-4aa4-84fb-02f258aca361.png" width="1687" height="521" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6a55381b-4a64-4aa4-84fb-02f258aca361.png"></p>
<p><span>We will cover the following topics:</span></p>
<ul>
<li>Configuring Spring Cloud Stream to handle challenges with messaging</li>
<li>Defining topics and events</li>
<li>Changes in Gradle build files</li>
<li>Publishing events in the composite service</li>
<li>Consuming events in the core services</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Configuring Spring Cloud Stream to handle challenges with messaging</h1>
                </header>
            
            <article>
                
<div>
<p>To implement the event-driven <span>create and delete services, we will use </span>Spring Cloud Stream. <span>In <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml">Chapter 2</a>, <em>Introduction to Spring Boot</em>, in the <em>Spring Cloud Stream</em> section, w</span><span>e have already seen how easy it is to publish and consume messages on a topic using Spring Cloud Stream.</span></p>
</div>
<p><span>For example, to publish a message on a topic defined by <kbd>mysource</kbd>, we only have to write the following:</span></p>
<pre style="padding-left: 60px"><span>mysource</span>.output().send(MessageBuilder.<span>withPayload</span>(message).build());</pre>
<p>For consuming a message, we write the following code:</p>
<pre style="padding-left: 60px"><span>@StreamListener</span>(target = Sink.<span>INPUT</span>)<br> <span>public void </span>receive(MyMessage message) {<br>   <span>LOG</span>.info(<span>"Received: {}"</span>,message);</pre>
<p>This programming model can be used independently of the messaging system used, for example, RabbitMQ or Apache Kafka!</p>
<p>Even though sending asynchronous messages is preferred over synchronous API calls, it comes with challenges. We will see how we can use Spring Cloud Stream to handle some of them. The following features in <span>Spring Cloud Stream will be covered:</span></p>
<ul>
<li class="mce-root"><span>Consumer groups</span></li>
<li class="mce-root">Retries and dead-letter queues</li>
<li class="mce-root">Guaranteed orders and partitions</li>
</ul>
<p>We'll study each of these in the following sections.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Retries and dead-letter queues&nbsp;</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will learn how retries and dead-letter queues are used by message consumers.</span></p>
<p>If a consumer fails to process a message, it may lost or be requeued for the failing consumer until it is successfully processed. If the content of the message is invalid, also known as a <strong>poisoned message</strong>, it will block the consumer from processing other messages until it is manually removed. If the failure is due to a temporary problem, for example, the database can't be reached due to a temporary network error, the processing will probably succeed after a number of retries.</p>
<p>It must be possible to specify the number of retries until a message is moved to another storage for fault analysis and correction. A failing message is typically moved to a dedicated queue called a dead-letter queue. To avoid overloading the infrastructure during temporary failure, for example, a network error, it must be possible to configure how often retries are performed and preferably with increasing time between each retry.</p>
<p><span>In Spring Cloud Stream, this can be configured on the consumer side, for example, for the&nbsp;</span><span>product microservice, as shown here</span><span>:</span></p>
<pre><span>spring.cloud.stream.bindings.input.consumer</span>:<br>  <span>maxAttempts</span>: 3<br>  <span>backOffInitialInterval</span>: 500<br>  <span>backOffMaxInterval</span>: 1000<br>  <span>backOffMultiplier</span>: 2.0<br><br><span>spring.cloud.stream.rabbit.bindings.input.consumer</span>:<br>  <span>autoBindDlq</span>: true<br>  <span>republishToDlq</span>: true<br><br><span>spring.cloud.stream.kafka.bindings.input.consumer</span>:<br>  <span>enableDlq</span>: true</pre>
<p>In the preceding example, we specify that Spring Cloud Stream shall perform <kbd>3</kbd> retries before placing a message on the dead-letter queue. The first retry shall be attempted after <kbd>500</kbd> ms and the two other attempts after <kbd>1000</kbd> ms.</p>
<p>Enabling the use of d<span>ead-letter queues is binding-specific; therefore, we have one configuration for RabbitMQ and one for Kafka.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Retries and dead-letter queues </h1>
                </header>
            
            <article>
                
<p><span>In this section, we will learn how retries and dead-letter queues are used by message consumers.</span></p>
<p>If a consumer fails to process a message, it may lost or be requeued for the failing consumer until it is successfully processed. If the content of the message is invalid, also known as a <strong>poisoned message</strong>, it will block the consumer from processing other messages until it is manually removed. If the failure is due to a temporary problem, for example, the database can't be reached due to a temporary network error, the processing will probably succeed after a number of retries.</p>
<p>It must be possible to specify the number of retries until a message is moved to another storage for fault analysis and correction. A failing message is typically moved to a dedicated queue called a dead-letter queue. To avoid overloading the infrastructure during temporary failure, for example, a network error, it must be possible to configure how often retries are performed and preferably with increasing time between each retry.</p>
<p><span>In Spring Cloud Stream, this can be configured on the consumer side, for example, for the </span><span>product microservice, as shown here</span><span>:</span></p>
<pre><span>spring.cloud.stream.bindings.input.consumer</span>:<br>  <span>maxAttempts</span>: 3<br>  <span>backOffInitialInterval</span>: 500<br>  <span>backOffMaxInterval</span>: 1000<br>  <span>backOffMultiplier</span>: 2.0<br><br><span>spring.cloud.stream.rabbit.bindings.input.consumer</span>:<br>  <span>autoBindDlq</span>: true<br>  <span>republishToDlq</span>: true<br><br><span>spring.cloud.stream.kafka.bindings.input.consumer</span>:<br>  <span>enableDlq</span>: true</pre>
<p>In the preceding example, we specify that Spring Cloud Stream shall perform <kbd>3</kbd> retries before placing a message on the dead-letter queue. The first retry shall be attempted after <kbd>500</kbd> ms and the two other attempts after <kbd>1000</kbd> ms.</p>
<p>Enabling the use of d<span>ead-letter queues is binding-specific; therefore, we have one configuration for RabbitMQ and one for Kafka.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Guaranteed order and partitions</h1>
                </header>
            
            <article>
                
<p><span>We can use partitions to ensure that messages are delivered in the same order as they were sent but without losing performance and scalability.</span></p>
<p>If the business logic requires that messages are consumed and processed in the same order as they were sent, we cannot use multiple instances per consumer to increase processing performance; for example, we cannot use consumer groups. This might, in some cases, lead to an unacceptable latency in the processing of incoming messages.</p>
<p>In most cases, strict order in the processing of messages is only required for messages that affect the same business entities, for example, products.</p>
<p>For example, messages affecting the product with product ID <kbd>1</kbd> can, in many cases, be processed independently of messages that affect the product with product ID <kbd>2</kbd>. This means that the order only needs to be guaranteed for messages that have the same product ID.</p>
<p>The solution to this is to make it possible to specify a key for each message that the messaging system can use in order to guarantee that the order is kept between messages with the same key. This can be solved by introducing sub-topics, also known as <strong>partitions</strong>, in a topic. The messaging system places messages in a specific partition based on its key. Messages with the same key are always placed in the same partition. The messaging system only needs to guarantee the delivery order for messages in one and the same partition. To ensure the order of the messages, we configure one consumer instance per partition within a consumer group. By increasing the number of partitions, we can allow a consumer to increase its number of instances. This increases its processing message performance without losing the delivery order. This is illustrated in the following  diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2962fb3e-d606-4550-8328-c846114f8965.png" width="1807" height="671" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2962fb3e-d606-4550-8328-c846114f8965.png"></p>
<p>In Spring Cloud Stream, this needs to be configured on both the publisher and consumer side. On the publisher side, the key and number of partitions must be specified. For example, for the <span><kbd>product-composite</kbd> service, we have the following</span>:</p>
<pre>spring.cloud.stream.bindings.output:<br>  destination: products<br>  producer:<br>    partition-key-expression: payload.key<br>    partition-count: 2</pre>
<p>The preceding configuration means that the key will be taken from the payload in the message using a field named <kbd>key</kbd> and that two partitions will be used.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>Each consumer can specify which partition it wants to consume messages from. <span>For example, for the </span><span><kbd>product</kbd> microservice, we have the following</span><span>:</span></p>
<pre>spring.cloud.stream.bindings.input:<br>  destination: products<br>  group:<span>productsGroup</span><br>  consumer:<br>    partitioned: true<br>    instance-index: 0</pre>
<p>The<span> preceding configuration tells Spring Cloud Stream that this consumer will only consume messages from partition number <kbd>0</kbd>, that is, the first partition.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Defining topics and events</h1>
                </header>
            
            <article>
                
<p><span>As we already mentioned in the <em>Spring Cloud Stream</em></span> section in <span><a href="7d969006-ea94-4bbb-858d-30dce8177a2c.xhtml">Chapter 2</a>, <em>Introduction to Spring Boot</em>, Spring Cloud Stream is based on the publishing and subscribe pattern, where a publisher publishes messages to topics and subscribers subscribe to topics they are interested in to receive messages.</span></p>
<p><span>We will use one <strong>topic</strong> per type of entity:</span><span>&nbsp;</span><kbd>products</kbd><span>,</span><span>&nbsp;</span><kbd>recommendations</kbd>,&nbsp;<span>and</span><span>&nbsp;</span><kbd>reviews</kbd><span>.</span></p>
<p>Messaging systems handle<span>&nbsp;</span><strong>messages</strong> that typically consist of headers and a body. <span>An <strong>event</strong>&nbsp;</span><span>is a message that describes something that has happened. For events, the message body can be used to describe the type of event, the event data, and a timestamp for when the event occurred.</span></p>
<p>An event is, for the scope of this book, defined by the following:</p>
<ul>
<li>The<span>&nbsp;</span><strong>type</strong><span>&nbsp;</span>of event, for example, create or delete an event</li>
<li>A<span>&nbsp;</span><strong>key</strong>, that identifies the data, for example, a product ID</li>
<li>A<span>&nbsp;</span><strong>data</strong><span>&nbsp;</span>element, that is, the actual data in the event</li>
<li>A<span>&nbsp;</span><strong>timestamp</strong>, which describes when the event occurred</li>
</ul>
<p class="mce-root">The event class we will use looks as follows:</p>
<pre><span>public class </span>Event&lt;<span>K</span>, <span>T</span>&gt; {<br><br>    <span>public enum </span>Type {<span>CREATE</span>, <span>DELETE</span>}<br><br>    <span>private </span>Event.Type <span>eventType</span>;<br>    <span>private </span><span>K </span><span>key</span>;<br>    <span>private </span><span>T </span><span>data</span>;<br>    <span>private </span>LocalDateTime <span>eventCreatedAt</span>;<br><br>    <span>public </span>Event() {<br>        <span>this</span>.<span>eventType </span>= <span>null</span>;<br>        <span>this</span>.<span>key </span>= <span>null</span>;<br>        <span>this</span>.<span>data </span>= <span>null</span>;<br>        <span>this</span>.<span>eventCreatedAt </span>= <span>null</span>;<br>    }<br><br>    <span>public </span>Event(Type eventType, <span>K </span>key, <span>T </span>data) {<br>        <span>this</span>.<span>eventType </span>= eventType;<br>        <span>this</span>.<span>key </span>= key;<br>        <span>this</span>.<span>data </span>= data;<br>        <span>this</span>.<span>eventCreatedAt </span>= <span>now</span>();<br>    }<br><br>    <span>public </span>Type getEventType() {<br>        <span>return </span><span>eventType</span>;<br>    }<br><br>    <span>public </span><span>K </span>getKey() {<br>        <span>return </span><span>key</span>;<br>    }<br><br>    <span>public </span><span>T </span>getData() {<br>        <span>return </span><span>data</span>;<br>    }<br><br>    <span>public </span>LocalDateTime getEventCreatedAt() {<br>        <span>return </span><span>eventCreatedAt</span>;<br>    }<br>}</pre>
<p>Let's explain the preceding source code in detail:</p>
<ul>
<li>The <kbd>Event</kbd> class is a generic class parameterized over the types of its <kbd>key</kbd> and <kbd>data</kbd> field, <kbd>K</kbd> and <kbd>T</kbd>.</li>
<li>The event type is declared as an enumerator with the allowed values, that is, <kbd>CREATE</kbd> and <kbd>DELETE</kbd>.</li>
<li>The class defines two constructors, one empty and one that can be used to initialize the type, key, and value members.</li>
<li>Finally, the class defines getter methods for its member variables.</li>
</ul>
<p>For the full source code, see the <kbd>se.magnus.api.event.Event</kbd> class in the <kbd>api</kbd> project.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Publishing events in the composite service</h1>
                </header>
            
            <article>
                
<p class="mce-root">When the composite service receives requests for the creation or deletion of products, it shall publish the&nbsp;<span>corresponding&nbsp;</span>events to the core services on their topics. To be able to publish events in the<span>&nbsp;</span>composite service, we need to perform the following steps:</p>
<ol>
<li class="mce-root"><span><span>Declare message sources and publish events<span>&nbsp;</span></span></span><span>in the integration layer.</span></li>
<li class="mce-root">Add configuration for publishing events.</li>
<li class="mce-root">Change our tests so that they can test the publishing of events.</li>
</ol>
<div class="packt_infobox"><span>No changes are required in the composite service implementation class!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Publishing events in the composite service</h1>
                </header>
            
            <article>
                
<p class="mce-root">When the composite service receives requests for the creation or deletion of products, it shall publish the <span>corresponding </span>events to the core services on their topics. To be able to publish events in the<span>&nbsp;</span>composite service, we need to perform the following steps:</p>
<ol>
<li class="mce-root"><span><span>Declare message sources and publish events<span>&nbsp;</span></span></span><span>in the integration layer.</span></li>
<li class="mce-root">Add configuration for publishing events.</li>
<li class="mce-root">Change our tests so that they can test the publishing of events.</li>
</ol>
<div class="packt_infobox"><span>No changes are required in the composite service implementation class!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding configuration for publishing events</h1>
                </header>
            
            <article>
                
<p>We also need to set up a configuration for the messaging system to be able to publish events. To do this, we need to complete the following steps:</p>
<ol>
<li>We declare that RabbitMQ is the default messaging system and that the default content type is JSON:</li>
</ol>
<pre style="padding-left: 60px">spring.cloud.stream:<br>  defaultBinder: rabbit<br>  default.contentType: application/json</pre>
<ol start="2">
<li>Next, we bind our output channels to specific topic names, as follows:</li>
</ol>
<pre style="padding-left: 60px">  bindings:<br>    output-products:<br>      destination: products<br>    output-recommendations:<br>      destination: recommendations<br>    output-reviews:<br>      destination: reviews</pre>
<ol start="3">
<li>Finally, we declare connectivity information for both Kafka and RabbitMQ:</li>
</ol>
<pre style="padding-left: 60px">spring.cloud.stream.kafka.binder:<br>  brokers: 127.0.0.1<br>  defaultBrokerPort: 9092<br><br>spring.rabbitmq:<br>  host: 127.0.0.1<br>  port: 5672<br>  username: guest<br>  password: guest<br><br>---<br>spring.profiles: docker<br><br>spring.rabbitmq.host: rabbitmq<br>spring.cloud.stream.kafka.binder.brokers: kafka</pre>
<p>In the default Spring profile, we specify hostnames to be used when we run our system landscape without Docker on <kbd>localhost</kbd>&nbsp;with the IP address <kbd>127.0.0.1</kbd>. In the&nbsp;<kbd>docker</kbd><span>&nbsp;Spring profile,&nbsp;</span>we specify the hostnames we will use when running in Docker and using Docker Compose, that is,&nbsp;<kbd>rabbitmq</kbd> and <kbd>kafka</kbd>.</p>
<p>For the full source code, see the&nbsp;<kbd>src/main/resources/application.yml</kbd>&nbsp;configuration file in the&nbsp;<span><kbd>product-composite</kbd> project.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding configuration for publishing events</h1>
                </header>
            
            <article>
                
<p>We also need to set up a configuration for the messaging system to be able to publish events. To do this, we need to complete the following steps:</p>
<ol>
<li>We declare that RabbitMQ is the default messaging system and that the default content type is JSON:</li>
</ol>
<pre style="padding-left: 60px">spring.cloud.stream:<br>  defaultBinder: rabbit<br>  default.contentType: application/json</pre>
<ol start="2">
<li>Next, we bind our output channels to specific topic names, as follows:</li>
</ol>
<pre style="padding-left: 60px">  bindings:<br>    output-products:<br>      destination: products<br>    output-recommendations:<br>      destination: recommendations<br>    output-reviews:<br>      destination: reviews</pre>
<ol start="3">
<li>Finally, we declare connectivity information for both Kafka and RabbitMQ:</li>
</ol>
<pre style="padding-left: 60px">spring.cloud.stream.kafka.binder:<br>  brokers: 127.0.0.1<br>  defaultBrokerPort: 9092<br><br>spring.rabbitmq:<br>  host: 127.0.0.1<br>  port: 5672<br>  username: guest<br>  password: guest<br><br>---<br>spring.profiles: docker<br><br>spring.rabbitmq.host: rabbitmq<br>spring.cloud.stream.kafka.binder.brokers: kafka</pre>
<p>In the default Spring profile, we specify hostnames to be used when we run our system landscape without Docker on <kbd>localhost</kbd> with the IP address <kbd>127.0.0.1</kbd>. In the <kbd>docker</kbd><span> Spring profile, </span>we specify the hostnames we will use when running in Docker and using Docker Compose, that is, <kbd>rabbitmq</kbd> and <kbd>kafka</kbd>.</p>
<p>For the full source code, see the <kbd>src/main/resources/application.yml</kbd> configuration file in the <span><kbd>product-composite</kbd> project.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the test code</h1>
                </header>
            
            <article>
                
<p>Testing asynchronous event-driven microservices is, by their nature, difficult. Tests typically need to synchronize on the asynchronous background processing in some way to be able to verify its result. Spring Cloud Stream comes with support, in terms of <kbd>TestSupportBinder</kbd>, for verifying what messages have been sent without using any messaging system during the tests!</p>
<p class="mce-root"></p>
<p class="mce-root">The test support includes a  <kbd>MessageCollector</kbd> helper class that can be used to get all the messages that were sent during a test. To see how it is done, check out these steps:</p>
<ol>
<li class="mce-root">In the <kbd>MessagingTests</kbd> test class, we set up a queue that can be used to inspect the messages<span> that are</span> sent to each topic, as follows:</li>
</ol>
<pre style="padding-left: 60px">  @Autowired<br>  private MessageCollector collector;<br><br>  BlockingQueue&lt;Message&lt;?&gt;&gt; queueProducts = null;<br>  BlockingQueue&lt;Message&lt;?&gt;&gt; queueRecommendations = null;<br>  BlockingQueue&lt;Message&lt;?&gt;&gt; queueReviews = null;<br><br>  @Before<br>  public void setUp() {<br>      queueProducts = getQueue(channels.outputProducts());<br>      queueRecommendations = <br>      getQueue(channels.outputRecommendations());<br>      queueReviews = getQueue(channels.outputReviews());<br>  }<br><br>  private BlockingQueue&lt;Message&lt;?&gt;&gt; getQueue(MessageChannel <br>  messageChannel) {<br>      return collector.forChannel(messageChannel);<br>  } </pre>
<ol start="2">
<li>An actual test can verify the content in the queue like the following test can for the creation of a product:</li>
</ol>
<pre style="padding-left: 60px">@Test<br>public void createCompositeProduct1() {<br><br>    ProductAggregate composite = new ProductAggregate(1, "name", 1, <br>    null, null, null);<br>    postAndVerifyProduct(composite, OK);<br><br>    // Assert one expected new product events queued up<br>    assertEquals(1, queueProducts.size());<br><br>    Event&lt;Integer, Product&gt; expectedEvent = new Event(CREATE, <br>    composite.getProductId(), new Product(composite.getProductId(), <br>    composite.getName(), composite.getWeight(), null));<br>    assertThat(queueProducts, <br>    is(receivesPayloadThat (sameEventExceptCreatedAt <br>    (expectedEvent))));<br><br>    // Assert none recommendations and review events<br>    assertEquals(0, queueRecommendations.size());<br>    assertEquals(0, queueReviews.size());<br>}</pre>
<p>The <kbd>receivesPayloadThat()</kbd> method is a static method in another test support class in Spring Cloud Stream, <kbd>MessageQueueMatcher</kbd>. This class contains a set of methods that simplify the verification of messages in a queue.</p>
<p>The <kbd>sameEventExceptCreatedAt()</kbd> method is a static method in the <kbd>IsSameEvent</kbd> <span>class </span>that compares <kbd>Event</kbd> objects and treats them as equal if all the fields are equal, except for the <kbd>eventCreatedAt</kbd> field.</p>
<p><span>For the full source code, see the following test classes in the <kbd>product-composite</kbd> project: </span></p>
<ul>
<li><kbd>se.magnus.microservices.composite.product.MessagingTests</kbd></li>
<li><kbd>se.magnus.microservices.composite.product.IsSameEvent</kbd></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Consuming events in the core services</h1>
                </header>
            
            <article>
                
<p class="mce-root">To be able to consume events in the<span>&nbsp;</span>core services, we need to do the following:</p>
<ol>
<li class="mce-root"><span>Declare message processors that listen for events on its topic.</span></li>
<li class="mce-root">Change our service implementations so it uses the reactive persistence layer correctly.</li>
<li class="mce-root">Add configuration for consuming events.</li>
<li class="mce-root">Change our tests so that they can test the asynchronous processing of the events.</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Declaring message processors</h1>
                </header>
            
            <article>
                
<p class="mce-root">The REST APIs for creating and deleting entities have been replaced with a message processor in each core microservice that listens for creating and deleting events on each entity's topic. To be able to consume messages that have been published to a topic, we need to bind to <kbd>SubscribableChannel</kbd>, similar to how we bind to <kbd>MessageChannel</kbd> when we want to publish messages. Since each message processor only listens to one topic, we can use the built-in <kbd>Sink</kbd> interface to bind to that topic. We use the <span><kbd>EnableBinding</kbd></span><span> annotation to declare the use of the <kbd>Sink</kbd> interface, as shown in the following source code:</span></p>
<pre>@EnableBinding(Sink.class)<br>public class MessageProcessor {</pre>
<p>To actually consume and process messages, we can annotate a method with the <kbd>StreamListener</kbd> annotation, where we specify what channel we shall listen to:</p>
<pre>@StreamListener(target = Sink.INPUT)<br>public void process(Event&lt;Integer, Product&gt; event) {</pre>
<p>The implementation of the <kbd>process()</kbd> method uses a <kbd>switch</kbd> statement to call the create method in the service component for creating events and the delete method for deleting events. The source code looks as follows:</p>
<pre>switch (event.getEventType()) {<br><br>case CREATE:<br>    Product product = event.getData();<br>    LOG.info("Create product with ID: {}", product.getProductId());<br>    productService.createProduct(product);<br>    break;<br><br>case DELETE:<br>    int productId = event.getKey();<br>    LOG.info("Delete recommendations with ProductID: {}", productId);<br>    productService.deleteProduct(productId);<br>    break;<br><br>default:<br>    String errorMessage = "Incorrect event type: " + <br>    event.getEventType() + ", expected a CREATE or DELETE event";<br>    LOG.warn(errorMessage);<br> throw new EventProcessingException(errorMessage);<br>}</pre>
<p>Let's explain the preceding source code in detail:</p>
<ol>
<li>The <kbd>switch</kbd> statement expects an event type that is either a <kbd>CREATE</kbd> or a <kbd>DELETE</kbd> event.</li>
<li>The <kbd>productService.createProduct()</kbd> <span>method </span>is called for create events.</li>
<li>The <kbd>productService.deleteProduct()</kbd><span> method </span>is called for delete events.</li>
<li>If the event type is neither a <span>&nbsp;</span><kbd>CREATE</kbd><span> or a </span><kbd>DELETE</kbd><span> event; an exception of the <kbd>EventProcessingException</kbd> type is thrown.</span></li>
</ol>
<p class="mce-root"></p>
<p>The service component is injected as usual using constructor injection, as shown here:</p>
<pre>private final ProductService productService;<br><br>@Autowired<br>public MessageProcessor(ProductService productService) {<br>    this.productService = productService;<br>}</pre>
<p>For the full source code, see the following classes:</p>
<ul>
<li class="mce-root"><kbd>se.magnus.microservices.core.product.services.MessageProcessor</kbd> in the <kbd><span>product</span></kbd> project</li>
<li class="mce-root"><kbd>se.magnus.microservices.core.recommendation.services.MessageProcessor</kbd> in the <kbd><span>recommendation</span></kbd> project</li>
<li class="mce-root"><kbd>se.magnus.microservices.core.review.services.MessageProcessor</kbd> in the <kbd><span>review</span></kbd> project</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the service implementations</h1>
                </header>
            
            <article>
                
<p>The service implementations of the create and delete methods for the <kbd>product</kbd> and <kbd>recommendation</kbd> service have been rewritten to use the non-blocking reactive persistence layer for MongoDB. For example, creating product entities is done as follows:</p>
<pre>public class ProductServiceImpl implements ProductService {<br><br>    @Override<br>    public Product createProduct(Product body) {<br><br>        if (body.getProductId() &lt; 1) throw new <br>        InvalidInputException("Invalid productId: " + <br>        body.getProductId());<br><br>        ProductEntity entity = mapper.apiToEntity(body);<br>        Mono&lt;Product&gt; newEntity = repository.save(entity)<br>            .log()<br>            .onErrorMap(<br>                DuplicateKeyException.class,<br>                ex -&gt; new InvalidInputException("Duplicate key, Product <br>                Id: " + body.getProductId()))<br>            .map(e -&gt; mapper.entityToApi(e));<br><br>        return newEntity.block();<br>    }</pre>
<p>The <kbd>onErrorMap()</kbd> method is used to map the <kbd>DuplicateKeyException</kbd>&nbsp;<span>persistence exception </span>to our own <kbd>InvalidInputException</kbd> exception.</p>
<p>Since our message processor is based on a blocking programming model, we need to call the <kbd>block()</kbd> method on the returned <kbd>Mono</kbd> object from the persistence layer before we return it to the message processor. If we don't call the <kbd>block()</kbd><span> method, we won't be able to trigger the error handling in the messaging system if the processing in the service implementation fails; the event will not be requeued, and eventually, it will be moved to the dead-letter queue, as expected.</span></p>
<div class="packt_tip"><span>The <kbd>review</kbd> service that uses the blocking </span><span>persistence layer for JPA, as before, does not need to be updated. </span></div>
<p class="mce-root">For the full source code, see the following classes:</p>
<ul>
<li class="mce-root"><kbd>se.magnus.microservices.core.product.services.ProductServiceImpl</kbd> in the <kbd><span>product</span></kbd> project</li>
<li class="mce-root"><kbd>se.magnus.microservices.core.recommendation.services.RecommendationServiceImpl</kbd> in the <kbd><span>recommendation</span></kbd> project</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the test code</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Since the core services now receive events for creating and deleting their entities, the tests need to be updated so that they send events instead of calling REST APIs, like they did previously. In the following source code, we can see how the <kbd>send()</kbd>&nbsp;method on the <kbd>input</kbd> method channel is used to send an event:</span></p>
<pre>private void sendCreateProductEvent(int productId) {<br>    Product product = new Product(productId, "Name " + productId, <br>    productId, "SA");<br>    Event&lt;Integer, Product&gt; event = new Event(CREATE, productId, <br>    product);<br>    input.send(new GenericMessage&lt;&gt;(event));<br>}<br><br>private void sendDeleteProductEvent(int productId) {<br>    Event&lt;Integer, Product&gt; event = new Event(DELETE, productId, null);<br>    input.send(new GenericMessage&lt;&gt;(event));<br>}</pre>
<p>The <kbd>input</kbd> channel is set up by the test class before any tests are run. It is based on the same built-in&nbsp;<kbd>Sink</kbd><span>&nbsp;interface that the message processor uses. In the following source code, we can see how the <kbd>input</kbd> channel is created in the <kbd>setupDb()</kbd> method. Since the&nbsp;<kbd>setupDb()</kbd>&nbsp;method is annotated with&nbsp;<kbd>@Before</kbd>, it will run before any tests are executed</span><span>:</span></p>
<pre><span>@Autowired<br></span><span>private </span>Sink <span>channels</span>;<br><br><span>private </span>AbstractMessageChannel <span>input </span>= <span>null</span>;<br><br><span>@Before<br></span><span>public void </span>setupDb() {<br>   <span>input </span>= (AbstractMessageChannel) <span>channels</span>.input();<br>   <span>repository</span>.deleteAll().block();<br>}</pre>
<p><span>This construction shortcuts the messaging system and the call to the <kbd>send()</kbd> method in the <kbd>input</kbd> channel will be processed synchronously by the message processor, that is, like a normal method call its&nbsp;</span><kbd>process()</kbd> method. This means that the test code doesn't need to implement any synchronization or <em>wait logic</em> for the asynchronous processing of an event. Instead, the test code can apply validation logic directly after calls to the&nbsp;<kbd>sendCreateProductEvent</kbd> and&nbsp;<kbd>sendDeleteProductEvent</kbd>&nbsp;send helper methods return.</p>
<p>For the full source code, see the following test classes:</p>
<ul>
<li><kbd>se.magnus.microservices.core.product.ProductServiceApplicationTests</kbd>&nbsp;in the&nbsp;<kbd><span>product</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.recommendation.RecommendationServiceApplicationTests</kbd>&nbsp;in the&nbsp;<kbd><span>recommendation</span></kbd> project</li>
<li><kbd>se.magnus.microservices.core.review.ReviewServiceApplicationTests</kbd>&nbsp;in the&nbsp;<kbd><span>review</span></kbd> project</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running manual tests of the reactive microservice landscape</h1>
                </header>
            
            <article>
                
<p>Now, we have fully reactive microservices, both in terms of non-blocking synchronous REST APIs and event-driven asynchronous services. Let's try them out!</p>
<p><span>Three different configurations are prepared as follows, each in a separate Docker Compose file:</span></p>
<ul>
<li><span>Using RabbitMQ without the use of partitions</span></li>
<li>Using RabbitMQ with two partitions per topic</li>
<li>Using Kafka with&nbsp;<span>two partitions per topic</span></li>
</ul>
<p>However, before testing these three configurations, we first need to simplify testing of the&nbsp;reactive microservice landscape. Once simplified, we can proceed with testing the microservices.</p>
<p>So accordingly, the following two features need to be checked:</p>
<ul>
<li>Saving events for later inspection when using RabbitMQ</li>
<li>A health API that can be used to monitor the state of the landscape</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Saving events</h1>
                </header>
            
            <article>
                
<p>After running some tests on&nbsp;<span>event-driven asynchronous services, it might be of interest to see what event was actually sent. When using Spring Cloud Stream with&nbsp;</span>Kafka, events are retained in the topics, even after consumers have processed them. However, when using&nbsp;<span>Spring Cloud Stream with&nbsp;</span><span>RabbitMQ, the events are removed after they have been processed successfully.</span></p>
<p>To be able to see what events have been published on each topic, Spring Cloud Stream is configured to save published events in a separate&nbsp;<kbd>auditGroup</kbd>&nbsp;<span>consumer group&nbsp;</span>per topic. For the <kbd>products</kbd> topic, the configuration looks like the following:</p>
<pre><span>spring.cloud.stream</span>:<br>  <span>bindings</span>:<br>    <span>output-products</span>:<br>      <span>destination</span>: products<br>      <span>producer</span>:<br>        <span>required-groups</span>: auditGroup</pre>
<p>When using RabbitMQ, this will result in extra queues being created where the events are stored for later inspection.</p>
<p><span>For the full source code, see the&nbsp;</span><kbd>src/main/resources/application.yml</kbd><span>&nbsp;configuration file in the&nbsp;<kbd>product-composite</kbd> project.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a health API</h1>
                </header>
            
            <article>
                
<p class="mce-root">Testing a system landscape of microservices that use a combination of synchronous APIs and asynchronous messaging is challenging. For example, how do we know when a newly started landscape of microservices, together with their databases and messaging system, are ready to process requests and messages?</p>
<p class="mce-root">To make it easier to know when all the microservices are ready to process requests and messages, we have added a health API in all the microservices. They are based on the support for health endpoints that comes with the Spring Boot module known as the Actuator. By default, a <kbd>health</kbd> endpoint based on the Actuator answers <kbd>UP</kbd>&nbsp;(and gives 200 as the HTTP return status) if the microservice itself and all the dependencies Spring Boot knows about are available, for example, dependencies to databases and messaging systems; otherwise, the health endpoint answers <kbd>DOWN</kbd>&nbsp;(and returns 500 as the HTTP return status).</p>
<p class="mce-root">We can also extend the <kbd>health</kbd> endpoint to cover dependencies that Spring Boot is not aware of. We will use this feature to extend to the product composite's <kbd>health</kbd> endpoint, which will also include the health of the three core services. This means that the product composite <kbd>health</kbd> endpoint will only respond with <kbd>UP</kbd>&nbsp;if itself and the three core microservices are healthy. This can be used either manually or automatically by the<span>&nbsp;</span><kbd>test-em-all.bash</kbd><span>&nbsp;</span>script to find out when all the microservices and their dependencies are up and running.</p>
<p class="mce-root"></p>
<p class="mce-root">In the <kbd>ProductCompositeIntegration</kbd>&nbsp;integration class, we have added helper methods for checking the health of the three core microservices, as follows:</p>
<pre>public Mono&lt;Health&gt; getProductHealth() {<br>    return getHealth(productServiceUrl);<br>}<br><br>public Mono&lt;Health&gt; getRecommendationHealth() {<br>    return getHealth(recommendationServiceUrl);<br>}<br><br>public Mono&lt;Health&gt; getReviewHealth() {<br>    return getHealth(reviewServiceUrl);<br>}<br><br>private Mono&lt;Health&gt; getHealth(String url) {<br>    url += "/actuator/health";<br>    LOG.debug("Will call the Health API on URL: {}", url);<br>    return webClient.get().uri(url).retrieve().bodyToMono(String.class)<br>        .map(s -&gt; new Health.Builder().up().build())<br>        .onErrorResume(ex -&gt; Mono.just(new <br>         Health.Builder().down(ex).build()))<br>        .log();<br>}</pre>
<p>This code is similar to the code we used previously to call the core services to read APIs.</p>
<p>For the full source code, see the <kbd>se.magnus.microservices.composite.product.services.ProductCompositeIntegration</kbd>&nbsp;class in the&nbsp;<span><kbd>product-composite</kbd> project.</span></p>
<p>In the main <kbd>ProductCompositeServiceApplication</kbd>&nbsp;application class, we use these helper methods to register three health checks, one for each core microservice:</p>
<pre>@Autowired<br>HealthAggregator healthAggregator;<br><br>@Autowired<br>ProductCompositeIntegration integration;<br><br>@Bean<br>ReactiveHealthIndicator coreServices() {<br><br>    ReactiveHealthIndicatorRegistry registry = new <br>    DefaultReactiveHealthIndicatorRegistry(new LinkedHashMap&lt;&gt;());<br><br>    registry.register("product", () -&gt; integration.getProductHealth());<br>    registry.register("recommendation", () -&gt; <br>    integration.getRecommendationHealth());<br>    registry.register("review", () -&gt; integration.getReviewHealth());<br><br>    return new CompositeReactiveHealthIndicator(healthAggregator, <br>    registry);<br>}</pre>
<p>For the full source code, see the&nbsp;<kbd>se.magnus.microservices.composite.product.ProductCompositeServiceApplication</kbd>&nbsp;class in the&nbsp;<span><kbd>product-composite</kbd> project.</span></p>
<p>Finally, in the<span>&nbsp;</span><kbd>application.yml</kbd>&nbsp;file of all four microservices, we configure the Spring Boot Actuator so that it does the following:</p>
<ul>
<li>Show details about the state of health, which not only includes&nbsp;<kbd>UP</kbd>&nbsp;or&nbsp;<kbd>DOWN</kbd>, but also information about its dependencies</li>
<li>Expose all its endpoints over HTTP</li>
</ul>
<p>The configuration for these two settings looks as follows:</p>
<pre style="padding-left: 60px">management.endpoint.health.show-details: "ALWAYS"<br>management.endpoints.web.exposure.include: "*"</pre>
<p><span>For an example of the full source code, see the&nbsp;</span><kbd>src/main/resources/application.yml</kbd>&nbsp;c<span>onfiguration file&nbsp;</span>in the&nbsp;<span><kbd>product-composite</kbd> project.</span></p>
<div class="packt_infobox"><strong>WARNING</strong>: These configuration settings are good during development, but it can be a security issue to reveal too much information in actuator endpoints in production systems. Therefore, plan for minimizing the information exposed by the actuator endpoints in production!</div>
<p class="mce-root">For details regarding&nbsp;the endpoints that are exposed by Spring Boot Actuator, see<span>&nbsp;</span><a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html">https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html</a>:</p>
<ul>
<li class="mce-root">Try it out (when you have all the microservices up and running using Docker Compose, as described in the next section):</li>
</ul>
<pre style="padding-left: 60px"><strong>curl localhost:8080/actuator/health -s | jq .</strong></pre>
<ul>
<li>This will result in the following response:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2ae85fa6-c021-4370-9850-fb6daec78d0c.png" style="width:20.83em;height:21.67em;" width="820" height="851" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2ae85fa6-c021-4370-9850-fb6daec78d0c.png"></p>
<p><span>In the preceding output</span>, we can see that the composite service reports that it is healthy, that is, its status is <kbd>UP</kbd>. At the end of the response, we can see that all three core microservices are <span>also&nbsp;</span>reported as healthy.</p>
<p>With a&nbsp;health API in place, we are ready to test our reactive microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a health API</h1>
                </header>
            
            <article>
                
<p class="mce-root">Testing a system landscape of microservices that use a combination of synchronous APIs and asynchronous messaging is challenging. For example, how do we know when a newly started landscape of microservices, together with their databases and messaging system, are ready to process requests and messages?</p>
<p class="mce-root">To make it easier to know when all the microservices are ready to process requests and messages, we have added a health API in all the microservices. They are based on the support for health endpoints that comes with the Spring Boot module known as the Actuator. By default, a <kbd>health</kbd> endpoint based on the Actuator answers <kbd>UP</kbd> (and gives 200 as the HTTP return status) if the microservice itself and all the dependencies Spring Boot knows about are available, for example, dependencies to databases and messaging systems; otherwise, the health endpoint answers <kbd>DOWN</kbd> (and returns 500 as the HTTP return status).</p>
<p class="mce-root">We can also extend the <kbd>health</kbd> endpoint to cover dependencies that Spring Boot is not aware of. We will use this feature to extend to the product composite's <kbd>health</kbd> endpoint, which will also include the health of the three core services. This means that the product composite <kbd>health</kbd> endpoint will only respond with <kbd>UP</kbd> if itself and the three core microservices are healthy. This can be used either manually or automatically by the<span>&nbsp;</span><kbd>test-em-all.bash</kbd><span>&nbsp;</span>script to find out when all the microservices and their dependencies are up and running.</p>
<p class="mce-root"></p>
<p class="mce-root">In the <kbd>ProductCompositeIntegration</kbd> integration class, we have added helper methods for checking the health of the three core microservices, as follows:</p>
<pre>public Mono&lt;Health&gt; getProductHealth() {<br>    return getHealth(productServiceUrl);<br>}<br><br>public Mono&lt;Health&gt; getRecommendationHealth() {<br>    return getHealth(recommendationServiceUrl);<br>}<br><br>public Mono&lt;Health&gt; getReviewHealth() {<br>    return getHealth(reviewServiceUrl);<br>}<br><br>private Mono&lt;Health&gt; getHealth(String url) {<br>    url += "/actuator/health";<br>    LOG.debug("Will call the Health API on URL: {}", url);<br>    return webClient.get().uri(url).retrieve().bodyToMono(String.class)<br>        .map(s -&gt; new Health.Builder().up().build())<br>        .onErrorResume(ex -&gt; Mono.just(new <br>         Health.Builder().down(ex).build()))<br>        .log();<br>}</pre>
<p>This code is similar to the code we used previously to call the core services to read APIs.</p>
<p>For the full source code, see the <kbd>se.magnus.microservices.composite.product.services.ProductCompositeIntegration</kbd> class in the <span><kbd>product-composite</kbd> project.</span></p>
<p>In the main <kbd>ProductCompositeServiceApplication</kbd> application class, we use these helper methods to register three health checks, one for each core microservice:</p>
<pre>@Autowired<br>HealthAggregator healthAggregator;<br><br>@Autowired<br>ProductCompositeIntegration integration;<br><br>@Bean<br>ReactiveHealthIndicator coreServices() {<br><br>    ReactiveHealthIndicatorRegistry registry = new <br>    DefaultReactiveHealthIndicatorRegistry(new LinkedHashMap&lt;&gt;());<br><br>    registry.register("product", () -&gt; integration.getProductHealth());<br>    registry.register("recommendation", () -&gt; <br>    integration.getRecommendationHealth());<br>    registry.register("review", () -&gt; integration.getReviewHealth());<br><br>    return new CompositeReactiveHealthIndicator(healthAggregator, <br>    registry);<br>}</pre>
<p>For the full source code, see the <kbd>se.magnus.microservices.composite.product.ProductCompositeServiceApplication</kbd> class in the <span><kbd>product-composite</kbd> project.</span></p>
<p>Finally, in the<span>&nbsp;</span><kbd>application.yml</kbd> file of all four microservices, we configure the Spring Boot Actuator so that it does the following:</p>
<ul>
<li>Show details about the state of health, which not only includes <kbd>UP</kbd> or <kbd>DOWN</kbd>, but also information about its dependencies</li>
<li>Expose all its endpoints over HTTP</li>
</ul>
<p>The configuration for these two settings looks as follows:</p>
<pre style="padding-left: 60px">management.endpoint.health.show-details: "ALWAYS"<br>management.endpoints.web.exposure.include: "*"</pre>
<p><span>For an example of the full source code, see the </span><kbd>src/main/resources/application.yml</kbd>&nbsp;c<span>onfiguration file </span>in the <span><kbd>product-composite</kbd> project.</span></p>
<div class="packt_infobox"><strong>WARNING</strong>: These configuration settings are good during development, but it can be a security issue to reveal too much information in actuator endpoints in production systems. Therefore, plan for minimizing the information exposed by the actuator endpoints in production!</div>
<p class="mce-root">For details regarding the endpoints that are exposed by Spring Boot Actuator, see<span>&nbsp;</span><a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html">https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html</a>:</p>
<ul>
<li class="mce-root">Try it out (when you have all the microservices up and running using Docker Compose, as described in the next section):</li>
</ul>
<pre style="padding-left: 60px"><strong>curl localhost:8080/actuator/health -s | jq .</strong></pre>
<ul>
<li>This will result in the following response:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2ae85fa6-c021-4370-9850-fb6daec78d0c.png" style="width:20.83em;height:21.67em;" width="820" height="851" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2ae85fa6-c021-4370-9850-fb6daec78d0c.png"></p>
<p><span>In the preceding output</span>, we can see that the composite service reports that it is healthy, that is, its status is <kbd>UP</kbd>. At the end of the response, we can see that all three core microservices are <span>also </span>reported as healthy.</p>
<p>With a health API in place, we are ready to test our reactive microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using RabbitMQ without using partitions</h1>
                </header>
            
            <article>
                
<p>In this section, we will test <span>the reactive microservices together with RabbitMQ but without using partitions.</span></p>
<p>The default <kbd>docker-compose.yml</kbd> Docker Co<span>mpose file </span>is used in this configuration. The following changes have been applied to the file:</p>
<ul>
<li>RabbitMQ has been added, as shown here:</li>
</ul>
<pre style="padding-left: 60px"><span>rabbitmq</span>:<br>  <span>image</span>: rabbitmq:3.7.8-management<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - 5672:5672<br>    - 15672:15672<br>  <span>healthcheck</span>:<br>    <span>test</span>: [<span>"CMD"</span>, <span>"rabbitmqctl"</span>, <span>"status"</span>]<br>    <span>interval</span>: 10s<br>    <span>timeout</span>: 5s<br>    <span>retries</span>: 10</pre>
<ul>
<li>The microservices now have a dependency declared to the RabbitMQ service. This means that Docker will not start the microservice containers until the RabbitMQ service is reported to be healthy:</li>
</ul>
<pre style="padding-left: 60px"><span>depends_on</span>:<br>  <span>rabbitmq</span>:<br>    <span>condition</span>: service_healthy</pre>
<p><span>To </span>run our tests, perform the following steps:</p>
<ol>
<li>Build and start the system landscape with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>cd $BOOK_HOME/</span><span>Chapter07<br></span><span>./gradlew build &amp;&amp; </span><span>docker-compose build </span><span>&amp;&amp; </span><span>docker-compose up -d</span></strong></pre>
<ol start="2">
<li>Now, we have to wait for the microservice landscape to be up and running.<br>
Try running the following command a few times:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl -s localhost:8080/actuator/health | jq -r .status</strong></pre>
<p style="padding-left: 60px">When it returns <kbd>UP</kbd>, we are ready to run our tests!</p>
<ol start="3">
<li>First, create a composite product with the following commands:</li>
</ol>
<pre style="padding-left: 60px">body='{"productId":1,"name":"product name C","weight":300, "recommendations":[<br> {"recommendationId":1,"author":"author 1","rate":1,"content":"content 1"},<br> {"recommendationId":2,"author":"author 2","rate":2,"content":"content 2"},<br> {"recommendationId":3,"author":"author 3","rate":3,"content":"content 3"}<br>], "reviews":[<br> {"reviewId":1,"author":"author 1","subject":"subject 1","content":"content 1"},<br> {"reviewId":2,"author":"author 2","subject":"subject 2","content":"content 2"},<br> {"reviewId":3,"author":"author 3","subject":"subject 3","content":"content 3"}<br>]}'<br><br><strong>curl -X POST localhost:8080/product-composite -H "Content-Type: application/json" --data "$body"</strong></pre>
<p style="padding-left: 60px">When using <span>Spring Cloud Stream together with RabbitMQ, it will create one RabbitMQ exchange per topic and a set of queues, depending on our configuration. </span></p>
<p style="padding-left: 60px">Let's see what queues that Spring Cloud Stream has created for us!</p>
<ol start="4">
<li>Open the following URL in a web browser: <kbd>http://localhost:15672/#/queues</kbd>. You should see the following queues:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/00372e65-3efd-4733-8c3a-773b383cde7e.png" width="2065" height="1563" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/00372e65-3efd-4733-8c3a-773b383cde7e.png"></p>
<p style="padding-left: 60px">For each topic, we can see one queue for <span class="packt_screen">auditGroup</span>, one for the consumer group that's used by the corresponding core microservice, and one dead-letter queue. We can also see that the <span class="packt_screen">auditGroup</span> queues contain messages, as expected!</p>
<ol start="5">
<li>Click on the <span class="packt_screen">products.auditGroup</span> queue and scroll down to <span class="packt_screen">Get Message(s)</span>, expand it, and click on the button named <span class="packt_screen">Get Message(s)</span> to see the message in the queue:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1ce813e3-39eb-4b9a-a69e-b2d4f0a6be85.png" width="2069" height="1486" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1ce813e3-39eb-4b9a-a69e-b2d4f0a6be85.png"></p>
<ol start="6">
<li>Next, try to get the product composite using the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl localhost:8080/product-composite/1 | jq</strong> </pre>
<ol start="7">
<li>Finally, delete it, like so:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl -X DELETE localhost:8080/product-composite/1</strong></pre>
<p style="padding-left: 60px">Trying to get the deleted product again should result in a <kbd>404 - "NotFound"</kbd> response!</p>
<p style="padding-left: 60px">If you look in the RabbitMQ audit queues again, you should be able to find new messages containing delete events.</p>
<p class="mce-root"></p>
<ol start="8">
<li>Wrap up the test by bringing down the microservice landscape with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose down</strong></pre>
<p>This completes the tests where we use RabbitMQ without partitions. Now, let's move on and test <span>RabbitMQ with partitions.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using RabbitMQ with two partitions per topic</h1>
                </header>
            
            <article>
                
<p>Now, let's try out the partitioning support in Spring Cloud Stream!</p>
<p>We have a separate Docker Compose file prepared for using RabbitMQ with two partitions per topic: <kbd>docker-compose-partitions.yml</kbd>. It will also start two instances per core microservice, one for each partition. For example, a second <kbd>product</kbd> instance is configured as follows:</p>
<pre><span>product-p1</span>:<br>  <span>build</span>: microservices/product-service<br>  mem_limit: 350m<br>  <span>environment</span>:<br>    - SPRING_PROFILES_ACTIVE=docker<br>    - SPRING_CLOUD_STREAM_BINDINGS_INPUT_CONSUMER_PARTITIONED=true<br>    - SPRING_CLOUD_STREAM_BINDINGS_INPUT_CONSUMER_INSTANCECOUNT=2<br>    - SPRING_CLOUD_STREAM_BINDINGS_INPUT_CONSUMER_INSTANCEINDEX=1<br>  <span>depends_on</span>:<br>    <span>mongodb</span>:<br>      <span>condition</span>: service_healthy<br>    <span>rabbitmq</span>:<br>      <span>condition</span>: service_healthy</pre>
<p>Here is an explanation of the preceding source code:</p>
<ul>
<li>We use the same source code and Dockerfile that we did for the first <span><kbd>product</kbd> instance but configure them differently.</span></li>
<li>Specifically, we assign the two <kbd>product</kbd> instances to different partitions using the <kbd>instance-index</kbd> property we described earlier in this chapter.</li>
<li>When using system environment variables to specify Spring properties, we must use an uppercase format where dots are replaced with underscores.</li>
<li>This <kbd>product</kbd> instance will only process asynchronous events; it will not respond to API calls. Since it has a different name, <kbd>product-p1</kbd> (also used as its DNS name), it will not respond to calls to a URL starting with <kbd>http://product:8080</kbd>.</li>
</ul>
<p class="mce-root"></p>
<p>Start up the <span>microservice landscape with the following command:</span></p>
<pre><strong><span>export COMPOSE_FILE=docker-compose-partitions.yml<br>docker-compose build </span><span>&amp;&amp; </span><span>docker-compose up -d</span></strong></pre>
<p>Repeat the tests from the previous section but also create a product with the product ID set to <kbd>2</kbd>. If you take a look into the queues set up by Spring Cloud Stream, you will see one queue per partition and that the product audit queues now contain one message each, that is, the event for product ID <kbd>1</kbd> was placed in one partition and the event for product ID <kbd>2</kbd> was placed in the other partition. If you go back to<span>&nbsp;</span><kbd>http://localhost:15672/#/queues</kbd> in your web browser, you should see something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4d333160-75e8-4a56-87f6-aeee8765402e.png" width="2044" height="1723" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4d333160-75e8-4a56-87f6-aeee8765402e.png"></p>
<p class="mce-root"></p>
<p>To end the test with RabbitMQ using partitions, bring down the microservice landscape with the following command:</p>
<pre><strong>docker-compose down</strong><br><strong>unset <span>COMPOSE_FILE</span></strong></pre>
<p>We are now done with tests using RabbitMQ, both with and without partitions. The final test configuration we shall try out is testing the microservices together with Kafka.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Kafka with two partitions per topic</h1>
                </header>
            
            <article>
                
<p>Now, we shall try out a very cool feature of Spring Cloud Stream: changing the messaging system from RabbitMQ to Apache Kafka!</p>
<p>This can be done simply by changing the value of the <kbd><span>spring.cloud.stream.defaultBinder</span></kbd><span> property from </span><kbd>rabbit</kbd> to <kbd>kafka</kbd>. This is handled by the<span>&nbsp;<kbd>docker-compose-kafka.yml</kbd></span><span>&nbsp;</span><span>Docker Compose file </span>that has also replaced RabbitMQ with Kafka and Zookeeper. The configuration of Kafka and Zookeeper looks as follows:</p>
<pre><span>kafka</span>:<br>  <span>image</span>: wurstmeister/kafka:2.12-2.1.0<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"9092:9092"<br></span><span>  </span><span>environment</span>:<br>    - KAFKA_ADVERTISED_HOST_NAME=kafka<br>    - KAFKA_ADVERTISED_PORT=9092<br>    - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181<br>  <span>depends_on</span>:<br>    - zookeeper<br><br><span>zookeeper</span>:<br>  <span>image</span>: wurstmeister/zookeeper:3.4.6<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"2181:2181"<br></span><span>  </span><span>environment</span>:<br>    - KAFKA_ADVERTISED_HOST_NAME=zookeeper</pre>
<p>Kafka is also configured to use two partitions per topic, and like before, we start up <span>two instances per core microservice, one for each partition. See the Docker Compose file, <kbd>docker-compose-kafka.yml</kbd>, for details!</span></p>
<p class="mce-root"></p>
<p>Start up the <span>microservice landscape with the following command:</span></p>
<pre><strong><span>export COMPOSE_FILE=docker-compose-kafka.yml<br>docker-compose build </span><span>&amp;&amp; </span><span>docker-compose up -d</span></strong></pre>
<p><span>Repeat the tests from the previous section, for example, create two products, one with the product ID set to <kbd>1</kbd>, and one with the product ID set to <kbd>2</kbd>.</span></p>
<div class="packt_infobox"><span>Unfortunately, Kafka doesn't come with any graphical tools that can be used to inspect topics, partitions, and the messages that are placed within them. Instead, we can run CLI commands in the Kafka Docker container.</span></div>
<p>To see a list of topics, run the following command:</p>
<pre><strong>docker-compose exec kafka /opt/kafka/bin/kafka-topics.sh --zookeeper zookeeper --list</strong></pre>
<p>Expect an output like the one shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/824bee38-2547-4e15-9c1f-d4a7550eb6a0.png" style="width:21.67em;height:9.42em;" width="866" height="378" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/824bee38-2547-4e15-9c1f-d4a7550eb6a0.png"></p>
<p>Here is an explanation of the preceding source code:</p>
<ul>
<li>The topics prefixed with <kbd>error</kbd> are the topics corresponding to d<span>ead-letter queues.</span></li>
<li>You will not find any <kbd>auditGroup</kbd> in the case of RabbitMQ; <span>instead, all messages the are available in the topics for any consumer to process.</span></li>
</ul>
<p>To see the partitions in a specific topic, for example, the <kbd>products</kbd> topic, run the following command:</p>
<pre><strong>docker-compose exec kafka /opt/kafka/bin/kafka-topics.sh --describe --zookeeper zookeeper --topic products</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p><span>Expect an output like the one shown here:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0ba3ea90-0eb6-49b9-94f2-71b377357e60.png" style="width:36.50em;height:6.42em;" width="1378" height="242" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0ba3ea90-0eb6-49b9-94f2-71b377357e60.png"></p>
<p>To see all the messages in a specific topic, for example, <span>the </span><kbd>products</kbd><span> topic, </span>run the following command:</p>
<pre><strong>docker-compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic products --from-beginning --timeout-ms 1000</strong></pre>
<p><span>Expect an output like the one shown here:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7570391d-189e-4c89-b544-547f054cfc71.png" width="1807" height="383" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7570391d-189e-4c89-b544-547f054cfc71.png"></p>
<p>To see all the messages in a specific partition, <span>for example, partition <kbd>1</kbd> in the </span><kbd>products</kbd><span> topic, </span>run the following command:</p>
<pre><strong>docker-compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic products --from-beginning --timeout-ms 1000 --partition 1</strong></pre>
<p><span>Expect an output like the one shown here:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9dd71100-be1f-4d1c-b3aa-06094d47fe58.png" width="1796" height="238" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9dd71100-be1f-4d1c-b3aa-06094d47fe58.png"></p>
<p><span>The output will end with a timeout exception since we stop the command by specifying a timeout for the command of <kbd>1000</kbd> ms.</span></p>
<p class="mce-root"></p>
<p>Bring down the microservice landscape with the following command:</p>
<pre><strong>docker-compose down</strong><br><strong>unset <span>COMPOSE_FIL</span><span>E</span></strong></pre>
<p><span><span>Now, we have learned how Spring Cloud Stream can be used to switch a message broker from RabbitMQ to Kafka without requiring any changes in the source code. It just requires a few changes in the Docker Compose file.</span></span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running automated tests of the reactive microservice landscape</h1>
                </header>
            
            <article>
                
<p>To be able to run tests of the <span>reactive microservice landscape</span> automatically instead of manually, the automated <kbd>test-em-all.bash</kbd> test script has been enhanced. The most important changes are as follows:</p>
<ul>
<li>The script uses the new <kbd>health</kbd> endpoint to know when the microservice landscape is operational, as shown here:</li>
</ul>
<pre style="padding-left: 60px"><span>waitForService </span>curl http://$HOST:$PORT/actuator/health</pre>
<ul>
<li>The script has a new <kbd>waitForMessageProcessing()</kbd> function, which is called after the test data is set up. Its purpose is simply to wait for the creation of the test data to be completed by the asynchronous create services.</li>
</ul>
<p style="padding-left: 60px">To use the test script to automatically run the tests with RabbitMQ and Kafka, perform the following steps:</p>
<ol>
<li>Run the tests using the default Docker Compose file, that is, with RabbitMQ without partitions, with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>unset COMPOSE_FILE</strong><br><strong>./test-em-all.bash start stop</strong></pre>
<ol start="2">
<li>Run the tests for RabbitMQ with two partitions per topic using the Docker Compose <kbd>docker-compose-partitions.yml</kbd> file with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>export COMPOSE_FILE=docker-compose-partitions.yml </strong><br><strong>./test-em-all.bash start stop</strong><br><strong>unset COMPOSE_FILE</strong></pre>
<p class="mce-root"></p>
<ol start="3">
<li>Finally, run the tests with Kafka and <span>two partitions per topic using the Docker Compose <kbd>docker-compose-kafka.yml</kbd> file with the following commands:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>export COMPOSE_FILE=docker-compose-kafka.yml </strong><br><strong>./test-em-all.bash start stop</strong><br><strong>unset COMPOSE_FILE</strong></pre>
<p>In this section, we have learned how to use the<span>&nbsp;</span><kbd>test-em-all.bash</kbd><span> test script to automatically run tests of the reactive microservice landscape that have been either configured to use RabbitMQ or Kafka as its message broker.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we have seen how we can develop reactive microservices!</span></p>
<p>Using Spring WebFlux and Spring WebClient, we can develop non-blocking synchronous APIs that can handle incoming HTTP requests and send outgoing HTTP requests without blocking any threads. Using Spring Data's reactive support for MongoDB, we can also access MongoDB databases in a non-blocking way, that is, without blocking any threads while waiting for responses from the database. <span>Spring WebFlux, Spring WebClient, and Spring Data rely on Spring Reactor to provide their reactive and non-blocking features. </span>When we must use blocking code, for example, when using Spring Data for JPA, we can encapsulate the processing of the blocking code by scheduling the processing of it in a dedicated thread pool.</p>
<p>We have also seen how Spring Data Stream can be used to develop event-driven asynchronous services that work on both RabbitMQ and Kafka as messing systems without requiring any changes in the code. By doing some configuration, we can use features in Spring Cloud Stream such as consumer groups, retries, dead-letter queues, and partitions to handle the various challenges of asynchronous messaging. </p>
<p>We have also learned how to manually and automatically test a system landscape consisting of reactive microservices.</p>
<p><span>This was the final chapter on how to use fundamental features in Spring Boot and Spring Framework.</span></p>
<p><span>Next up is an introduction to Spring Cloud and how it can be used to make our services production-ready, scalable, robust, configurable, secure, and resilient!</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Why is it important to know how to develop reactive microservices?</li>
<li>How do you choose between non-blocking synchronous APIs and event/message-driven asynchronous services?</li>
<li>What makes a message different from an event?</li>
<li>Name some challenges with <span>message-driven asynchronous services. How do we handle them?</span></li>
<li class="mce-root"><span>Why is the following test failing?</span></li>
</ol>
<pre style="color: black;padding-left: 60px">    @Test<br>    public void TestFlux() {<br><br>        List&lt;Integer&gt; list = new ArrayList&lt;&gt;();<br><br>        Flux.just(1, 2, 3, 4)<br>            .filter(n -&gt; n % 2 == 0)<br>            .map(n -&gt; n * 2)<br>            .log();<br><br>        assertThat(list).containsExactly(4, 8);</pre>
<ol start="6">
<li>What are the challenges of writing tests with reactive code using JUnit, and how can we handle them?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Section 2: Leveraging Spring Cloud to Manage Microservices</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, you'll gain an understanding of how Spring Cloud can be used to manage the challenges faced when developing microservices (that is, building a distributed system).</p>
<p>This section includes the following chapters:</p>
<ul>
<li><a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml">Chapter 8</a>, <em>Introduction to Spring Cloud</em></li>
<li><a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml"><span>Chapter </span>9</a>, <em>Adding Service Discovery Using Netflix Eureka and Ribbon</em></li>
<li><a href="a3383211-405d-4319-b142-ddb8cf3674fd.xhtml"><span>Chapter </span>10</a>, <em>Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server </em></li>
<li><a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml"><span>Chapter </span>11</a>, <em>Securing Access to APIs</em></li>
<li><a href="a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml"><span>Chapter </span>12</a>, <em>Centralized Configuration</em></li>
<li><a href="23795d34-4068-4961-842d-989cde26b642.xhtml"><span>Chapter </span>13</a>, <em>Improving Resilience Using Resilience4j </em></li>
<li><a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml"><span>Chapter </span>14</a>, <em>Understanding Distributed Tracing</em></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to Spring Cloud</h1>
                </header>
            
            <article>
                
<p>So far, we have seen how we can use Spring Boot<span> to </span>build microservices with well-documented APIs, along with Spring WebFlux and SpringFox; persist data in MongoDB and SQL databases using Spring Data for MongoDB and JPA; build reactive microservices either as non-blocking APIs using Project Reactor or as event-driven asynchronous services using Spring Cloud Stream with RabbitMQ or Kafka, together with Docker; and manage and test a system landscape consisting of microservices, databases, and messaging systems. </p>
<p class="mce-root">Now, it's time to see how we can use <strong>Spring Cloud</strong> to make our services production-ready, scalable, robust, configurable, secure, and resilient.</p>
<p class="mce-root"><span>In this chapter, we will introduce you to how Spring Cloud can be used to implement the following design patterns from <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em>, the <em>Design patterns for microservices</em> section</span>:</p>
<ul>
<li>Service discovery</li>
<li>Edge server</li>
<li>Centralized configuration</li>
<li>Circuit breaker</li>
<li>Distributed tracing</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter does not contain any source code, and so no tools need to be installed.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The evolution of Spring Cloud</h1>
                </header>
            
            <article>
                
<p>In its initial 1.0 release in March 2015, Spring Cloud was mainly a wrapper around the Netflix OSS tools, which are as follows:</p>
<ul>
<li>Netflix Eureka, a discovery server</li>
<li><span>Netflix </span>Ribbon, a client-side load balancer</li>
<li><span>Netflix </span>Zuul, an edge server</li>
<li><span>Netflix </span>Hystrix, a circuit breaker</li>
</ul>
<p>The initial release of Spring Cloud also contained a configuration server and integration with Spring Security that provided OAuth 2.0 protected APIs. In May 2016, the Brixton release (V1.1) of Spring Cloud was made generally available. With the Brixton release, Spring Cloud got support for distributed tracing based on Spring Cloud Sleuth and Zipkin, which originated from Twitter. <span>These initial Spring Cloud components could be used to implement the preceding design patterns. </span>For more details, see <a href="https://spring.io/blog/2015/03/04/spring-cloud-1-0-0-available-now">https://spring.io/blog/2015/03/04/spring-cloud-1-0-0-available-now</a> and <a href="https://spring.io/blog/2016/05/11/spring-cloud-brixton-release-is-available">https://spring.io/blog/2016/05/11/spring-cloud-brixton-release-is-available</a>.&nbsp;</p>
<p><span>Since its inception, Spring Cloud has grown considerably over the years and has added support for the following, among others:</span></p>
<ul>
<li>Service discovery and centralized configuration based on HashiCorp Consul and Apache Zookeeper</li>
<li><span>Event-driven microservices using Spring Cloud Stream</span></li>
<li>Cloud providers such as Microsoft Azure, <span>Amazon Web Services, and Google Cloud Platform</span></li>
</ul>
<div class="packt_infobox">See <a href="https://spring.io/projects/spring-cloud">https://spring.io/projects/spring-cloud</a> for a complete list of tools.</div>
<p class="mce-root">Since the release of Spring Cloud Greenwich (V2.1) in January 2019, some of the Netflix tools mentioned previously have been placed in maintenance mode in Spring Cloud. The following replacements are recommended by the Spring Cloud project:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 41.2434%"><strong>Current component</strong></td>
<td style="width: 56.7566%"><strong>Replaced by</strong></td>
</tr>
<tr>
<td style="width: 41.2434%">Netflix Hystrix </td>
<td style="width: 56.7566%">Resilience4j</td>
</tr>
<tr>
<td style="width: 41.2434%"><span><span>Netflix Hystrix Dashboard/</span></span><span>Netflix Turbine</span></td>
<td style="width: 56.7566%"><span>Micrometer and monitoring system</span></td>
</tr>
<tr>
<td style="width: 41.2434%"><span>Netflix Ribbon</span></td>
<td style="width: 56.7566%"><span>Spring Cloud load balancer</span></td>
</tr>
<tr>
<td style="width: 41.2434%"><span>Netflix Zuul</span></td>
<td style="width: 56.7566%"><span>Spring Cloud Gateway</span></td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p><span>For more details, for example on what maintenance mode means</span>, see <a href="https://spring.io/blog/2019/01/23/spring-cloud-greenwich-release-is-now-available">https://spring.io/blog/2019/01/23/spring-cloud-greenwich-release-is-now-available</a>.</p>
<p class="mce-root">In this book, we will use the replacement alternatives to implement the design patterns mentioned previously. The following table maps each design pattern to the software components that will be used to implement them:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 22.5368%"><strong>Design pattern</strong></td>
<td style="width: 75.4632%"><strong>Software component</strong></td>
</tr>
<tr>
<td style="width: 22.5368%"><span>Service discovery</span></td>
<td style="width: 75.4632%">Netflix Eureka and <span>Spring Cloud load balancer</span></td>
</tr>
<tr>
<td style="width: 22.5368%"><span>Edge server</span></td>
<td style="width: 75.4632%"><span>Spring Cloud Gateway </span>and Spring Security OAuth</td>
</tr>
<tr>
<td style="width: 22.5368%"><span>Centralized configuration</span></td>
<td style="width: 75.4632%">Spring Cloud Configuration Server</td>
</tr>
<tr>
<td style="width: 22.5368%"><span>Circuit breaker</span></td>
<td style="width: 75.4632%"><span>Resilience4j</span></td>
</tr>
<tr>
<td style="width: 22.5368%"><span>Distributed tracing</span></td>
<td style="width: 75.4632%">Spring Cloud Sleuth and Zipkin</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p class="mce-root">Now, let's go through the design patterns and introduce the software components that will be used to implement them!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Netflix Eureka as a discovery service</h1>
                </header>
            
            <article>
                
<p>A discovery service is probably the most important support function required to make a landscape of cooperating microservices <span>production-ready</span>. As we already described in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em><span>, in the <em>Service discovery</em> section, a discovery service can be used to keep track of existing microservices and their instances. The first discovery service that Spring Cloud supported was <em>Netflix Eureka</em><strong>.</strong></span></p>
<p class="mce-root"></p>
<p><span>We will use this in <a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml">Chapter 9</a>, <em>Adding Service Discovery Using Netflix Eureka and Ribbon</em><em>,</em> along with a<em>&nbsp;</em>load balancer and the new Spring Cloud load balancer.</span></p>
<p>We will see how easy it is to register microservices with Netflix Eureka when using Spring Cloud, and as a client <span>sends HTTP requests such as a call to a RESTful API to one of the instances registered in Netflix E</span><span>ureka. We will also see how to scale up the number of instances of a microservice, and how requests to a microservice will be load-balanced over its available instances (based on, by default, round-robin scheduling).</span></p>
<p>The following screenshot demonstrates the web UI from Eureka, where we can see what microservices we have registered:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9d6bf02d-f466-40cf-9bf1-140cbf7d1766.png" width="1950" height="1231" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9d6bf02d-f466-40cf-9bf1-140cbf7d1766.png"></p>
<p><span>The review service has three instances available, while the other two services only have one instance each.</span></p>
<p>With Netflix Eureka introduced, let's introduce how to use Spring Cloud Gateway as an edge server.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Spring Cloud Gateway as an edge server</h1>
                </header>
            
            <article>
                
<p>Another very important support function is an edge server. As we already described in <span><a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em></span><span>, in the </span><span><em>Edge server</em>&nbsp;</span><span>section, it can be used to secure a microservice landscape, that is, hide private services from external usage and protect public services when they're used by external clients.</span></p>
<p><span>Initially, Spring Cloud used Netflix Zuul v1 as its edge server. Since the Spring Cloud Greenwich release, it's recommended to use <strong>Spring Cloud Gateway</strong> instead. Spring Cloud Gateway comes with similar support for critical features, such as URL path-based routing and the protection of endpoints via the use of OAuth 2.0 and <strong>OpenID Connect</strong> (<strong>OIDC</strong>).</span></p>
<p><span>One important difference between Netflix Zuul v1 and Spring Cloud Gateway is that Spring Cloud Gateway is based on non-blocking APIs that use Spring 5, Project Reactor, and Spring Boot 2, while Netflix Zuul v1 is based on blocking APIs. This means that Spring Cloud Gateway should be able to handle larger amounts of concurrent requests than Netflix Zuul v1, which is important for an edge server that all external traffic goes through.</span></p>
<p>The following diagram shows how all requests from external clients go through <span>Spring Cloud Gateway as an edge server. Based on URL paths, it routes requests to the intended microservice:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1d389b62-e8c6-4111-bce0-9406d0c5f0fc.png" width="1876" height="900" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1d389b62-e8c6-4111-bce0-9406d0c5f0fc.png"></p>
<p class="mce-root"></p>
<p>In the preceding diagram, we can see how the edge server will send external requests that have a URL path that starts with <kbd>/product-composite/</kbd> to the <strong>Product Composite</strong> microservice. The core services <strong>Product</strong>, <strong>Recommendation</strong>, and <strong>Review</strong> are not reachable from external clients.</p>
<p class="mce-root"><span>In <a href="a3383211-405d-4319-b142-ddb8cf3674fd.xhtml" target="_blank">Chapter 10</a>, <em>Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server</em>, we will look at how to set up Spring Cloud Gateway with our microservices.</span></p>
<p>In <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml">Chapter 11</a>, <em>Secure Access to APIs</em>, we will see how we can use <span>Spring Cloud Gateway together with Spring Security OAuth2 to protect access to the edge server using OAuth 2.0 and OIDC. We will also see how Spring Cloud Gateway can propagate identity information of the caller down to our microservices, for example, the username or email address of the caller.</span></p>
<p>With Spring Cloud Gateway introduced, let's introduce how to use<span> Spring Cloud Config for centralized configuration</span>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Spring Cloud Config for centralized configuration</h1>
                </header>
            
            <article>
                
<p>To manage the configuration of a system landscape of microservices, Spring Cloud contains Spring Cloud Config, which provides the centralized management of configuration files <span>according to the requirements described in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em>, in the</span><span>&nbsp;</span><span><em>Central configuration</em>&nbsp;</span><span>sect</span><span>ion.</span></p>
<p>Spring Cloud Config supports storing configuration files in a number of different backends, such as the following:</p>
<ul>
<li>A Git repository, for example, on GitHub or Bitbucket</li>
<li>Local filesystem</li>
<li>HashiCorp Vault</li>
<li>A JDBC database</li>
</ul>
<p>Spring Cloud Config allows us to handle configuration in a hierarchical structure; for example, we can place common parts of the configuration in a common file and microservice-specific settings in separate configuration files.</p>
<p class="mce-root"></p>
<p><span>Spring Cloud Config also supports detecting changes in the configuration and p</span>ushing notifications to the affected microservices. It uses <strong>Spring Cloud Bus</strong> to transport the notifications. <span>Spring Cloud Bus is an abstraction on top of Spring Cloud Stream that we already are familiar with; that is, it supports the use of either RabbitMQ or Kafka as the messaging system for transporting notifications out of the box.</span></p>
<p>The following diagram illustrates the cooperation between Spring Cloud Config, its clients, a Git repository, and Spring Cloud Bus:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a6dc8c02-132d-482d-bf64-2b2994b29eca.png" style="width:41.58em;height:21.92em;" width="1518" height="799" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a6dc8c02-132d-482d-bf64-2b2994b29eca.png"></p>
<p>The diagram shows the following:</p>
<ol>
<li>When the microservices starts up, they ask the configuration server for its configuration.</li>
<li>T<span>he configuration server gets the configuration from, in this case, a Git repository.</span></li>
<li>Optionally, the Git repository can be configured to send notifications to the <span>configuration server when Git commits are pushed to the Git repository.</span></li>
<li>The<span>&nbsp;</span><span>configuration server will publish change events using Spring Cloud Bus. The microservices that are affected by the change will react and retrieve its updated configuration from the configuration server.</span></li>
</ol>
<p class="mce-root"></p>
<p><span>Finally, Spring Cloud Config also supports the encryption of sensitive information in the configuration, such as credentials.</span></p>
<p>We will learn about Spring Cloud Config in <a href="a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml">Chapter 12</a>, <em>Centralized Configuration</em>.</p>
<p>With <span>Spring Cloud Config</span> introduced, let's get introduced to how to use <span>Resilience4j for improved resilience</span>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Resilience4j for improved resilience</h1>
                </header>
            
            <article>
                
<p>As we already mentioned in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml" target="_blank">Chapter 1</a>,<span>&nbsp;<em>Introduction to Microservices</em>, in </span><span>the<em>&nbsp;</em></span><span><em>Circuit breaker</em>&nbsp;</span><span>sect</span><span>ion, t</span>hings go wrong occasionally. In a fairly large-scaled system landscape of cooperating microservices, we must assume that there is something going wrong all of the time. Failures must be seen as a normal state, and the system landscape must be designed to handle it!</p>
<div class="packt_infobox">Initially, Spring Cloud came with Netflix Hystrix, a well-proven circuit breaker. But since the <span>Spring Cloud Greenwich release, it is recommended to replace Netflix Hystrix with Resilience4j. The reason for this is that Netflix recently put Hystrix into maintenance mode. For more details, see <a href="https://github.com/Netflix/Hystrix#hystrix-status">https://github.com/Netflix/Hystrix#hystrix-status</a>.</span></div>
<p><strong>Resilience4j</strong> is an open source-based<span> fault tolerance library</span>. You can discover more about it at <a href="http://resilience4j.github.io/resilience4j/">https://github.com/resilience4j/resilience4j</a>. It comes with the following fault tolerance mechanisms built-in:</p>
<ul>
<li><strong>Circuit breaker</strong> is used to prevent a chain of failure reactions if a remote service stops to respond.</li>
<li><strong>Rate limiter</strong> is used to limit the number of requests <span>to service </span>during a specified time period.</li>
<li><strong>Bulkhead</strong> is used to limit the number of concurrent requests to a service.</li>
<li><strong>Retries</strong> is used to handle random errors that might happen from time to time.</li>
<li><strong>Timeout</strong> is used to avoid waiting too long for a response from slow or not responding service.</li>
</ul>
<p><span>In <a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em>, we will focus on the</span> circuit breaker in Resilience4j.<strong>&nbsp;</strong>It follows the classic design of a circuit breaker, as illustrated in the following state diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/39bdb1ab-dd3a-4238-8fe8-9b112d5f2125.png" style="width:30.58em;height:16.58em;" width="1024" height="554" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/39bdb1ab-dd3a-4238-8fe8-9b112d5f2125.png"></p>
<p>Let's take a look at the state diagram in more detail:</p>
<ol>
<li>A&nbsp;<span>circuit breaker starts as</span> <strong>Closed</strong><span>, that is, allowing requests to be processed.</span></li>
<li>As long as the requests are processed successfully, it stays in the <strong>Closed</strong> state.</li>
<li>If failures start to happen, a counter starts to count up.</li>
<li><span>If a configured threshold of failures is reached</span>, the circuit breaker will <strong>trip</strong>, that is, go to the <strong>Open</strong> state, not allowing further requests to be processed.</li>
<li>Instead, a request will <strong>fast fail</strong><span>, that is,</span> return immediately with an exception.</li>
<li>After a configurable time, the circuit breaker will enter a <strong>Half Open</strong> state and allow one request to go through, such as a probe, to see whether the failure has been resolved.</li>
<li>If the probe request fails, the circuit breaker goes back to the <strong>Open</strong> state.</li>
<li>If the probe request succeeds, the <span>circuit breaker goes to the initial</span> <strong>Closed</strong> <span>state, that is, allowing new requests to be processed.</span></li>
</ol>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Sample usage of the circuit breaker in Resilience4j</h1>
                </header>
            
            <article>
                
<p>Let's assume we have a REST service that is protected by a circuit breaker via <span>Resilience4j called <kbd>myService</kbd>.</span></p>
<p>If the service starts to produce internal errors, for example, because it can't reach a service it depends on, we might get a response from the service such as <kbd>500 Internal Server Error</kbd>. After a number of configurable attempts, the circuit will open and we will get a fast failure that returns an error message such as <kbd>CircuitBreaker '<span>myService</span>' is open</kbd>. When the error is resolved and we make a new attempt (<span>after the configurable wait time)</span>, the circuit breaker will allow a new attempt as a probe. If the call succeeds, the circuit breaker will be closed again; that is, it is operating normally.</p>
<p>When using <span>Resilience4j together with Spring Boot, we will be able to monitor the state of the circuit breakers in a microservice using its Spring Boot Actuator <kbd>health</kbd> endpoint. We can, for example, use <kbd>curl</kbd> to see the state of the circuit breaker, that is, <kbd>myService</kbd>:</span></p>
<pre><strong>curl $HOST:$PORT/actuator/health -s | jq .details.<span>myService</span>CircuitBreaker</strong></pre>
<p>If it operates normally, that is, the circuit is <kbd>closed</kbd>, it will respond with something such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3fb19393-c8f6-46e4-a14b-f0850f75131e.png" style="width:21.00em;height:17.08em;" width="751" height="611" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3fb19393-c8f6-46e4-a14b-f0850f75131e.png"></p>
<p>If something is wrong and the circuit is <strong>open</strong>, it will respond with something such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0256fdb6-55bb-4ad8-9e24-532f801c9916.png" style="width:22.58em;height:17.58em;" width="781" height="607" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0256fdb6-55bb-4ad8-9e24-532f801c9916.png"></p>
<p>With Resilience4j <span>and specifically its circuit breaker</span> introduced, we have seen an example of how <span>the circuit breaker</span><span> can be used to handle errors for a REST client</span>. Let's get introduced to how to use <span>Spring Cloud Sleuth and Zipkin for distributed tracing.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Spring Cloud Sleuth and Zipkin for distributed tracing</h1>
                </header>
            
            <article>
                
<p><span>To understand what is going on in a distributed system such as a system landscape of cooperating microservices, it is crucial to be able to track and visualize how requests and messages flow between microservices when processing an external call to the system landscape.</span></p>
<div class="packt_infobox"><span>Refer to <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>,<em> Introduction to Microservices</em>, the </span><em>Distributed tracing</em><span><em>&nbsp;</em>section, for more information on this subject.</span><span>&nbsp;</span></div>
<p><span>Spring Cloud comes with </span><strong>Spring Cloud Sleuth</strong>, which can mark r<span>equests and messages/events that are part of the same processing flow with a common <em>correlation ID</em>.&nbsp;</span></p>
<p class="mce-root"></p>
<p><span>Spring Cloud Sleuth can also decorate log messages with correlation IDs to make it easier to track log messages from different microservices that come from the same processing flow. </span><span class="text-gray-dark mr-2"><strong>Zipkin</strong> is a distributed tracing system (</span><span><a href="http://zipkin.io/">http://zipkin.io</a>) that Spring Cloud Sleuth can send tracing data to for storage and visualization.</span></p>
<p>The infrastructure for handling distributed tracing information in Spring Cloud Sleuth and Zipkin<strong>&nbsp;</strong>is based on <span>Google Dapper (</span><span><a href="https://ai.google/research/pubs/pub36356">https://ai.google/research/pubs/pub36356</a>). In Dapper, the tracing information from a complete workflow is called a <strong>trace tree</strong>, and subparts of the tree, such as the basic units of work, are called <strong>spans</strong>. Spans can, in turn, consist of sub-spans, which form the trace tree. A correlation ID is called <kbd>TraceId</kbd>, and a span is identified by its own unique <kbd>SpanId</kbd>, along with <kbd>TraceId</kbd> of the trace tree it belongs to.</span></p>
<p>Spring Cloud Sleuth can send requests to Zipkin either synchronously over HTTP or asynchronously using either RabbitMQ or Kafka. To avoid creating runtime dependencies to the Zipkin server from our microservices, we prefer sending trace information to Zipkin asynchronously using either RabbitMQ or Kafka. This is illustrated by the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0b718675-03d1-4367-8114-0def71168052.png" width="1802" height="623" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0b718675-03d1-4367-8114-0def71168052.png"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>In <a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml">Chapter 14</a>, <em>Understanding Distributed Tracing</em>, we will see how we can use Spring Cloud Sleuth and Zipkin to trace the processing that goes on in our microservice landscape. The following is a screenshot from the Zipkin UI, which visualizes the trace tree that was created as a result of processing the creation of an aggregated product:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6b7b0b09-bac1-4528-ad88-ab30d6410fde.png" width="1120" height="569" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6b7b0b09-bac1-4528-ad88-ab30d6410fde.png"></p>
<p><span>An HTTP <kbd>POST</kbd> request is sent to the <span class="packt_screen">product-composite</span> service and responds by publishing create events to the topics for products, recommendations, and reviews. These events are consumed by the three core microservices </span><span>in parallel </span><span>and the data in the create events are stored in each microservice's database.</span></p>
<p><span>With Spring Cloud Sleuth and Zipkin for distributed tracing being introduced, we have seen an example of distributed tracing of the processing of an external synchronous HTTP request that includes asynchronous passing of events between the involved microservices.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the purpose of Netflix Eureka?</li>
<li>What are the main features of Spring Cloud Gateway?</li>
<li>What backends are supported by Spring Cloud Config?</li>
<li>What are the capabilities that Resilience4j provides?</li>
<li>What are the concepts of trace&nbsp;tree and span used for in distributed tracing, and what is the paper called that defined them?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the purpose of Netflix Eureka?</li>
<li>What are the main features of Spring Cloud Gateway?</li>
<li>What backends are supported by Spring Cloud Config?</li>
<li>What are the capabilities that Resilience4j provides?</li>
<li>What are the concepts of trace tree and span used for in distributed tracing, and what is the paper called that defined them?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding Service Discovery Using Netflix Eureka and Ribbon</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this chapter, we will learn how to use Netflix Eureka as a discovery server for microservices based on Spring Boot. To allow our </span><span>microservices to communicate with Netflix Eureka, we will use the Spring Cloud module for Netflix Eureka clients. Before we delve into the details, we will elaborate on why a discovery server is needed and why a DNS server isn't sufficient.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introduction to service discovery
<ul>
<li>The problem with DNS-based service discovery</li>
<li>Challenges with service discovery</li>
<li>Service discovery with Netflix Eureka in Spring Cloud</li>
</ul>
</li>
<li>Setting up a Netflix Eureka server</li>
<li>Connecting <span>microservices to a </span><span>Netflix Eureka server</span></li>
<li>Setting up configuration for use in the development process</li>
<li>Trying out the discovery service</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All of the commands that are described in this book<span> have been run on a MacBook Pro using macOS Mojave, but it should be straightforward to modify it so that it can run on another platform, such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p class="mce-root"></p>
<p>The source code for this chapter can be found <span>on GitHub</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter09">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter09</a>.</p>
<p>To be able to run the commands that are described in this book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter09</span></strong></pre>
<p>The Java source code is written for Java 8 and has been tested on Java 12. This chapter uses Spring Cloud 2.1.0 (also known as the <span><strong>Greenwich</strong> release</span>), Spring Boot 2.1.3, and Spring 5.1.5âthat is, the latest available version of the Spring components at the time of writing this chapter.</p>
<p>The source code contains the following Gradle projects:</p>
<ul>
<li><kbd>api</kbd></li>
<li><kbd>util</kbd></li>
<li><kbd>microservices/product-service</kbd></li>
<li><kbd>microservices/review-service</kbd></li>
<li><kbd>microservices/recommendation-service</kbd></li>
<li><kbd>microservices/product-composite-service</kbd></li>
<li><kbd>spring-cloud/eureka-server</kbd></li>
</ul>
<p>The code examples in this chapter all come from the source code in the <span><kbd>$BOOK_HOME/Chapter09</kbd> directory but have been edited in several places in order to remove irrelevant parts of the source code, such as comments and import and log statements.</span></p>
<p>If you want to look at the changes that were applied to the source code in <a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml"></a><a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml" target="_blank">Chapter 9</a>,&nbsp;<em>Adding Service Discovery Using Netflix Eureka and Ribbon</em>, to see <span>what it took to add Netflix Eureka as a discovery service to the microservices landscape, </span>you can compare it with the source code for <a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml">Chapter 7</a>, <em>Developing Reactive Microservices</em>.&nbsp;<span>You can use your favorite</span> <kbd>diff</kbd> <span>tool and compare the two folders,</span> <kbd>$BOOK_HOME/Chapter07</kbd><span> and </span><kbd>$BOOK_HOME/Chapter09</kbd><span>, respectively.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing service discovery</h1>
                </header>
            
            <article>
                
<p><span>The concept of service discovery was described in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em>; please refer to the <em>Service discovery</em></span> section<span> for more information. Netflix Eureka was introduced as a discovery service in <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml">Chapter 8</a>,&nbsp;<em>Introduction to Spring Cloud</em>; please refer to the <em>Netflix Eureka as a discovery service</em> section for more information. Before we jump into the implementation details, we will look at the following topics</span>:</p>
<ul>
<li>The problem with DNS-based service discovery</li>
<li>Challenges with service discovery</li>
<li>Service discovery with Netflix Eureka in Spring Cloud</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">The problem with DNS-based service discovery</h1>
                </header>
            
            <article>
                
<p class="p1">So, what's the problem?</p>
<p>Why can't we simply start new instances of a microservice and rely on round-robin DNS? In essence, given that the <span>microservice instances have the same DNS name, the DNS server will resolve it to a list of IP addresses for the available instances. Due to this, the client can call the service instances in a round-robin fashion.</span></p>
<p>Let's try it out and see what happens, shall we? Follow these steps:</p>
<ol>
<li>Assuming that you have followed the instructions from <a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml">Chapter 7</a><span>, <em>Developing Reactive Microservices</em></span>, start the system landscape and insert some test data with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd $BOOK_HOME/chapter07</strong><br><strong>./test-em-all.bash start</strong></pre>
<ol start="2">
<li>Scale up the <kbd>review</kbd> microservice to two instances:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose up -d --scale review=2</strong></pre>
<ol start="3">
<li>Ask the composite product service for the IP addresses it finds for the <kbd>review</kbd> microservice:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose exec product-composite getent hosts review</strong></pre>
<ol start="4">
<li>Expect an answer like the following:</li>
</ol>
<pre style="padding-left: 60px">172.19.0.9 review<br>172.19.0.8 review</pre>
<p style="padding-left: 60px">Great, the <span>composite product service sees two IP addressesâin my case, </span><kbd>172.19.0.8</kbd> and <span><kbd>172.19.0.9</kbd></span><span>âone for each instance of the <kbd>review</kbd> microservice instances!</span></p>
<ol start="5">
<li>If you want to, you can verify that these are the correct IP addresses by using the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose exec --index=1 review cat /etc/hosts</strong><br><strong>docker-compose exec --index=2 review cat /etc/hosts</strong></pre>
<p style="padding-left: 60px">The last line in the output from each command should contain one of the IP addresses, as shown in the preceding code.</p>
<ol start="6">
<li>Now, let's try out a couple of calls to the <span>composite product service and see whether it uses both instances of the <kbd>review</kbd> microservice:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>curl localhost:8080/product-composite/2 -s | jq -r .serviceAddresses.rev</strong></pre>
<p style="padding-left: 60px">Unfortunately, we will only get responses from one of the microservice instances, as in this example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/70f0682d-4f5b-4fae-a1a0-d96ea3c3c254.png" style="width:14.75em;height:3.58em;" width="580" height="140" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/70f0682d-4f5b-4fae-a1a0-d96ea3c3c254.png"></p>
<p>That was disappointing! </p>
<p>Okay, so what is going on here?</p>
<p>A DNS client typically caches the resolved IP addresses and hangs on to the first working IP address it tries out when it receives a list of IP addresses that have been resolved for a DNS name. Neither the DNS servers nor the DNS protocol is well-suited for handling volatile microservices instances that come and go all of the time. Because of this, DNS-based service discovery isn't very<span> appealing from a practical standpoint.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Challenges with service discovery</h1>
                </header>
            
            <article>
                
<p>So, we need something a bit more powerful than a plain DNS to keep track of available microservice instances!</p>
<p>We must take the following into consideration when we're keeping track of many small moving parts, that is, microservice instances:</p>
<ul>
<li>New instances can start up at any point in time.</li>
<li>Existing instances can stop responding and eventually crash at any point in time.</li>
<li>Some of the failing instances might be okay after a while and should start to receive traffic again, while others will not and should be removed from the service registry.</li>
<li>Some microservice instances might take some time to start up; that is, just because they can receive HTTP requests doesn't mean that traffic should be routed to them.</li>
<li>Unintended network partitioning and other network-related errors can occur at any time.</li>
</ul>
<p>Building a robust and resilient discovery server is not an easy task, to say the least. Let's see how we can use Netflix Eureka to handle these challenges!</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up a Netflix Eureka server</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to set up a&nbsp;Netflix Eureka server for service discovery.&nbsp;Setting up a Netflix Eureka server using Spring Cloud is really easyâjust follow these steps:</p>
<ol>
<li>Create a Spring Boot project using Spring Initializr, as described in <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml">Chapter 3</a>, <em>Creating a Set of Cooperating Microservices</em>, in the<em>&nbsp;</em><span><em>Using Spring Initializr to generate skeleton code</em> section.</span></li>
<li>Add a dependency to <kbd>spring-cloud-starter-netflix-eureka-server</kbd>.</li>
<li>Add the&nbsp;<kbd>@EnableEurekaServer</kbd>&nbsp;annotation to the application class.</li>
<li>Add a Dockerfile, similar to the Dockerfiles that are used for our microservices, with the exception that we export the Eureka default port, <kbd>8761</kbd>, instead of the default port for our microservices, <kbd>8080</kbd>.</li>
<li>&nbsp;<span>Add the Eureka server to our three Docker Compose files, that is,&nbsp;<kbd>docker-compose.yml</kbd>, <kbd>docker-compose-partitions.yml</kbd>, and <kbd>docker-compose-kafka.yml</kbd>:</span></li>
</ol>
<pre style="padding-left: 60px"><span>eureka</span>:<br>  <span>build</span>: spring-cloud/eureka-server<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"8761:8761"</span></pre>
<ol start="6">
<li>Finally, add some configuration. Please go to the&nbsp;<em>Setting up configuration for use in the development process</em>&nbsp;section in this chapter, where we will go through the configuration for both the Eureka server and our microservices.</li>
</ol>
<p>That's all it takes!</p>
<p>You can find the source code for the Eureka server in the <span><kbd>$BOOK_HOME/Chapter09/spring-cloud/eureka-server</kbd> folder.</span></p>
<p>Knowing how to set up a&nbsp;<span>Netflix Eureka server for service discovery, we are ready to learn how to connect microservices to a Netflix Eureka server.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up a Netflix Eureka server</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to set up a Netflix Eureka server for service discovery. Setting up a Netflix Eureka server using Spring Cloud is really easyâjust follow these steps:</p>
<ol>
<li>Create a Spring Boot project using Spring Initializr, as described in <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml">Chapter 3</a>, <em>Creating a Set of Cooperating Microservices</em>, in the<em>&nbsp;</em><span><em>Using Spring Initializr to generate skeleton code</em> section.</span></li>
<li>Add a dependency to <kbd>spring-cloud-starter-netflix-eureka-server</kbd>.</li>
<li>Add the <kbd>@EnableEurekaServer</kbd> annotation to the application class.</li>
<li>Add a Dockerfile, similar to the Dockerfiles that are used for our microservices, with the exception that we export the Eureka default port, <kbd>8761</kbd>, instead of the default port for our microservices, <kbd>8080</kbd>.</li>
<li>&nbsp;<span>Add the Eureka server to our three Docker Compose files, that is, <kbd>docker-compose.yml</kbd>, <kbd>docker-compose-partitions.yml</kbd>, and <kbd>docker-compose-kafka.yml</kbd>:</span></li>
</ol>
<pre style="padding-left: 60px"><span>eureka</span>:<br>  <span>build</span>: spring-cloud/eureka-server<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"8761:8761"</span></pre>
<ol start="6">
<li>Finally, add some configuration. Please go to the <em>Setting up configuration for use in the development process</em> section in this chapter, where we will go through the configuration for both the Eureka server and our microservices.</li>
</ol>
<p>That's all it takes!</p>
<p>You can find the source code for the Eureka server in the <span><kbd>$BOOK_HOME/Chapter09/spring-cloud/eureka-server</kbd> folder.</span></p>
<p>Knowing how to set up a <span>Netflix Eureka server for service discovery, we are ready to learn how to connect microservices to a Netflix Eureka server.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up configuration for use in the development process</h1>
                </header>
            
            <article>
                
<p>Now, it's time for the trickiest part of setting up Netflix Eureka as a discovery service, that is, setting up a working configuration for both the Eureka server and its clients: our microservice instances.</p>
<p class="mce-root"></p>
<p>Netflix Eureka is a highly configurable discovery server that can be set up for a number of different use cases, and it provides robust, resilient, and fault-tolerant runtime characteristics. One downside of this flexibility and robustness is that it has an almost overwhelming number of configuration options. Fortunately,&nbsp;<span>Netflix Eureka comes with good default values for most of the configurable parametersâat least when it comes to using them in a production environment.</span></p>
<p>When it comes to using&nbsp;<span>Netflix Eureka during development, the default values cause long startup times. For example, it can take a long time for a client to make an initial successful call to a microservices instance that is registered in the Eureka server.</span></p>
<p><span>Up to two minutes of wait time can be experienced when using the default configuration values.</span>&nbsp;This wait time is added to the time it takes for the Eureka service and the microservices to start up. The reason for this wait time is that the involved processes need to synchronize registration information with each other.</p>
<p>The&nbsp;<span>microservices instances need to register with the Eureka server, and the client needs to gather information from the Eureka server. This communication is mainly based on heartbeats, which happen every 30 seconds by default. A couple of caches are also involved, which slows down the propagation of updates.</span></p>
<p>We will use a configuration that minimizes this wait time, which is useful during development. For use in production environments, the default values should be used as a starting point!</p>
<div class="packt_infobox">We will only use one Netflix Eureka server instance, which is okay in a development environment. In a production environment, you should always use two or more instances<span>&nbsp;</span>to ensure high availability for the&nbsp;<span>Netflix Eureka server.</span></div>
<p>Let's start to learn what types of configuration parameters we need to know about.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Eureka configuration parameters</h1>
                </header>
            
            <article>
                
<p>The configuration parameters for Eureka are divided into three groups:</p>
<ul>
<li><span>There are parameters for the Eureka&nbsp;server, prefixed with&nbsp;<kbd>eureka.server</kbd>.</span></li>
<li>There are parameters for Eureka clients,&nbsp;<span>prefixed with&nbsp;</span><kbd>eureka.client</kbd><span>.&nbsp;</span>This is for clients who want to communicate with a Eureka server<span><span>.</span></span></li>
<li><span><span>There are parameters for Eureka&nbsp;instances, prefixed with&nbsp;<kbd>eureka.instance</kbd>. This is for the microservices instances that want to register themselves in the Eureka server.</span></span></li>
</ul>
<div class="packt_infobox">Some of the available parameters are described in the Spring Cloud documentation: <em>Service Discovery: Eureka Server</em>: <a href="https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#spring-cloud-eureka-server">https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#spring-cloud-eureka-server<br></a><em>Service Discovery: Eureka Clients</em>:&nbsp;<a href="https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#_service_discovery_eureka_clients">https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#_service_discovery_eureka_clients</a></div>
<p>For an extensive list of available parameters, I recommend reading the source code:</p>
<ul>
<li>For Eureka server parameters, look at the <kbd>org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean</kbd><span>&nbsp;class for default values and the</span> <kbd>com.netflix.eureka.EurekaServerConfig</kbd> interface for the relevant documentation.</li>
<li>For Eureka client parameters, look at the <kbd><span>org.springframework.cloud.netflix.eureka.EurekaClientConfigBean</span></kbd> <span>class for the default values and</span>documentation.</li>
<li>For Eureka instance parameters, look at the <kbd>org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean</kbd> <span>class for default values and</span> documentation.</li>
</ul>
<p>Let's start to learn about configuration parameters for the Eureka server.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Eureka configuration parameters</h1>
                </header>
            
            <article>
                
<p>The configuration parameters for Eureka are divided into three groups:</p>
<ul>
<li><span>There are parameters for the Eureka server, prefixed with <kbd>eureka.server</kbd>.</span></li>
<li>There are parameters for Eureka clients, <span>prefixed with </span><kbd>eureka.client</kbd><span>.&nbsp;</span>This is for clients who want to communicate with a Eureka server<span><span>.</span></span></li>
<li><span><span>There are parameters for Eureka instances, prefixed with <kbd>eureka.instance</kbd>. This is for the microservices instances that want to register themselves in the Eureka server.</span></span></li>
</ul>
<div class="packt_infobox">Some of the available parameters are described in the Spring Cloud documentation: <em>Service Discovery: Eureka Server</em>: <a href="https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#spring-cloud-eureka-server">https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#spring-cloud-eureka-server<br></a><em>Service Discovery: Eureka Clients</em>:&nbsp;<a href="https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#_service_discovery_eureka_clients">https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#_service_discovery_eureka_clients</a></div>
<p>For an extensive list of available parameters, I recommend reading the source code:</p>
<ul>
<li>For Eureka server parameters, look at the <kbd>org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean</kbd><span> class for default values and the</span> <kbd>com.netflix.eureka.EurekaServerConfig</kbd> interface for the relevant documentation.</li>
<li>For Eureka client parameters, look at the <kbd><span>org.springframework.cloud.netflix.eureka.EurekaClientConfigBean</span></kbd> <span>class for the default values and</span>documentation.</li>
<li>For Eureka instance parameters, look at the <kbd>org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean</kbd> <span>class for default values and</span> documentation.</li>
</ul>
<p>Let's start to learn about configuration parameters for the Eureka server.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Configuring the Eureka server</h1>
                </header>
            
            <article>
                
<p>To configure the Eureka server <span>for use in a development environment, the following configuration can be used:</span></p>
<pre><span>server</span>:<br>  <span>port</span>: <span>8761<br></span><span><br></span><span>eureka</span>:<br>  <span>instance</span>:<br>    <span>hostname</span>: localhost<br>  <span>client</span>:<br>    <span>registerWithEureka</span>: <span>false<br></span><span>    fetchRegistry</span>: <span>false<br></span><span>    serviceUrl</span>:<br>      <span>defaultZone</span>: http://${<span>eureka.instance.hostname</span>}:${<span>server.port</span>}/eureka/<br> <span><br></span><span>  </span><span>server</span>:<br>    <span>waitTimeInMsWhenSyncEmpty</span>: <span>0<br></span><span>    </span><span>response-cache-update-interval-ms</span>: <span>5000</span></pre>
<p>The first part of the configuration, for a Eureka <kbd>instance</kbd> and <kbd>client</kbd> is a standard configuration of a standalone Eureka server. For details, see the Spring Cloud documentation that we referred to previously. The last two parameters used for the Eureka <kbd>server</kbd>,&nbsp;<kbd>waitTimeInMsWhenSyncEmpty</kbd> and <kbd>response-cache-update-interval-ms</kbd>, are used to minimize the startup time.</p>
<p>With the Eureka server configured, we are ready to see how clients to the Eureka server, that is, the microservice instances, can be configured.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Configuring clients to the Eureka server</h1>
                </header>
            
            <article>
                
<p>To be able to connect to the Eureka server, the microservices have the following configuration:</p>
<pre><span>eureka</span>:<br>  <span>client</span>:<br>    <span>s</span><span>erviceUrl</span>:<br> <span>defaultZone</span>: http://localhost:8761/eureka/<br> <span>initialInstanceInfoReplicationIntervalSeconds</span>: <span>5<br></span><span> </span><span>registryFetchIntervalSeconds</span>: <span>5<br></span><span> </span><span>instance</span>:<br> <span>leaseRenewalIntervalInSeconds</span>: <span>5<br></span><span> </span><span>leaseExpirationDurationInSeconds</span>: <span>5<br></span><span><br></span>---<br><span>spring.profiles</span>: docker<br><br><span>eureka.client.serviceUrl.defaultZone</span>: http://eureka:8761/eureka/</pre>
<p>The <kbd>eureka.client.serviceUrl.defaultZone</kbd> parameter is used to find the Eureka server, whereas the other parameters <span>are used to minimize the startup time and the time it takes to deregister a microservice instance that is stopped.</span></p>
<p>The <kbd>product-composite</kbd> microservice, which uses the Eureka server to look up the other microservices, also has two Netflix Ribbon-specific parameters:</p>
<pre><span>ribbon.ServerListRefreshInterval</span>: 5000<br><span>ribbon.NFLoadBalancerPingInterval</span>: 5</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>These two parameters are also used to <span>minimize startup time.</span></p>
<p>Now, we have everything in place required to actually try out discovery service using the Netflix Eureka server together with our microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the discovery service</h1>
                </header>
            
            <article>
                
<p>With all of the details in place, we are ready to try out the service:</p>
<ol>
<li>First, build the Docker images with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><span><strong>cd $BOOK_HOME/</strong></span><strong><span>Chapter09<br></span><span>./gradlew build &amp;&amp; </span><span>docker-compose build</span></strong></pre>
<ol start="2">
<li>Next, start the system landscape and run the usual tests with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>./t</strong><strong>est-em-all.bash start</strong></pre>
<p style="padding-left: 60px">Expect output similar to what we have seen in previous chapters:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/841fe8fc-968b-40e7-85fa-60fa73153679.png" width="1924" height="482" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/841fe8fc-968b-40e7-85fa-60fa73153679.png"></p>
<p>With <span>the system landscape up and running, we can start with testing how to scale up the number of instances for one of the microservices.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Scaling up</h1>
                </header>
            
            <article>
                
<p>Now, we can try out the discovery service by launching two extra <kbd>review</kbd> microservice instances:</p>
<pre><strong><span>docker-compose up -d --scale review=3</span></strong></pre>
<div class="packt_infobox">With the preceding command, we ask Docker Compose to run three instances of the <kbd>review</kbd> service. Since one instance is already running, two new instances will be started up.</div>
<p>Once the new instances are up and running, browse to <kbd>http://localhost:8761/</kbd> and expect something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b53f6c5c-f964-4731-8907-8482ddd11cca.png" style="width:45.67em;height:28.67em;" width="2217" height="1392" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b53f6c5c-f964-4731-8907-8482ddd11cca.png"></p>
<p><span>After running this localhost, verify that you can see three <kbd>review</kbd> instances in the Netflix Eureka web UI, as shown in the preceding screenshot.</span></p>
<p class="mce-root">One way of knowing when the new instances are up and running is to run the <kbd>docker-compose logs -f review</kbd> command and look for output that looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/249647b6-1f89-4d9a-800c-d84d740893f6.png" width="1820" height="151" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/249647b6-1f89-4d9a-800c-d84d740893f6.png"></p>
<p>We can also use a REST API that the Eureka service exposes. To get a list of instance IDs, we can issue a <kbd>curl</kbd> command, like so:</p>
<pre><strong>curl -H "accept:application/json" localhost:8761/eureka/apps -s | jq -r .applications.application[].instance[].instanceId</strong></pre>
<p>Expect a response that looks similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/66ef4c5e-20f9-4056-8bdb-e4398a194d4e.png" style="width:20.67em;height:10.58em;" width="727" height="373" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/66ef4c5e-20f9-4056-8bdb-e4398a194d4e.png"></p>
<p>Now that we have all of the instances up and running, let's try out the client-side load balancer by making some requests and focusing on the address of the <kbd>review</kbd> service in the responses, as follows:</p>
<pre><strong>curl localhost:8080/product-composite/2 -s | jq -r .serviceAddresses.rev</strong></pre>
<p>Expect responses similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ccd79bc6-a3e1-43aa-891f-553c6f727741.png" style="width:17.92em;height:6.67em;" width="648" height="240" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ccd79bc6-a3e1-43aa-891f-553c6f727741.png"></p>
<p>Note that the address of the <kbd>review</kbd> service changes in each response; that is, the load balancer uses a round-robin to call the available <kbd>review</kbd> instances, one at a time!</p>
<p>We can also take a look into the <kbd>review</kbd> instances log with the following command:</p>
<pre><strong>docker-compose logs -f review</strong></pre>
<p>After this, you will see output that looks similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cadd482e-aa89-4949-972e-d8fb4f3f1145.png" style="width:40.42em;height:5.00em;" width="1532" height="189" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cadd482e-aa89-4949-972e-d8fb4f3f1145.png"></p>
<p class="mce-root"></p>
<p>In the preceding output, we can see how the three review microservice instances, <span><kbd>review_1</kbd>,&nbsp;</span><span><kbd>review_2</kbd>, and </span><span><kbd>review_3</kbd>, in turn, have responded to the requests. </span></p>
<p>After trying out scaling up microservice instances we will try out what happens when we scale down the instances.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Disruptive tests with the Eureka server</h1>
                </header>
            
            <article>
                
<p>Let's bring some disorder to our Eureka server and see how the system landscape manages it!</p>
<p>To start with, what happens if we crash the Eureka server?</p>
<p>As long as clients have read the information regarding available microservice instances from the Eureka server before it is stopped, the clients will be fine since they cache the information locally. However, new instances will not be made available to clients, and they will not be notified if any running instances are terminated. So, calls to instances that are no longer running will cause failures.&nbsp;</p>
<p>Let's try this out!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Disruptive tests with the Eureka server</h1>
                </header>
            
            <article>
                
<p>Let's bring some disorder to our Eureka server and see how the system landscape manages it!</p>
<p>To start with, what happens if we crash the Eureka server?</p>
<p>As long as clients have read the information regarding available microservice instances from the Eureka server before it is stopped, the clients will be fine since they cache the information locally. However, new instances will not be made available to clients, and they will not be notified if any running instances are terminated. So, calls to instances that are no longer running will cause failures. </p>
<p>Let's try this out!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Stopping the Eureka server</h1>
                </header>
            
            <article>
                
<p>To simulate that the Eureka server crashes, follow these steps:</p>
<ol>
<li>First, stop the Eureka server and keep the two <kbd>review</kbd> instances up and running:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose up -d --scale review=2 --scale eureka=0</strong></pre>
<ol start="2">
<li>Try a couple of calls to the API and extract the service address of the <kbd>review</kbd> service:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl localhost:8080/product-composite/2 -s | jq -r .serviceAddresses.rev</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<ol start="3">
<li>The response willâ<span>just </span>like before we stopped the Eureka serverâcontain the addresses of the two <kbd>review</kbd> instances, like so:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/91291ded-566c-4cea-b151-aeea0e4c6390.png" style="width:20.42em;height:4.58em;" width="641" height="143" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/91291ded-566c-4cea-b151-aeea0e4c6390.png"></p>
<p>This shows that the client can make calls to existing instances, even though the Eureka server is no longer running!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Stopping a review instance </h1>
                </header>
            
            <article>
                
<p>To further investigate what the effects are of a crashed Eureka server, let's simulate that one of the remaining <kbd>review</kbd> microservice instances also crashes. Terminate one of the two<span>&nbsp;<kbd>review</kbd> instances with the following command:</span></p>
<pre><strong>docker-compose up -d --scale review=1 --scale eureka=0</strong></pre>
<p>The client, that is, the <kbd>product-composite</kbd> service, will not be notified that one of the <kbd>review</kbd> instances has disappeared since no Eureka server is running. Due to this, it still thinks that there are two instances up and running. Every second call to the client will cause it to call a <kbd>review</kbd> instance that no longer exists, resulting in the response from the client not containing any information from the <kbd>review</kbd> service. The service address of the <kbd>review</kbd> service will be empty.</p>
<p>Try out the preceding <kbd>curl</kbd> command to verify that the <span>service address of the <kbd>review</kbd> service will be empty every second time. This can be prevented, as described previously, by using resilience mechanisms such as timeouts and retries. </span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Starting up an extra instance of the product service</h1>
                </header>
            
            <article>
                
<p>As a final test of the effects of a crashed <span>Eureka server, let's see what happens if we start up a new instance of the <kbd>product</kbd> microservice. </span>Perform the following steps:</p>
<ol>
<li>Let's try starting a new instance of the <kbd>product</kbd> service:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose up -d --scale review=1 --scale eureka=0 --scale product=2</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<ol start="2">
<li>Call the API a couple of times and extract the address of the <kbd>product</kbd> service with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl localhost:8080/product-composite/2 -s | jq -r .serviceAddresses.pro</strong></pre>
<ol start="3">
<li>Since no Eureka server is running, the client will not be notified of the new <kbd>product</kbd> instance, and so all calls will go to the first instance, as in the following example:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a783eca8-e0b7-48be-947d-c972032e4c9b.png" style="width:24.25em;height:4.42em;" width="769" height="141" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a783eca8-e0b7-48be-947d-c972032e4c9b.png"></p>
<p>Now we have seen some of the most important aspects of not having a Netflix Eureka server up and running. Let's conclude the section on disruptive tests by starting up the <span>Netflix Eureka server again and seeing how the system landscape handles self-heals, that is, resilience.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Starting up the Eureka server again</h1>
                </header>
            
            <article>
                
<p>In this section, we will wrap up the <span>disruptive tests</span> by starting up the Eureka server again. We shall also verify that the <span>system landscape self-heals, that is,</span> verify that the new instance of the <kbd>product</kbd> microservice gets registered with the Netflix Eureka server and that the client gets updated by the Eureka server. Perform the following steps:</p>
<ol>
<li>Start the Eureka server with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose up -d --scale review=1 --scale eureka=1 --scale product=2</strong></pre>
<p style="padding-left: 60px"><span>Make some new calls to the API and verify that the following happens:</span></p>
<ul>
<li style="padding-left: 30px">All calls go to the remaining <kbd>review</kbd> instance, that is, the client has detected that the second <kbd>review</kbd> instance has gone.</li>
<li style="padding-left: 30px">Calls to the <kbd>product</kbd> service are load-balanced over the two <kbd>product</kbd> instances, that is, <span><span>the client has detected that there are two <kbd>product</kbd> instances available.</span></span></li>
</ul>
<ol start="2">
<li>Make the following call a couple of times to extract the addresses of the product and the <kbd>review</kbd> service:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl localhost:8080/product-composite/2 -s | jq -r .serviceAddresses</strong></pre>
<ol start="3">
<li><span>Verify that the responses from the API calls contain addresses to the involved <kbd>product</kbd> and <kbd>review</kbd> instances, like so:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4c8c141a-2404-4b8c-bf33-b7a0b59c5476.png" style="width:24.08em;height:4.17em;" width="843" height="145" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4c8c141a-2404-4b8c-bf33-b7a0b59c5476.png"></p>
<p style="padding-left: 60px">This is the second response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/03c25c97-63a7-49d6-82b1-72ea5a69ac79.png" style="width:23.08em;height:4.00em;" width="834" height="144" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/03c25c97-63a7-49d6-82b1-72ea5a69ac79.png"></p>
<p style="padding-left: 60px">The <kbd>192.168.128.3</kbd> and <kbd>192.168.128.7</kbd> IP addresses belong to the two <kbd>product</kbd> instances. <kbd>192.168.128.9</kbd> is the IP address of the <kbd>review</kbd> instance.</p>
<p style="padding-left: 60px">To summarize, the Eureka server provides a very robust and resilient implementation of a discovery service. If even higher availability is desired, multiple Eureka servers can be launched and configured to communicate with each other. Details on how to set up multiple <span>Eureka servers can be found in the Spring Cloud documentation: <a href="https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#spring-cloud-eureka-server-peer-awareness">https://cloud.spring.io/spring-cloud-static/Greenwich.RELEASE/single/spring-cloud.html#spring-cloud-eureka-server-peer-awareness</a>.</span></p>
<ol start="4">
<li>Finally, shut down the system landscape with the command:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose down</strong></pre>
<p>This completes the tests of the discovery server, Netflix Eureka, where we have learned both how to scale up and scale down microservice instances and learned what happens if a Netflix Eureka server crashes and later on comes back online.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to use Netflix Eureka for service discovery. First, we looked into the shortcomings of a simple DNS-based <span>service discovery solution and the challenges that a robust and resilient service discovery solution must be able to handle.</span></p>
<p>Netflix Eureka is a very capable <span>service discovery solution that provides robust, resilient, and fault-tolerant runtime characteristics. However, it can be challenging to configure correctly, especially for smooth developer experience. With Spring Cloud, it becomes easy to set up a Netflix Eureka server and adapt Spring Boot-based microservices, both so that they can register themselves to Eureka during startup and, when acting as a client to other microservices, to keep track of available microservices instances.</span></p>
<p><span>With a discovery service in place, it's time to see how we can handle external traffic using Spring Cloud Gateway as an edge server. Head over to the next chapter to find out how!</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is required to turn a Spring Boot application created with Spring Initializr into a fully-fledged Netflix Eureka Server?</li>
<li>What is required to make a Spring Boot-based microservice register itself automatically as a startup with <span>Netflix Eureka?</span></li>
<li>What is required to make it possible for a <span>Spring Boot-based microservice to call another microservice that is registered in a Netflix Eureka server?</span></li>
<li>Let's assume that you have a Netflix Eureka server up and running, along with one instance of microservice <em>A</em> and two instances of microservice <em>B</em>. All microservice instances register themselves with the Netflix Eureka server. Microservice <em>A</em> makes HTTP requests to m<span>icroservice <em>B</em> based on the information it gets from the Eureka server. What will happen if, in turn, the following happens:</span>
<ul>
<li>The Netflix Eureka server crashes</li>
<li>One of the instances of <span>m</span><span>icroservice <em>B</em> crashes</span></li>
<li>A new instance of m<span>icroservice <em>A</em> starts up</span></li>
<li>A new instance of m<span>icroservice <em>B</em> starts up</span></li>
<li>The Netflix Eureka server starts up again</li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All commands described in this book<span>&nbsp;are run on a MacBook Pro using macOS Mojave, but should be straightforward to modify in order to run on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p>The source code for this chapter&nbsp;can be found&nbsp;<span>on GitHub</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter10">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter10</a>.</p>
<p>To be able to run the commands as described in this book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. The following are sample commands:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter10</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1.0, SR1 (also known as the <span><strong>Greenwich</strong>&nbsp;release</span>), Spring Boot 2.1.3, and Spring 5.1.5, which is the latest available version of the Spring components at the time of writing this chapter.</p>
<p>The source code contains the following Gradle projects:</p>
<ul>
<li><kbd>api</kbd></li>
<li><kbd>util</kbd></li>
<li><kbd>microservices/product-service</kbd></li>
<li><kbd>microservices/review-service</kbd></li>
<li><kbd>microservices/recommendation-service</kbd></li>
<li><kbd>microservices/product-composite-service</kbd></li>
<li><kbd>spring-cloud/eureka-server</kbd></li>
<li><kbd>spring-cloud/gateway</kbd></li>
</ul>
<p>The code examples in this chapter all come from source code in <span><kbd>$BOOK_HOME/Chapter10</kbd>&nbsp;but are, in several cases, edited to remove non-relevant parts of the source code, such as comments, and import and log statements.</span></p>
<p>If you want to see the changes applied to the source code in <a href="a3383211-405d-4319-b142-ddb8cf3674fd.xhtml" target="_blank">Chapter 10</a>,&nbsp;<em>Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server</em>, that is, see&nbsp;<span>what it took to add Spring Cloud Gateway as an edge server to the microservices landscape,&nbsp;</span>you can compare it with the source code for <a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml" target="_blank">Chapter 9</a>, <em>Adding Service Discovery Using Netflix Eureka and Ribbon</em>.<span>&nbsp;You can use your favorite&nbsp;</span><kbd>diff</kbd><span>&nbsp;tool and compare the two folders,</span> <kbd>$BOOK_HOME/Chapter09</kbd><span>&nbsp;and&nbsp;</span><kbd>$BOOK_HOME/Chapter10</kbd><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All commands described in this book<span> are run on a MacBook Pro using macOS Mojave, but should be straightforward to modify in order to run on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p>The source code for this chapter can be found <span>on GitHub</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter10">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter10</a>.</p>
<p>To be able to run the commands as described in this book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. The following are sample commands:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter10</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1.0, SR1 (also known as the <span><strong>Greenwich</strong> release</span>), Spring Boot 2.1.3, and Spring 5.1.5, which is the latest available version of the Spring components at the time of writing this chapter.</p>
<p>The source code contains the following Gradle projects:</p>
<ul>
<li><kbd>api</kbd></li>
<li><kbd>util</kbd></li>
<li><kbd>microservices/product-service</kbd></li>
<li><kbd>microservices/review-service</kbd></li>
<li><kbd>microservices/recommendation-service</kbd></li>
<li><kbd>microservices/product-composite-service</kbd></li>
<li><kbd>spring-cloud/eureka-server</kbd></li>
<li><kbd>spring-cloud/gateway</kbd></li>
</ul>
<p>The code examples in this chapter all come from source code in <span><kbd>$BOOK_HOME/Chapter10</kbd> but are, in several cases, edited to remove non-relevant parts of the source code, such as comments, and import and log statements.</span></p>
<p>If you want to see the changes applied to the source code in <a href="a3383211-405d-4319-b142-ddb8cf3674fd.xhtml" target="_blank">Chapter 10</a>,&nbsp;<em>Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server</em>, that is, see <span>what it took to add Spring Cloud Gateway as an edge server to the microservices landscape, </span>you can compare it with the source code for <a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml" target="_blank">Chapter 9</a>, <em>Adding Service Discovery Using Netflix Eureka and Ribbon</em>.<span> You can use your favorite </span><kbd>diff</kbd><span> tool and compare the two folders,</span> <kbd>$BOOK_HOME/Chapter09</kbd><span> and </span><kbd>$BOOK_HOME/Chapter10</kbd><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding an edge server to our system landscape</h1>
                </header>
            
            <article>
                
<p>In this section we will see how the edge server is added to the system landscape and how it affects the way external clients access the public APIs that the microservices expose. All incoming requests will now be routed through the edge server, as illustrated by the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3ec7ce9c-d8a1-48bd-a0af-932a2b44ed6b.png" width="1877" height="897" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3ec7ce9c-d8a1-48bd-a0af-932a2b44ed6b.png"></p>
<p>As we can see from the preceding diagram, external clients send all their requests to the edge server. The edge server can route the incoming requests based on the URL path. For example, requests with a URL that starts with <kbd>/product-composite/</kbd> are routed to the <strong>Product Composite</strong> microservice, and a request with a URL that starts with <kbd>/eureka/</kbd> is routed to the <strong>Discovery Server</strong> based on Netflix Eureka.</p>
<p>In the previous <a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml" target="_blank"></a><a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml">Chapter 9</a>, <em>Adding Service Discovery Using Netflix Eureka and Ribbon</em>, we exposed both the <kbd>product-composite</kbd> service and the discovery service, Netflix Eureka, to the outside. When we introduce the edge server in this chapter, this will no longer be the case.  This is implemented by removing the following port declarations for the two services in the Docker Compose files:</p>
<pre>  product-composite:<br>    build: microservices/product-composite-service<br>    ports:<br>      - "8080:8080"<br><br>  eureka:<br>    build: spring-cloud/eureka-server<br>    ports:<br>      - "8761:8761"</pre>
<p>With the edge serve added to the system landscape, we will learn how to set up an edge server based on Spring Cloud Gateway in the next section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up a Spring Cloud Gateway</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to set up <span>an edge server based on Spring Cloud Gateway.</span></p>
<p>Setting up a Spring Cloud <span>Gateway as an edge server is straightforward and can be done by means of the following steps:</span></p>
<ol>
<li class="mce-root"><span>Create a Spring Boot project using Spring Initializr as described in <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml">Chapter 3</a>, <em>Creating a Set of Cooperating Microservicesâ</em>refer to the </span><span><em>Using Spring Initializr to generate skeleton code</em> section.</span></li>
<li class="mce-root">Add a dependency to<span>&nbsp;</span><kbd>spring-cloud-starter-gateway</kbd>.</li>
<li class="mce-root">To be able to locate microservices instances though Netflix Eureka, a<span>lso add the </span><kbd>spring-cloud-starter-netflix-eureka-client</kbd>&nbsp;<span>dependency.</span></li>
<li class="mce-root">Add the <span>edge server to the common build file, <kbd>settings.gradle</kbd></span><span>:</span></li>
</ol>
<pre style="padding-left: 60px">include ':spring-cloud:gateway'</pre>
<ol start="5">
<li>Add a Dockerfile with the same content as for our microservices.</li>
<li>Add the edge server to our three Docker Compose files:</li>
</ol>
<pre style="padding-left: 60px"><span>gateway</span>:<br>  <span>environment</span>:<br>    - SPRING_PROFILES_ACTIVE=docker<br>  <span>build</span>: spring-cloud/gateway<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"8080:8080"</span></pre>
<div class="packt_tip">Port <kbd>8080</kbd> of the edge server is exposed outside the Docker engine. <span>The memory limit of 350 MB is to ensure that all containers in this and the coming chapters will fit in the 6 GB of memory that we have allocated to the Docker engine</span></div>
<ol start="7">
<li>Add configuration for routing rules and more; refer to the <em>Configuring a Spring Cloud Gateway</em>&nbsp;<span>section </span>as we proceed in this chapter.</li>
<li>Since the edge server will handle all incoming traffic, we will move the composite health check from the product composite service to the edge server. This is described in <em>Adding a <span>composite</span> health check</em><span> section.</span></li>
</ol>
<p>You can find the source code for the <span>Spring Cloud Gateway</span> in <span><kbd>$BOOK_HOME/Chapter10/spring-cloud/gateway</kbd>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a composite health check</h1>
                </header>
            
            <article>
                
<p>With an edge server in place, external health check requests also have to go through the edge server. Therefore, the <span>composite</span> health check that checks the status of all microservices been moved from the <kbd>product-composite</kbd> service to the edge server. See <a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml">Chapter 7</a>, <em>Developing Reactive Microservicesâ</em>refer to the <em>Adding a health API</em> section for implementation details of the <span>composite</span><span> health check.</span></p>
<p>The following has been added to the edge server: </p>
<ol>
<li>The <kbd>se.magnus.springcloud.gateway.HealthCheckConfiguration</kbd> class has been added, which declares the health indicator:</li>
</ol>
<pre style="padding-left: 60px"><span>@Bean<br></span>ReactiveHealthIndicator healthcheckMicroservices() {<br><br>    ReactiveHealthIndicatorRegistry registry = <br>        new <span>DefaultReactiveHealthIndicatorRegistry<br>            (</span><span>new </span><span>LinkedHashMap&lt;&gt;());<br></span><br>    registry.register(<span>"product"</span>,() -&gt; <br>        getHealth(<span>"http://product"</span>));<br>    registry.register(<span>"recommendation"</span>, () -&gt;<br>        getHealth(<span>"http://recommendation"</span>));<br>    registry.register(<span>"review"</span>, () -&gt;<br>        getHealth(<span>"http://review"</span>));<br>    registry.register(<span>"product-composite"</span>, () -&gt;<br>        getHealth(<span>"http://product-composite"</span>));<br><br>    <span>return new </span>CompositeReactiveHealthIndicator<br>        (<span>healthAggregator</span>, registry);<br>}<br><br><span>private </span>Mono&lt;Health&gt; getHealth(String url) {<br>    url += <span>"/actuator/health"</span>;<br>    <span>LOG</span>.debug(<span>"Will call the Health API on URL: {}"</span>, url);<br>    <span>return </span>getWebClient().get().uri(url)<br>        .retrieve().bodyToMono(String.<span>class</span>)<br>        .map(s -&gt; <span>new </span>Health.Builder().up().build())<br>        .onErrorResume(ex -&gt; <br>            Mono.<span>just</span>(<span>new </span>Health.Builder().down(ex).build()))<br>        .log();<br>}</pre>
<div class="packt_infobox">We have added the <kbd>product-composite</kbd> service to the composite health check!</div>
<ol start="2">
<li>The main application class, <kbd>se.magnus.springcloud.gateway.GatewayApplication</kbd>, declares a <kbd>WebClient.builder</kbd> bean to be used by the implementation of the health<span> indicator as follows</span>:</li>
</ol>
<pre style="padding-left: 60px"><span>@Bean<br></span><span>@LoadBalanced<br></span><span>public </span>WebClient.Builder loadBalancedWebClientBuilder() {<br>   <span>final </span>WebClient.Builder builder = WebClient.<span>builder</span>();<br>   <span>return </span>builder;<br>}</pre>
<p>From the preceding source code, we see that <span><span><kbd>WebClient.builder</kbd> is annotated with <kbd>@LoadBalanced</kbd>, which makes it aware of microservice instances registered in the discovery server, Netflix Eureka. Refer to the <em>Service discovery with Netflix Eureka in Spring Cloud</em> section in</span></span> <span><a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml">Chapter 9</a>, <em>Adding Service Discovery Using Netflix Eureka and Ribbon, </em>for details.</span></p>
<p>With a composite health check in place in the edge server, we are ready to look at the configuration that can be set up for a <span>Spring Cloud Gateway.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Configuring a Spring Cloud Gateway</h1>
                </header>
            
            <article>
                
<p><span>When it comes to configuring a </span><span>Spring Cloud Gateway, t</span>he most important thing <span>is </span>setting up the routing rules. We also need to set up a few other things in the configuration:</p>
<ol>
<li>Since <span><span>Spring Cloud Gateway will use Netflix Eureka to find the microservices it will route traffic to, so it must be configured as a Eureka client in the same way as described in </span></span><a href="9a514887-19f2-4962-9736-2c0d457bfd9e.xhtml">Chapter 9</a>, <em>Adding Service Discovery Using Netflix Eureka and Ribbon</em>ârefer to the <em>Configuration of clients to the Eureka server</em> section.</li>
</ol>
<ol start="2">
<li>Configure Spring Boot Actuator for development usage as described in <span><a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml" target="_blank">Chapter 7</a>, <em>Developing Reactive Microservicesâ</em>refer to the <em>Adding a health API</em> section</span>:</li>
</ol>
<pre style="padding-left: 60px"><span>management.endpoint.health.show-details</span>: <span>"</span><span>ALWAYS</span><span>"<br></span><span>management.endpoints.web.exposure.include</span>: <span>"*"<br></span></pre>
<ol start="3">
<li>Configure log levels so that we can see log messages from interesting parts of the internal processing in the<span>&nbsp;</span><span><span>Spring Cloud Gateway, for example, how it decides where to route incoming requests to:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span>logging</span>:<br>  <span>level</span>:<br>    <span>root</span>: INFO<br>    <span>org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator</span>: INFO<br>    <span>org.springframework.cloud.gateway</span>: TRACE</pre>
<p><span>For the full source code, refer to the configuration file:</span> <kbd>src/main/resources/application.yml</kbd>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Routing rules</h1>
                </header>
            
            <article>
                
<p>Setting up routing rules can be done in two ways; programmatically, using a Java DSL, or by configuration. Using the Java DSL to set up the routing rules programmatically can be useful in cases where the rules are stored in external storage, such as a database, or are given at runtime, for example, via a RESTful API or a message sent to the gateway. In most cases, I find it convenient to declare the routes in the configuration file, <kbd><span>src/main/resources/application.yml</span></kbd>.</p>
<p>A <strong>route</strong> is defined by the following:</p>
<ol>
<li><strong>Predicates</strong>, which select a route based on information in the incoming HTTP request</li>
<li><strong>Filters</strong>,&nbsp;<span>which</span> can modify both the request and/or the response</li>
<li>A <strong>destination URI</strong>,&nbsp;<span>which describes where to send a request</span></li>
<li>An <strong>ID</strong>, that is, the name of the route</li>
</ol>
<p>For a full list of available predicates and filters, refer to the reference documentation: <a href="https://cloud.spring.io/spring-cloud-gateway/single/spring-cloud-gateway.html">https://cloud.spring.io/spring-cloud-gateway/single/spring-cloud-gateway.html</a>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Routing requests to the Eureka server's API and web page</h1>
                </header>
            
            <article>
                
<p>Eureka exposes both an API and a web page for its clients. To provide a clean separation between the API and the web page in Eureka, we will set up routes as follows:</p>
<ul>
<li>Requests sent to the edge server with the path starting with <kbd>/eureka/api/</kbd> should be handled as a call to the Eureka API.&nbsp;</li>
<li><span>Requests sent to the edge server with the path starting with <kbd>/eureka/web/</kbd> should be handled as a call to&nbsp;the Eureka web page.</span></li>
</ul>
<p><span>API requests will be routed to&nbsp;<kbd>http://${app.eureka-server}:8761/eureka</kbd>. The route rule for the Eureka API looks like this:</span></p>
<pre>- <span>id</span>: eureka-api<br>  <span>uri</span>: http://${<span>app.eureka-server</span>}:8761<br>  <span>predicates</span>:<br>  - Path=/eureka/api/{segment}<br>  <span>filters</span>:<br>  - SetPath=/eureka/{segment}</pre>
<p><span>The <kbd>{segment}</kbd> part in the <kbd>Path</kbd> value matches zero or more elements in the path and will be used to replace the <kbd>{segment}</kbd> part in the <kbd>SetPath</kbd> value.</span></p>
<p><span>Web page requests will be routed to&nbsp;</span><kbd>http://${app.eureka-server}:8761</kbd><span>. The web page will load several web resources, such as&nbsp;<kbd>.js</kbd>,&nbsp;<kbd>.css</kbd></span>,&nbsp;<span>and&nbsp;<kbd>.png</kbd>&nbsp;files. These requests will be routed to&nbsp;<kbd>http://${app.eureka-server}:8761/eureka</kbd>.&nbsp;The route rules for the Eureka web page look like this:</span></p>
<pre>- <span>id</span>: eureka-web-start<br>  <span>uri</span>: http://${<span>app.eureka-server</span>}:8761<br>  <span>predicates</span>:<br>  - Path=/eureka/web<br>  <span>filters</span>:<br>  - SetPath=/<br><br>- <span>id</span>: eureka-web-other<br>  <span>uri</span>: http://${<span>app.eureka-server</span>}:8761<br>  <span>predicates</span>:<br>  - Path=/eureka/**</pre>
<p>From the preceding configuration, we can take the following notes:&nbsp;The <kbd>${<span>app.eureka-server</span>}</kbd> <span>property&nbsp;</span>is resolved by Spring's property mechanism depending on what Spring profile is activated:</p>
<ol>
<li>When running the services on the same host without using Docker, for example, for debugging purposes, the property will be translated to <kbd>localhost</kbd> using the <kbd>default</kbd> profile.&nbsp;</li>
<li><span>When running the services as Docker containers, the Netflix Eureka server will run in a container with the DNS name <kbd>eureka</kbd>. Therefore, the property will be translated into <kbd>eureka</kbd> using the <kbd>docker</kbd> profile.</span></li>
</ol>
<p>The relevant parts in the <kbd>application.yml</kbd> file that defines this translation look like this:</p>
<pre><span>app.eureka-server</span>: localhost<br>---<br><span>spring.profiles</span>: docker<br><span>app.eureka-server</span>: eureka</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Routing requests to the Eureka server's API and web page</h1>
                </header>
            
            <article>
                
<p>Eureka exposes both an API and a web page for its clients. To provide a clean separation between the API and the web page in Eureka, we will set up routes as follows:</p>
<ul>
<li>Requests sent to the edge server with the path starting with <kbd>/eureka/api/</kbd> should be handled as a call to the Eureka API. </li>
<li><span>Requests sent to the edge server with the path starting with <kbd>/eureka/web/</kbd> should be handled as a call to the Eureka web page.</span></li>
</ul>
<p><span>API requests will be routed to <kbd>http://${app.eureka-server}:8761/eureka</kbd>. The route rule for the Eureka API looks like this:</span></p>
<pre>- <span>id</span>: eureka-api<br>  <span>uri</span>: http://${<span>app.eureka-server</span>}:8761<br>  <span>predicates</span>:<br>  - Path=/eureka/api/{segment}<br>  <span>filters</span>:<br>  - SetPath=/eureka/{segment}</pre>
<p><span>The <kbd>{segment}</kbd> part in the <kbd>Path</kbd> value matches zero or more elements in the path and will be used to replace the <kbd>{segment}</kbd> part in the <kbd>SetPath</kbd> value.</span></p>
<p><span>Web page requests will be routed to </span><kbd>http://${app.eureka-server}:8761</kbd><span>. The web page will load several web resources, such as <kbd>.js</kbd>,&nbsp;<kbd>.css</kbd></span>,&nbsp;<span>and <kbd>.png</kbd> files. These requests will be routed to <kbd>http://${app.eureka-server}:8761/eureka</kbd>. The route rules for the Eureka web page look like this:</span></p>
<pre>- <span>id</span>: eureka-web-start<br>  <span>uri</span>: http://${<span>app.eureka-server</span>}:8761<br>  <span>predicates</span>:<br>  - Path=/eureka/web<br>  <span>filters</span>:<br>  - SetPath=/<br><br>- <span>id</span>: eureka-web-other<br>  <span>uri</span>: http://${<span>app.eureka-server</span>}:8761<br>  <span>predicates</span>:<br>  - Path=/eureka/**</pre>
<p>From the preceding configuration, we can take the following notes: The <kbd>${<span>app.eureka-server</span>}</kbd> <span>property </span>is resolved by Spring's property mechanism depending on what Spring profile is activated:</p>
<ol>
<li>When running the services on the same host without using Docker, for example, for debugging purposes, the property will be translated to <kbd>localhost</kbd> using the <kbd>default</kbd> profile. </li>
<li><span>When running the services as Docker containers, the Netflix Eureka server will run in a container with the DNS name <kbd>eureka</kbd>. Therefore, the property will be translated into <kbd>eureka</kbd> using the <kbd>docker</kbd> profile.</span></li>
</ol>
<p>The relevant parts in the <kbd>application.yml</kbd> file that defines this translation look like this:</p>
<pre><span>app.eureka-server</span>: localhost<br>---<br><span>spring.profiles</span>: docker<br><span>app.eureka-server</span>: eureka</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the edge server</h1>
                </header>
            
            <article>
                
<p>To try out the edge server, we perform the following steps:</p>
<ol>
<li>First, build the Docker images with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>cd $BOOK_HOME/</span><span>Chapter10<br></span><span>./gradlew build &amp;&amp; </span><span>docker-compose build</span></strong></pre>
<ol start="2">
<li>Next, start the system landscape in Docker and run the usual tests with the following&nbsp;command:</li>
</ol>
<pre style="padding-left: 60px"><strong>./test-em-all.bash start</strong></pre>
<p style="padding-left: 60px">Expect output similar to what we have seen in previous chapters:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/fa1b108b-8607-4ced-a8b6-b24fc43ce34d.png" style="width:48.75em;height:11.92em;" width="1935" height="473" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/fa1b108b-8607-4ced-a8b6-b24fc43ce34d.png"></p>
<p>With the system landscape including the edge server, let's explore the following topics:</p>
<ul>
<li>Examine what is exposed by the edge server outside of the system landscape running in the Docker engine.&nbsp;</li>
<li>Try out some of the most frequently used routing rules as follows:
<ul>
<li>Use URL-based routing to call our APIs through the edge server.</li>
<li><span>Use URL-based routing to c</span>all Netflix Eureka through the edge server, both using its API and web-based UI.</li>
<li>Use header-based routing to see how we can route requests based on the hostname in the request.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the edge server</h1>
                </header>
            
            <article>
                
<p>To try out the edge server, we perform the following steps:</p>
<ol>
<li>First, build the Docker images with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>cd $BOOK_HOME/</span><span>Chapter10<br></span><span>./gradlew build &amp;&amp; </span><span>docker-compose build</span></strong></pre>
<ol start="2">
<li>Next, start the system landscape in Docker and run the usual tests with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>./test-em-all.bash start</strong></pre>
<p style="padding-left: 60px">Expect output similar to what we have seen in previous chapters:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/fa1b108b-8607-4ced-a8b6-b24fc43ce34d.png" style="width:48.75em;height:11.92em;" width="1935" height="473" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/fa1b108b-8607-4ced-a8b6-b24fc43ce34d.png"></p>
<p>With the system landscape including the edge server, let's explore the following topics:</p>
<ul>
<li>Examine what is exposed by the edge server outside of the system landscape running in the Docker engine. </li>
<li>Try out some of the most frequently used routing rules as follows:
<ul>
<li>Use URL-based routing to call our APIs through the edge server.</li>
<li><span>Use URL-based routing to c</span>all Netflix Eureka through the edge server, both using its API and web-based UI.</li>
<li>Use header-based routing to see how we can route requests based on the hostname in the request.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Examining what is exposed outside the Docker engine</h1>
                </header>
            
            <article>
                
<p>To understand what the edge server exposes to the outside of the system landscape, perform the following steps:</p>
<ol>
<li>Use the <kbd>docker-compose ps</kbd> command to see that what ports are exposed by our services:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose ps gateway eureka product-composite product recommendation review</strong></pre>
<ol start="2">
<li>As we can see in the following output, only the edge server <span>(named <kbd>gateway</kbd>)</span> exposes its port (<kbd>8080</kbd>) outside the Docker engine:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/635db56b-5896-497a-a15d-2ca63833a1e8.png" style="width:27.25em;height:8.50em;" width="1209" height="374" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/635db56b-5896-497a-a15d-2ca63833a1e8.png"></p>
<ol start="3">
<li>If we want to see what routes the edge server has set up, we can use the <kbd>/actuator/gateway/routes</kbd> API. The response from this API is rather verbose. To limit the response to information we are interested in, we can apply a <kbd>jq</kbd> filter. <span>In the following example, I have selected the <kbd>id</kbd> of the route and the first predicate in the route</span>:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl localhost:8080/actuator/gateway/routes -s | jq '.[] | {"\(.route_id)": "\(.route_definition.predicates[0].args._genkey_0)"}'</strong></pre>
<ol start="4">
<li>This command will respond with the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7e1c7cfb-3eb6-4299-9199-b4415b1b20b3.png" style="width:17.75em;height:20.17em;" width="953" height="1079" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7e1c7cfb-3eb6-4299-9199-b4415b1b20b3.png"></p>
<p>This gives us a good overview of the actual routes configured in the edge server. Now, let's try out the routes!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Calling the product composite API through the edge server</h1>
                </header>
            
            <article>
                
<p>Let's perform the following steps to call the product composite API through the edge server as follows:</p>
<ol>
<li>To be able to see what is going on in the edge server, we can follow its log output:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose logs -f --tail=0 gateway</strong></pre>
<ol start="2">
<li>Now, make the call to the product composite API through the edge server:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl http://localhost:8080/product-composite/2</strong></pre>
<ol start="3">
<li>Expect the normal type of response from the composite product API:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/20e4c6a4-4714-4990-b85f-964445d04b6a.png" style="width:43.17em;height:3.75em;" width="1716" height="148" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/20e4c6a4-4714-4990-b85f-964445d04b6a.png"></p>
<ol start="4">
<li>We should be able to find the following interesting information in the log output:</li>
</ol>
<pre style="padding-left: 60px">Pattern "/product-composite/**" matches against value "/product-composite/2"<br>Route matched: product-composite<br>LoadBalancerClientFilter url chosen: http://b8013440aea0:8080/product-composite/2</pre>
<p>From the log output, we can see the pattern matching based on the predicate we specified in the configuration, and we can see what microservice instance the edge server selected from the available instances in the discovery serverâin this case, <kbd>http://b8013440aea0:8080/product-composite/2</kbd>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Calling the product composite API through the edge server</h1>
                </header>
            
            <article>
                
<p>Let's perform the following steps to call the product composite API through the edge server as follows:</p>
<ol>
<li>To be able to see what is going on in the edge server, we can follow its log output:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose logs -f --tail=0 gateway</strong></pre>
<ol start="2">
<li>Now, make the call to the product composite API through the edge server:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl http://localhost:8080/product-composite/2</strong></pre>
<ol start="3">
<li>Expect the normal type of response from the composite product API:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/20e4c6a4-4714-4990-b85f-964445d04b6a.png" style="width:43.17em;height:3.75em;" width="1716" height="148" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/20e4c6a4-4714-4990-b85f-964445d04b6a.png"></p>
<ol start="4">
<li>We should be able to find the following interesting information in the log output:</li>
</ol>
<pre style="padding-left: 60px">Pattern "/product-composite/**" matches against value "/product-composite/2"<br>Route matched: product-composite<br>LoadBalancerClientFilter url chosen: http://b8013440aea0:8080/product-composite/2</pre>
<p>From the log output, we can see the pattern matching based on the predicate we specified in the configuration, and we can see what microservice instance the edge server selected from the available instances in the discovery serverâin this case, <kbd>http://b8013440aea0:8080/product-composite/2</kbd>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Calling Eureka through the edge server</h1>
                </header>
            
            <article>
                
<p>To call Eureka through an edge server, perform the following steps:</p>
<ol>
<li>First, call the Eureka API <span>through the edge server </span>to see what instances are currently registered in the discovery server:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl -H "accept:application/json" localhost:8080/eureka/api/apps -s | \ jq -r .applications.application[].instance[].instanceId</strong></pre>
<ol start="2">
<li>Expect a response along the lines of the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d8b40a4a-a897-408e-9131-8dd8b7f52041.png" style="width:18.00em;height:8.33em;" width="725" height="334" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d8b40a4a-a897-408e-9131-8dd8b7f52041.png"></p>
<div class="packt_tip">Note that the edge server (named <kbd>gateway</kbd>) is also present in the response.</div>
<ol start="3">
<li><span>Next, open the Eureka web page in a web browser using the URL,</span> <kbd>http://localhost:8080/eureka/web</kbd>:&nbsp;</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c1f75e90-3c3a-42c1-aac1-432c7428230e.png" style="width:41.00em;height:29.42em;" width="1950" height="1398" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c1f75e90-3c3a-42c1-aac1-432c7428230e.png"></p>
<p><span>From the preceding screenshot, we can see the </span><span>Eureka web page reporting the same available instances as the API response in the previous step.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Routing based on the host header</h1>
                </header>
            
            <article>
                
<p>Let's wrap up by testing the route setup based on the hostname used in the requests!</p>
<p>Normally, the hostname in the request is set automatically in the <kbd>Host</kbd> <span>header </span>by the HTTP client. When testing the edge server locally, the hostname will be <kbd>localhost</kbd>âthat is not so useful when testing hostname-based routing. But we can cheat by specifying another hostname in the <kbd>Host</kbd> header in the call to the API. Let's see how this can be done:</p>
<ol>
<li>To call for the <kbd>i.feel.lucky</kbd>&nbsp;<span>hostname, </span>use this code:</li>
</ol>
<pre style="padding-left: 60px"><span><strong>curl http://localhost:8080/headerrouting -H "Host: i.feel.lucky:8080"</strong></span></pre>
<ol start="2">
<li>Expect the response 200 OK. F<span>or the hostname </span><kbd>im.a.teapot</kbd><span>, use the following command:</span></li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>curl http://localhost:8080/headerrouting -H "Host: im.a.teapot:8080"</span></strong></pre>
<p style="padding-left: 60px">Expect the response <kbd>418 I'm a teapot</kbd>.</p>
<ol start="3">
<li>Finally, if not specifying any <kbd>Host</kbd> header, use <kbd>localhost</kbd> as the <kbd>Host</kbd> header: </li>
</ol>
</div>
<div>
<pre style="padding-left: 60px"><span><strong>curl http://localhost:8080/headerrouting</strong></span></pre></div>
<p style="padding-left: 60px"><span>Expect the response</span>&nbsp;<kbd>501 Not Implemented</kbd><span>.</span></p>
<ol start="4">
<li>We can also use <kbd>i.feel.lucky</kbd> <span>and</span> <kbd>im.a.teapot</kbd> <span>as real hostnames in the requests if we add them to the local</span> <kbd>/etc/hosts</kbd> <span>file and specify that they should be translated into the same IP address as</span> <kbd>localhost</kbd><span>, that is </span><kbd>127.0.0.1</kbd><span>.&nbsp;</span>Run the following command to <span>add a row to the </span><kbd>/etc/hosts</kbd><span> file with the required information:</span></li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>sudo bash -c "echo '127.0.0.1 i.feel.lucky im.a.teapot' &gt;&gt; /etc/hosts"</span></strong></pre></div>
<ol start="5">
<li>We can now perform the same routing based on the hostname, but without specifying the <kbd>Host</kbd> header. Try it out by running the following commands:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>curl http://i.feel.lucky:8080/headerrouting<br></span><span>curl http://im.a.teapot:8080/headerrouting</span></strong></pre></div>
<p style="padding-left: 60px">Expect the same response as previously, that is, 200 OK <span>and </span><kbd>418 I'm a teapot</kbd>.</p>
<ol start="6">
<li>Wrap up the tests by shutting down the system landscape with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose down</strong></pre>
<ol start="7">
<li>Also, clean up the <kbd>/etc/hosts</kbd><span> file from the DNS name translation we added for the hostnames, </span><span><kbd>i.feel.lucky</kbd> and <kbd>im.a.teapot</kbd>. Edit the <kbd>/etc/hosts</kbd> file </span><span>and remove the line we added: </span><span><kbd>127.0.0.1 i.feel.lucky im.a.teapot</kbd>.</span></li>
</ol>
<p class="mce-root">These tests of the routing capabilities in the system landscape's edge server end the chapter.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how Spring Cloud Gateway can be used as an edge server to control what services are allowed to be called from the outside of the system landscape. Based on predicates, filters, and destination URIs, we can define routing rules in a very flexible way. If we want to, we can configure Spring Cloud Gateway to use a discovery service such as Netflix Eureka to look up the target microservice instances.</p>
<p>One important question still unanswered is how we prevent unauthorized access to the APIs exposed by the edge server and how we can prevent third parties from intercepting the traffic.</p>
<p>In the next chapter, we will see how we can secure access to the edge server using standard security mechanisms such as HTTPS, OAuth, and OpenID Connect.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the elements used to build a routing rule in Spring Cloud Gateway called?</li>
<li>What are they used for?</li>
<li>How can we instruct <span>Spring Cloud Gateway to locate microservice instances through a discovery service such as Netflix Eureka?</span></li>
<li><span>In a Docker environment, how can we ensure that external HTTP requests to the Docker engine can only reach the edge server?</span></li>
<li>How do we change the routing rules so that the edge server accepts calls to the <kbd>product-composite</kbd> service on the <kbd>http://$HOST:$PORT/api/product</kbd><span> URL instead of the currently used </span><kbd>http://$HOST:$PORT/product-composite</kbd><span>?</span></li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Securing Access to APIs</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will see how we can secure access to the APIs and web pages exposed by the edge server introduced in the previous chapter. We will learn to use HTTPS to protect against eavesdropping on external access to our APIs and also how to use OAuth 2.0 and OpenID Connect to authenticate and authorize users and client applications to access our APIs. Finally, we will study the use of HTTP basic authentication to secure access to the discovery service, Netflix Eureka.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>An introduction to the OAuth 2.0 and OpenID Connect standard<span>s</span></li>
<li>A general discussion on how to secure the system landscape</li>
<li>Adding an authorization server to our system landscape</li>
<li>Protecting external communication with HTTPS</li>
<li>Securing access to the discovery service, Netflix Eureka</li>
<li>Authenticating and authorizing API access using OAuth 2.0 and OpenID Connect</li>
<li>Testing with the local authorization server</li>
<li>Testing with an OpenID Connect provider, Auth0</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All commands described in this book<span> are run on a MacBook Pro using macOS Mojave but modifying them so they run on another platform such as Linux or Windows should be straightforward.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p>The source code for this chapter can be found <span>on GitHub at</span><span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter11">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter11</a>.</p>
<p class="mce-root"></p>
<p>To be able to run the commands as described in the book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. The following commands can be used to perform these steps:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter11</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1.0, SR1 (also known as the <span><strong>Greenwich</strong> release</span>), Spring Boot 2.1.3, and Spring 5.1.5, that is, the latest available version of the Spring components at the time of writing.</p>
<p>The source code contains the following Gradle projects:</p>
<ul>
<li><kbd>api</kbd></li>
<li><kbd>util</kbd></li>
<li><kbd>microservices/product-service</kbd></li>
<li><kbd>microservices/review-service</kbd></li>
<li><kbd>microservices/recommendation-service</kbd></li>
<li><kbd>microservices/product-composite-service</kbd></li>
<li><kbd>spring-cloud/eureka-server</kbd></li>
<li><kbd>spring-cloud/gateway</kbd></li>
<li><kbd>spring-cloud/authorization-server</kbd></li>
</ul>
<p>The code examples in this chapter all come from source code in <span><kbd>$BOOK_HOME/Chapter11</kbd>, but are, in several cases, edited to remove non-relevant parts of the source code, such as comments, imports, and log statements.</span></p>
<p>If you want to see the changes applied to the source code in <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml">Chapter 11</a>,&nbsp;<em>Secure Access to APIs</em>, that is, to see <span>what it took to secure access to the APIs in the microservice landscape, </span>you can compare it with the source code for <a href="a3383211-405d-4319-b142-ddb8cf3674fd.xhtml">Chapter 10</a>, <em>Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server</em>.&nbsp;<span>You can use your favorite </span><kbd>diff</kbd><span> tool and compare the two folders, </span><kbd>$BOOK_HOME/Chapter10</kbd><span> and </span><kbd>$BOOK_HOME/Chapter11</kbd><span>.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to OAuth 2.0 and OpenID Connect</h1>
                </header>
            
            <article>
                
<p>Before introducing OAuth 2.0 and OpenID Connect, let's clarify what we mean with authentication and authorization. <strong>Authentication</strong> means identifying a user by validating credentials supplied by the user, such as a username and password. <strong>Authorization</strong> is about giving access to various parts of, in our case, an API to an authenticated, that is, an identified user. In our case, a user will be assigned a set of privileges based on OAuth 2.0 scopes, as explained hereinafter. The microservices will be based on these privileges determine whether the user is allowed to access an API. </p>
<p><strong>OAuth 2.0</strong><span> is an open standard for authorization, and <strong>OpenID Connect</strong> is an add-on to OAuth 2.0 that enables client applications to verify the identity of users based on the authentication performed by the authorization server. </span>Let's look briefly at OAuth 2.0 and OpenID Connect separately to get an initial understanding of their purposes!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to OAuth 2.0</h1>
                </header>
            
            <article>
                
<p><strong>OAuth</strong> <em>2.0</em> is a widely accepted open standard for authorization that enables a user to give consent for a third-party client application to access protected resources in the name of the user.</p>
<p>So, what does this mean?</p>
<p class="mce-root">Let's start with sorting out the concepts used:</p>
<ul>
<li><strong>Resource owner</strong>: The end user.</li>
<li><strong>Client</strong>: The third-party client application, <span>for example, a web app or a native mobile app, that wants to call some protected APIs in the name of the end user.</span></li>
<li><strong>Resource server</strong>: The server that exposes the APIs that we want to protect.</li>
<li><span><strong>Authorization server:</strong> The authorization server issues tokens to the client after the resource owner, that is, the end user, has been authenticated. The management of user information and the authentication of users are typically delegated, behind the scenes, to an <strong>Identity Provider</strong>&nbsp;</span>(<strong>IdP</strong>). </li>
</ul>
<p><span>A client is registered in the authorization server and is given a </span><strong>client ID</strong><span> and a </span><strong>client secret</strong>. <span>The client secret must be protected by the client, like a password. A client also gets registered with a set of allowed </span><strong>redirect-URIs</strong><span> that the authorization server will use after a user has been authenticated to send</span> <strong>grant codes</strong><span> and </span><strong>tokens</strong><span> that have been issued back to the client application. </span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<p>The following is an example by way of illustration. Let's say that a user accesses a third-party client application and the client application wants to call a protected API to serve the user. To be allowed to access these APIs, the client application needs a way to tell the APIs that it is acting in the name of the user. To avoid solutions where the user must share their credentials with the client application for authentication, an <strong>access token</strong> is issued by an authorization server that gives the client application limited access to a selected set of APIs in the name of the user.</p>
<p>This means that the user never has to reveal their credentials to the client application. The user can also give consent to the client application to access specific APIs on behalf of the user. An access token represents a time-constrained set of access rights, expressed as a <em>scope</em> in OAuth 2.0 terms. A <strong>refresh token</strong> can also be issued to a client application by the authorization server. A refresh token can be used by the client application to obtain new access tokens without having to involve the user.</p>
<p class="mce-root">The OAuth 2.0 specification defines four authorization grant flows for issuing access tokens, explained as follows:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><strong>Authorization Code grant flow</strong>: This is the safest, but also the most complex, grant flow. This grant flow requires that the user interact with the authorization server using a web browser for authentication and giving consent to the client application, as illustrated by the following diagram:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/186b42c1-4690-4407-99c1-c0f71c1894c4.png" style="width:19.33em;height:25.33em;" width="692" height="906" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/186b42c1-4690-4407-99c1-c0f71c1894c4.png"></p>
<p style="padding-left: 60px">Explanations for this diagram are as follows:</p>
<ol>
<li style="list-style-type: none">
<ol>
<li>The client application initiates the grant flow by sending the user to the authorization server in the web browser.</li>
<li><span>The authorization server will authenticate the user and ask for the user's consent.</span></li>
<li>The authorization server will redirect the user back to the client application with a grant code. The <span>authorization server will use the redirect-URI specified by the client in step 1 to know where to send the grant code. </span>Since the grant code is passed back to the client application using the web browser, that is, to an insecure environment where malicious JavaScript code potentially can pick up the grant code, it is only allowed to be used once and only during a short time period.</li>
<li>To exchange the grant code for an access token, the client application is expected to call the authorization server again, using server-side code. The client application must present its client ID and client secret together with the grant code for the authorization server.</li>
<li>T<span>he authorization server issues an access token and sends it back to the client application. The authorization server can also, optionally, issue and return a refresh token.</span></li>
<li>Using the access token, the client can send a request to the protected API exposed by the resource server.</li>
<li>The resource server validates the access token and serves the request in the event of a successful validation. Steps 6 and 7 can be repeated as long as the access token is valid. When the lifetime of the access token has expired, the client can use their refresh token to acquire a new access token.</li>
</ol>
</li>
</ol>
<ul>
<li><strong>Implicit grant flow</strong><span>: This flow is also web browser-based, but intended for client applications that are not able to keep a client secret protected, for example, a single-page web application. It gets an access token back from the authorization server instead of a grant code, but cannot request a refresh token, since it is using the implicit grant flow that is less secure than the code grant flow.</span></li>
<li><strong>Resource Owner Password Credentials grant flow</strong><span>:</span> <span>If a client application can't interact with a web browser, it can fall back on this grant flow. In this grant flow, the user must share their credentials with the client application and the client application will use these credentials to acquire an access token.</span></li>
<li><strong>Client Credentials grant flow:</strong> <span><span>In the case where a client application needs to call an API unrelated to a specific user, it can use this grant flow to acquire an access token using its own client ID and client secret.</span></span></li>
</ul>
<div class="mce-root packt_infobox">When it comes to automating tests against APIs that are protected by OAuth 2.0, the <strong>Resource Owner Password Credentials grant flow</strong> is very handy since it doesn't require manual interaction using a web browser. We will use this later on in this chapter with our test script; see the <em>Changes in the test script</em> section.</div>
<p class="mce-root">The full specification can be found here: <a href="https://tools.ietf.org/html/rfc6749">https://tools.ietf.org/html/rfc6749</a>. There are also a number of additional specifications that detail various aspects of OAuth 2.0; for an overview, refer to <a href="https://www.oauth.com/oauth2-servers/map-oauth-2-0-specs/">https://www.oauth.com/oauth2-servers/map-oauth-2-0-specs/</a>.</p>
<div class="mce-root packt_infobox">One additional specification that is worth some extra attention is <em>RFC 7636 â Proof Key for Code Exchange by OAuth Public Clients (PKCE), </em><a href="https://tools.ietf.org/html/rfc7636">https://tools.ietf.org/html/rfc7636</a>. This specification describes how an otherwise insecure public client, such as a mobile native app or desktop application, can utilize the code grant flow by adding an extra layer of security.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing OpenID Connect</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>OpenID Connect</strong><span> (abbreviated to <strong>OIDC</strong>) is, as has already been mentioned, an add-on to OAuth 2.0 that enables client applications to verify the identity of users. OIDC adds an extra token, an ID token, that the client application gets back from the authorization server after a completed grant flow.</span></p>
<p class="mce-root"><span>The ID token is encoded as a <strong>JSON Web Token</strong> (<strong>JWT</strong>) and contains a number of claims, such as the ID and email address of the user. The ID token is digitally signed using JSON web signatures. This makes it possible for a client application to trust the information in the ID token by validating the digital signature using public keys from the authorization server.</span></p>
<p class="mce-root"><span>Optionally, access tokens can also be encoded and signed in the same way as ID tokens, but it is not mandatory according to the specification. Finally, OIDC defines a <strong>discovery endpoint</strong>, which is a standardized way to establish URLs to important endpoints, such as initiating a grant flow, getting the public keys to verify a digitally signed JWT token, and a <strong>user-info endpoint</strong>, which can be used to get extra information about an authenticated user given an access token for that user.</span></p>
<p>For an overview of the available specifications, see <a href="https://openid.net/developers/specs/">https://openid.net/developers/specs/</a>.</p>
<p>This concludes our introduction to the <span>OAuth 2.0 and OpenID Connect standards. In the next section, we will get a high-level view of how the system landscape will be secured.</span>&nbsp;</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Securing the system landscape</h1>
                </header>
            
            <article>
                
<p>To secure the system landscape as described in the introduction to this chapter, we will perform the following steps: </p>
<ul>
<li>Encrypt external requests and responses to and from our external API using HTTPS to protect against eavesdropping</li>
<li><span>Authenticate and authorize users and client applications that access our APIs using OAuth 2.0 and OpenID Connect</span></li>
<li>Secure access to the discovery service, Netflix Eureka, using HTTP basic authentication</li>
</ul>
<p><span>We will only apply HTTPS for external communication to our edge server, using plain HTTP for communication inside our system landscape.</span></p>
<div class="packt_infobox">In the chapter on service mesh (<a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>) that will appear later in this book, we will see how we can get help from a service mesh product to automatically provision HTTPS to secure communication inside a system landscape.</div>
<p>For test purposes, we will add a local OAuth <em>2.0</em> authorization server to our system landscape. All external communication with the authorization server will be routed through the edge server. The edge server and the product-composite service will act as OAuth 2.0 resource servers; that is, they will require a <span>valid </span>OAuth 2.0 access token to allow access.</p>
<p>To minimize the overhead of validating access tokens, we will assume that they are encoded as signed JWT tokens and that the authorization server exposes an endpoint that the resource servers can use to access the public keys, also known as <kbd>jwk-set</kbd>, required to validate the signing. </p>
<p class="mce-root"></p>
<p>The system landscape will look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c45c1b73-3816-4923-aca4-414cef12a865.png" width="1920" height="861" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c45c1b73-3816-4923-aca4-414cef12a865.png"></p>
<p>From the preceding diagram, we can note that:</p>
<ol>
<li>HTTPS is used for external communication, while plain text HTTP is used inside the system landscape.</li>
<li>The local OAuth 2.0 authorization server will be accessed externally through the edge server.</li>
<li>Both the edge server and the product composite microservice will validate access tokens as signed JWT tokens.</li>
<li><span>The edge server and the product composite microservice</span> will get the <span>authorization server's public keys from its <kbd>jwk-set</kbd> endpoint, and use them to validate the signature of the JWT-based access tokens.</span></li>
</ol>
<div class="packt_infobox">Note that we will focus on securing access to APIs over HTTP, not on covering general best practices for securing <span>web applications, for example, managing web application security risks pointed out by the <em>Category:OWASP Top Ten Project</em>. Refer to <a href="https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project">https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project</a> for more information on the OWASP Top Ten.</span></div>
<p><span>With</span>&nbsp;<span>this</span> <span>overview of how the</span><span>&nbsp;</span><span>system landscape</span>&nbsp;<span>will be secured, let's start by adding a local authorization server to </span><span>the system landscape</span><span>.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Protecting external communication with HTTPS</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to prevent&nbsp;<span>eavesdropping on external communication, for example from the internet,&nbsp;via the public APIs exposed by the edge server.&nbsp;</span><span>We will use HTTPS</span>&nbsp;<span>to encrypt communication. To use HTTPS</span><span>, we need to do the following:</span></p>
<ul>
<li class="mce-root"><span><strong>Create a certificate</strong>: We will create our own self-signed certificate, sufficient for development purposes.</span></li>
<li class="mce-root"><strong>Configure the edge server</strong>: It has to be configured to accept&nbsp;<span>only&nbsp;</span><span>HTTPS-based external traffic using the certificate.</span></li>
</ul>
<p>The&nbsp;<span>self-signed certificate&nbsp;is created with the following command:</span></p>
<pre><strong>keytool -genkeypair -alias localhost -keyalg RSA -keysize 2048 -storetype PKCS12 -keystore edge.p12 -validity 3650</strong></pre>
<div class="packt_infobox"><span>The source code comes with a sample certificate file, so you don't need to run this command to run the following examples.</span></div>
<p>The command will ask for a number of parameters. When asked for a password, I entered<span>&nbsp;</span><kbd>password</kbd><span>. For the rest of the parameters, I simply entered an</span>&nbsp;empty value to accept the default value. The certificate file <span>created</span>, <kbd>edge.p12</kbd>, is placed in the <kbd>gateway</kbd> projects folder,&nbsp;<kbd>src/main/resources/keystore</kbd>. This means that the certificate file will be placed in the <kbd>.jar</kbd> file when it is built and will be available on the classpath in runtime at:&nbsp;<kbd>keystore/edge.p12</kbd>.</p>
<div class="packt_infobox">Providing certificates using the classpath is sufficient during development, but not applicable to other environments, for example, a production environment. The following shows how we can replace this certificate with an external certificate at runtime!</div>
<p><span>To configure the edge server to use the certificate and HTTPS, the following is added to <kbd>application.yml</kbd> in the <kbd>gateway</kbd> project:</span></p>
<pre><span>server.port</span>: <span>8443<br></span><span><br></span><span>server.ssl</span>:<br> <span>key-store-type</span>: PKCS12<br> <span>key-store</span>: classpath:keystore/edge.p12<br> <span>key-store-password</span>: password<br> <span>key-alias</span>: localhost</pre>
<p><span>The following are the explanations for the preceding source code:</span></p>
<ul>
<li><span>The path to the certificate is specified in the&nbsp;</span><kbd>server.ssl.key-store</kbd><span>&nbsp;parameter, and is set to the&nbsp;</span><kbd>classpath:keystore/edge.p12</kbd><span>&nbsp;value. This means that the certificate will be picked up on the classpath from the location,</span> <span><kbd>keystore/edge.p12</kbd>.</span></li>
<li>The password for the certificate is specified in the&nbsp;<kbd>server.ssl.key-store-password</kbd><span>&nbsp;parameter.</span></li>
<li>To indicate that the edge server talks HTTPS and not HTTP, we also change the port from <kbd>8080</kbd> <span>to</span> <kbd>8443</kbd> <span>in the</span>&nbsp;<kbd>server.port</kbd><span>&nbsp;parameter.</span></li>
</ul>
<p>In addition to these changes in the edge server, changes are also required in the following files to reflect the changes to the port and HTTP protocol:</p>
<ul>
<li><span>The three Docker Compose files,&nbsp;</span><kbd>docker-compose*.yml</kbd></li>
<li>The test script,&nbsp;<kbd>test-em-all.bash</kbd></li>
</ul>
<p>Providing certificates using the classpath is as already mentioned previously only sufficient during development; let's see how we can replace this certificate with an external certificate in runtime!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Protecting external communication with HTTPS</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to prevent <span>eavesdropping on external communication, for example from the internet, via the public APIs exposed by the edge server. </span><span>We will use HTTPS</span>&nbsp;<span>to encrypt communication. To use HTTPS</span><span>, we need to do the following:</span></p>
<ul>
<li class="mce-root"><span><strong>Create a certificate</strong>: We will create our own self-signed certificate, sufficient for development purposes.</span></li>
<li class="mce-root"><strong>Configure the edge server</strong>: It has to be configured to accept <span>only </span><span>HTTPS-based external traffic using the certificate.</span></li>
</ul>
<p>The <span>self-signed certificate is created with the following command:</span></p>
<pre><strong>keytool -genkeypair -alias localhost -keyalg RSA -keysize 2048 -storetype PKCS12 -keystore edge.p12 -validity 3650</strong></pre>
<div class="packt_infobox"><span>The source code comes with a sample certificate file, so you don't need to run this command to run the following examples.</span></div>
<p>The command will ask for a number of parameters. When asked for a password, I entered<span>&nbsp;</span><kbd>password</kbd><span>. For the rest of the parameters, I simply entered an</span> empty value to accept the default value. The certificate file <span>created</span>, <kbd>edge.p12</kbd>, is placed in the <kbd>gateway</kbd> projects folder, <kbd>src/main/resources/keystore</kbd>. This means that the certificate file will be placed in the <kbd>.jar</kbd> file when it is built and will be available on the classpath in runtime at: <kbd>keystore/edge.p12</kbd>.</p>
<div class="packt_infobox">Providing certificates using the classpath is sufficient during development, but not applicable to other environments, for example, a production environment. The following shows how we can replace this certificate with an external certificate at runtime!</div>
<p><span>To configure the edge server to use the certificate and HTTPS, the following is added to <kbd>application.yml</kbd> in the <kbd>gateway</kbd> project:</span></p>
<pre><span>server.port</span>: <span>8443<br></span><span><br></span><span>server.ssl</span>:<br> <span>key-store-type</span>: PKCS12<br> <span>key-store</span>: classpath:keystore/edge.p12<br> <span>key-store-password</span>: password<br> <span>key-alias</span>: localhost</pre>
<p><span>The following are the explanations for the preceding source code:</span></p>
<ul>
<li><span>The path to the certificate is specified in the </span><kbd>server.ssl.key-store</kbd><span> parameter, and is set to the </span><kbd>classpath:keystore/edge.p12</kbd><span> value. This means that the certificate will be picked up on the classpath from the location,</span> <span><kbd>keystore/edge.p12</kbd>.</span></li>
<li>The password for the certificate is specified in the <kbd>server.ssl.key-store-password</kbd><span> parameter.</span></li>
<li>To indicate that the edge server talks HTTPS and not HTTP, we also change the port from <kbd>8080</kbd> <span>to</span> <kbd>8443</kbd> <span>in the</span>&nbsp;<kbd>server.port</kbd><span> parameter.</span></li>
</ul>
<p>In addition to these changes in the edge server, changes are also required in the following files to reflect the changes to the port and HTTP protocol:</p>
<ul>
<li><span>The three Docker Compose files, </span><kbd>docker-compose*.yml</kbd></li>
<li>The test script, <kbd>test-em-all.bash</kbd></li>
</ul>
<p>Providing certificates using the classpath is as already mentioned previously only sufficient during development; let's see how we can replace this certificate with an external certificate in runtime!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Replacing a self-signed certificate in runtime</h1>
                </header>
            
            <article>
                
<p>Placing a self-signed certificate in the <kbd>.jar</kbd> file is only useful for development. For a working solution in runtime environments, for example, for test or production, it must be possible to use certificates signed by authorized <strong>CAs</strong> (short for <strong>Certificate Authorities</strong>).</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>It must also be possible to specify the certificates to be used during runtime without the need to rebuild the <kbd>.jar</kbd> files and, when using Docker, the Docker image that contains the <kbd>.jar</kbd> file. <span>When using Docker Compose to manage the Docker container, we can map a volume in the Docker container to a certificate that resides on the Docker host. We can also set up environment variables for the Docker container that points to the new certificate in the Docker volume.</span></p>
<div class="packt_infobox">In <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>,&nbsp;<em>Introduction to Kubernetes</em>, we will learn about Kubernetes, where we will see more powerful solutions for how to handle secrets, such as certificates, that are suitable for running Docker containers in a cluster; that is, where containers are scheduled on a group of Docker hosts and not on a single Docker host.<br>
<br>
The changes described in this topic have <strong>not</strong> been applied to the source code in the book's GitHub repository; that is, you need to make them yourself to see them in action!</div>
<p><span>To replace the certificate packaged in the <kbd>.jar</kbd> file, perform the following steps:</span></p>
<ol>
<li>
<p>Create a second certificate and set the password to <kbd>testtest</kbd>, when asked for it:</p>
</li>
</ol>
<pre style="padding-left: 60px"><strong>cd <span>$BOOK_HOME/</span><span>Chapter11<br>mkdir keystore<br></span></strong><br><strong>keytool -genkeypair -alias localhost -keyalg RSA -keysize 2048 -storetype PKCS12 -keystore keystore/edge-test.p12 -validity 3650</strong></pre>
<ol start="2">
<li>Update the Docker Compose file, <kbd>docker-compose.yml</kbd>, with environment variables for the location and password for the new certificate and a volume that maps to the folder where the new certificate is placed. The configuration of the edge server will look like the following after the change:</li>
</ol>
<pre style="padding-left: 60px"><span>gateway</span>:<br>  <span>environment</span>:<br>    - SPRING_PROFILES_ACTIVE=docker<br>    - SERVER_SSL_KEY_STORE=file:/keystore/edge-test.p12<br>    - SERVER_SSL_KEY_STORE_PASSWORD=testtest<br>  <span>volumes</span>:<br>    - $PWD/keystore:/keystore<br>  <span>build</span>: spring-cloud/gateway<br>  mem_limit: 350m<br>  <span>ports</span>:<br>    - <span>"8443:8443"</span></pre>
<ol start="3">
<li>
<p><span>If the edge server is up and running, it needs to be restarted with the following commands</span>:</p>
</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose up -d --scale gateway=0</strong><br><strong>docker-compose up -d --scale gateway=1</strong></pre>
<div class="packt_infobox">The <kbd>docker-compose restart gateway</kbd> <span>command </span>might look like a good candidate for restarting the <kbd>gateway</kbd> service, but it actually does not take changes in <kbd>docker-compose.yml</kbd> into consideration. Hence, it is not a useful command in this case.</div>
<p>The new certificate is now in use!</p>
<p>This concludes the section on how to protect external communication with HTTPS. In the next section we will learn how to secure access to the discovery service, Netflix Eureka, using <span>HTTP basic authentication.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the Eureka server</h1>
                </header>
            
            <article>
                
<p>To protect the Eureka servers, the following changes have been applied:</p>
<ol>
<li>A dependency in <kbd>build.gradle</kbd>&nbsp;has been added to Spring Security:</li>
</ol>
<pre style="color: black;padding-left: 60px">implementation 'org.springframework.boot:spring-boot-starter-security'</pre>
<ol start="2">
<li>Security configuration <span>has been added to</span>&nbsp;the&nbsp;<kbd>se.magnus.springcloud.eurekaserver.SecurityConfig</kbd>&nbsp;class:<br></li>
</ol>
<ul>
<li style="padding-left: 30px" class="mce-root">The user is defined as follows:<br></li>
</ul>
<pre style="color: black;padding-left: 90px">public void configure(AuthenticationManagerBuilder auth) throws Exception {<br>  auth.inMemoryAuthentication()<br>   .passwordEncoder(NoOpPasswordEncoder.getInstance())<br>   .withUser(username).password(password)<br>   .authorities("USER");<br>}</pre>
<ul>
<li style="padding-left: 30px">The <kbd>username</kbd> <span>and</span> <kbd>password</kbd> <span><span>are injected into the constructor from the configuration file:<br></span></span></li>
</ul>
<pre style="color: black;padding-left: 90px">@Autowired<br>public SecurityConfig(<br>  @Value("${app.eureka-username}") String username,<br>  @Value("${app.eureka-password}") String password<br>) {<br>  this.username = username;<br>  this.password = password;<br>}</pre>
<ul>
<li style="padding-left: 30px"><span>All&nbsp;APIs and web pages are&nbsp;protected using HTTP basic authentication by means of the following definition:<br></span></li>
</ul>
<pre style="color: black;padding-left: 90px">protected void configure(HttpSecurity http) throws Exception {<br>  http<br>    .authorizeRequests()<br>      .anyRequest().authenticated()<br>      .and()<br>      .httpBasic();<br>}</pre>
<ol start="3">
<li>Credentials for the user are set up in the configuration file,&nbsp;<kbd>application.yml</kbd>:</li>
</ol>
<pre style="color: black;padding-left: 60px">app:<br> eureka-username: u<br> eureka-password: p</pre>
<ol start="4">
<li>Finally, the test class, <kbd>se.magnus.springcloud.eurekaserver.EurekaServerApplicationTests</kbd>, uses the credentials from the configuration file when testing the APIs of the Eureka server:</li>
</ol>
<pre style="color: black;padding-left: 60px">@Value("${app.eureka-username}")<br>private String username;<br> <br>@Value("${app.eureka-password}")<br>private String password;<br> <br>@Autowired<br>public void setTestRestTemplate(TestRestTemplate testRestTemplate) {<br>   this.testRestTemplate = testRestTemplate.withBasicAuth(username, password);<br>}</pre>
<p>The preceding are the steps required for&nbsp;<span>restricting access to the APIs and web pages of the discovery server, Netflix Eureka. It will now use HTTP basic authentication and&nbsp;require a user to supply a username and password to get access.&nbsp;In the next section, we will learn how to configure Netflix&nbsp;</span>Eureka clients so that they pass credentials when accessing the&nbsp;<span>Netflix&nbsp;</span>Eureka server.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the Eureka server</h1>
                </header>
            
            <article>
                
<p>To protect the Eureka servers, the following changes have been applied:</p>
<ol>
<li>A dependency in <kbd>build.gradle</kbd> has been added to Spring Security:</li>
</ol>
<pre style="color: black;padding-left: 60px">implementation 'org.springframework.boot:spring-boot-starter-security'</pre>
<ol start="2">
<li>Security configuration <span>has been added to</span> the <kbd>se.magnus.springcloud.eurekaserver.SecurityConfig</kbd> class:<br></li>
</ol>
<ul>
<li style="padding-left: 30px" class="mce-root">The user is defined as follows:<br></li>
</ul>
<pre style="color: black;padding-left: 90px">public void configure(AuthenticationManagerBuilder auth) throws Exception {<br>  auth.inMemoryAuthentication()<br>   .passwordEncoder(NoOpPasswordEncoder.getInstance())<br>   .withUser(username).password(password)<br>   .authorities("USER");<br>}</pre>
<ul>
<li style="padding-left: 30px">The <kbd>username</kbd> <span>and</span> <kbd>password</kbd> <span><span>are injected into the constructor from the configuration file:<br></span></span></li>
</ul>
<pre style="color: black;padding-left: 90px">@Autowired<br>public SecurityConfig(<br>  @Value("${app.eureka-username}") String username,<br>  @Value("${app.eureka-password}") String password<br>) {<br>  this.username = username;<br>  this.password = password;<br>}</pre>
<ul>
<li style="padding-left: 30px"><span>All APIs and web pages are protected using HTTP basic authentication by means of the following definition:<br></span></li>
</ul>
<pre style="color: black;padding-left: 90px">protected void configure(HttpSecurity http) throws Exception {<br>  http<br>    .authorizeRequests()<br>      .anyRequest().authenticated()<br>      .and()<br>      .httpBasic();<br>}</pre>
<ol start="3">
<li>Credentials for the user are set up in the configuration file, <kbd>application.yml</kbd>:</li>
</ol>
<pre style="color: black;padding-left: 60px">app:<br> eureka-username: u<br> eureka-password: p</pre>
<ol start="4">
<li>Finally, the test class, <kbd>se.magnus.springcloud.eurekaserver.EurekaServerApplicationTests</kbd>, uses the credentials from the configuration file when testing the APIs of the Eureka server:</li>
</ol>
<pre style="color: black;padding-left: 60px">@Value("${app.eureka-username}")<br>private String username;<br> <br>@Value("${app.eureka-password}")<br>private String password;<br> <br>@Autowired<br>public void setTestRestTemplate(TestRestTemplate testRestTemplate) {<br>   this.testRestTemplate = testRestTemplate.withBasicAuth(username, password);<br>}</pre>
<p>The preceding are the steps required for <span>restricting access to the APIs and web pages of the discovery server, Netflix Eureka. It will now use HTTP basic authentication and require a user to supply a username and password to get access. In the next section, we will learn how to configure Netflix </span>Eureka clients so that they pass credentials when accessing the <span>Netflix </span>Eureka server.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in Eureka clients</h1>
                </header>
            
            <article>
                
<p>For Eureka clients, the credentials have to be specified in the connection URL for the Eureka server. This is specified in each client's configuration file, <kbd>application.yml</kbd>, as follows:</p>
<pre><span>app</span>:<br>  <span>eureka-username</span>: u<br>  <span>eureka-password</span>: p<br> <br><span>eureka</span>:<br>  <span>client</span>:<br>     <span>serviceUrl</span>:<br>       <span>defaultZone</span>: <span>"http://${</span><span>app.eureka-username</span><span>}:${</span><span>app.eureka-<br>                     pa</span><span>ssword</span><span>}@${</span><span>app.eureka-server</span><span>}:8761/eureka/"</span><span><br></span></pre>
<p>We will see this configuration in use by Netflix Eureka clients when we test the secured system landscape in the <em><span>Testing with the local authorization server</span></em> section.</p>
<p><span>In the next section, we will learn how to add credentials when we manually access the Netflix Eureka server, either using its API or its Web pages.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Authenticating and authorizing API access using OAuth 2.0 and OpenID Connect</h1>
                </header>
            
            <article>
                
<p>With the authorization server in place, we can enhance the edge server and the <kbd>product-composite</kbd> service so they become OAuth 2.0 resource servers; that is, they require a valid access token to allow access. We will configure the edge server to accept any access token it can validate using the signature provided by the authorization server. The <kbd>product-composite</kbd> service will also require the access token to contain some OAuth 2.0 scopes:</p>
<ul>
<li class="mce-root"><span>The</span> <kbd>product:read</kbd> <span>scope</span><span>&nbsp;</span><span>will be required for accessing the read-only APIs.</span></li>
<li class="mce-root">The <kbd>product:write</kbd> <span>scope</span><span>&nbsp;</span><span>will be required for accessing the create and delete APIs.</span></li>
</ul>
<p>We also need to enhance our test script,&nbsp;<kbd>test-em-all.bash</kbd>, so that it acquires access tokens before it runs the tests.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in both the edge server and the product-composite service</h1>
                </header>
            
            <article>
                
<p>The following changes have been applied:</p>
<ul>
<li class="mce-root">
<p>Spring Security 5.1 dependencies have been added to&nbsp;<kbd>build.gradle</kbd>&nbsp;to support OAuth 2.0 resource servers:</p>
</li>
</ul>
<pre style="padding-left: 60px"><span>implementation('org.springframework.boot:spring-boot-starter-security')</span><br><span>implementation('org.springframework.security:spring-security-oauth2-resource-server')</span><br><span>implementation('org.springframework.security:spring-security-oauth2-jose')</span></pre>
<ul>
<li>
<p>Security configurations have been added to the <kbd>se.magnus.springcloud.<span>gateway</span>.SecurityConfig</kbd> and <kbd>se.magnus.microservices.composite.product.<span>SecurityConfig</span></kbd><span><span>&nbsp;classes:</span></span></p>
</li>
</ul>
<pre style="padding-left: 60px"><span>@EnableWebFluxSecurity<br></span><span>public class</span> SecurityConfig {<br> <br>  <span>@Bean<br></span>  SecurityWebFilterChain springSecurityFilterChain(ServerHttpSecurity http) {<br>    http<br>      .authorizeExchange()<br>        .pathMatchers("/actuator/**").permitAll()<br>        .anyExchange().authenticated()<br>        .and()<br>      .oauth2ResourceServer()<br>        .jwt();<br>    <span>return</span> http.build();<br>  }<br>}</pre>
<p class="mce-root">Explanations <span>for the preceding source code are as follows:</span></p>
<ul>
<li><kbd>.pathMatchers("/actuator/**").permitAll()</kbd> is used to allow access to URLs that should be unprotected, for example, the <kbd>actuator</kbd> endpoints in this case. Refer to the source code for URLs that are treated as unprotected. Be careful about which URLs&nbsp;are exposed unprotected. For example, the <kbd>actuator</kbd> endpoints should be protected before going to production:
<ul>
<li><kbd>.anyExchange().authenticated()</kbd> <span>ensures that the user is authenticated before being allowed access to all other URLs.</span></li>
<li><kbd>.oauth2ResourceServer().jwt()</kbd> <span>specifies that&nbsp;authentication and authorization will be based on a JWT-encoded OAuth 2.0 access token.</span></li>
</ul>
</li>
<li>The endpoint of the authorization server's&nbsp;<kbd>jwk-set</kbd>&nbsp;endpoint has been registered in the configuration file, <kbd>application.yml</kbd>:</li>
</ul>
<pre style="color: black;padding-left: 60px">spring.security.oauth2.resourceserver.jwt.jwk-set-uri: http://${app.auth-server}:9999/.well-known/jwks.json</pre>
<p>With these changes applied to both&nbsp;the edge server and the <kbd>product-composite</kbd> service to make them act as&nbsp;<span>OAuth 2.0 resource servers, we also need to make some changes that only apply to the&nbsp;</span><kbd>product-composite</kbd> service.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in both the edge server and the product-composite service</h1>
                </header>
            
            <article>
                
<p>The following changes have been applied:</p>
<ul>
<li class="mce-root">
<p>Spring Security 5.1 dependencies have been added to <kbd>build.gradle</kbd> to support OAuth 2.0 resource servers:</p>
</li>
</ul>
<pre style="padding-left: 60px"><span>implementation('org.springframework.boot:spring-boot-starter-security')</span><br><span>implementation('org.springframework.security:spring-security-oauth2-resource-server')</span><br><span>implementation('org.springframework.security:spring-security-oauth2-jose')</span></pre>
<ul>
<li>
<p>Security configurations have been added to the <kbd>se.magnus.springcloud.<span>gateway</span>.SecurityConfig</kbd> and <kbd>se.magnus.microservices.composite.product.<span>SecurityConfig</span></kbd><span><span> classes:</span></span></p>
</li>
</ul>
<pre style="padding-left: 60px"><span>@EnableWebFluxSecurity<br></span><span>public class</span> SecurityConfig {<br> <br>  <span>@Bean<br></span>  SecurityWebFilterChain springSecurityFilterChain(ServerHttpSecurity http) {<br>    http<br>      .authorizeExchange()<br>        .pathMatchers("/actuator/**").permitAll()<br>        .anyExchange().authenticated()<br>        .and()<br>      .oauth2ResourceServer()<br>        .jwt();<br>    <span>return</span> http.build();<br>  }<br>}</pre>
<p class="mce-root">Explanations <span>for the preceding source code are as follows:</span></p>
<ul>
<li><kbd>.pathMatchers("/actuator/**").permitAll()</kbd> is used to allow access to URLs that should be unprotected, for example, the <kbd>actuator</kbd> endpoints in this case. Refer to the source code for URLs that are treated as unprotected. Be careful about which URLs are exposed unprotected. For example, the <kbd>actuator</kbd> endpoints should be protected before going to production:
<ul>
<li><kbd>.anyExchange().authenticated()</kbd> <span>ensures that the user is authenticated before being allowed access to all other URLs.</span></li>
<li><kbd>.oauth2ResourceServer().jwt()</kbd> <span>specifies that authentication and authorization will be based on a JWT-encoded OAuth 2.0 access token.</span></li>
</ul>
</li>
<li>The endpoint of the authorization server's <kbd>jwk-set</kbd> endpoint has been registered in the configuration file, <kbd>application.yml</kbd>:</li>
</ul>
<pre style="color: black;padding-left: 60px">spring.security.oauth2.resourceserver.jwt.jwk-set-uri: http://${app.auth-server}:9999/.well-known/jwks.json</pre>
<p>With these changes applied to both the edge server and the <kbd>product-composite</kbd> service to make them act as <span>OAuth 2.0 resource servers, we also need to make some changes that only apply to the </span><kbd>product-composite</kbd> service.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the product-composite service</h1>
                </header>
            
            <article>
                
<p>In addition to the common changes applied in the previous section, t<span>he following changes have also been applied to the <kbd>product-composite</kbd> service:</span></p>
<ul>
<li>The security configuration has been refined by requiring OAuth 2.0 scopes in the access token in order to allow access:</li>
</ul>
<pre style="color: black;padding-left: 60px">.pathMatchers(POST, "/product-composite/**").hasAuthority("SCOPE_product:write")<br>.pathMatchers(DELETE, "/product-composite/**").hasAuthority("SCOPE_product:write")<br>.pathMatchers(GET, "/product-composite/**").hasAuthority("SCOPE_product:read")</pre>
<div class="packt_infobox"><span>By convention, OAuth 2.0 scopes should be prefixed with <kbd>SCOPE_</kbd> when checked for authority using Spring Security.</span></div>
<ul>
<li class="mce-root"><span>A method, </span><kbd>logAuthorizationInfo()</kbd><span>, has been added to log relevant parts from the JWT-encoded access token upon each call to the API. The access token can be acquired using the standard Spring Security, </span><kbd>SecurityContext</kbd><span>, which, in a reactive environment, can be acquired using the static helper method,</span> <kbd>ReactiveSecurityContextHolder.getContext()</kbd><span>. R</span><span>efer to the</span> <kbd>se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl</kbd><span> class for details.</span></li>
<li class="mce-root">The use of OAuth has been d<span>isabled</span> when running Spring-based integration tests. To prevent the OAuth machinery from kicking in when we are running integration tests, we disable it as follows:</li>
<li style="padding-left: 30px">A security configuration, <kbd>TestSecurityConfig</kbd>, <span>is added to be used during tests </span>that permit access to all resources:</li>
</ul>
<pre style="color: black;padding-left: 90px">http.csrf().disable().authorizeExchange().anyExchange().permitAll();</pre>
<ul>
<li style="padding-left: 30px">In each Spring integration test class, we configure <kbd>TestSecurityConfig</kbd> to override the existing security configuration with the following:</li>
</ul>
<pre style="color: black;padding-left: 90px">@SpringBootTest( classes = <br>{ProductCompositeServiceApplication.class, TestSecurityConfig.class },<br> properties = {"spring.main.allow-bean-definition-overriding=true"})</pre>
<p><span>With these changes in place, both the edge server and the <kbd>product-composite</kbd> service can act as </span><span>OAuth 2.0 resource servers. The last step we need to take to introduce the usage of OAuth 2.0 and OpenID Connect is to update the test script so it acquires access tokens and uses them when running the tests.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the test script</h1>
                </header>
            
            <article>
                
<p>To start with, we need to acquire an access token before we can call any of the APIs, except the health API. This is done using the OAuth 2.0 password flow. To be able to call the create and delete APIs, we acquire an access token as the <kbd>writer</kbd> client, as follows:</p>
<pre><strong>ACCESS_TOKEN=<span>$</span>(curl -k https://writer:secret@$HOST:$PORT/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong><span><br></span></pre>
<p>To verify that the scope-based authorization works, two tests have been added to the test script:</p>
<ul>
<li class="mce-root"><span>The first test calls an API without supplying an access token. The API is expected to return the </span>401 Unauthorized HTTP status<span>.&nbsp;</span></li>
<li class="mce-root">The other test calls an updating API using the <kbd>reader</kbd> client, which <span>is </span><span>only granted a read scope. The API is expected to return the</span><span>&nbsp;</span>403 Forbidden<span> HTTP status</span><span>.</span></li>
</ul>
<p><span>For the full source code, see <kbd>test-em-all.bash</kbd></span>:</p>
<pre><span># Verify that a request without access token fails on 401, Unauthorized<br></span><span>assertCurl</span> <span>401</span> <span>"curl -k https://$HOST:$PORT/product-composite/$PROD_ID_REVS_RECS -s"<br></span><span><br></span><span># Verify that the reader - client with only read scope can call the read API but not delete API.<br></span>READER_ACCESS_TOKEN=<span>$</span>(curl -k https://reader:secret@$HOST:$PORT/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)<br> READER_AUTH=<span>"-H \"Authorization: Bearer $READER_ACCESS_TOKEN\""<br></span><span><br></span><span>assertCurl</span> <span>200</span> <span>"curl -k https://$HOST:$PORT/product-composite/$PROD_ID_REVS_RECS $READER_AUTH -s"<br></span><span>assertCurl</span> <span>403</span> <span>"curl -k https://$HOST:$PORT/product-composite/$PROD_ID_REVS_RECS $READER_AUTH -X DELETE -s"<br></span></pre>
<p>With the test scripts updated to acquire and use OAuth 2.0 access tokens, we are ready to try them out in the next section!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing with the local authorization server</h1>
                </header>
            
            <article>
                
<p>In this section we will try out the secured system landscape; that is, we will test all the security components together. We will use the local authorization server to issue access tokens.<span>&nbsp;</span><span>The following tests will be performed:</span></p>
<ol>
<li>First, we build from source and run the test script to ensure that everything fits together.</li>
<li>Next, we learn how to acquire access tokens using OAuth 2.0 grant flows: password, implicit, and code grant flows.</li>
<li>Finally, we will use access tokens to call APIs. We will also verify that an access token issued for a reader client can't be used to call an updating API.</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Acquiring access tokens</h1>
                </header>
            
            <article>
                
<p><span>Now we can acquire access tokens using the various grant flows defined by OAuth 2.0. We will try out the following grant flows: password, implicit, and code grant.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Acquiring access tokens using the password grant flow</h1>
                </header>
            
            <article>
                
<p>To get an access token for the <kbd>writer</kbd> client, that is, with both <span>the</span> <kbd>product:read</kbd><span>&nbsp;</span>and<span>&nbsp;</span><kbd>product:write</kbd><span>&nbsp;</span>scopes, issue the following command:</p>
<pre><strong>curl -k https://writer:secret@localhost:8443/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .</strong></pre>
<p><span>The client identifies itself using HTTP basic authentication, passing its <kbd>writer</kbd> client ID</span><span>, and its secret, <kbd>secret</kbd>. It sends the credentials of the resource owners, that is the end user, using the&nbsp;<kbd>username</kbd> and <kbd>password</kbd>&nbsp;parameters.</span></p>
<p>A sample response is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/aabd7ac5-4ce6-4f92-ae33-0393200e9116.png" style="width:25.00em;height:11.25em;" width="934" height="420" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/aabd7ac5-4ce6-4f92-ae33-0393200e9116.png"></p>
<p>Set the value of the <kbd>access_token</kbd> <span>field&nbsp;</span>in the response as the access token in an&nbsp;<span>environment variable</span>:</p>
<pre><span>ACCESS_TOKEN=</span>eyJ...SyIlQ</pre>
<p><span>To get an access token for the <kbd>reader</kbd> client, that is, with only the</span> <kbd>product:read</kbd> <span>scope, simply replace <kbd>writer</kbd>&nbsp;with&nbsp;<kbd>reader</kbd> in the preceding command:</span></p>
<pre><strong>curl -k https://reader:secret@localhost:8443/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Acquiring access tokens using the password grant flow</h1>
                </header>
            
            <article>
                
<p>To get an access token for the <kbd>writer</kbd> client, that is, with both <span>the</span> <kbd>product:read</kbd><span>&nbsp;</span>and<span>&nbsp;</span><kbd>product:write</kbd><span>&nbsp;</span>scopes, issue the following command:</p>
<pre><strong>curl -k https://writer:secret@localhost:8443/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .</strong></pre>
<p><span>The client identifies itself using HTTP basic authentication, passing its <kbd>writer</kbd> client ID</span><span>, and its secret, <kbd>secret</kbd>. It sends the credentials of the resource owners, that is the end user, using the <kbd>username</kbd> and <kbd>password</kbd> parameters.</span></p>
<p>A sample response is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/aabd7ac5-4ce6-4f92-ae33-0393200e9116.png" style="width:25.00em;height:11.25em;" width="934" height="420" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/aabd7ac5-4ce6-4f92-ae33-0393200e9116.png"></p>
<p>Set the value of the <kbd>access_token</kbd> <span>field </span>in the response as the access token in an <span>environment variable</span>:</p>
<pre><span>ACCESS_TOKEN=</span>eyJ...SyIlQ</pre>
<p><span>To get an access token for the <kbd>reader</kbd> client, that is, with only the</span> <kbd>product:read</kbd> <span>scope, simply replace <kbd>writer</kbd> with <kbd>reader</kbd> in the preceding command:</span></p>
<pre><strong>curl -k https://reader:secret@localhost:8443/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Acquiring access tokens using the code grant flow</h1>
                </header>
            
            <article>
                
<p>Finally, let's try out the most secure grant flow in OAuth 2.0 â the code grant flow!</p>
<p><span>When it comes to the code grant flow, things are a bit more complicated in order to make the flow more secure. In the first insecure step, we will use the web browser to acquire a code that can be used only once, when it is exchanged with an access token. The code shall be passed from the web browser to a secure layer, for example, server-side code, which can make a new request the authorization server again to exchange the code with an access token. In this exchange, the server has to supply a client secret to verify its origin.</span></p>
<p>To get a code for the <kbd>reader</kbd> client, use the following URL in the web browser:&nbsp;<kbd>https://localhost:8443/oauth/authorize?response_type=code&amp;client_id=reader&amp;redirect_uri=http://my.redirect.uri&amp;scope=product:read&amp;state=35725</kbd>.</p>
<p>This time, you will get back a much shorter URL, for example, <kbd>http://my.redirect.uri/?code=T2pxvW&amp;state=72489</kbd>.</p>
<p>Extract the authorization code from the <kbd>code</kbd><span>&nbsp;parameter&nbsp;</span>and define an environment variable,<span>&nbsp;</span><kbd>CODE</kbd>, with its value:</p>
<pre class="mce-root">CODE=<span>T2pxvW</span></pre>
<p>Next, pretend you are the backend server that exchanges the authorization code with an access token using the following <kbd>curl</kbd> command:</p>
<pre class="mce-root"><strong>curl -k https://reader:secret@localhost:8443/oauth/token \</strong><br><strong> -d grant_type=authorization_code \</strong><br><strong> -d client_id=reader \</strong><br><strong> -d redirect_uri=http://<span>my.redirect.uri</span><span> </span>\</strong><br><strong> -d code=$CODE -s | jq .</strong></pre>
<p class="mce-root">A sample response is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b06c2376-bab9-452d-8081-fce44ce0b426.png" style="width:23.58em;height:10.25em;" width="953" height="412" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b06c2376-bab9-452d-8081-fce44ce0b426.png"></p>
<p>Finally, save the access token in an environment variable as previously:</p>
<pre class="mce-root">ACCESS_TOKEN=eyJh...KUBA</pre>
<p>To get a code for the <kbd>writer</kbd> client, use the following URL:&nbsp;<kbd>https://localhost:8443/oauth/authorize?response_type=code&amp;client_id=writer&amp;redirect_uri=http://my.redirect.uri&amp;scope=product:read+product:write&amp;state=72489</kbd><a href="https://localhost:8443/oauth/authorize?response_type=code&amp;client_id=writer&amp;redirect_uri=http://my.redirect.uri&amp;scope=message:read+message:write&amp;state=72489" class="totri-footnote">.</a></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Acquiring access tokens using the code grant flow</h1>
                </header>
            
            <article>
                
<p>Finally, let's try out the most secure grant flow in OAuth 2.0 â the code grant flow!</p>
<p><span>When it comes to the code grant flow, things are a bit more complicated in order to make the flow more secure. In the first insecure step, we will use the web browser to acquire a code that can be used only once, when it is exchanged with an access token. The code shall be passed from the web browser to a secure layer, for example, server-side code, which can make a new request the authorization server again to exchange the code with an access token. In this exchange, the server has to supply a client secret to verify its origin.</span></p>
<p>To get a code for the <kbd>reader</kbd> client, use the following URL in the web browser: <kbd>https://localhost:8443/oauth/authorize?response_type=code&amp;client_id=reader&amp;redirect_uri=http://my.redirect.uri&amp;scope=product:read&amp;state=35725</kbd>.</p>
<p>This time, you will get back a much shorter URL, for example, <kbd>http://my.redirect.uri/?code=T2pxvW&amp;state=72489</kbd>.</p>
<p>Extract the authorization code from the <kbd>code</kbd><span> parameter </span>and define an environment variable,<span>&nbsp;</span><kbd>CODE</kbd>, with its value:</p>
<pre class="mce-root">CODE=<span>T2pxvW</span></pre>
<p>Next, pretend you are the backend server that exchanges the authorization code with an access token using the following <kbd>curl</kbd> command:</p>
<pre class="mce-root"><strong>curl -k https://reader:secret@localhost:8443/oauth/token \</strong><br><strong> -d grant_type=authorization_code \</strong><br><strong> -d client_id=reader \</strong><br><strong> -d redirect_uri=http://<span>my.redirect.uri</span><span> </span>\</strong><br><strong> -d code=$CODE -s | jq .</strong></pre>
<p class="mce-root">A sample response is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b06c2376-bab9-452d-8081-fce44ce0b426.png" style="width:23.58em;height:10.25em;" width="953" height="412" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b06c2376-bab9-452d-8081-fce44ce0b426.png"></p>
<p>Finally, save the access token in an environment variable as previously:</p>
<pre class="mce-root">ACCESS_TOKEN=eyJh...KUBA</pre>
<p>To get a code for the <kbd>writer</kbd> client, use the following URL: <kbd>https://localhost:8443/oauth/authorize?response_type=code&amp;client_id=writer&amp;redirect_uri=http://my.redirect.uri&amp;scope=product:read+product:write&amp;state=72489</kbd><a href="https://localhost:8443/oauth/authorize?response_type=code&amp;client_id=writer&amp;redirect_uri=http://my.redirect.uri&amp;scope=message:read+message:write&amp;state=72489" class="totri-footnote">.</a></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Calling protected APIs using access tokens</h1>
                </header>
            
            <article>
                
<p>Now, let's use the access tokens we have acquired to call the protected APIs!</p>
<ol>
<li>First, call an API to retrieve <span>a composite product </span>without a valid access token:</li>
</ol>
<pre style="padding-left: 60px"><span>ACCESS_TOKEN=an-invalid-token<br></span><span>curl https://localhost:8443/product-composite/2 -k -H "Authorization: Bearer $ACCESS_TOKEN" -i</span>  </pre>
<ol start="2">
<li>It should return the following response:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a11b7747-9134-464d-956b-8d9ea3ee632b.png" style="width:28.42em;height:5.75em;" width="933" height="187" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a11b7747-9134-464d-956b-8d9ea3ee632b.png"></p>
<p style="padding-left: 60px">The error message clearly states that the access token is invalid!</p>
<ol start="3">
<li>Next, try using the API to retrieve a composite product using one of the access tokens acquired for the <kbd>reader</kbd> client from the previous section:</li>
</ol>
<pre style="padding-left: 60px"><span>ACCESS_TOKEN={a-reader-access-token}<br></span><span>curl https://localhost:8443/product-composite/2 -k -H "Authorization: Bearer $ACCESS_TOKEN" -i</span> </pre>
<ol start="4">
<li>Now we will get the <kbd>200 OK</kbd> status code and the expected response body will be returned:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/35945d10-f681-4265-99d7-8c71e352e889.png" style="width:10.25em;height:5.17em;" width="462" height="234" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/35945d10-f681-4265-99d7-8c71e352e889.png"></p>
<p>If we try to access an updating API, for example, the delete API, with an access token acquired for the <kbd>reader</kbd> client, the call will fail:</p>
<pre><strong><span>ACCESS_TOKEN={a-reader-access-token}<br></span><span>curl https://localhost:8443/product-composite/999 -k -H "Authorization: Bearer $ACCESS_TOKEN" -X DELETE -i</span>  </strong></pre>
<p>It will fail with a response similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/41ac8596-5ddd-4a60-a2f4-48f0ec0e69e9.png" style="width:39.92em;height:5.92em;" width="1892" height="280" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/41ac8596-5ddd-4a60-a2f4-48f0ec0e69e9.png"></p>
<p>If we repeat the call to the delete API, but with <span>an access token acquired for</span> <span>the</span> <kbd>writer</kbd> <span>client, the call will succeed with</span> 200 OK<span> in the response.</span></p>
<div class="packt_infobox">The delete operation should return <kbd>200</kbd> even if the product with the specified product ID does not exist in the underlying database, since the delete operation is idempotent, as described in <a href="6c495e97-f473-4cb1-a404-11fd938f5478.xhtml"></a><a href="6c495e97-f473-4cb1-a404-11fd938f5478.xhtml">Chapter 6</a>, <em>Adding Persistence</em>. Refer to the <em>Adding new APIs</em> section.</div>
<p>If you look into the log output using the <kbd>docker-compose logs -f product-composite</kbd>&nbsp;<span>command, </span>you should be able to find authorization information such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/fa8061ea-8387-4bda-b2bd-ad5bab5f2230.png" style="width:44.83em;height:5.42em;" width="1570" height="189" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/fa8061ea-8387-4bda-b2bd-ad5bab5f2230.png"></p>
<p>This information was extracted by the new method, <kbd><span>logAuthorizationInfo()</span></kbd>, in the <kbd>product-composite</kbd> service from the JWT-encoded access token; that is, the <span><kbd>product-composite</kbd> service did not need to communicate with the authorization server to get this information!</span></p>
<p><span>With these tests, we have seen how to acquire an access token with the various grant flows, that is, password, implicit, and code grant flow. We have also seen how scopes can be used to limit what a client can do with a specific access token, for example only use is for reading operations.</span></p>
<p><span>In the next section, we will replace the local authorization server used in this section to an external OpenID Connect provider.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up an account and OAuth 2.0 client in Auth0</h1>
                </header>
            
            <article>
                
<p>Perform the following steps to sign up for a free account in Auth0, configure both an OAuth 2.0 client and the <kbd>product-composite</kbd> API, and finally register a user:</p>
<ol>
<li>Open the URL,&nbsp;<a href="https://auth0.com">https://auth0.com</a>, in your browser.</li>
<li>Click on the <span class="packt_screen">SIGN UP&nbsp;</span><span>button:</span>&nbsp;<br>
<ol>
<li>Sign up with an account of your choice.</li>
<li>After a successful sign-up, you will be asked to create a tenant domain.<br>
Enter the name of the tenant of your choice, in my case:&nbsp;<kbd>dev-ml<span>.eu.auth0.com</span></kbd>.</li>
<li>Fill in information about your account as requested.</li>
</ol>
</li>
</ol>
<ol start="3">
<li>Following sign-up, you will be directed to your dashboard. Select the <span class="packt_screen">Applications</span> tab (on the left) to see the default client application that was created for you during the sign-up process.</li>
<li>Click on the <span class="packt_screen">Default App</span> <span>to configure it:</span>
<ol>
<li>Copy the<span>&nbsp;</span><span class="packt_screen">Client ID</span>&nbsp;and <span class="packt_screen">Client Secret</span>; you will need them later on.</li>
<li>As <span class="packt_screen">Application Type</span>, select <span class="packt_screen">Machine to Machine</span>.</li>
<li>As <span class="packt_screen">Token Endpoint Authentication Method</span>, select <span class="packt_screen">POST</span>.</li>
<li>Enter&nbsp;<kbd>http://my.redirect.uri</kbd> <span>as the allowed callback URL.</span></li>
<li>Click on <span class="packt_screen">Show Advanced Settings</span>, go to the <span class="packt_screen">Grant Types</span> tab, deselect <span class="packt_screen">Client Credentials</span>, and select the <span class="packt_screen">Password</span> box.</li>
<li>Click on <span class="packt_screen">SAVE CHANGES</span>.</li>
</ol>
</li>
<li>Now define authorizations for our API:
<ol>
<li>Click on the <span class="packt_screen">APIs</span> tab (on the left) and click on the&nbsp;<span class="packt_screen">+</span> <span class="packt_screen">CREATE API</span> button.</li>
<li>Name the API&nbsp;<kbd>product-composite</kbd>, give it the identifier <kbd>https://localhost:8443/product-composite</kbd>, and click on the <span class="packt_screen">CREATE</span> button.</li>
<li>Click on the <span class="packt_screen">Permissions</span> tab and create two permissions (that is, OAuth scopes) for&nbsp;<span class="sc-kafWEX bPqrTI"><kbd>product:read</kbd> and&nbsp;<kbd>product:write</kbd>.</span></li>
</ol>
</li>
</ol>
<ol start="6">
<li>Next, create a user:
<ol>
<li>Click on the <span class="packt_screen"><span class="packt_screen">Users &amp;</span>&nbsp;Roles</span>&nbsp;<span>and</span> <span class="packt_screen">-&gt; Users</span><span>&nbsp;</span><span>tab (on the left) and then on the</span> <span class="packt_screen">+ CREATE YOUR FIRST USER</span> <span>button.</span></li>
<li>Enter an <kbd>email</kbd> and <kbd>password</kbd> of your preference and click on the <span class="packt_screen">SAVE</span> button.</li>
<li>Look for a verification mail from Auth0 in the <span class="packt_screen">Inbox</span> for the email address you supplied.</li>
</ol>
</li>
<li>Finally, validate your <span class="packt_screen">Default Directory</span> setting, used for the password grant flow:
<ol>
<li>Click on your tenant profile in the upper-right corner and select <span class="packt_screen">Settings</span>.</li>
<li>In the tab named <span class="packt_screen">General</span>, scroll down to the field named&nbsp;<span class="packt_screen">Default Directory</span> <span>and verify that it contains the&nbsp;<kbd>Username-Password-Authentication</kbd>&nbsp;value. If not, update the field and save the change.</span></li>
</ol>
</li>
<li>That's it!&nbsp;<span>Note that both the default app and the API get a client ID and secret. We will use the client ID and secret for the default app; that is, the OAuth client.&nbsp;</span></li>
</ol>
<p>With an Auth0 account created and configured we can move on and apply the necessary configuration&nbsp;changes in the system landscape.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Applying the necessary&nbsp;changes to use&nbsp;Auth0 as an OpenID provider</h1>
                </header>
            
            <article>
                
<p>In this section we will learn what configuration changes are required to be able to replace the local authorization server with Auth0. We only need to change the configuration for the two services that act as OAuth resource servers, the <kbd>product-composite</kbd>,&nbsp;<span>and the&nbsp;</span><kbd>gateway</kbd> services.&nbsp;We also need to change our test script a bit, so that it acquires the access tokens from Auth0 instead of from our&nbsp;local authorization server.&nbsp;Let's start with the&nbsp;<span>OAuth resource servers, that is, the&nbsp;<kbd>product-composite</kbd></span><span>&nbsp;and the&nbsp;</span><span><kbd>gateway</kbd> services.</span></p>
<div class="packt_infobox"><span>The changes described in this topic have&nbsp;</span><strong>not</strong><span>&nbsp;been applied to the source code in the book's Git repository; that is, you need to make them yourself to see them in action!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Applying the necessary changes to use Auth0 as an OpenID provider</h1>
                </header>
            
            <article>
                
<p>In this section we will learn what configuration changes are required to be able to replace the local authorization server with Auth0. We only need to change the configuration for the two services that act as OAuth resource servers, the <kbd>product-composite</kbd>,&nbsp;<span>and the </span><kbd>gateway</kbd> services. We also need to change our test script a bit, so that it acquires the access tokens from Auth0 instead of from our local authorization server. Let's start with the <span>OAuth resource servers, that is, the <kbd>product-composite</kbd></span><span> and the </span><span><kbd>gateway</kbd> services.</span></p>
<div class="packt_infobox"><span>The changes described in this topic have </span><strong>not</strong><span> been applied to the source code in the book's Git repository; that is, you need to make them yourself to see them in action!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changing the test script so it acquires access tokens from Auth0</h1>
                </header>
            
            <article>
                
<p><span>We also need to update the test script so it acquires access tokens from the Auth0 OIDC provider. This is done by performing the following changes in <kbd>test-em-all.bash</kbd>.</span></p>
<p><span>Take the following command</span>:</p>
<pre><strong>ACCESS_TOKEN=<span>$</span>(curl http://writer:secret@$HOST:$PORT/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong></pre>
<p><span>Replace it with this command</span>:</p>
<pre><strong>ACCESS_TOKEN=<span>$</span>(curl --request POST \</strong><br><strong> --url <span>'https://</span><span>${TENANT_DOMAIN_NAME}</span><span>/oauth/token'</span> \</strong><br><strong> --header <span>'content-type: application/json'</span> \</strong><br><strong> --data <span>'{"grant_type":"password", "username":"${USER_EMAIL}", "password":"${USER_PASSWORD}", "audience":"https://localhost:8443/product-composite", "scope":"openid email product:read product:write", "client_id": "</span><span>${CLIENT_ID}</span><span>", "client_secret": "</span><span>${CLIENT_SECRET}</span><span>"}'</span> -s | jq -r .access_token)</strong></pre>
<p><span>Now, replace&nbsp;</span><span><kbd>${TENANT_DOMAIN_NAME}</kbd>, <kbd>${USER_EMAIL}</kbd>, <kbd>${USER_PASSWORD}</kbd>, <kbd>${CLIENT_ID}</kbd>, and <kbd>${CLIENT_SECRET}</kbd> in the preceding command with the values you collected during the registration process in Auth0, as described previously. Then, take the following command</span>:</p>
<pre><strong>READER_ACCESS_TOKEN=<span>$</span>(curl -k https://reader:secret@$HOST:$PORT/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong></pre>
<p>Replace it with this command:</p>
<pre><strong>READER_ACCESS_TOKEN=<span>$</span>(curl --request POST \</strong><br><strong> --url <span>'https://</span><span>${TENANT_DOMAIN_NAME}</span><span>/oauth/token'</span> \</strong><br><strong> --header <span>'content-type: application/json'</span> \</strong><br><strong> --data <span>'{"grant_type":"password", "username":"${USER_EMAIL}", "password":"${USER_PASSWORD}", </span><span>"audience":"https://localhost:8443/product-composite", "scope":"openid email product:read", "client_id": "</span><span>${CLIENT_ID}</span><span>", "client_secret": "</span><span>${CLIENT_SECRET}</span><span>"}'</span> -s | jq -r .access_token)</strong></pre>
<p class="mce-root">Apply the preceding changes to the command. Also note that we only require the <span><kbd>product:read</kbd> scope and not the&nbsp;</span><span><kbd>product:write</kbd></span>&nbsp;scope. <span>This is to simulate a client with read-only access.</span></p>
<p>Now access tokens are issued by Auth0 instead of our local authorization server, and our API implementations can verify that the access tokens (have been correctly signed by Auth0 and have not expired), using information from Auth0's discovery service flagged in the <kbd>application.yml</kbd> files. The API&nbsp;<span>implementations can, as before, use the scopes in the access tokens to authorize the client to perform the call to the API, or not.</span></p>
<p><span>Now we have all the required changes in place, let's run some tests to verify that we can acquire access tokens from Auth0.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changing the test script so it acquires access tokens from Auth0</h1>
                </header>
            
            <article>
                
<p><span>We also need to update the test script so it acquires access tokens from the Auth0 OIDC provider. This is done by performing the following changes in <kbd>test-em-all.bash</kbd>.</span></p>
<p><span>Take the following command</span>:</p>
<pre><strong>ACCESS_TOKEN=<span>$</span>(curl http://writer:secret@$HOST:$PORT/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong></pre>
<p><span>Replace it with this command</span>:</p>
<pre><strong>ACCESS_TOKEN=<span>$</span>(curl --request POST \</strong><br><strong> --url <span>'https://</span><span>${TENANT_DOMAIN_NAME}</span><span>/oauth/token'</span> \</strong><br><strong> --header <span>'content-type: application/json'</span> \</strong><br><strong> --data <span>'{"grant_type":"password", "username":"${USER_EMAIL}", "password":"${USER_PASSWORD}", "audience":"https://localhost:8443/product-composite", "scope":"openid email product:read product:write", "client_id": "</span><span>${CLIENT_ID}</span><span>", "client_secret": "</span><span>${CLIENT_SECRET}</span><span>"}'</span> -s | jq -r .access_token)</strong></pre>
<p><span>Now, replace </span><span><kbd>${TENANT_DOMAIN_NAME}</kbd>, <kbd>${USER_EMAIL}</kbd>, <kbd>${USER_PASSWORD}</kbd>, <kbd>${CLIENT_ID}</kbd>, and <kbd>${CLIENT_SECRET}</kbd> in the preceding command with the values you collected during the registration process in Auth0, as described previously. Then, take the following command</span>:</p>
<pre><strong>READER_ACCESS_TOKEN=<span>$</span>(curl -k https://reader:secret@$HOST:$PORT/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong></pre>
<p>Replace it with this command:</p>
<pre><strong>READER_ACCESS_TOKEN=<span>$</span>(curl --request POST \</strong><br><strong> --url <span>'https://</span><span>${TENANT_DOMAIN_NAME}</span><span>/oauth/token'</span> \</strong><br><strong> --header <span>'content-type: application/json'</span> \</strong><br><strong> --data <span>'{"grant_type":"password", "username":"${USER_EMAIL}", "password":"${USER_PASSWORD}", </span><span>"audience":"https://localhost:8443/product-composite", "scope":"openid email product:read", "client_id": "</span><span>${CLIENT_ID}</span><span>", "client_secret": "</span><span>${CLIENT_SECRET}</span><span>"}'</span> -s | jq -r .access_token)</strong></pre>
<p class="mce-root">Apply the preceding changes to the command. Also note that we only require the <span><kbd>product:read</kbd> scope and not the </span><span><kbd>product:write</kbd></span> scope. <span>This is to simulate a client with read-only access.</span></p>
<p>Now access tokens are issued by Auth0 instead of our local authorization server, and our API implementations can verify that the access tokens (have been correctly signed by Auth0 and have not expired), using information from Auth0's discovery service flagged in the <kbd>application.yml</kbd> files. The API <span>implementations can, as before, use the scopes in the access tokens to authorize the client to perform the call to the API, or not.</span></p>
<p><span>Now we have all the required changes in place, let's run some tests to verify that we can acquire access tokens from Auth0.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running the test script with Auth0 as the OpenID Connect provider</h1>
                </header>
            
            <article>
                
<p>Now, we are ready to give Auth0 a try!</p>
<p>Run the usual tests against Auth0 with the following command:</p>
<pre><strong>./test-em-all.bash</strong></pre>
<p>In the logs (using the <kbd>docker-compose logs -f product-composite</kbd>&nbsp;<span>command),</span> you will be able to find authorization information from the access tokens issued by Auth0:</p>
<p><span>From calls using an access token with both the <kbd>product:read</kbd> and <kbd>product:write</kbd> scopes, we will see that both scopes are listed as follows: </span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9c98cb07-9e00-4611-8c25-0db0aa384d1d.png" style="width:41.75em;height:6.50em;" width="1817" height="281" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9c98cb07-9e00-4611-8c25-0db0aa384d1d.png"></p>
<p><span>From</span><span> calls using an access token with only the <kbd>product:read</kbd> scope, we will see that only that scope is listed as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/568e2fc6-35fa-43e1-8247-1d2b9cad167f.png" style="width:42.25em;height:6.50em;" width="1813" height="279" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/568e2fc6-35fa-43e1-8247-1d2b9cad167f.png"></p>
<div class="packt_infobox">As we can see from the log output, we now get information regarding the intended audience for this access token. To strengthen security, we could add a test to our service that verifies that its URL, <kbd>https://localhost:8443/product-composite</kbd> in this case, is part of the audience list. This would prevent the situation where someone tries to use an access token issued for another purpose to get access to our API.</div>
<p>With the automated tests working together with Auth0, we can move on and learn how to acquire access tokens using the different types of grant flow. Let's start with the password grant flow.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Acquiring access tokens using the password grant flow</h1>
                </header>
            
            <article>
                
<p>In this section we will learn how to acquire an access token from Auth0 using the password grant flow. </p>
<p>If you want to acquire an access token from Auth0 yourself, you can do so by running the following command:</p>
<pre><strong>curl --request POST \</strong><br><strong> --url <span>'https://</span><span>${TENANT_DOMAIN_NAME}</span><span>/oauth/token'</span> \</strong><br><strong> --header <span>'content-type: application/json'</span> \</strong><br><strong> --data <span>'{"grant_type":"password", "username":"${USER_EMAIL}", "password":"${USER_PASSWORD}", "audience":"https://localhost:8443/product-composite", "scope":"openid email product:read", "client_id": "</span><span>${CLIENT_ID}</span><span>", "client_secret": "</span><span>${CLIENT_SECRET}</span><span>"}'</span> -s | jq</strong></pre>
<p>Following the instruction in the <em>Calling protected APIs using the access tokens</em>&nbsp;<span>section, </span>you should be able to call the APIs using the acquired access token. The next grant flow we'll try out is the implicit grant flow.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Acquiring access tokens using the implicit grant flow</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will learn how to acquire an access token from Auth0 using the implicit grant flow. </span></p>
<p>If you want to try out the more involved implicit grant flow, you can open the following URL in a web browser:</p>
<pre>https://<span>${TENANT_DOMAIN_NAME}</span>/authorize?response_type=token&amp;scope=openid email product:read product:write&amp;client_id=<span>${CLIENT_ID}</span>&amp;state=98421&amp;&amp;nonce=jxdlsjfi0fa&amp;redirect_uri=http://my.redirect.uri&amp;audience=https://localhost:8443/product-composite</pre>
<div class="packt_tip"><span>Replace </span><span><kbd>${TENANT_DOMAIN_NAME}</kbd>&nbsp;</span><span>and <kbd>${CLIENT_ID}</kbd> in the preceding URL with the tenant domain name and client ID you collected during the registration process in Auth0 as described previously.</span></div>
<p class="mce-root">Let's have a look at the following steps:</p>
<ol>
<li>Auth0 should present the following login screen:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/51fa7e05-b536-4ae8-8108-403cb016fa7f.png" style="width:41.08em;height:33.67em;" width="1511" height="1239" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/51fa7e05-b536-4ae8-8108-403cb016fa7f.png"></p>
<ol start="2">
<li>Following a successful login, Auth0 will ask you to give the client application your consent:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/88af32b9-cb29-4cea-afba-1e337b85d14f.png" style="width:47.83em;height:42.25em;" width="1547" height="1367" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/88af32b9-cb29-4cea-afba-1e337b85d14f.png"></p>
<p>The access token is now in the URL in the browser, just like when we tried out the implicit flow in our local authorization server:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2124688f-d39d-431b-982b-6f5f786ac8d7.png" style="width:31.92em;height:13.08em;" width="658" height="267" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2124688f-d39d-431b-982b-6f5f786ac8d7.png"></p>
<div class="packt_infobox"><span>To get an access token that corresponds to the <kbd>reader</kbd> client, remove the</span> <kbd>product:write</kbd><span> scope </span>from the preceding URL that we used to initiate the implicit grant flow.</div>
<p>Now that we know how to acquire an access token using the implicit grant flow, we can move on to the third and last grant flow that we will try out, the <span>authorization c</span><span>ode grant flow.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Acquiring access tokens using the authorization code grant flow</h1>
                </header>
            
            <article>
                
<p class="mce-root">Finally, we come to the most secure grant flow â the<span> authorization c</span>ode grant flow. We will follow the same procedure as with the local authorization server; that is, we first acquire code and then exchange it for an access token. Get the code by opening the following URL in a web browser, as follows:</p>
<pre>https://${TENANT_DOMAIN_NAME}/authorize?audience=https://localhost:8443/product-composite&amp;scope=openid email product:read product:write&amp;response_type=code&amp;client_id=${CLIENT_ID}&amp;redirect_uri=http://my.redirect.uri&amp;state=845361</pre>
<div class="packt_infobox"><span>Replace </span><span><kbd>${TENANT_DOMAIN_NAME}</kbd>&nbsp;</span><span>and <kbd>${CLIENT_ID}</kbd> in the preceding URL with the tenant domain name and client ID you collected during the registration process in Auth0 as described previously.</span></div>
<p>Expect a redirect attempt in the web browser to a URL as follows:</p>
<pre>http://my.redirect.uri/?code=6mQ7HK--WyX9fMnv&amp;state=845361</pre>
<p>Extract the code and run the following command to get the access token:</p>
<pre><strong>curl --request POST \</strong><br><strong>   --url 'https://${TENANT_DOMAIN_NAME}/oauth/token' \</strong><br><strong>   --header 'content-type: application/json' \</strong><br><strong>   --data '{"grant_type":"authorization_code","client_id": "${CLIENT_ID}","client_secret": "${CLIENT_SECRET}","code": "${CODE}","redirect_uri": "http://my.redirect.uri"}' -s | jq .</strong></pre>
<p><span>Replace </span><span><kbd>${TENANT_DOMAIN_NAME}</kbd>,&nbsp;<kbd>${CLIENT_ID}</kbd>,&nbsp;<kbd>${CLIENT_SECRET}</kbd>, and <kbd>${CODE}</kbd>&nbsp;</span><span>in the preceding URL with the tenant domain name, client ID, and the client code you collected during the registration process in Auth0 as described previously.</span></p>
<p>Now that we have learned how to acquire access tokens using all three types of grant flows, we are ready to try calling the external API using an access token acquired from Auth0 in the next section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Getting extra information about the user</h1>
                </header>
            
            <article>
                
<p>As you can see in the log output, the ID of the subject, that is, the user, is a bit cryptic, for example,&nbsp;<kbd>auth0|5ca0b73c97f31e11bc85a5e6</kbd>.</p>
<p>If you want your API implementation to know a bit more about the user, it can call Auth0's&nbsp;<kbd><span>userinfo_endpoint</span></kbd>&nbsp;as described in the response to the discovery request made previously:</p>
<pre><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" https://<span>${TENANT_DOMAIN_NAME}</span>/userinfo -s | jq</strong></pre>
<p><span>Replace&nbsp;</span><span><kbd>${TENANT_DOMAIN_NAME}</kbd>&nbsp;</span><span>in the preceding command with the tenant domain name you collected during the registration process in Auth0 as described previously.</span></p>
<p>A sample response is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/85766cca-2c09-4b6a-9ead-a450d32154af.png" style="width:16.50em;height:9.92em;" width="548" height="328" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/85766cca-2c09-4b6a-9ead-a450d32154af.png"></p>
<div class="packt_infobox">This endpoint can also be used to verify that the user hasn't revoked the access token in Auth0.</div>
<div>
<p>&nbsp;Wrap up the tests by shutting down the system landscape with the following command:</p>
<pre><strong>docker-compose down<br></strong></pre>
<p><span>This concludes the section where we have learned how to replace the local OAuth 2.0 Authorization server, only used for tests, with an external alternative. We have also seen how to reconfigure the microservice landscape to validate access tokens using an external&nbsp;</span><span>OIDC provider</span><span>.</span></p>
</div>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Getting extra information about the user</h1>
                </header>
            
            <article>
                
<p>As you can see in the log output, the ID of the subject, that is, the user, is a bit cryptic, for example, <kbd>auth0|5ca0b73c97f31e11bc85a5e6</kbd>.</p>
<p>If you want your API implementation to know a bit more about the user, it can call Auth0's <kbd><span>userinfo_endpoint</span></kbd> as described in the response to the discovery request made previously:</p>
<pre><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" https://<span>${TENANT_DOMAIN_NAME}</span>/userinfo -s | jq</strong></pre>
<p><span>Replace </span><span><kbd>${TENANT_DOMAIN_NAME}</kbd>&nbsp;</span><span>in the preceding command with the tenant domain name you collected during the registration process in Auth0 as described previously.</span></p>
<p>A sample response is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/85766cca-2c09-4b6a-9ead-a450d32154af.png" style="width:16.50em;height:9.92em;" width="548" height="328" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/85766cca-2c09-4b6a-9ead-a450d32154af.png"></p>
<div class="packt_infobox">This endpoint can also be used to verify that the user hasn't revoked the access token in Auth0.</div>
<div>
<p> Wrap up the tests by shutting down the system landscape with the following command:</p>
<pre><strong>docker-compose down<br></strong></pre>
<p><span>This concludes the section where we have learned how to replace the local OAuth 2.0 Authorization server, only used for tests, with an external alternative. We have also seen how to reconfigure the microservice landscape to validate access tokens using an external </span><span>OIDC provider</span><span>.</span></p>
</div>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we have learned how to use Spring Security to protect our APIs.</span></p>
<div>
<p><span>We have seen how easy it is to enable HTTPS to prevent eavesdropping by third parties using Spring Security. With Spring Security, we have also learned that it is straightforward to restrict access and the discovery server, Netflix Eureka, using HTTP basic authentication. Finally, we have seen how we can use Spring Security to simplify the use of OAuth 2.0 and OpenID Connect to allow third-party client applications to access our APIs in the name of a user, but without requiring that the user share credentials with the client application. We have learned both how to set up a local OAuth 2.0 authorization server based on Spring Security and also how to change the configuration so that an external OpenID Connect provider, Auth0, can be used instead.</span></p>
</div>
<p>One concern, however, is how to manage the configuration required. Many small pieces of configuration must be set up for the microservices <span>involved </span>and the configuration must be synchronized to match. Added to the scattered configuration is the fact that some of the configuration contains sensitive information, such as credentials or certificates. It seems like we need a better way to handle the configuration for a number of cooperating microservices and also a solution for how to handle sensitive parts of the configuration.</p>
<p>In the next chapter, we will explore the Spring Cloud Configuration server and see how it can be used to handle these types of requirement.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Centralized Configuration</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to use the Spring Cloud Configuration server to centralize managing the configuration of our microservices. As already described in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em>, in the <em>Central configuration</em> section, an increasing number of microservices typically come with an increasing number of configuration files that need to be managed and updated.</p>
<p>With the&nbsp;<span>Spring Cloud Configuration server, we can place the configuration files for all our microservices in a central configuration repository that will make it much easier to handle them. Our microservices will be updated to retrieve their configuration from the configuration server at startup.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introduction to the Spring Cloud Configuration server</li>
<li>Setting up a config server</li>
<li>Configuring clients of a config server</li>
<li>Structuring the configuration repository</li>
<li>Trying out the Spring Cloud Configuration server</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All the commands described in this book<span>&nbsp;are run on a MacBook Pro using macOS Mojave but should be straightforward enough to modify in order to be able to run on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p>The source code for this chapter can be found <span>on GitHub at</span><span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter12">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter12</a>.</p>
<p class="mce-root"></p>
<p>To be able to run the commands as described in the book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Sample commands include the following:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter12</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1.0, SR1 (also known as the <span><strong>Greenwich</strong>&nbsp;release</span>), Spring Boot 2.1.4, and Spring 5.1.6, that is, the latest available versions of the Spring components at the time of writing this chapter.</p>
<p>The base Docker image, <kbd>openjdk:12.0.2</kbd><span>, is used in all Dockerfiles.</span></p>
<p>The source code contains the following Gradle projects:</p>
<ul>
<li><kbd>api</kbd></li>
<li><kbd>util</kbd></li>
<li><kbd>microservices/product-service</kbd></li>
<li><kbd>microservices/review-service</kbd></li>
<li><kbd>microservices/recommendation-service</kbd></li>
<li><kbd>microservices/product-composite-service</kbd></li>
<li><kbd>spring-cloud/eureka-server</kbd></li>
<li><kbd>spring-cloud/gateway</kbd></li>
<li><kbd>spring-cloud/authorization-server</kbd></li>
<li><kbd>spring-cloud/config-server</kbd></li>
</ul>
<p>All source code examples in this chapter come from the source code in <span><kbd>$BOOK_HOME/Chapter12</kbd>, but are, in several cases, edited to remove non-relevant parts of the source code, such as comments, import statements, and log statements.</span></p>
<p>If you want to see the changes applied to the source code in <a href="a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml">Chapter 12</a>, <em>Centralized Configuration</em>, that is, see&nbsp;<span>what it took to add a configuration server,&nbsp;</span>you can compare that with the source code for <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml">Chapter 11</a>, <em>Securing Access to APIs</em>.&nbsp;<span>You can use your favorite</span> <kbd>diff</kbd> <span>tool and compare the two folders,</span> <kbd>$BOOK_HOME/Chapter11</kbd> <span>and</span> <kbd>$BOOK_HOME/Chapter12</kbd><span>.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All the commands described in this book<span> are run on a MacBook Pro using macOS Mojave but should be straightforward enough to modify in order to be able to run on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p>The source code for this chapter can be found <span>on GitHub at</span><span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter12">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter12</a>.</p>
<p class="mce-root"></p>
<p>To be able to run the commands as described in the book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Sample commands include the following:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter12</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1.0, SR1 (also known as the <span><strong>Greenwich</strong> release</span>), Spring Boot 2.1.4, and Spring 5.1.6, that is, the latest available versions of the Spring components at the time of writing this chapter.</p>
<p>The base Docker image, <kbd>openjdk:12.0.2</kbd><span>, is used in all Dockerfiles.</span></p>
<p>The source code contains the following Gradle projects:</p>
<ul>
<li><kbd>api</kbd></li>
<li><kbd>util</kbd></li>
<li><kbd>microservices/product-service</kbd></li>
<li><kbd>microservices/review-service</kbd></li>
<li><kbd>microservices/recommendation-service</kbd></li>
<li><kbd>microservices/product-composite-service</kbd></li>
<li><kbd>spring-cloud/eureka-server</kbd></li>
<li><kbd>spring-cloud/gateway</kbd></li>
<li><kbd>spring-cloud/authorization-server</kbd></li>
<li><kbd>spring-cloud/config-server</kbd></li>
</ul>
<p>All source code examples in this chapter come from the source code in <span><kbd>$BOOK_HOME/Chapter12</kbd>, but are, in several cases, edited to remove non-relevant parts of the source code, such as comments, import statements, and log statements.</span></p>
<p>If you want to see the changes applied to the source code in <a href="a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml">Chapter 12</a>, <em>Centralized Configuration</em>, that is, see <span>what it took to add a configuration server, </span>you can compare that with the source code for <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml">Chapter 11</a>, <em>Securing Access to APIs</em>.&nbsp;<span>You can use your favorite</span> <kbd>diff</kbd> <span>tool and compare the two folders,</span> <kbd>$BOOK_HOME/Chapter11</kbd> <span>and</span> <kbd>$BOOK_HOME/Chapter12</kbd><span>.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Selecting the storage type of the configuration repository</h1>
                </header>
            
            <article>
                
<p>As already described in&nbsp;<a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml">Chapter 8</a>, <em>Introduction to Spring Cloud</em>, in the <em>Spring Cloud Config for centralized configuration</em> section, the config server supports storing configuration files in a number of different backends, for example:</p>
<ul>
<li>Git repository</li>
<li>Local filesystem</li>
<li>HashiCorp Vault</li>
<li>A JDBC database</li>
</ul>
<p>In this chapter, we will use a local filesystem. To use the local filesystem, the config server needs to be launched with the Spring profile,&nbsp;<kbd>native</kbd>, enabled. The location of the configuration repository is specified using the property, <span><kbd>spring.cloud.config.server.native.searchLocations</kbd>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Selecting the storage type of the configuration repository</h1>
                </header>
            
            <article>
                
<p>As already described in <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml">Chapter 8</a>, <em>Introduction to Spring Cloud</em>, in the <em>Spring Cloud Config for centralized configuration</em> section, the config server supports storing configuration files in a number of different backends, for example:</p>
<ul>
<li>Git repository</li>
<li>Local filesystem</li>
<li>HashiCorp Vault</li>
<li>A JDBC database</li>
</ul>
<p>In this chapter, we will use a local filesystem. To use the local filesystem, the config server needs to be launched with the Spring profile, <kbd>native</kbd>, enabled. The location of the configuration repository is specified using the property, <span><kbd>spring.cloud.config.server.native.searchLocations</kbd>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deciding on the initial client connection</h1>
                </header>
            
            <article>
                
<p>By default, a client connects first to the config server to retrieve its configuration. Based on the configuration, it connects to the discovery server, that is, Netflix Eureka in our case, to register itself. It is also possible to do this the other way around, that is, the client first connects to the <span>discovery server to find a config server instance and then connects to the config server to get its configuration. There are pros and cons to both approaches.</span></p>
<p><span>In this chapter, the clients will first connect to the config server. With this approach, it will be possible to store the configuration of the discovery server, that is, Netflix Eureka, in the config server.</span></p>
<p><span>To learn more about the other alternative, see <a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-config/2.1.0.RELEASE/single/spring-cloud-config.html#_environment_repository">https://cloud.spring.io/spring-cloud-static/spring-cloud-config/2.1.0.RELEASE/single/spring-cloud-config.html#discovery-first-bootstrap</a></span><span>.</span></p>
<p class="mce-root"></p>
<div class="packt_infobox"><span>One concern with connecting to the config server first is that the </span><span>config server </span><span>can become a single point of failure. If the clients connect first to a discovery service, such as Netflix Eureka, there can be multiple config server instances registered, so that a single point of failure can be avoided. When, later on in this book, we learn about the </span><em>service</em><span> concept in Kubernetes, we will see how we can avoid a single point of failure by running multiple containers, for example, config servers, behind each Kubernetes service.</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Securing the configuration in transit</h1>
                </header>
            
            <article>
                
<p>When the&nbsp;<span>configuration information&nbsp;is asked for by a microservice, or anyone using the API of the config server, it will be protected against eavesdropping by the edge server since it already uses HTTPS.</span></p>
<p><span>To ensure that the API user is a known client, we will use HTTP basic authentication. We can set up&nbsp;HTTP basic authentication by using Spring Security in the config server and specifying the environment variables,&nbsp;<kbd>SPRING_SECURITY_USER_NAME</kbd> and <kbd>SPRING_SECURITY_USER_PASSWORD</kbd>, with the permitted credentials.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Securing the configuration in transit</h1>
                </header>
            
            <article>
                
<p>When the <span>configuration information is asked for by a microservice, or anyone using the API of the config server, it will be protected against eavesdropping by the edge server since it already uses HTTPS.</span></p>
<p><span>To ensure that the API user is a known client, we will use HTTP basic authentication. We can set up HTTP basic authentication by using Spring Security in the config server and specifying the environment variables, <kbd>SPRING_SECURITY_USER_NAME</kbd> and <kbd>SPRING_SECURITY_USER_PASSWORD</kbd>, with the permitted credentials.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Securing the configuration at rest</h1>
                </header>
            
            <article>
                
<p>To avoid a situation whereby anyone with access to the configuration repository can steal sensitive information, such as passwords, the config server supports encryption of configuration information when stored on disk. The config server supports using both symmetric and asymmetric keys. Asymmetric keys are more secure but harder to manage.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>In this chapter, we will use a symmetric key. The symmetric key is given to the config server at startup by specifying an environment variable, <kbd>ENCRYPT_KEY</kbd>. The encrypted key is just a plain text string that needs to be protected in the same way as any sensitive information.</p>
<p><span>To learn more about the use of asymmetric keys, see </span><a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-config/2.1.0.RELEASE/single/spring-cloud-config.html#_key_management">https://cloud.spring.io/spring-cloud-static/spring-cloud-config/2.1.0.RELEASE/single/spring-cloud-config.html#_key_management</a>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing the config server API</h1>
                </header>
            
            <article>
                
<p><span>The config server exposes a REST API that can be used by its clients to retrieve their configuration. In this chapter, we will use the following endpoints in the API:</span></p>
<ul>
<li><kbd>/actuator</kbd>: The standard actuator endpoints exposed by all microservices.<br>
As always, these should be used with care. They are very useful during development but must be locked down before being used in production.</li>
<li><kbd>/encrypt</kbd> and <kbd>/decrypt</kbd>: Endpoints for encrypting and decrypting sensitive information. These must also be locked down before being used in production.</li>
<li><kbd>/{microservice}/{profile}</kbd>: Returns the configuration for the specified microservice and the specified Spring profile.</li>
</ul>
<p>We will see some sample uses for the APIs when we try out the config server.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up a config server</h1>
                </header>
            
            <article>
                
<p><span>S</span><span>etting up a</span> config server on the basis of the decisions discussed <span>is straightforward:</span></p>
<ol>
<li>Create a Spring Boot project using Spring Initializr as described in <a href="d26f4e61-20bf-4f55-b96d-060c7dd6f20c.xhtml" target="_blank">Chapter 3</a>, <em>Creating a Set of Cooperating Microservices</em>. Refer to the <span><em>Using Spring Initializr to generate skeleton</em> <em>code</em> section.</span></li>
<li>Add the dependencies, <kbd>spring-cloud-config-server</kbd> and <kbd>spring-boot-starter-security</kbd>, to the Gradle build file, <kbd>build.gradle</kbd>.</li>
</ol>
<ol start="3">
<li>Add the annotation, <kbd>@EnableConfigServer</kbd>, to the application class, <kbd>ConfigServerApplication</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>@EnableConfigServer<br></span><span>@SpringBootApplication<br></span><span>public class </span>ConfigServerApplication {</pre>
<ol start="4">
<li>Add the configuration for the config server to the default property file, <kbd>application.yml</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>server.port</span>: <span>8888<br></span><span><br></span><span>spring.cloud.config.server.native.searchLocations</span>: file:${<span>PWD</span>}/config-repo<br><span><br></span><span>management.endpoint.health.show-details</span>: <span>"</span><span>ALWAYS</span><span>"<br></span><span>management.endpoints.web.exposure.include</span>: <span>"*"<br></span><span><br></span><span>logging.</span><span>level.</span><span>root</span>: info<br>---<br><span>spring.profiles</span>: docker<br><span>spring.cloud.config.server.native.searchLocations</span>: file:/config-repo<br><br></pre>
<p style="padding-left: 60px">The most important configuration is to specify where to find the configuration repository, specified by the <kbd>spring.cloud.config.server.native.searchLocations</kbd> property.</p>
<ol start="5">
<li>Add a routing rule to the edge server to make the API of the config server accessible from outside the microservice landscape. </li>
<li>Add a Dockerfile and a definition of the config server to the three Docker Compose files.</li>
<li>Externalize sensitive configuration parameters to the standard Docker Compose environment file, <kbd>.env</kbd></li>
<li>Add the <span>config server to the common build file, <kbd>settings.gradle</kbd></span><span><span>:</span></span></li>
</ol>
<pre style="padding-left: 60px">include <span>':spring-cloud:config-server'</span></pre>
<p><span>The source code for the</span> <span>Spring Cloud</span> Configuration server can be found <span>in</span> <span><kbd>$BOOK_HOME/Chapter12/spring-cloud/config-server</kbd>.&nbsp;</span></p>
<p>Now, let's look a bit more into how to set up the routing rule and how to configure the config server for use in Docker.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up a routing rule in the edge server </h1>
                </header>
            
            <article>
                
<p>To be able to access the API of the config server from outside the microservice landscape, we add a routing rule to the edge server. All requests to the edge server that begin with <kbd>/config</kbd> will be routed to the config server with the following routing rule:</p>
<pre> - <span>id</span>: config-server<br>   <span>uri</span>: http://${<span>app.config-server</span>}:8888<br>  <span>predicates</span>:<br>  - Path=/config/**<br>  <span>filters</span>:<br>  - RewritePath=/config/(?&lt;segment&gt;.*), /$\{segment}</pre>
<p>The <kbd>RewritePath</kbd> filter in the preceding routing rule will remove the leading part, <kbd>/config</kbd>, from the incoming URL before it sends it to the config server.</p>
<p>With this routing rule in place, we can use the API of the config server; for example, run the following command to ask for the configuration of the product service when it uses the Docker Spring profile:</p>
<pre><strong>curl https://dev-usr:dev-pwd@localhost:8443/config/product/docker -ks | jq</strong></pre>
<p>We will run the preceding command when we try out the config server.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Configuring the config server for use with Docker</h1>
                </header>
            
            <article>
                
<p>The Dockerfile of the config server looks the same as for the other microservices, except for the fact that it exposes port <kbd>8888</kbd> instead of port <kbd>8080</kbd>.</p>
<p>When it comes to adding the config server to the Docker Compose files, it looks a bit different from what we have seen for the other microservices:</p>
<pre><span>config-server</span>:<br>  <span>environment</span>:<br>    - SPRING_PROFILES_ACTIVE=docker,native<br>    - ENCRYPT_KEY=${CONFIG_SERVER_ENCRYPT_KEY}<br>    - SPRING_SECURITY_USER_NAME=${CONFIG_SERVER_USR}<br>    - SPRING_SECURITY_USER_PASSWORD=${CONFIG_SERVER_PWD}<br>  <span>volumes</span>:<br>    - $PWD/config-repo:/config-repo<br>  <span>build</span>: spring-cloud/config-server<br>  mem_limit: 350m</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>Here are the explanations for the preceding source code:</p>
<ol>
<li>The Spring profile, <kbd>native</kbd>, is added to signal to the config server that the config repository is based on plain files; in other words, it is not a Git repository.</li>
<li>The environment variable <kbd>ENCRYPT_KEY</kbd> is used to specify the symmetric encryption key that shall be used by the config server to encrypt and decrypt sensitive configuration information.</li>
<li><span><span>The environment variables, <kbd>SPRING_SECURITY_USER_NAME</kbd> and <kbd>SPRING_SECURITY_USER_PASSWORD</kbd>, are used to specify the credentials to be used for protecting the APIs using basic HTTP authentication.</span></span></li>
<li><span><span><span>The volume declaration will make the <kbd>config-repo</kbd> folder accessible in the Docker container at </span></span></span><kbd>/config-repo</kbd>.</li>
</ol>
<p>The values of the three preceding <span>environment variables defined are fetched by Docker Compose from the <kbd>.env</kbd> file:</span></p>
<pre>CONFIG_SERVER_ENCRYPT_KEY=my-very-secure-encrypt-key<br>CONFIG_SERVER_USR=dev-usr<br>CONFIG_SERVER_PWD=dev-pwd</pre>
<div class="packt_infobox">The information stored in the <kbd>.env</kbd> file, that is, the username, password, and encryption key, is sensitive information and must be protected if used for something other than development and testing. Also, note that losing the encryption key will lead to a situation whereby the encrypted information in the config repository cannot be decrypted! </div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Configuring clients of a config server</h1>
                </header>
            
            <article>
                
<p><span>To be able to get their configurations from the config server, our microservices need to be updated. This can be done through the following steps:</span></p>
<ol>
<li>Add the <kbd>spring-cloud-starter-config</kbd>, and<span><span>&nbsp;<kbd>spring-retry</kbd> dependencies </span></span>to the Gradle build file,<span>&nbsp;</span><kbd>build.gradle</kbd>.</li>
<li>Move the configuration file, <kbd>application.yml</kbd>, to the config repository and rename it to the name of the client as specified by the property, <kbd>spring.application.name</kbd>.</li>
</ol>
<ol start="3">
<li>Add a file named <kbd>bootstrap.yml</kbd> to the <kbd>src/main/resources</kbd>&nbsp;<span>folder. </span>This file holds the configuration required to connect to the config server. Refer to the following for an explanation of its content.</li>
<li>Add credentials for accessing the config server to the Docker Compose files, for example, the <kbd>product</kbd> service:</li>
</ol>
<pre style="padding-left: 60px"><span>product</span>:<br>  <span>environment</span>:<br> - CONFIG_SERVER_USR=${CONFIG_SERVER_USR}<br> - CONFIG_SERVER_PWD=${CONFIG_SERVER_PWD}</pre>
<ol start="5">
<li>Disable the use of the config server when running Spring Boot-based automated tests. This is done by adding<span>&nbsp;</span><kbd>spring.cloud.config.enabled=false</kbd><span>&nbsp;</span>to the <kbd><span>@D</span>ataMongoTest</kbd><span>,&nbsp;</span><span><kbd>@DataJpaTest</kbd></span>, and <kbd>@SpringBootTest</kbd> annotations.  For example, execute the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>@DataMongoTest(properties = {"spring.cloud.config.enabled=false"})</strong><br><br><strong><span>@DataJpaTest</span>(properties = {<span>"spring.cloud.config.enabled=false"</span>})</strong><br><br><strong>@SpringBootTest(webEnvironment=RANDOM_PORT, properties = {"eureka.client.enabled=false", "spring.cloud.config.enabled=false"})</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Moving the partitioning configuration from Docker Compose files to the configuration repository</h1>
                </header>
            
            <article>
                
<p><span>The Docker Compose files,&nbsp;<kbd>docker-compose-partitions.yml</kbd>, and&nbsp;<kbd>docker-compose-kafka.yml</kbd>, contain some extra configuration for handling&nbsp;partitions in the message brokers, RabbitMQ and Kafka. For details, refer to the&nbsp;<em>Guaranteed order and partitions</em>&nbsp;section in <a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml">Chapter 7</a>, <em>Developing Reactive Microservices</em>. This configuration has also been moved to the centralized configuration repository.&nbsp;</span></p>
<p class="mce-root"></p>
<p>For example, in&nbsp;<span><kbd>docker-compose-kafka.yml</kbd>,&nbsp;</span>the configuration for the product consumer that reads messages from the first partition in the product topic in Kafka appears as follows:</p>
<pre>  product:<br>    environment:<br>      - SPRING_PROFILES_ACTIVE=docker<br>      - MANAGEMENT_HEALTH_RABBIT_ENABLED=false<br>      - SPRING_CLOUD_STREAM_DEFAULTBINDER=kafka<br>      - SPRING_CLOUD_STREAM_BINDINGS_INPUT_CONSUMER_PARTITIONED=true<br>      - SPRING_CLOUD_STREAM_BINDINGS_INPUT_CONSUMER_INSTANCECOUNT=2<br>      - SPRING_CLOUD_STREAM_BINDINGS_INPUT_CONSUMER_INSTANCEINDEX=0</pre>
<p>This configuration has been structured into a number of Spring profiles for increased reusability and moved to the corresponding configuration files in the configuration repository. The Spring profiles added are as follows:</p>
<ul>
<li><kbd>streaming_partitioned</kbd>&nbsp;contains properties for enabling the use of partitions in a message broker.</li>
<li><kbd>streaming_instance_0</kbd>&nbsp;<span>contains properties required for consuming messages from the first partition.</span></li>
<li><kbd>streaming_instance_1</kbd><span>&nbsp;</span><span>contains properties required for consuming messages from the second partition.</span></li>
<li><kbd>kafka</kbd>&nbsp;<span>contains properties that are specific for the use of Kafka as the messaging broker.</span></li>
</ul>
<p>The following configuration has been added to the configuration files of the message consumers, that is, the product, review, and recommendation services:</p>
<pre>---<br><span>spring.profiles</span>: streaming_partitioned<br><span>spring.cloud.stream.bindings.input.consumer</span>:<br>  <span>partitioned</span>: true<br>  <span>instanceCount</span>: 2<br><br>---<br><span>spring.profiles</span>: streaming_instance_0<br><span>spring.cloud.stream.bindings.input.consumer.instanceIndex</span>: 0<br><br>---<br><span>spring.profiles</span>: streaming_instance_1<br><span>spring.cloud.stream.bindings.input.consumer.instanceIndex</span>: 1<br><br>---<br><span>spring.profiles</span>: kafka<br><br><span>management.health.rabbit.enabled</span>: false<br><span>spring.cloud.stream.defaultBinder</span>: kafka</pre>
<p><span>The following configuration has been added to the configuration file of the message producer, that is, the product-composite service:</span></p>
<pre>---<br><span>spring.profiles</span>: streaming_partitioned<br><br><span>spring.cloud.stream.bindings.output-products.producer</span>:<br>  <span>partition-key-expression</span>: payload.key<br>  <span>partition-count</span>: 2<br><br><span>spring.cloud.stream.bindings.output-recommendations.producer</span>:<br>  <span>partition-key-expression</span>: payload.key<br>  <span>partition-count</span>: 2<br><br><span>spring.cloud.stream.bindings.output-reviews.producer</span>:<br>  <span>partition-key-expression</span>: payload.key<br>  <span>partition-count</span>: 2<br><br>---<br><span>spring.profiles</span>: kafka<br><br><span>management.health.rabbit.enabled</span>: false<br><span>spring.cloud.stream.defaultBinder</span>: kafka</pre>
<p>The Docker Compose files are now cleaner and only contain the configuration of credentials for accessing the configurations server and a list of Spring profiles to activate. For example, t<span>he configuration for the product consumer that read messages from the first partition in the product topic in Kafka is now reduced to the following:</span></p>
<pre>product:<br>    environment:<br>      - SPRING_PROFILES_ACTIVE=docker,streaming_partitioned,streaming_instance_0,kafka<br>      - CONFIG_SERVER_USR=${CONFIG_SERVER_USR}<br>      - CONFIG_SERVER_PWD=${CONFIG_SERVER_PWD}</pre>
<p><span>For the full source code, refer to the following:&nbsp;</span></p>
<ul>
<li><kbd>docker-compose-partitions.yml</kbd><span>&nbsp;</span></li>
<li><kbd>docker-compose-kafka.yml</kbd></li>
<li><kbd>config-repo/product-composite.yml</kbd></li>
<li><span><kbd>config-repo/product.yml</kbd><br></span></li>
<li><kbd><span>config-repo/</span>recommendation.yml</kbd></li>
<li><kbd><span>config-repo/</span>review.yml</kbd></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Structuring the configuration repository</h1>
                </header>
            
            <article>
                
<p>After moving the configuration files from each client to the configuration repository, we will have some level of consistent configuration in many of the configuration files, for example, for the configuration of actuator endpoints and how to connect to Eureka, RabbitMQ, and Kafka. The common parts have been placed in a configuration file named <kbd>application.yml</kbd> that is shared by all clients. The configuration repository contains the following files:</p>
<ul>
<li><kbd>application.yml</kbd></li>
<li><kbd>eureka-server.yml</kbd></li>
<li><kbd>product-composite.yml</kbd></li>
<li><kbd>recommendation.yml</kbd></li>
<li><kbd>auth-server.yml</kbd></li>
<li><kbd>gateway.yml</kbd></li>
<li><kbd>product.yml</kbd></li>
<li><kbd>review.yml</kbd></li>
</ul>
<p>The&nbsp;<span>configuration repository</span><span>&nbsp;can be found&nbsp;</span><span>in&nbsp;</span><span><kbd>$BOOK_HOME/Chapter12/config-repo</kbd>.&nbsp;</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Structuring the configuration repository</h1>
                </header>
            
            <article>
                
<p>After moving the configuration files from each client to the configuration repository, we will have some level of consistent configuration in many of the configuration files, for example, for the configuration of actuator endpoints and how to connect to Eureka, RabbitMQ, and Kafka. The common parts have been placed in a configuration file named <kbd>application.yml</kbd> that is shared by all clients. The configuration repository contains the following files:</p>
<ul>
<li><kbd>application.yml</kbd></li>
<li><kbd>eureka-server.yml</kbd></li>
<li><kbd>product-composite.yml</kbd></li>
<li><kbd>recommendation.yml</kbd></li>
<li><kbd>auth-server.yml</kbd></li>
<li><kbd>gateway.yml</kbd></li>
<li><kbd>product.yml</kbd></li>
<li><kbd>review.yml</kbd></li>
</ul>
<p>The <span>configuration repository</span><span> can be found </span><span>in </span><span><kbd>$BOOK_HOME/Chapter12/config-repo</kbd>.&nbsp;</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the Spring Cloud Configuration server</h1>
                </header>
            
            <article>
                
<p>Now it is time to try out the config server:</p>
<ol>
<li>First, we build from source and run the test script to ensure that everything fits together.</li>
<li>Next, we will try out the config server API to retrieve the configuration for our microservices.</li>
<li>Finally, we will see how we can encrypt and decrypt sensitive information, for example, passwords.</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building and running automated tests</h1>
                </header>
            
            <article>
                
<p>So now we build and run, as follows:</p>
<ol>
<li>First, build the Docker images with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>cd $BOOK_HOME/</span><span>Chapter12<br></span><span>./gradlew build &amp;&amp;</span> <span>docker-compose build</span></strong></pre>
<ol start="2">
<li>Next, <span>start the system landscape in Docker and run the usual tests with the following command:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>./test-em-all.bash start</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Encrypting and decrypting sensitive information</h1>
                </header>
            
            <article>
                
<p>Information can be encrypted and decrypted using the <kbd>/encrypt</kbd>,&nbsp;and <kbd>/decrypt</kbd> endpoints exposed by the config server.&nbsp;<span>The <kbd>/encrpyt</kbd>&nbsp;endpoint can be used to create encrypted values to be placed in the property file in the config repository. Refer to the preceding example where the passwords to Eureka and RabbitMQ are stored encrypted on disk. The <kbd>/decrypt</kbd>&nbsp;endpoint can be used to verify encrypted information that is stored on disk in the config repository.</span></p>
<p>To encrypt the <kbd>hello world</kbd>&nbsp;<span>string,&nbsp;</span>run the following command:</p>
<pre><strong>curl -k https://dev-usr:dev-pwd@localhost:8443/config/encrypt --data-urlencode "hello world"</strong></pre>
<div class="packt_infobox">It is important to use the <kbd>--data-urlencode</kbd>&nbsp;flag when using <kbd>curl</kbd> to call the <kbd>/encrypt</kbd> endpoint, so as to ensure the correct handling of special characters such as <kbd>'+'</kbd>.</div>
<p>Expect a response along the lines of the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/733638b7-6ddf-4111-b1b8-9ea27d16f42e.png" style="width:39.50em;height:4.50em;" width="650" height="74" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/733638b7-6ddf-4111-b1b8-9ea27d16f42e.png"></p>
<p>To decrypt the encrypted value, run the following command:</p>
<pre><strong>curl -k https://dev-usr:dev-pwd@localhost:8443/config/decrypt -d 9eca39e823957f37f0f0f4d8b2c6c46cd49ef461d1cab20c65710823a8b412ce</strong></pre>
<p>Expect the <kbd>hello world</kbd> string as the response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/db7066ec-de99-4008-91ad-a347b02ff206.png" style="width:6.33em;height:3.75em;" width="125" height="73" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/db7066ec-de99-4008-91ad-a347b02ff206.png"></p>
<p>If you want to use an encrypted value in a configuration file, you need to prefix it with <kbd>{cipher}</kbd>&nbsp;and&nbsp;<span>wrap it in&nbsp;</span><kbd>''</kbd><span>.</span>&nbsp;For example, to store the encrypted version of <kbd>hello world</kbd>, execute the following command:</p>
<pre>my-secret:'{cipher}9eca39e823957f37f0f0f4d8b2c<br>6c46cd49ef461d1cab20c65710823a8b412ce'</pre>
<p>These tests conclude the chapter on centralized configuration. Wrap it up by shutting down the system landscape:</p>
<pre><strong>docker-compose down</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Encrypting and decrypting sensitive information</h1>
                </header>
            
            <article>
                
<p>Information can be encrypted and decrypted using the <kbd>/encrypt</kbd>, and <kbd>/decrypt</kbd> endpoints exposed by the config server. <span>The <kbd>/encrpyt</kbd> endpoint can be used to create encrypted values to be placed in the property file in the config repository. Refer to the preceding example where the passwords to Eureka and RabbitMQ are stored encrypted on disk. The <kbd>/decrypt</kbd> endpoint can be used to verify encrypted information that is stored on disk in the config repository.</span></p>
<p>To encrypt the <kbd>hello world</kbd>&nbsp;<span>string, </span>run the following command:</p>
<pre><strong>curl -k https://dev-usr:dev-pwd@localhost:8443/config/encrypt --data-urlencode "hello world"</strong></pre>
<div class="packt_infobox">It is important to use the <kbd>--data-urlencode</kbd> flag when using <kbd>curl</kbd> to call the <kbd>/encrypt</kbd> endpoint, so as to ensure the correct handling of special characters such as <kbd>'+'</kbd>.</div>
<p>Expect a response along the lines of the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/733638b7-6ddf-4111-b1b8-9ea27d16f42e.png" style="width:39.50em;height:4.50em;" width="650" height="74" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/733638b7-6ddf-4111-b1b8-9ea27d16f42e.png"></p>
<p>To decrypt the encrypted value, run the following command:</p>
<pre><strong>curl -k https://dev-usr:dev-pwd@localhost:8443/config/decrypt -d 9eca39e823957f37f0f0f4d8b2c6c46cd49ef461d1cab20c65710823a8b412ce</strong></pre>
<p>Expect the <kbd>hello world</kbd> string as the response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/db7066ec-de99-4008-91ad-a347b02ff206.png" style="width:6.33em;height:3.75em;" width="125" height="73" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/db7066ec-de99-4008-91ad-a347b02ff206.png"></p>
<p>If you want to use an encrypted value in a configuration file, you need to prefix it with <kbd>{cipher}</kbd> and <span>wrap it in </span><kbd>''</kbd><span>.</span> For example, to store the encrypted version of <kbd>hello world</kbd>, execute the following command:</p>
<pre>my-secret:'{cipher}9eca39e823957f37f0f0f4d8b2c<br>6c46cd49ef461d1cab20c65710823a8b412ce'</pre>
<p>These tests conclude the chapter on centralized configuration. Wrap it up by shutting down the system landscape:</p>
<pre><strong>docker-compose down</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What API call can we expect from a review service to the config server during startup to retrieve its configuration?&nbsp;The review service was started up using the following command: <kbd>docker compose up -d</kbd>.</li>
<li>What configuration information should we expect back from an API call to the config server using this command:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl https://dev-usr:dev-pwd@localhost:8443/config/application/default -ks | jq</strong> </pre>
<ol start="3">
<li>What types of repository backend does&nbsp;<span>Spring Cloud Config</span>&nbsp;support?</li>
<li>How can we encrypt sensitive information on disk using Spring Cloud Config?</li>
<li>How can we protect the config server API from misuse?</li>
<li>Mention some pros and cons for clients that first connect to the config server as opposed to those that first connect to th<span>e discovery server.</span></li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Improving Resilience Using Resilience4j</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to use Resilience4j to make our microservices more resilient, that is, how to mitigate and recover from errors. As we already discussed in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em>, in the&nbsp;<em>Circuit breaker</em> section, and <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml" target="_blank">Chapter 8</a>, <em>Introduction to Spring Cloud,</em> the <em>Resilience4j for improved resilience</em> section, a circuit breaker can be used to minimize the damage that a slow or not-responding downstream <span>microservice</span> can cause in a large-scale system landscape of synchronously communicating microservices. We will see how the circuit breaker in&nbsp;<span>Resilience4j can be used together with a timeout and retry mechanism to prevent twoâin my experienceâof the most common error situations:</span></p>
<ul>
<li><span>Microservices that start to respond slowly&nbsp;or not at all</span></li>
<li>Requests that randomly fail from time to time, for example, due to temporary network problems</li>
</ul>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introducing the Resilience4j circuit breaker and retry mechanism</li>
<li>Adding a&nbsp;<span>circuit breaker and retry</span> <span>mechanism to the source code</span></li>
<li>Trying out the&nbsp;<span>circuit breaker and retry&nbsp;</span><span>mechanism&nbsp;</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Improving Resilience Using Resilience4j</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to use Resilience4j to make our microservices more resilient, that is, how to mitigate and recover from errors. As we already discussed in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em>, in the <em>Circuit breaker</em> section, and <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml" target="_blank">Chapter 8</a>, <em>Introduction to Spring Cloud,</em> the <em>Resilience4j for improved resilience</em> section, a circuit breaker can be used to minimize the damage that a slow or not-responding downstream <span>microservice</span> can cause in a large-scale system landscape of synchronously communicating microservices. We will see how the circuit breaker in <span>Resilience4j can be used together with a timeout and retry mechanism to prevent twoâin my experienceâof the most common error situations:</span></p>
<ul>
<li><span>Microservices that start to respond slowly or not at all</span></li>
<li>Requests that randomly fail from time to time, for example, due to temporary network problems</li>
</ul>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introducing the Resilience4j circuit breaker and retry mechanism</li>
<li>Adding a <span>circuit breaker and retry</span> <span>mechanism to the source code</span></li>
<li>Trying out the <span>circuit breaker and retry </span><span>mechanism </span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All the commands that are described in this book<span> are run on a MacBook Pro using macOS Mojave but should be straightforward to modify if you want to run them on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p class="mce-root"></p>
<p>The source code for this chapter can be found in this book's <span>GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter13">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter13</a>.</p>
<p>To be able to run the commands that are described in this book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter13</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1.0, SR1 (also known as the <span><strong>Greenwich</strong> release</span>), Spring Boot 2.1.4, and Spring 5.1.6, that is, the latest available version of the Spring components at the time of writing this chapter.</p>
<p>The <kbd><span>openjdk:12.0.2</span></kbd><span> base Docker image is used in all the Dockerfiles.</span></p>
<p>The source code contains the following Gradle projects:</p>
<ul>
<li class="mce-root"><span><kbd><span>api</span></kbd></span></li>
<li class="mce-root"><kbd>util</kbd></li>
<li class="mce-root"><kbd>microservices/product-service</kbd></li>
<li class="mce-root"><kbd>microservices/review-service</kbd></li>
<li class="mce-root"><kbd>microservices/recommendation-service</kbd></li>
<li class="mce-root"><kbd>microservices/product-composite-service</kbd></li>
<li class="mce-root"><kbd>spring-cloud/eureka-server</kbd></li>
<li class="mce-root"><kbd>spring-cloud/gateway</kbd></li>
<li class="mce-root"><kbd>spring-cloud/authorization-server</kbd></li>
<li class="mce-root"><kbd>spring-cloud/config-server</kbd></li>
</ul>
<p>The configuration files can be found in the config repository, <kbd>config-repo</kbd>.</p>
<p>All the source code examples in this chapter come from the source code in <span><kbd>$BOOK_HOME/Chapter13</kbd> but in several cases have been edited to remove irrelevant parts of the source code, such as comments, imports, and log statements.</span></p>
<p class="mce-root"></p>
<p>If you want to see the changes that were applied to the source code in this chapter, that is, see <span>what it took to add resilience using Resilience4j, </span>you can compare it with the source code for <a href="a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml" target="_blank">Chapter 12</a>, <em>Centralized Configuration</em>.&nbsp;<span>You can use your favorite </span><kbd>diff</kbd><span> tool and compare the two folders, </span><kbd>$BOOK_HOME/Chapter12</kbd><span> and </span><kbd>$BOOK_HOME/Chapter13</kbd><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing the Resilience4j circuit breaker and retry mechanism</h1>
                </header>
            
            <article>
                
<p>Retries and circuit breakers are potentially useful in any synchronous communication between two software components, for example, microservices. Resilience4j can be used by all our microservices except for the edge server since Spring Cloud Gateway currently only supports the older circuit breaker, Netflix Hystrix. In this chapter, we will apply a circuit breaker and a retry mechanism in one place,<span> in calls to the <kbd>product</kbd> service from the <kbd>product-composite</kbd> service.</span> This is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1242dd3a-010d-4a6d-8ce2-edc8fd88c290.png" style="width:48.92em;height:22.25em;" width="1889" height="860" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1242dd3a-010d-4a6d-8ce2-edc8fd88c290.png"></p>
<p>Note that the synchronous calls to the discovery and config servers from the other microservices are not shown in the preceding diagram (to make it easier to read).</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<div class="mce-root packt_infobox"><span>Work is ongoing as this chapter was written to add an abstraction layer for circuit breakers in Spring Cloud, something </span><span>Spring Cloud Gateway will probably be able to benefit from. For details, see <a href="https://spring.io/blog/2019/04/16/introducing-spring-cloud-circuit-breaker">https://spring.io/blog/2019/04/16/introducing-spring-cloud-circuit-breaker</a>.</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing the circuit breaker</h1>
                </header>
            
            <article>
                
<p>Let's quickly revisit the state diagram for a circuit breaker from <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml">Chapter 8</a>, <em>Introduction to Spring Cloud</em>, in the <em>Resilience4j for improved resilience</em> section:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/87b2c812-ab46-45fe-a6d7-0f06a030d556.png" style="width:31.25em;height:17.00em;" width="1001" height="544" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/87b2c812-ab46-45fe-a6d7-0f06a030d556.png"></p>
<p>The key features of a circuit breaker are as follows:</p>
<ul>
<li>If a <span>circuit breaker detects too many faults, it will open its circuit, that is, not allow new calls.</span></li>
<li><span>When the circuit is open, a circuit breaker</span> will perform fast failure logic. This means that it doesn't wait for a new fault, for example, a timeout, to happen on subsequent calls. Instead, it directly redirects the call to a <strong>fallback</strong> <strong>method</strong>. The fallback method can apply various business logic to produce the best effort response. For example, a fallback method can return data from a local cache or simply return an immediate error message. This will prevent a microservice from getting unresponsive if the services it depends on stop responding normally. This is specifically useful under high load.</li>
<li>After a while, the circuit breaker will be half-open<em>,</em> allowing new calls to see whether the issue that caused the failures is gone. If new failures are detected by the circuit breaker, it will open the circuit again and go back to the fast failure logic. Otherwise, it will close the circuit and go back to normal operation. This makes a microservice resilient to faults, a capability that is indispensable in a system landscape of microservices that communicate synchronously with each other!</li>
</ul>
<p><span>Resilience4j exposes information about circuit breakers at runtime in a number of ways:</span></p>
<ul>
<li>The current state of a circuit breaker can be monitored using the microservice's <span>actuator </span><kbd>health</kbd> endpoint, <kbd>/actuator/health</kbd>.</li>
<li>The circuit breaker also publishes events on an <kbd>actuator</kbd> endpoint, for example, state transitions, <kbd>/actuator/circuitbreakerevents</kbd>.</li>
<li>Finally, circuit breakers are integrated with Spring Boot's metrics system and can use it to publish metrics to monitoring tools such as Prometheus.</li>
</ul>
<p>We will try out the <kbd>health</kbd> and <kbd>event</kbd> endpoints in this chapter. In <a href="5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml" target="_blank">Chapter 20</a>, <em>Monitoring Microservices</em>, we will see Prometheus in action and how it can collect metrics that are exposed by Spring Boot, for example, metrics from our circuit breaker.</p>
<p>To control the logic in a circuit breaker, Resilience4J can be configured using standard Spring Boot configuration files. We will use the following configuration parameters:</p>
<ul>
<li class="mce-root"><kbd>ringBufferSizeInClosedState</kbd>: Number of calls in a closed state, which are used to determine whether the circuit shall be opened.</li>
<li class="mce-root"><kbd>failureRateThreshold</kbd>: The threshold, in percent, for failed calls that will cause the circuit to be opened.</li>
<li class="mce-root"><kbd>waitInterval</kbd>: Specifies how long the circuit stays in an open state, that is, before it transitions to the half-open state.</li>
<li class="mce-root"><kbd>ringBufferSizeInHalfOpenState</kbd>: The number of calls in the half-open state that are used to determine whether the circuit shall be opened again or go back to the normal, closed state.</li>
<li class="mce-root"><kbd>automaticTransitionFromOpenToHalfOpenEnabled</kbd>: Determines whether the circuit automatically will transition to half-open once the wait period is over or wait for the first call after the waiting period until it transitions to the half-open state.</li>
<li class="mce-root"><kbd>ignoreExceptions</kbd>: Can be used to specify exceptions that should not be counted as faults. Expected business exceptions such as not found or invalid input are typical exceptions that the circuit breaker should ignore, that is, users that search for non-existing data or enter invalid input should not cause the circuit to open.</li>
</ul>
<div class="packt_infobox"><span>Resilience4j</span> keeps track of successful and failed calls when in the closed and half-open state using a ring buffer, hence the parameter names <kbd>ringBufferSizeInClosedState</kbd> and <kbd>ringBufferSizeInHalfOpenState</kbd>.</div>
<p class="mce-root">In this chapter, we will use the following settings:</p>
<ul>
<li><kbd>ringBufferSizeInClosedState = 5</kbd> and <kbd>failureRateThreshold = 50%</kbd>, meaning that if three or more of the last five calls are faults, then the circuit will open.</li>
<li><kbd>waitInterval = 10000</kbd> <span>and</span> <kbd>automaticTransitionFromOpenToHalfOpenEnabled = true</kbd><span>, meaning that the circuit breaker will keep the circuit open for 10 seconds and then transition to the half-open state.</span></li>
<li class="mce-root"><kbd>ringBufferSizeInHalfOpenState = 3</kbd><span>, meaning that the circuit breaker will decide whether the circuit shall be opened or closed based on the three first calls after the circuit has transitioned to the half-open state. Since the </span><span><kbd>failureRateThreshold</kbd> parameters are set to 50%, </span><span>the circuit will be open again if two or all three calls fail. Otherwise,</span><span> the circuit will be closed.</span></li>
<li class="mce-root"><kbd>ignoreExceptions = InvalidInputException</kbd> <span>and</span> <kbd>NotFoundException</kbd><span>, meaning that our two business exceptions will not be counted as faults in the circuit breaker.</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing the retry mechanism</h1>
                </header>
            
            <article>
                
<p>The <strong>retry</strong> mechanism is very useful for random and infrequent faults, such as temporary network glitches. The retry mechanism can simply retry a failed request a number of times with a configurable delay between the attempts. One very important restriction on the use of the retry mechanism is that the services that it retries must be <strong>idempotent</strong>, that is, calling the service one or many times with the same request parameters gives the same result. For example, reading information is idempotent but creating information is typically not. You don't want a retry mechanism to accidentally create two orders just because the response from the first order's creation got lost in the network. </p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>Resilience4j exposes retry information in the same way as it does for circuit breakers when it comes to events and metrics but does not provide any health information. Retry events are accessible on the <kbd>actuator</kbd> endpoint, </span><kbd>/actuator/retryevents</kbd><span>.&nbsp;</span>To control the retry logic, Resilience4J can be configured using standard Spring Boot configuration files. We will use the following configuration parameters:</p>
<ul>
<li><kbd>maxRetryAttempts</kbd>: Number of retries before giving up, including the first call</li>
<li><kbd>waitDuration</kbd>: Wait time before the next retry attempt</li>
<li><kbd>retryExceptions</kbd>: A list of exceptions that shall trigger a retry</li>
</ul>
<p>In this chapter, we will use the following values:</p>
<ul>
<li><kbd>maxRetryAttempts = 3</kbd>: We will make a maximum of two retry attempts.</li>
<li><kbd>waitDuration= 1000</kbd>: We will wait one second between retries.</li>
<li><kbd>retryExceptions = InternalServerError</kbd>: We will only trigger retries on <kbd>InternalServerError</kbd> exceptions, that is, when HTTP requests respond with a 500 status code.</li>
</ul>
<div class="packt_infobox"><span>Be careful when configuring retry and circuit breaker settings so that, for example, the circuit breaker doesn't open the circuit before the intended number of retries have been completed!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a circuit breaker and retry mechanism to the source code</h1>
                </header>
            
            <article>
                
<p>Before we add a <span>circuit bre</span><span>aker and a retry </span><span>mechanism to the source code, we will add code that makes it possible to force an error to occurâeither a delay and/or a random fault. Next, we will add a circuit breaker to handle slow or not responding APIs, as well as a retry mechanism that can handle faults that happens randomly. Adding these features from Resilience4j follows the traditional Spring Boot way:</span></p>
<ul>
<li><span>Add a starter dependency for Resilience4j in the build file.</span></li>
<li><span>Add annotations in the source code where the circuit breaker and retry mechanism shall be applied.</span></li>
<li><span>Add configuration that controls the behavior of the circuit breaker and retry mechanism.</span></li>
</ul>
<p class="mce-root"></p>
<p>Once we have the <span>circuit breaker and retry </span><span>mechanism in place, we will extend our test script, <kbd>test-em-all.bash</kbd>, with the tests of the circuit breaker.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding programmable delays and random errors</h1>
                </header>
            
            <article>
                
<p>To be able to test our circuit breaker and retry mechanism, we need a way to control when errors happen. A simple way to achieve this is by adding optional query parameters in the API in order to retrieve a product and a composite product. The composite product API will simply pass on the parameters to the product API. The following query parameters have been added to the two APIs:</p>
<ul>
<li><span><kbd>delay</kbd>: Causes the <kbd>getProduct</kbd> API on the <kbd>product</kbd> microservice to delay its response. The parameter is specified in seconds. For example, if the parameter is set to <kbd>3</kbd>, it will cause a delay of three seconds before the response is returned.</span></li>
<li><kbd>faultPercentage</kbd><span>: Causes the </span><kbd>getProduct</kbd><span> API on the <kbd>product</kbd> microservice to throw an exception randomly with the probability specified by the query parameter, from 0 to 100%. For example, if the parameter is set to <kbd>25</kbd>, it will cause every fourth call to the API, on average, to fail with an exception. It will return an HTTP error</span> <span>500 inter</span>nal server error in these cases.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the API definitions</h1>
                </header>
            
            <article>
                
<p><span>The two query parameters that we introduced previously, <kbd>delay</kbd> and <kbd>faultPercentage</kbd>, have been defined in the <kbd>api</kbd> project in the following two Java interfaces:</span></p>
<ul>
<li class="mce-root"><kbd>se.magnus.api.composite.product.ProductCompositeService</kbd><span>:</span></li>
</ul>
<pre style="color: black;padding-left: 60px">Mono&lt;ProductAggregate&gt; getCompositeProduct(<br>    @PathVariable int productId,<br>    @RequestParam(value = "delay", required = false, defaultValue = <br>    "0") int delay,<br>    @RequestParam(value = "faultPercent", required = false, <br>    defaultValue = "0") int faultPercent<br>);</pre>
<ul>
<li class="mce-root"><kbd>se.magnus.api.core.product.ProductService</kbd><span>:</span></li>
</ul>
<pre style="color: black;padding-left: 60px">Mono&lt;Product&gt; getProduct(<br>     @PathVariable int productId,<br>     @RequestParam(value = "delay", required = false, defaultValue<br>     = "0") int delay,<br>     @RequestParam(value = "faultPercent", required = false, <br>     defaultValue = "0") int faultPercent<br>);</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the product composite microservice</h1>
                </header>
            
            <article>
                
<p>The <kbd>product-composite</kbd><span> microservice simply passes the parameters to the product API. The service implementation receives the API request and passes on the parameters to the integration component that makes the call to the product API:</span></p>
<ul>
<li>The call to the <kbd>se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl</kbd> class:</li>
</ul>
<pre style="color: black;padding-left: 60px">public Mono&lt;ProductAggregate&gt; getCompositeProduct(int productId, int delay, int faultPercent) {<br>    return Mono.zip(<br>        ...<br>        integration.getProduct(productId, delay, faultPercent),<br>        ....</pre>
<ul>
<li>The call to the <kbd>se.magnus.microservices.composite.product.services.ProductCompositeIntegration</kbd> class:</li>
</ul>
<pre style="padding-left: 60px"><span>public </span>Mono&lt;Product&gt; getProduct(<span>int </span>productId, <span>int </span>delay, <span>int </span>faultPercent) {<br>    URI url = UriComponentsBuilder<br>        .<span>fromUriString</span>(<span>productServiceUrl </span>+ <span>"/product/{pid}?delay=<br>         {delay}&amp;faultPercent={fp}"</span>)<br>        .build(productId, delay, faultPercent);<br>    <span>return </span>getWebClient().get().uri(url)...</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a circuit breaker</h1>
                </header>
            
            <article>
                
<p>As we mentioned previously, we need to add dependencies, annotations, and configuration. We also need to add some code for handling timeouts and fallback logic. We will see how to do this in the following sections.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a circuit breaker</h1>
                </header>
            
            <article>
                
<p>As we mentioned previously, we need to add dependencies, annotations, and configuration. We also need to add some code for handling timeouts and fallback logic. We will see how to do this in the following sections.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding dependencies to the build file</h1>
                </header>
            
            <article>
                
<p>To add a circuit breaker, we have to add dependencies to the appropriate Resilience4j libraries in the build file, <kbd>build.gradle</kbd>:</p>
<pre>ext {<br> <span>  </span>resilience4jVersion = <span>"0.14.1"<br></span>}<br>dependencies {<br>   implementation(<span>"io.github.resilience4j:resilience4j-spring-<br>    boot2:</span>${resilience4jVersion}<span>"</span>)<br>   implementation(<span>"io.github.resilience4j:resilience4j-<br>    reactor:</span>${resilience4jVersion}<span>"</span>)<br>   ...</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding the circuit breaker and timeout logic</h1>
                </header>
            
            <article>
                
<p>The circuit breaker can be applied by annotating the method it is expected to protect with <span><kbd>@CircuitBreaker(name="nnn")</kbd></span>, which in this case is the <kbd>getProduct()</kbd> method in the <kbd>se.magnus.microservices.composite.product.services.ProductCompositeIntegration</kbd> class. The circuit breaker is triggered by an exception, not by a timeout itself. To be able to trigger the circuit breaker after a timeout, we have to add code that generates an exception after a timeout. Using <kbd>WebClient</kbd>, which is based on Project Reactor, allows us to do that conveniently by using its <kbd>timeout(Duration)</kbd> method. The source code looks as follows:</p>
<pre><span>@CircuitBreaker</span>(name = <span>"product"</span>)<br><span>public </span>Mono&lt;Product&gt; getProduct(<span>int </span>productId, <span>int </span>delay, <span>int </span>faultPercent) {<br>    ...<br>    <span>return </span>getWebClient().get().uri(url)<br>        .retrieve().bodyToMono(Product.<span>class</span>).log()<br>        .onErrorMap(WebClientResponseException.<span>class</span>, ex -&gt; <br>         handleException(ex))<br>        .timeout(Duration.<span>ofSeconds</span>(<span>productServiceTimeoutSec</span>));<br>}</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>The name of the circuit breaker, <kbd>"product"</kbd>, is used to identify the configuration that we will go through. The timeout parameter, <kbd><span>productServiceTimeoutSec</span></kbd>, is injected into the constructor as a configurable parameter value:</p>
<pre><span>private final int </span><span>productServiceTimeoutSec</span>;<br><span><br>@Autowired<br></span><span>public </span>ProductCompositeIntegration(<br>    ...<br>    <span>@Value</span>(<span>"${app.product-service.timeoutSec}"</span>) <span>int </span>productServiceTimeoutSec<br>) {<br>    ...<br>    <span>this</span>.<span>productServiceTimeoutSec </span>= productServiceTimeoutSec;<br>}</pre>
<div class="packt_infobox"><span><span>To activate the circuit breaker, the annotated method must be invoked as a Spring Bean. In our case, it's the integration class that's injected by Spring into the service implementation class and therefore used as a Spring Bean:<br></span></span>
<pre><span>private final </span>ProductCompositeIntegration <span>integration</span>;<br><br><span>@Autowired<br></span><span>public </span>ProductCompositeServiceImpl(... ProductCompositeIntegration integration) {<br>    <span>this</span>.<span>integration </span>= integration;<br>}<br><span><br>public </span>Mono&lt;ProductAggregate&gt; getCompositeProduct(<span>int </span>productId, <span>int </span>delay, <span>int </span>faultPercent) {<br>    <span>return </span>Mono.<span>zip</span>(..., <span>integration</span>.getProduct(productId, delay, faultPercent), ...</pre></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding configuration</h1>
                </header>
            
            <article>
                
<p>Finally, the configuration of the circuit breaker is added to the <kbd>product-composite.yml</kbd>&nbsp;file in the config repository, as follows:</p>
<pre><span>app.product-service.timeoutSec</span>: 2<br><br><span>resilience4j.circuitbreaker</span>:<br>  <span>backends</span>:<br>    <span>product</span>:<br>      <span>registerHealthIndicator</span>: true<br>      <span>ringBufferSizeInClosedState</span>: 5<br>      <span>failureRateThreshold</span>: 50<br>      <span>waitInterval</span>: 10000<br>      <span>ringBufferSizeInHalfOpenState</span>: 3<br>      <span>automaticTransitionFromOpenToHalfOpenEnabled</span>: true<br>      <span>ignoreExceptions</span>:<br>        - se.magnus.util.exceptions.InvalidInputException<br>        - se.magnus.util.exceptions.NotFoundException</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p>Most of the values in the configuration have already been described in&nbsp;<em>Introducing the circuit breaker</em>&nbsp;section, except for the following:</p>
<ul>
<li><span><kbd>app.product-service.timeoutSec</kbd>: Used to configure the timeout we introduced previously. This is set to two seconds.</span></li>
<li><span><span>&nbsp;</span></span><kbd>registerHealthIndicator</kbd>: Determines whether the circuit breaker shall display information in the <kbd>health</kbd> endpoint or not. This is set to <kbd>true</kbd>.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a retry mechanism</h1>
                </header>
            
            <article>
                
<p>In the same way as for the circuit breaker, a retry mechanism is set up by<span>&nbsp;adding dependencies, annotations, and configuration. The dependencies were added previously, so we only need to add the annotation and set up some configuration. We, however, also need to add some error handling logic due to retry-specific exceptions that the&nbsp;</span>retry mechanism throws.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding a retry mechanism</h1>
                </header>
            
            <article>
                
<p>In the same way as for the circuit breaker, a retry mechanism is set up by<span> adding dependencies, annotations, and configuration. The dependencies were added previously, so we only need to add the annotation and set up some configuration. We, however, also need to add some error handling logic due to retry-specific exceptions that the </span>retry mechanism throws.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding the retry annotation</h1>
                </header>
            
            <article>
                
<p><span>The retry mechanism can be applied to a method by annotating it with <kbd>@Retry(name="nnn")</kbd>, where <kbd>nnn</kbd> is the name of the configuration entry to be used for this method. See the <em>Adding configuration</em> section for details on the configuration. The method, in our case, is the same as it is for the circuit breaker, that is, <kbd>getProduct()</kbd> in the <kbd>se.magnus.microservices.composite.product.services.ProductCompositeIntegration</kbd> class:</span></p>
<pre><span>@Retry</span>(name = <span>"product"</span>)<br><span>@CircuitBreaker</span>(name = <span>"product"</span>)<span><br></span><span>public </span>Mono&lt;Product&gt; getProduct(<span>int </span>productId, <span>int </span>delay, <span>int </span>faultPercent) {</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Handling retry-specific exceptions</h1>
                </header>
            
            <article>
                
<p><span>Exceptions that are thrown by a method annotated with </span><span><kbd>@Retry</kbd> can be wrapped by the retry mechanism with a <kbd>RetryExceptionWrapper</kbd> exception. To be able to handle the actual exception that the method threw, for example, to apply a fallback method when </span><kbd>CircuitBreakerOpenException</kbd> is thrown, the caller needs to add logic that unwraps <span><kbd>RetryExceptionWrapper</kbd> exceptions and replaces them with the actual exception. </span></p>
<p class="mce-root"></p>
<p>In our case, it is the <kbd>getCompositeProduct</kbd> method in the <kbd>ProductCompositeServiceImpl</kbd> class that makes the call using the Project Reactor API for <kbd>Mono</kbd> objects. The <kbd>Mono</kbd> API has a convenient method, <kbd>onErrorMap</kbd>, that can be used to unwrap <kbd>RetryExceptionWrapper</kbd> <span>exceptions. It is used in the</span> <kbd>getCompositeProduct</kbd> method like so:</p>
<pre><span>public </span>Mono&lt;ProductAggregate&gt; getCompositeProduct(<span>int </span>productId, <span>int </span>delay, <span>int </span>faultPercent) {<br>    <span>return </span>Mono.<span>zip</span>(<br>        ...<br>        <span>integration</span>.getProduct(productId, delay, faultPercent)<br>            .onErrorMap(RetryExceptionWrapper.<span>class</span>, retryException -&gt; <br>             retryException.getCause())<br>            .onErrorReturn(CircuitBreakerOpenException.<span>class</span>, <br>             getProductFallbackValue(productId)),</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding configuration</h1>
                </header>
            
            <article>
                
<p>Configuration for the retry mechanism is added in the same way as it is for the circuit breaker, that is, in the<span>&nbsp;</span><kbd>product-composite.yml</kbd><span> file </span>in the config repository, like so:</p>
<p>&nbsp;</p>
<pre><span>resilience4j.retry</span>:<br>  <span>backends</span>:<br>    <span>product</span>:<br>      <span>maxRetryAttempts</span>: 3<br>      <span>waitDuration</span>: 1000<br>      <span>retryExceptions</span>:<br>      - org.springframework.web.reactive.function.client.WebClientResponseException$InternalServerError</pre>
<p>&nbsp;</p>
<p>The actual values were discussed in <em>Introducing the retry mechanism</em> section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding automated tests</h1>
                </header>
            
            <article>
                
<p>Automated tests for the circuit breaker have been added to the <kbd>test-em-all.bash</kbd> test script in a separate function, <kbd><span>testCircuitBreaker</span>()</kbd>:</p>
<pre><span>...<br></span><span>function </span><span>testCircuitBreaker</span>() {<br>    <span>echo </span><span>"Start Circuit Breaker tests!"<br>    ...<br>}<br></span><span>...<br>testCircuitBreaker</span><span><br></span><span>echo </span><span>"End, all tests OK:" </span>`date`</pre>
<p>To be able to perform some of the required verifications, we need to have access to the <kbd>actuator</kbd> endpoints of the <kbd>product-composite</kbd> microservice, which are not exposed through the edge server. Therefore, we will access the <span><kbd>actuator</kbd> endpoints</span> through a separate Docker container that will be attached to the internal network that was set up by Docker Compose for our microservices.</p>
<p>By default, the name of the network is based on the name of the folder where the <span>Docker Compose file is placed</span>. To avoid that uncertain dependency, an explicit network name, <kbd>my-network</kbd>, is defined in the <kbd>docker-compose</kbd> files. All container definitions have been updated to specify that they shall attach to the <kbd><span>my-network</span></kbd> network. The following is an example from <kbd>docker-compose.yml</kbd>:</p>
<pre>...<br>  <span>product</span>:<br>    <span>build</span>: microservices/product-service<br>    <span>networks</span>:<br>      - my-network<br>...<br><span>networks</span>:<br>  <span>my-network</span>:<br>    <span>name</span>: my-network</pre>
<p>Since the container is attached to the internal network, it can access the <span><kbd>actuator</kbd> endpoints of the product composite without going through the edge server. We will use Alpine as our Docker image and use <kbd>wget</kbd> instead of <kbd>curl</kbd> since <kbd>curl</kbd> isn't included in the Alpine distribution by default. For example, to be able to find out the state of the circuit breaker named <kbd>product</kbd> in the <kbd>product-composite</kbd> microservice, we can run the following command:</span></p>
<pre><strong>docker run --rm -it --network=my-network alpine wget product-composite:8080/actuator/health -qO - | jq -r .details.productCircuitBreaker.details.state</strong></pre>
<p>The command is expected to return a value of <kbd>CLOSED</kbd>.</p>
<div class="packt_infobox">Since we have created the Docker container with the <kbd>--rm</kbd> flag, it will be stopped and destroyed by the Docker engine after the <kbd>wget</kbd> command completes. </div>
<p class="mce-root"></p>
<p>The test starts by doing exactly this, that is, verifying that the circuit breaker is closed before the tests are executed:</p>
<pre><strong>EXEC=<span>"docker run --rm -it --network=my-network alpine"<br></span><span>assertEqual </span></strong><span><strong>"CLOSED" "$($EXEC wget product-composite:8080/actuator/health -qO - | jq -r .details.productCircuitBreaker.details.state)"</strong><br></span></pre>
<p>Next, the test will force the circuit breaker to open up by running three commands in a row, all of which will fail on a slow response from the <kbd>product</kbd> service:</p>
<pre><strong><span>for ((</span>n=<span>0</span>; n&lt;<span>3</span>; n++<span>))<br></span><span>do<br></span><span>    </span><span>assertCurl </span><span>500 </span><span>"curl -k https://$HOST:$PORT/product-<br>    composite/$PROD_ID_REVS_RECS?delay=3 $AUTH -s"<br></span><span>    </span>message=<span>$</span>(<span>echo </span>$RESPONSE | jq -r .message)</strong><br><strong>    <span>assertEqual </span><span>"Did not observe any item or terminal signal within <br>    2000ms" "${message:0:57}"<br></span><span>done</span></strong></pre>
<div class="packt_infobox"><strong>Quick repetition of the configuration</strong>: The timeout of the <kbd>product</kbd> service is set to two seconds so that a delay of three seconds will cause a timeout. The circuit breaker is configured to evaluate the last five last calls when closed. The tests in the script that precede the circuit breaker-specific tests have already performed a couple of successful calls. The failure threshold is set to 50%, that is, three calls with a three-second delay is enough to open the circuit. </div>
<p>With the circuit open, we expect a fast failure, that is, we won't need to wait for the timeout before we get a response. We also expect the <kbd>fallback</kbd> method to be called to return a best-effort response. This should also apply for a normal call, that is, without requesting a delay. This is verified with the following code:</p>
<pre><span>assertCurl </span><span>200 </span><span>"curl -k https://$HOST:$PORT/product-composite/$PROD_ID_REVS_RECS?delay=3 $AUTH -s"<br></span><span>assertEqual </span><span>"Fallback product2" "$(echo "$RESPONSE" | jq -r .name)"<br></span><span><br></span><span>assertCurl </span><span>200 </span><span>"curl -k https://$HOST:$PORT/product-composite/$PROD_ID_REVS_RECS $AUTH -s"<br></span><span>assertEqual </span><span>"Fallback product2" "$(echo "$RESPONSE" | jq -r .name)"<br></span></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>We can also verify that the simulated not found error logic works as expected in the fallback method, that is, the fallback method returns <kbd>404</kbd>, <kbd>NOT_FOUND</kbd> for product ID <kbd>13</kbd>:</p>
<pre><strong><span>assertCurl </span><span>404 </span><span>"curl -k https://$HOST:$PORT/product-composite/$PROD_ID_NOT_FOUND $AUTH -s"<br></span><span>assertEqual </span><span>"Product Id: $PROD_ID_NOT_FOUND not found in fallback cache!" "$(echo $RESPONSE | jq -r .message)"</span></strong></pre>
<p>As configured, the circuit breaker will change its state to half-open after <kbd>10</kbd> seconds. To be able to verify that, the test waits for <kbd>10</kbd> seconds:</p>
<pre><span>echo </span><span>"Will sleep for 10 sec waiting for the CB to go Half Open..."<br></span>sleep <span>10<br></span></pre>
<p>After verifying the expected state (half-closed), the test runs three normal requests to make the circuit breaker go back to its normal state, which is also verified:</p>
<pre><strong><span>assertEqual </span><span>"HALF_OPEN" "$($EXEC wget product-composite:8080/actuator/health -qO - | jq -r .details.productCircuitBreaker.details.state)"<br></span><span><br></span><span>for ((</span>n=<span>0</span>; n&lt;<span>3</span>; n++<span>))<br></span><span>do<br></span><span>    </span><span>assertCurl </span><span>200 </span><span>"curl -k https://$HOST:$PORT/product-<br>    composite/$PROD_ID_REVS_RECS $AUTH -s"<br></span><span>    </span><span>assertEqual </span><span>"product name C" "$(echo "$RESPONSE" | jq -r .name)"<br></span><span>done<br></span><span><br></span><span>assertEqual </span><span>"CLOSED" "$($EXEC wget product-composite:8080/actuator/health -qO - | jq -r .details.productCircuitBreaker.details.state)"</span></strong></pre>
<div class="packt_infobox"><strong>Quick repetition of the configuration:</strong><span> The circuit breaker is configured to evaluate the first three calls when in the half-open state. Therefore,</span> we need to run three requests where more than 50% are successful before the circuit is closed.</div>
<p>The test wraps up by using the <span><kbd>/actuator/circuitbreakerevents</kbd> actuator API, which is </span>exposed by the circuit breaker to reveal internal events. It can, for example, be used to find out what state transitions the circuit breaker has performed. We expect the last three state transitions to be as follows:</p>
<ul>
<li>First <span>state transitions</span>: Closed to open</li>
<li>Next <span>state transitions</span>: Open to half-closed</li>
<li>Last <span>state transitions</span><span>:</span> Half-closed to closed</li>
</ul>
<p class="mce-root"></p>
<p>This is verified by the following code:</p>
<pre><strong><span>assertEqual </span><span>"CLOSED_TO_OPEN"      "$($EXEC wget product-composite:8080/actuator/circuitbreakerevents/product/STATE_TRANSITION -qO - | jq -r .circuitBreakerEvents[-3].stateTransition)"<br></span><span>assertEqual </span><span>"OPEN_TO_HALF_OPEN"   "$($EXEC wget product-composite:8080/actuator/circuitbreakerevents/product/STATE_TRANSITION -qO - | jq -r .circuitBreakerEvents[-2].stateTransition)"<br></span><span>assertEqual </span><span>"HALF_OPEN_TO_CLOSED" "$($EXEC wget product-composite:8080/actuator/circuitbreakerevents/product/STATE_TRANSITION -qO - | jq -r .circuitBreakerEvents[-1].stateTransition)"</span></strong></pre>
<div class="packt_infobox">The <kbd>jq</kbd> expression, <kbd>circuitBreakerEvents[-1]</kbd>, means the last entry in the array of circuit breaker events, <kbd>[-2]</kbd>, is the second to last event, while <kbd>[-3 ]</kbd> is the third to last event. Together, they are the three latest events, that is, the ones we are interested in. By default, Resilience4j keeps the last 100 events per circuit breaker. This can be customized using the <span><kbd>eventConsumerBufferSize</kbd> configuration parameter.</span></div>
<p><span>We added quite a lot of steps to the test script, but with this, we can automatically verify that the expected basic behavior of our circuit breaker is in place. In the next section, we will try it out!</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building and running the automated tests</h1>
                </header>
            
            <article>
                
<p>In order to build and run the automated tests, we need to do the following:</p>
<ol>
<li>First, build the Docker images with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>cd $BOOK_HOME/</span><span>Chapter13<br></span><span>./gradlew build &amp;&amp;</span> <span>docker-compose build</span></strong></pre>
<ol start="2">
<li>Next,&nbsp;<span>start the system landscape in Docker and run the usual tests with the following command:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>./test-em-all.bash start</strong></pre>
<div class="packt_infobox">When the test script prints out&nbsp;<kbd>Start Circuit Breaker tests!</kbd>, the tests we described previously are executed!</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building and running the automated tests</h1>
                </header>
            
            <article>
                
<p>In order to build and run the automated tests, we need to do the following:</p>
<ol>
<li>First, build the Docker images with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>cd $BOOK_HOME/</span><span>Chapter13<br></span><span>./gradlew build &amp;&amp;</span> <span>docker-compose build</span></strong></pre>
<ol start="2">
<li>Next, <span>start the system landscape in Docker and run the usual tests with the following command:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>./test-em-all.bash start</strong></pre>
<div class="packt_infobox">When the test script prints out <kbd>Start Circuit Breaker tests!</kbd>, the tests we described previously are executed!</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Verifying that the circuit is closed under normal operations</h1>
                </header>
            
            <article>
                
<p>Before we can call the API, we need an access token. Run the following commands to acquire an access token:</p>
<pre><strong>unset ACCESS_TOKEN</strong><br><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@localhost:8443/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq -r .access_token)</strong><br><strong>echo $ACCESS_TOKEN</strong></pre>
<p>Try a normal request and verify that it returns the HTTP response code <kbd>200</kbd>:</p>
<pre><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://localhost:8443/product-composite/2 -w "%{http_code}\n" -o /dev/null -s</strong></pre>
<div class="packt_infobox">The <kbd>-w "%{http_code}\n"</kbd> switch is used to print the HTTP return status. As long as the command returns <kbd>200</kbd>, we are not interested in the response body, and so we suppress it with the switch, that is, <kbd>-o /dev/null</kbd>.</div>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>Verify that the circuit breaker is closed using the <kbd>health</kbd> API:</span></p>
<pre><strong>docker run --rm -it --network=my-network alpine wget product-composite:8080/actuator/health -qO - | jq -r .details.productCircuitBreaker.details.state</strong></pre>
<p>We expect it to respond with <kbd>CLOSED</kbd>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Forcing the circuit breaker to open when things go wrong</h1>
                </header>
            
            <article>
                
<p>Now, it's time to make things go wrong! By<span> that, I mean it's time to try out some negative tests in order to verify that the circuit opens up when things start to go wrong. </span>Call the API three times and direct the <kbd>product</kbd> service to cause a timeout on every call, that is, delay the response with <kbd>3</kbd> seconds. This should be enough to trip the circuit breaker:</p>
<pre><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://localhost:8443/product-composite/2?delay=3 -s | jq .</strong></pre>
<p>We expect a response such as the following each time:</p>
<pre>{<br>  "timestamp": "2019-05-03T15:12:57.554+0000",<br>  "path": "/product-composite/2",<br>  "status": 500,<br>  "error": "Internal Server Error",<br>  "message": "Did not observe any item or terminal signal within 2000ms <br>   in 'onErrorResume' (and no fallback has been configured)"<br>}</pre>
<p>The <span>circuit breaker</span> is now open, so if you make a fourth attempt (within <kbd>waitInterval</kbd>, that is, <kbd>10</kbd> seconds), you will see a fast fail and the <kbd>fallback</kbd> method in action. You will get a response back immediately, instead of an error message once the timeout kicks in after <kbd>2</kbd> seconds:</p>
<pre>{<br>  "productId": 2,<br>  "name": "Fallback product2",<br>  ...<br>}</pre>
<p><span>The response will come from the fallback method. This can be recognized by looking at the value in the name field, that is, <kbd>Fallback product2</kbd>.</span></p>
<p class="mce-root"></p>
<div class="packt_infobox">Fast fail and fallback methods are key capabilities of a circuit breaker!<br>
<span>Given our configuration with a wait time set to only 10 seconds requires you to be rather quick to be able to see fast</span><span> fail and fallback methods</span><span> in action! Once in a half-open state, you can always submit three new requests that cause a timeout, forcing the circuit breaker back to the open state, and then quickly try the fourth request. Then, you should get a fast fail response from the fallback method! You can also increase the wait time to a minute or two, but it can be rather boring to wait that amount of time before the circuit switches to the half-open state.</span></div>
<p>Wait 10 seconds, for the circuit breaker to transition to half-open and then run the following command to verify that the circuit now is in a half-open state:</p>
<pre><strong>docker run --rm -it --network=my-network alpine wget product-composite:8080/actuator/health -qO - | jq -r .details.productCircuitBreaker.details.state</strong></pre>
<p><span>Expect it to respond with <kbd>HALF_OPEN</kbd></span><span>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Closing the circuit breaker again</h1>
                </header>
            
            <article>
                
<p>Once the <span>circuit breaker</span> is in a half-open state, it waits for three calls to see whether it should open the <span>circuit </span>again or go back to normal, that is, close it.</p>
<p>Let's submit three normal requests to close the <span>circuit breaker</span>:</p>
<pre><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://localhost:8443/product-composite/2 -w "%{http_code}\n" -o /dev/null -s</strong></pre>
<p>They should all response with <kbd>200</kbd>.&nbsp;<span>Verify that the circuit is closed again by using the <kbd>health</kbd> API:</span></p>
<pre><strong>docker run --rm -it --network=my-network alpine wget product-composite:8080/actuator/health -qO - | jq -r .details.productCircuitBreaker.details.state</strong></pre>
<p><span>We expect it to respond with</span>&nbsp;<kbd>CLOSED</kbd>.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>Wrap this up by listing the last three state transitions using the following command: </p>
<pre><strong>docker run --rm -it --network=my-network alpine wget product-composite:8080/actuator/circuitbreakerevents/product/STATE_TRANSITION -qO - | jq -r '.circuitBreakerEvents[-3].stateTransition, .circuitBreakerEvents[-2].stateTransition, .circuitBreakerEvents[-1].stateTransition'</strong></pre>
<p><span>Expect it to respond with the following command</span>:</p>
<pre>CLOSED_TO_OPEN<br>OPEN_TO_HALF_OPEN<br>HALF_OPEN_TO_CLOSED</pre>
<p>This response tells us that we have taken our circuit breaker through a full lap of its state diagram:</p>
<ul>
<li>From closed to open when an error starts to prevent requests from succeeding</li>
<li>From open to half-open to see whether the error is gone</li>
<li>From half-open to closed when the error is gone, that is, when we are back to normal operation</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen Resilience4j and its circuit breaker and retry mechanism in action.</p>
<p>A <span>circuit breaker can, using fast fail and <kbd>fallback</kbd> methods when it is open, prevent a microservice from becoming unresponsive if the synchronous services it depends on stop responding normally. A circuit breaker can also make a microservice resilient by allowing requests when it is half-open to see whether the failing service operates normally again and close the circuit if so.</span></p>
<p>A retry mechanism can retry requests that randomly fail from time to time, for example, due to temporary network problems. It is very important to only apply retry requests on idempotent services, that is, services that can handle that the same request is sent two or more times.</p>
<p><span>Circuit breakers and retry mechanisms are implemented by following Spring Boot conventions, that is, declaring dependencies, and adding annotations and configuration.</span> <span>Resilience4j exposes information about its circuit breakers and retry mechanisms at runtime, using <kbd>actuator</kbd> endpoints for health, events, and metrics for circuit breakers and events and metrics for retries.</span></p>
<p>We have seen the usage of both endpoints for health and events in this chapter, but we will have to wait until <a href="5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml">Chapter 20</a><span>, <em>Monitoring Microservices</em></span>, before we use any of the metrics.</p>
<p>In the next chapter, we will cover the last part of using Spring Cloud, where we will learn how to trace call chains through a set of cooperating microservices using Spring Cloud Sleuth and Zipkin. Head over to <a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml">Chapter 14</a>, <em>Understanding Distributed Tracing</em>, to get started!</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ul>
<li>What are the states of a circuit breaker and how are they used?</li>
<li>How can we handle timeout errors in the circuit breaker?</li>
<li>How can we apply <span>f</span><span>allback logic when a circuit breaker fast fails?</span></li>
<li>How can a retry mechanism and a circuit breaker interfere with each other?</li>
<li>Provide an example of a service that you can't apply a retry mechanism for.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ul>
<li>What are the states of a circuit breaker and how are they used?</li>
<li>How can we handle timeout errors in the circuit breaker?</li>
<li>How can we apply <span>f</span><span>allback logic when a circuit breaker fast fails?</span></li>
<li>How can a retry mechanism and a circuit breaker interfere with each other?</li>
<li>Provide an example of a service that you can't apply a retry mechanism for.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Understanding Distributed Tracing</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to use distributed tracing to better understand how our microservices cooperate; for example, fulfilling a request sent to the external API. Being able to utilize distributed tracing is essential for being able to manage a system landscape of cooperating microservices. A<span>s already des</span><span>cribed in <a href="9878a36a-5760-41a4-a132-1a2387b61037.xhtml" target="_blank">Chapter 8</a>, <em>Introduction to Spring Cloud</em>, in reference to the <em>Spring Cloud Sleuth and Zipkin for distributed tracing</em>&nbsp;</span><span>section</span><span>, Spring Cloud Sleuth will be used to collect trace information, and Zipkin will be used for the storage and visualizat</span><span>ion of said trace information.</span></p>
<p><span>In this chapter, we will learn about the following topics:</span></p>
<ul>
<li>Introducing distributed tracing with Spring Cloud Sleuth and Zipkin</li>
<li>How to add distributed tracing to the source code</li>
<li>How to perform <span>distributed tracing:</span>
<ul>
<li><span>We will learn how to visualize trace information using Zipkin in relation to the following:</span>
<ul>
<li>Successful and unsuccessful API requests</li>
<li>Synchronous and a<span>synchronous</span> processing of API requests</li>
</ul>
</li>
<li>We will use both RabbitMQ and Kafka to send trace events from our microservices to the Zipkin server</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All commands described in this book<span> are run on a MacBook Pro using macOS Mojave but should be straightforward to modify so that they can be run on another platform such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p>The source code for this chapter can be found <span>on GitHub at</span><span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter14">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter14</a>.</p>
<p>To be able to run the commands as described in the book, download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter14</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1.0, SR1 (also known as the <span><strong>Greenwich</strong> release</span>), Spring Boot 2.1.4, and Spring 5.1.6, that is, the latest available version of the Spring components at the time of writing this chapter.</p>
<p>The base Docker image, <kbd><span>openjdk:12.0.2</span></kbd><span>, is used in all</span> <span>Dockerfiles.</span></p>
<p>All source code examples in this chapter come from the source code in <span><kbd>$BOOK_HOME/Chapter14</kbd></span>&nbsp;<span>but are, in several cases, edited to remove non-relevant parts of the source code, such as comments and import and log statements.</span></p>
<p>If you want to see the changes applied to the source code in this chapter, that is, see <span>what it took to add distributed tracing using Spring Cloud Sleuth and Zipkin, </span>you can compare it with the source code for <a href="23795d34-4068-4961-842d-989cde26b642.xhtml" target="_blank">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em>.&nbsp;<span>You can use your favorite </span><kbd>diff</kbd><span> tool and compare the two folders â </span><kbd>$BOOK_HOME/Chapter13</kbd><span> and </span><kbd>$BOOK_HOME/Chapter14</kbd><span>.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding distributed tracing to the source code</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will learn how to update the source code to enable distributed tracing using Spring Cloud Sleuth and Zipkin. This can be done through the following steps:</span></p>
<ol>
<li>Add dependencies to the build files to bring in&nbsp;<span>Spring Cloud Sleuth and the capability of sending trace information to Zipkin.</span></li>
<li>Add&nbsp;<span>dependencies to RabbitMQ and Kafka for the projects that haven't used them before, that is, the Spring Cloud projects&nbsp;<kbd>authorization-server</kbd>, <kbd>eureka-server</kbd></span>,<span><span>&nbsp;</span></span><span>and <kbd>gateway</kbd>.</span></li>
<li>Configure the microservices to send trace information to Zipkin using either RabbitMQ or Kafka.</li>
<li>Add a Zipkin server to the Docker compose files.</li>
<li>Add the <kbd>kafka</kbd> Spring profile&nbsp;<span>in&nbsp;</span><kbd>docker-compose-kafka.yml</kbd>&nbsp;to&nbsp;<span>the Spring Cloud projects&nbsp;<kbd>authorization-server</kbd>,&nbsp;<kbd>eureka-server</kbd></span>,&nbsp;<span>and&nbsp;<kbd>gateway</kbd>.</span></li>
</ol>
<p>Adding the Zipkin server will be effected using a Docker image from Docker Hub that has been published by the Zipkin project. Refer to&nbsp;<a href="https://hub.docker.com/r/openzipkin/zipkin">https://hub.docker.com/r/openzipkin/zipkin</a> for details.</p>
<div class="packt_infobox">Zipkin is itself a Spring Boot application and is, at the time of writing,&nbsp;<span>undergoing incubation at the <strong>Apache Software Foundation</strong> (<strong>ASF</strong>). R</span>efer to&nbsp;<a href="https://zipkin.apache.org/">https://zipkin.apache.org/</a> for more information.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding dependencies to build files</h1>
                </header>
            
            <article>
                
<p>&nbsp;<span>To be able to utilize Spring Cloud Sleuth and the ability to send trace information to Zipkin, we need to add a couple of dependencies to the Gradle project build files, <kbd>build.gradle</kbd>.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>This is accomplished by adding the following two lines:</span>&nbsp;</p>
<pre>implementation('org.springframework.cloud:spring-cloud-starter-sleuth')   implementation('org.springframework.cloud:spring-cloud-starter-zipkin')</pre>
<p>For the Gradle projects&nbsp;<span>that haven't used RabbitMQ and Kafka before, that is, the Spring Cloud projects&nbsp;<kbd>authorization-server</kbd>,&nbsp;<kbd>eureka-server</kbd>, and&nbsp;<kbd>gateway</kbd>, the following dependencies have to be added:</span></p>
<pre>implementation('org.springframework.cloud:spring-cloud-starter-stream-rabbit')<br>implementation('org.springframework.cloud:spring-cloud-starter-stream-kafka')</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding dependencies to build files</h1>
                </header>
            
            <article>
                
<p>&nbsp;<span>To be able to utilize Spring Cloud Sleuth and the ability to send trace information to Zipkin, we need to add a couple of dependencies to the Gradle project build files, <kbd>build.gradle</kbd>.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>This is accomplished by adding the following two lines:</span>&nbsp;</p>
<pre>implementation('org.springframework.cloud:spring-cloud-starter-sleuth')   implementation('org.springframework.cloud:spring-cloud-starter-zipkin')</pre>
<p>For the Gradle projects <span>that haven't used RabbitMQ and Kafka before, that is, the Spring Cloud projects <kbd>authorization-server</kbd>,&nbsp;<kbd>eureka-server</kbd>, and <kbd>gateway</kbd>, the following dependencies have to be added:</span></p>
<pre>implementation('org.springframework.cloud:spring-cloud-starter-stream-rabbit')<br>implementation('org.springframework.cloud:spring-cloud-starter-stream-kafka')</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding configuration for Spring Cloud Sleuth and Zipkin</h1>
                </header>
            
            <article>
                
<p>Configuration for using Spring Cloud Sleuth and Zipkin is added to the common configuration file, <kbd>config-repo/application.yml</kbd>. In the default profile, it is specified that trace information shall be sent to Zipkin using RabbitMQ:</p>
<pre>spring.zipkin.sender.type: rabbit</pre>
<p>By default, Spring Cloud Sleuth only sends 10% of the traces to Zipkin. To ensure that all traces are sent to Zipkin, the following property is added in the default profile:</p>
<pre>spring.sleuth.sampler.probability: 1.0</pre>
<p>When sending traces to Zipkin using Kafka, the Spring profile <kbd>kafka</kbd> will be used. In earlier chapters, the <kbd>kafka</kbd> Spring profile was defined in the configuration files specific to the composite and core microservices. In this chapter, where the Spring Cloud services will also use Kafka to send trace information to Zipkin, the <kbd>kafka</kbd> Spring profile is moved to the common configuration file, <kbd>config-repo/application.yml</kbd>. The following two properties have also been added to the <kbd>kafka</kbd> Spring profile:</p>
<ul>
<li><kbd>spring.zipkin.sender.type: kafka</kbd> tells Spring Cloud Sleuth to send trace information to Zipkin using Kafka. </li>
<li><kbd>spring.kafka.bootstrap-servers: kafka:9092</kbd> specifies where to find the Kafka server.</li>
</ul>
<p class="mce-root"></p>
<p>All in all, the <kbd>kafka</kbd><span> Spring profile appears as follows:</span></p>
<pre>--- <br>spring.profiles: kafka<br><br>management.health.rabbit.enabled: false<br>spring.cloud.stream.defaultBinder: kafka<br>spring.zipkin.sender.type: kafka<br>spring.kafka.bootstrap-servers: kafka:9092</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out&nbsp;distributed tracing</h1>
                </header>
            
            <article>
                
<p><span>With the necessary changes to the source code in place, we can try out distributed tracing! We will do this by performing the following steps:</span></p>
<ol>
<li>Build, start, and verify the system landscape with RabbitMQ as the queue manager.</li>
<li>Send a successful API request and see what trace information we can find in Zipkin related to this&nbsp;<span>API request.</span></li>
<li><span>Send</span> an un<span>successful API request and see what the&nbsp;trace information in Zipkin looks like.</span></li>
<li><span>Send a successful API request that triggers asynchronous processing and see how its&nbsp;trace information is represented in Zipkin.</span></li>
<li>Investigate how we can monitor trace information that's passed to Zipkin in RabbitMQ.</li>
<li>Switch the queue manager to Kafka and repeat the preceding steps.</li>
</ol>
<p>We will discuss these steps in detail in the upcoming sections.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out distributed tracing</h1>
                </header>
            
            <article>
                
<p><span>With the necessary changes to the source code in place, we can try out distributed tracing! We will do this by performing the following steps:</span></p>
<ol>
<li>Build, start, and verify the system landscape with RabbitMQ as the queue manager.</li>
<li>Send a successful API request and see what trace information we can find in Zipkin related to this <span>API request.</span></li>
<li><span>Send</span> an un<span>successful API request and see what the trace information in Zipkin looks like.</span></li>
<li><span>Send a successful API request that triggers asynchronous processing and see how its trace information is represented in Zipkin.</span></li>
<li>Investigate how we can monitor trace information that's passed to Zipkin in RabbitMQ.</li>
<li>Switch the queue manager to Kafka and repeat the preceding steps.</li>
</ol>
<p>We will discuss these steps in detail in the upcoming sections.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Starting up the system landscape with RabbitMQ as the queue manager</h1>
                </header>
            
            <article>
                
<p>Let's start up the system landscape. Build the Docker images with the following commands:</p>
<pre><strong><span>cd $BOOK_HOME/</span><span>Chapter14<br></span><span>./gradlew build &amp;&amp;</span> <span>docker-compose build</span></strong></pre>
<p><span>Start the system landscape in Docker and run the usual tests with the following command:</span></p>
<pre><strong>./test-em-all.bash start</strong></pre>
<p>Before we can call the API, we need an access token. Run the following commands to acquire an access token:</p>
<pre><strong>unset ACCESS_TOKEN</strong><br><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@localhost:8443/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq -r .access_token)</strong><br><strong>echo $ACCESS_TOKEN</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Sending a successful API request</h1>
                </header>
            
            <article>
                
<p>Now, we are ready to send a normal request to the API. Run the following command:</p>
<pre><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://localhost:8443/product-composite/2 -w "%{http_code}\n" -o /dev/null -s</strong></pre>
<p><span>Expect the command to returns the HTTP status code for success,</span><span>&nbsp;</span>200.</p>
<p>We can now launch the Zipkin UI to look into what trace information has been sent to Zipkin:</p>
<ol>
<li>Open the following URL in your web browser: <kbd>http://localhost:9411/zipkin/.</kbd></li>
<li>To find the trace information for our request, implement the following steps:
<ol>
<li>Select<span><span class="packt_screen"> Service Name</span>:&nbsp;</span><span class="packt_screen">gateway</span>.</li>
<li>Set <span class="packt_screen">Sort</span> order: to <span class="packt_screen">Newest First</span>.</li>
<li>Click on the <span class="packt_screen">Find Traces</span> button.</li>
</ol>
</li>
</ol>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>The response from finding traces should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ee509be8-dcc5-41bb-b758-361379f8fe94.png" width="1950" height="1261" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ee509be8-dcc5-41bb-b758-361379f8fe94.png"></p>
<p>The trace information from our preceding API request is the first one in the list. Click on it to see details pertaining to the trace:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8fa726b9-d1b5-424a-ade5-c5324f925c2c.png" width="1950" height="853" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8fa726b9-d1b5-424a-ade5-c5324f925c2c.png"></p>
<p>In the detailed trace information view, we can observe the following:</p>
<ol>
<li>The request was received by the<span>&nbsp;</span><span class="packt_screen">gateway</span><span> service.</span></li>
<li>It delegated the processing of the request to the<span>&nbsp;</span><span class="packt_screen">product-composite</span> service.</li>
<li>The<span>&nbsp;</span><span class="packt_screen">product-composite</span> service, in turn, sent three parallel requests to the core services:<span>&nbsp;</span><span class="packt_screen">product</span>,<span>&nbsp;</span><span class="packt_screen">recommendation</span><span>,&nbsp;</span>and<span>&nbsp;</span><span class="packt_screen">review</span>.</li>
<li>Once the <span class="packt_screen">product-composite</span><span> service received the response from all </span>three core services, it created a composite response.</li>
<li>The composite response was sent back to the caller through the <span class="packt_screen">gateway</span> service.</li>
</ol>
<div class="packt_infobox">When using Safari, I have noticed that the trace tree isn't always rendered correctly. Switching to either Chrome or Firefox resolved the issue.</div>
<p class="mce-root"></p>
<p>If we click on the first span, <span class="packt_screen">gateway</span>, we can see even more details:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f6f693ce-7cea-43ea-ba6c-8faa8db74269.png" style="width:39.33em;height:22.42em;" width="1212" height="690" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f6f693ce-7cea-43ea-ba6c-8faa8db74269.png"></p>
<p>Here, we can see the actual request we sent: <span class="packt_screen">product-composite/2</span>. This is very valuable when analyzing traces that, for example, take a long time to complete!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Sending an unsuccessful API request</h1>
                </header>
            
            <article>
                
<p>Let's see what the trace information looks like if we make an unsuccessful API request; for example, searching for a product that does not exist:</p>
<ol>
<li>Send an API request for product ID <kbd>12345</kbd> and <span>verify that it returns the HTTP status code for</span> Not Found, 404:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://localhost:8443/product-composite/12345 -w "%{http_code}\n" -o /dev/null -s</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<ol start="2">
<li>In the Zipkin UI, go back to the search page <span>(use the back button in the web browser) </span>and click on the <span class="packt_screen">Find Traces</span> button. You should see the failed request at the top of the returned list, in red:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3b7bf196-16fe-4277-8e51-3f393696278f.png" width="1950" height="1270" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3b7bf196-16fe-4277-8e51-3f393696278f.png"></p>
<ol start="3">
<li>Click on the top trace marked in red:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4ca53dfa-35ad-4c3c-b50f-8e3a3c357f61.png" width="1950" height="846" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4ca53dfa-35ad-4c3c-b50f-8e3a3c357f61.png"></p>
<ol start="4">
<li>In the detailed trace view, we can see by the color-coding that the request went wrong when <span class="packt_screen">product-composite</span> called the <span class="packt_screen">product</span> service. Click on the <span class="packt_screen">product</span> span to see details of what went wrong:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b33855c9-3b3e-4aaf-81ed-8a080c05dec7.png" width="1950" height="1305" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b33855c9-3b3e-4aaf-81ed-8a080c05dec7.png"></p>
<p>Here, we can see what request caused the error, <span class="packt_screen">product/12345</span>, as well as the error code and the reason returned: <span class="packt_screen">404</span> Not Found. This is very useful when analyzing the root cause of a failure!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Sending an API request that triggers asynchronous processing</h1>
                </header>
            
            <article>
                
<p>The third type of request that is interesting to see how it is represented in the Zipkin UI is a request where parts of its processing are done asynchronously. Let's try a delete request, where the delete process in the core services is done asynchronously. The <kbd>product-composite</kbd> service sends a delete event to each of the three core services over the message broker and each core service picks up the delete event and processes it asynchronously. Thanks to Spring Cloud Sleuth, trace information is added to the events that are sent to the message broker, resulting in a coherent view of the total processing of the delete request.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>Run the following command to delete the product with a product ID of <kbd>12345</kbd> and <span>verify that it returns the HTTP status code for success,</span><span>&nbsp;</span>200<span>:</span></p>
<pre><strong>curl -X DELETE -H "Authorization: Bearer $ACCESS_TOKEN" -k https://localhost:8443/product-composite/12345 -w "%{http_code}\n" -o /dev/null -s</strong></pre>
<div class="packt_infobox">Remember that the delete operation is idempotent, that is, it will succeed even if the product doesn't exist!</div>
<p><span>In the Zipkin UI, go back to the search page </span><span>(use the back button in the web browser) </span><span>and click on the </span><span class="packt_screen">Find Traces</span><span> button. You should see the trace from the delete request at the top of the returned list:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c8304d85-6234-4bf4-9470-e18b2f92a68a.png" width="1950" height="1406" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c8304d85-6234-4bf4-9470-e18b2f92a68a.png"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>Click on the first trace to see its trace information:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b06714c3-aec0-4166-a066-2001d3c14c3d.png" width="1950" height="1065" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b06714c3-aec0-4166-a066-2001d3c14c3d.png"></p>
<p>Here, we can see the trace information for processing the delete request:</p>
<ol>
<li>The request was received by the<span>&nbsp;</span><span class="packt_screen">gateway</span><span> service.</span></li>
<li>It delegated the processing of the request to the<span>&nbsp;</span><span class="packt_screen">product-composite</span> service.</li>
<li>The<span>&nbsp;</span><span class="packt_screen">product-composite</span> service, in turn, published three events on the message broker (RabbitMQ, in this case).</li>
<li>The <span class="packt_screen">product-composite</span><span> service is now done and returns a success HTTP status code,</span> <span class="packt_screen">200</span><span>, through the</span> <span class="packt_screen">gateway</span><span> service back to the caller.</span></li>
<li>The core services,<span>&nbsp;</span><span class="packt_screen">product</span>,&nbsp;<span class="packt_screen">recommendation</span>, and <span class="packt_screen">review</span>, receive the delete events and start to process them asynchronously, that is, independent of one another.</li>
</ol>
<p>To see more detailed information, click on the <span class="packt_screen">product</span> span:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7fd269c2-3bc7-4c83-91cd-6a16c6010757.png" style="width:42.83em;height:24.50em;" width="1359" height="778" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7fd269c2-3bc7-4c83-91cd-6a16c6010757.png"></p>
<p>Here, we can see that the <span class="packt_screen">product</span> service was triggered by an event coming in to its <span class="packt_screen">input</span> channel, which was sent from the message broker.</p>
<div class="packt_tip">The Zipkin UI contains much more functionality for finding traces of interest!<br>
To get more accustomed to the Zipkin UI, try out the <kbd>Annotation Query</kbd> parameter; for example, search for a specific request using <kbd>http.path=/product-composite/214</kbd> or <kbd>error=401</kbd> to find requests that failed due to authorization failures. Watch out for the <span><kbd>Limit</kbd> parameter, which is set to <kbd>10</kbd> by default; this can hide results of interest if not raised. Also, ensure that the <kbd>Lookback</kbd> parameter doesn't remove traces of interest!</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Monitoring trace information passed to Zipkin in RabbitMQ</h1>
                </header>
            
            <article>
                
<p>To monitor trace information that's sent to Zipkin over RabbitMQ, we can use the RabbitMQ management web UI. Open the following URL in your web browser: <kbd>http://localhost:15672/#/queues/%2F/zipkin</kbd>. If required, log in using the username <kbd>guest</kbd> and the password <kbd>guest</kbd>. Expect a web page that looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0ebbf762-280a-457c-b52e-7344c666c249.png" width="1862" height="1334" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0ebbf762-280a-457c-b52e-7344c666c249.png"></p>
<p>In the graph named <kbd>Message Rates</kbd>, we can see that trace messages are sent to Zipkin, currently at an average rate of 1.2 messages per second.</p>
<p>Wrap up the tests of distributed tracing using RabbitMQ by bringing down the system landscape. Run the following command:</p>
<pre><strong>docker-compose down</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Kafka as a message broker</h1>
                </header>
            
            <article>
                
<p>Let's also verify that we can send trace information to Zipkin using Kafka instead of RabbitMQ!</p>
<p>Start up the system landscape using the following commands:</p>
<pre><strong><span>export COMPOSE_FILE=docker-compose-kafka.yml<br></span>./test-em-all.bash start</strong></pre>
<p>Repeat the commands we performed in the previous <span>sections</span>, where we used RabbitMQ, and verify that you can see the same trace information in the Zipkin UI when using Kafka!</p>
<p><span>Kafka doesn't come with</span><span> a management web UI like RabbitMQ. Therefore, we need to run a few Kafka commands </span>to be able to verify that the trace events actually were passed to the Zipkin server using Kafka:</p>
<div class="packt_infobox"><span>For a recap on how to run Kafka commands when running Kafka as a Docker container, refer to</span>&nbsp;<span>the </span><em>Using Kafka with two partitions per topic</em><span> section in </span><a href="436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml">Chapter 7</a>, <em>Developing Reactive Microservices.</em></div>
<ol>
<li>First, list the available topics in Kafka:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong>docker-compose exec kafka /opt/kafka/bin/kafka-topics.sh --zookeeper zookeeper --list</strong></pre></div>
<ol start="2">
<li>Expect to find a topic named <kbd>zipkin</kbd>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8ce689f9-3d7a-49de-b487-817b43554f35.png" width="1835" height="550" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8ce689f9-3d7a-49de-b487-817b43554f35.png"></div>
<ol start="3">
<li>Next, ask for trace events that were sent to the <kbd>zipkin</kbd> topic:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic zipkin --from-beginning --timeout-ms 1000</strong></pre>
<ol start="4">
<li>Expect a lot of events similar to the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8fcdde07-be13-497a-8b95-1b56e4fe3a95.png" width="1950" height="601" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8fcdde07-be13-497a-8b95-1b56e4fe3a95.png"></div>
<p>The details of a trace event are not important. The Zipkin server sorts that out for us and makes the information presentable in the Zipkin UI. The important point here is that we can see that the trace events actually were sent to the Zipkin server using Kafka.</p>
<p>Now, bring down the system landscape and unset the <kbd>COMPOSE_FILE</kbd> environment variable:</p>
<pre><strong>docker-compose down</strong><br><strong>unset <span>COMPOSE_FILE</span></strong></pre>
<p>That concludes this chapter on distributed tracing!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we have learned how to use distributed tracing to understand how our microservices cooperate. We have learned how to use Spring Cloud Sleuth to collect trace information, and how to use Zipkin to store and visualize the trace information.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p><span>To promote the decoupling of runtime components, we have learned how to configure microservices to send trace information to the Zipkin server asynchronously while using RabbitMQ and Kafka as message brokers. We have seen how adding Spring Cloud Sleuth to microservices is effected by adding a couple of dependencies to the build files and setting up a few configuration parameters. We have also seen how the Zipkin UI</span><span> makes it very easy to identify what part of a complex workflow caused either an unexpectedly long response time or an error. Both synchronous and asynchronous workflows can be visualized by Zipkin UI.</span></p>
<p>In the next chapter, we will <span>learn about container orchestrators, specifically Kubernetes. We will learn how to use Kubernetes to deploy and manage microservices, while also improving important runtime characteristics such as scalability, high availability, and resilience.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What configuration parameter is used to control how trace information is sent to Zipkin?</li>
<li>What is the purpose of the <kbd>spring.sleuth.sampler.probability</kbd> configuration parameter<span>?</span></li>
<li>How can you identify the longest-running request after executing the <kbd>test-em-all.bash</kbd><span> test script?</span></li>
<li>How can we find requests that have been interrupted by the timeout introduced in <a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em>?</li>
<li>What does the trace look like for an API request when the circuit breaker <span>introduced in <a href="23795d34-4068-4961-842d-989cde26b642.xhtml"></a><a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>,&nbsp;</span><span><em>Improving Resilience Using Resilience4j</em>,&nbsp;</span><span>is open?</span></li>
<li>How can we locate APIs that failed on the caller not being authorized to perform the request?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to Kubernetes</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will start to learn about Kubernetes, the most popular and widely used container orchestrator at the time of writing this book. Since the subjects on cont<span>ainer orchestrators in general and&nbsp;Kubernetes itself are too big to be covered in one chapter, I will focus on introducing the a</span>reas that I have found to be the m<span>ost important when I used Kubernetes over the last few years.</span></p>
<p><span>The following topics will be covered in this chapter:</span></p>
<ul>
<li>Introducing Kubernetes concepts</li>
<li>Introducing Kubernetes API objects&nbsp;</li>
<li>Introducing Kubernetes runtime components</li>
<li>Creating a local Kubernetes cluster</li>
<li>Trying out a sample deployment and getting used to the <kbd>kubectl</kbd> Kubernetes CLI tool</li>
<li>Managing a&nbsp;<span>Kubernetes cluster</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To work with Kubernetes locally, we will use Minikube running on VirtualBox. We will also use the Kubernetes CLI tool known as&nbsp;<kbd>kubectl</kbd> a lot. <kbd>kubectl</kbd> comes with Docker for macOS, but unfortunately with a version that's too old (at least as of when this chapter was written). Therefore, we need to install a newer version. In total, we need the following:</p>
<ul>
<li>Minikube version 1.2 or later</li>
<li>kubectl&nbsp;version 1.15 or later</li>
<li>VirtualBox version 6.0 or later</li>
</ul>
<p class="mce-root"></p>
<p>These tools can be installed using Homebrew with the following commands:</p>
<div>
<pre><strong>brew install kubectl</strong><br><strong>brew cask install minikube</strong><br><strong>brew cask install virtualbox</strong></pre></div>
<p>After installing <kbd>kubectl</kbd>, run the following command to ensure that the newer version of <kbd>kubectl</kbd> is used:</p>
<pre class="p1"><strong><span class="s1">brew link --overwrite kubernetes-cli</span></strong></pre>
<p>The installation of VirtualBox will ask you to rely on the system extensions that come with VirtualBox:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d6ccf0f8-41b0-4145-8956-2f97719b5556.png" style="width:26.92em;height:10.92em;" width="412" height="166" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d6ccf0f8-41b0-4145-8956-2f97719b5556.png"></p>
<p>Click on the <span class="packt_screen">OK</span> button and then on the <span class="packt_screen">Allow</span> button in the next dialog window:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/23dd883f-5cb1-4078-be74-65f77336496c.png" style="width:35.75em;height:24.00em;" width="661" height="444" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/23dd883f-5cb1-4078-be74-65f77336496c.png"></p>
<p>Conclude the tool's installation by verifying the versions of the installed tools with the following commands:</p>
<pre><strong>kubectl version --client --short</strong><br><strong>minikube version</strong><br><strong>vboxmanage --version</strong></pre>
<p><span>Expect a response such as the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/860bf0d5-1a10-4dc4-9a58-95c7651f5921.png" style="width:17.25em;height:9.08em;" width="361" height="190" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/860bf0d5-1a10-4dc4-9a58-95c7651f5921.png"></p>
<p>The source code for this chapter&nbsp;can be found&nbsp;<span>in this book's GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter15">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter15</a>.</p>
<p>To be able to run the commands that are described in this book, you need to download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter15</span></strong></pre>
<p><span>All the source code examples in this chapter come from the source code in <kbd>$BOOK_HOME/Chapter15</kbd> and have been tested using Kubernetes 1.15.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To work with Kubernetes locally, we will use Minikube running on VirtualBox. We will also use the Kubernetes CLI tool known as <kbd>kubectl</kbd> a lot. <kbd>kubectl</kbd> comes with Docker for macOS, but unfortunately with a version that's too old (at least as of when this chapter was written). Therefore, we need to install a newer version. In total, we need the following:</p>
<ul>
<li>Minikube version 1.2 or later</li>
<li>kubectl version 1.15 or later</li>
<li>VirtualBox version 6.0 or later</li>
</ul>
<p class="mce-root"></p>
<p>These tools can be installed using Homebrew with the following commands:</p>
<div>
<pre><strong>brew install kubectl</strong><br><strong>brew cask install minikube</strong><br><strong>brew cask install virtualbox</strong></pre></div>
<p>After installing <kbd>kubectl</kbd>, run the following command to ensure that the newer version of <kbd>kubectl</kbd> is used:</p>
<pre class="p1"><strong><span class="s1">brew link --overwrite kubernetes-cli</span></strong></pre>
<p>The installation of VirtualBox will ask you to rely on the system extensions that come with VirtualBox:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d6ccf0f8-41b0-4145-8956-2f97719b5556.png" style="width:26.92em;height:10.92em;" width="412" height="166" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d6ccf0f8-41b0-4145-8956-2f97719b5556.png"></p>
<p>Click on the <span class="packt_screen">OK</span> button and then on the <span class="packt_screen">Allow</span> button in the next dialog window:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/23dd883f-5cb1-4078-be74-65f77336496c.png" style="width:35.75em;height:24.00em;" width="661" height="444" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/23dd883f-5cb1-4078-be74-65f77336496c.png"></p>
<p>Conclude the tool's installation by verifying the versions of the installed tools with the following commands:</p>
<pre><strong>kubectl version --client --short</strong><br><strong>minikube version</strong><br><strong>vboxmanage --version</strong></pre>
<p><span>Expect a response such as the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/860bf0d5-1a10-4dc4-9a58-95c7651f5921.png" style="width:17.25em;height:9.08em;" width="361" height="190" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/860bf0d5-1a10-4dc4-9a58-95c7651f5921.png"></p>
<p>The source code for this chapter can be found <span>in this book's GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter15">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter15</a>.</p>
<p>To be able to run the commands that are described in this book, you need to download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter15</span></strong></pre>
<p><span>All the source code examples in this chapter come from the source code in <kbd>$BOOK_HOME/Chapter15</kbd> and have been tested using Kubernetes 1.15.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing Kubernetes concepts</h1>
                </header>
            
            <article>
                
<p>At a high level, as a container orchestrator, Kubernetes makes a cluster of servers (physical or virtual) that run containers appear as one big logical server running containers. As an operator, we declare a desired state to the Kubernetes cluster by creating objects using the Kubernetes API. Kubernetes continuously compares the desired state with the current state. If it detects differences, it takes actions to ensure that the current state is the same as the desired state.</p>
<p>One of the main purposes of a Kubernetes cluster is to deploy and run containers, but also to support zero-downtime rolling upgrades using techniques such as green/blue and canary deployments. Kubernetes can schedule containers, that is, <strong>pods</strong> that contain one or more co-located containers, to the available nodes in the cluster. To be able to monitor the health of running containers, Kubernetes assumes that containers implement a <strong>liveness</strong> <strong>probe</strong>. If a liveness probe reports an unhealthy container, Kubernetes will restart the container. Containers can be scaled in the cluster manually or automatically using a horizontal autoscaler. To optimize the use of the available hardware resources in a cluster, for example, memory and CPU, containers can be configured with <strong>quotas</strong> that specify how much resources a container needs. On the other hand, limits regarding how much a group of containers is allowed to consume can be specified on a <strong>namespace</strong> level. Namespaces will be introduced as we proceed through this chapter. This is of extra importance if several teams share a common Kubernetes cluster.</p>
<p>Another main purpose of Kubernetes is to provide service discovery of the running pods and its containers. Kubernetes <kbd>Service</kbd> objects can be defined for services discovery and will also load balance incoming requests over the available pods. <kbd>Service</kbd> objects can be exposed externally of a Kubernetes cluster. However, as we will see, an Ingress object is, in many cases, better suited to handling externally incoming traffic to a group of services. To help Kubernetes find out whether a container is ready to accept incoming requests, a container can implement a <strong><span>readiness</span> probe</strong>.</p>
<p>Internally, a Kubernetes cluster provides one big flat IP network where each pod gets its own IP address and can reach all the other pods, independent of what node they run on. To support multiple network vendors, Kubernetes allows the use of network plugins that comply with the <strong>Container Network Interface</strong> (<strong>CNI</strong>) specification (<a href="https://github.com/containernetworking/cni">https://github.com/containernetworking/cni</a>). Pods are not isolated by default, that is, they accept all incoming requests. CNI plugins that support the use of network policy definitions can be used to lock down access to pods, for example, only allowing traffic from pods in the same namespace.</p>
<p>To allow multiple teams to work on the same Kubernetes cluster in a safe way, <strong>Role-Based Access Control</strong> (<strong>RBAC</strong>,&nbsp;<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac">https://kubernetes.io/docs/reference/access-authn-authz/rbac</a>/) can be applied. For example, administrators can be authorized to access resources on a cluster level, while the access of team members can be locked down to resources that are created in a namespace owned by the teams.</p>
<p>In total, these concepts provide a platform for running containers that is scalable, secure, highly available, and resilient.</p>
<p>Let's look a bit more into API objects that are available in Kubernetes and after that, what runtime components make up a Kubernetes cluster.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing Kubernetes API objects</h1>
                </header>
            
            <article>
                
<p>Kubernetes defines an API that is used to manage different types <span>of <em>objects</em> or <em>resources</em>, as they are also known as. </span>Some of the most commonly used types, <span>or</span><span>&nbsp;</span><em>kinds</em><span>,&nbsp;</span><span>as they are referred to in the API,</span> are as follows in my experience:</p>
<ul>
<li class="mce-root"><strong>Node:</strong> A node represents a server, <span>virtual or </span>physical, in the cluster.</li>
<li class="mce-root"><strong>Pod:</strong> A pod represents the smallest possible deployable component in Kubernetes, consisting of one or more co-located containers. Typically, a pod consists of one container, but there are use cases for extending the functionality of the main container by running the second container in a pod. In <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>, a second container will be used in the pods, running a sidecar that makes the main container join the service mesh.<strong>&nbsp;<br></strong></li>
<li><strong>Deployment</strong>: Deployment is used to deploy and upgrade pods. The deployment objects hand over the responsibility of creating and monitoring the pods to a ReplicaSet. When creating a deployment for the first time, the work performed by the deployment object is no much more than creating the ReplicaSet object. When performing a rolling upgrade of deployment, the role of the deployment object is more involved.</li>
<li><strong>ReplicaSet</strong>: A ReplicaSet is used to ensure that a specified number of pods are running at all times. If a pod is deleted, it will be replaced with a new pod by the ReplicaSet.</li>
<li class="mce-root"><strong>Service</strong>: A service is a stable network endpoint that you can use to connect to one or multiple pods. A service is assigned an IP address and a DNS name in the internal network of the Kubernetes cluster. The IP address of the service will stay the same for the lifetime of the service. Requests that are sent to a service will be forwarded to one of the available pods using round-robin-based load balancing. By default, a service is only exposed inside the cluster using a cluster IP address. It is also possible to expose a service outside the cluster, either on a dedicated port on each node in the cluster or <span>â</span> even better <span>â</span> through an external load balancer that is aware of Kubernetes, that is, it can automatically provision a public IP address and/or DNS name for the service. Cloud providers that offer Kubernetes as a service, in general, support this type of load balancer.</li>
<li><strong>Ingress<em>:</em></strong> Ingress can manage external access to services in a Kubernetes cluster, typically using HTTP. F<span>or example, i</span>t can route traffic to the underlying services based on URL paths or HTTP headers such as the hostname. Instead of exposing a number of services externally, either using node ports or through load balancers, it is, in general, more convenient to set up an Ingress in front of the services. To handle the actual communication defined by the Ingress objects, an Ingress controller must be running in the cluster. We will see an example of <span>an </span>Ingress controller as we proceed.</li>
<li class="mce-root"><strong>Namespace</strong>: A namespace is used to group and, on some levels, isolate resources in a Kubernetes cluster. The names of resources must be unique in their namespaces, but not between namespaces.</li>
<li><strong>ConfigMap</strong>: ConfigMap is used to store configuration that's used by containers. ConfigMaps can be mapped into a running container as environment variables or files.</li>
<li class="mce-root"><strong>Secret: </strong>This is used to store sensitive data used by containers, such as credentials. Secrets can be made available to containers in the same way as ConfigMaps. Anyone with full access to the API server can access the values of created secrets, so they are not as safe as the name might imply.</li>
<li><strong>DaemonSet</strong>: This ensures that one pod is running on each node in a set of nodes in the cluster. In <a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml">Chapter 19</a>, <em>Centralized Logging with the EFK Stack</em>, we will see an example of a log collector, Fluentd, that will run on each worker node.</li>
</ul>
<p>For a full list of resource objects that the Kubernetes API covers in v1.15, see <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/</a>.</p>
<p>The following diagram summarizes the Kubernetes resources that are involved in handling incoming requests:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cdf2a486-253b-4f11-abc7-2ee25e3694e6.png" style="width:26.42em;height:17.17em;" width="1722" height="1120" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cdf2a486-253b-4f11-abc7-2ee25e3694e6.png"></p>
<p>In the preceding diagram, we can see the following:</p>
<ul>
<li>Two deployments, <strong>Deployment A</strong> and <strong>Deployment B</strong>, have been deployed to a cluster with two nodes, <strong>Node 1</strong> and <strong>Node </strong><strong>2</strong>.</li>
<li><strong>Deployment A</strong> contains two pods, <strong>Pod A1</strong> and <strong>Pod</strong>&nbsp;<strong>A2</strong>.</li>
<li><strong>Deployment B</strong> contains one <strong>Pod</strong>&nbsp;<strong>B1</strong>.</li>
<li><strong>Pod A1</strong> is scheduled to <strong>Node 1</strong>.</li>
<li><strong>Pod A2</strong> and <strong>Pod B1</strong> are <span>scheduled to <strong>Node 2</strong>.</span></li>
<li>Each deployment has a corresponding service deployed, <strong>Service A</strong> and <strong>Service B</strong>, and they are available on all nodes.</li>
<li>An Ingress is defined to route incoming requests to the two services.</li>
<li>A client typically sends requests to the cluster via an external load balancer.</li>
</ul>
<p>These objects are not, by themselves, running components; instead, they are definitions of different types of desired states. To reflect the desired state in the cluster's current state, Kubernetes comes with an architecture consisting of a number of runtime components, as described in the next section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing Kubernetes runtime components</h1>
                </header>
            
            <article>
                
<p>A Kubernetes cluster contains two types of nodes: master nodes and worker nodes. Master nodes manage the cluster, while the main purpose of worker nodes is to run the actual workload, for example, the containers we deploy in the cluster. Kubernetes is built up by a number of runtime components. The most important components are as follows:</p>
<ul>
<li class="mce-root">There are components that run on master nodes, constituting the control plane:
<ul>
<li class="mce-root"><kbd>api-server</kbd>, the entry point to the control plane. This exposes a RESTful API, which, for example, the Kubernetes CLI tool known as <kbd>kubectl</kbd><span> uses.</span></li>
<li class="mce-root"><kbd>etcd</kbd>, a highly available and distributed key/value store, used as a database for all cluster data.</li>
<li class="mce-root">A controller manager, which contains a number of controllers that continuously <span><span>evaluate the desired state versus the current state for the objects defined in the <kbd>etcd</kbd> database.</span></span></li>
<li><span>Whenever the desired or the current state changes, a controller that's responsible for that type of state </span><span>takes actions to move the current state to the desired state. For example, a replication controller that's responsible for managing pods will react if a new pod is added through the API server or a running pod dies and ensures that new pods are started. </span><span>Another example of a controller is the node controller. It is responsible for acting if a node becomes unavailable, ensuring that pods running on a failing node are rescheduled on other nodes in the cluster.<strong><br></strong></span></li>
<li class="mce-root">A <strong>Scheduler</strong>, which is responsible for assigning newly created pods to a node with available capacity, for example, in terms of memory and CPU. Affinity rules can be used to control how pods are assigned to nodes. For example, pods that perform a lot of disks I/O can be assigned to a group of worker nodes that have fast SSD disks. Anti-affinity rules can be defined to separate pods, for example, to avoid scheduling pods from the same deployment to the same worker node.<strong><br></strong></li>
</ul>
</li>
<li class="mce-root"><span>Components that run on all the nodes that constitute the data plane are as follows:<br></span>
<ul>
<li class="mce-root"><kbd>kubelet</kbd>, <span>a node agent that executes as a process directly in the nodes operating system and not as a container. It is responsible for that the containers that are up and running in the pods being assigned to the node where <kbd>kubelet</kbd> runs</span>. It acts as a conduit between the <kbd>api-server</kbd> and the container runtime on its node.</li>
<li class="mce-root"><kbd>kube-proxy</kbd>, a network proxy that enables the service concept in Kubernetes and is capable of forwarding requests to the appropriate pods, typically in a round-robin fashion if more than one pod is available for the specific service. <kbd>kube-proxy</kbd> is deployed as a DaemonSet.</li>
<li class="mce-root"><strong>Con</strong><strong>tainer</strong> <strong>runtime</strong>, which is the software that runs containers on the node. Typically, this is Docker, but any implementation of the Kubernetes <strong>Container Runtime Interface</strong> (<strong>CRI</strong>) can be used, for example, <kbd>cri-o</kbd>&nbsp;(<a href="https://cri-o.io">https://cri-o.io</a>), <kbd>containerd</kbd>&nbsp;(<a href="https://containerd.io/">https://containerd.io/</a>), or <kbd>rktlet</kbd>&nbsp;(<a href="https://github.com/kubernetes-incubator/rktlet">https://github.com/kubernetes-incubator/rktlet</a>).</li>
<li><strong>Kubernetes DNS</strong>, which is a DNS server that's used in the cluster's internal network. Services and pods are assigned a DNS name, and pods are configured to use this DNS server to resolve the internal DNS names. The DNS server is deployed as a deployment object and a service object.</li>
</ul>
</li>
</ul>
<p>The following diagram summarizes the Kubernetes runtime components:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6d8bfda7-c2b2-4b85-b4d7-5f30250bc9b9.png" style="width:46.17em;height:25.58em;" width="1967" height="1090" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6d8bfda7-c2b2-4b85-b4d7-5f30250bc9b9.png"></p>
<p>Now that we understand the Kubernetes runtime components and what they support and run on, let's move on to creating a Kubernetes cluster with Minikube.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Creating a Kubernetes cluster using Minikube</h1>
                </header>
            
            <article>
                
<p>Now, we are ready to create a Kubernetes cluster! We will use Minikube to create a local single-node cluster running on VirtualBox.</p>
<p>Before creating the Kubernetes cluster, we need to learn a bit about Minikube profiles, the Kubernetes CLI tool known as <kbd>kubectl</kbd>, and its use of contexts.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Working with Minikube profiles</h1>
                </header>
            
            <article>
                
<p>In order to run multiple Kubernetes clusters locally, Minikube comes with the concept of profiles. For example, if you want to work with multiple versions of Kubernetes, you can create multiple Kubernetes clusters using Minikube. Each cluster will be assigned a separate Minikube profile. Most of the Minikube commands accept a <kbd>--profile</kbd> flag (or <kbd>-p</kbd> for short) that can be used to specify which of the <span>Kubernetes clusters the command shall be applied to. If you plan to work with one specific profile for a while, a more convenient alternative exists, where you specify the current profile with the following command:</span></p>
<pre><strong>minikube profile my-profile</strong></pre>
<p>The preceding command will set the <kbd>my-profile</kbd> profile as the current profile.</p>
<p>To get the current profile, run the following command:</p>
<pre><strong>minikube config get profile</strong></pre>
<p>If no profile is specified, neither using the <kbd>minikube profile</kbd> command nor the <kbd>--profile</kbd> switch, a default profile named <kbd>minikube</kbd> will be used.</p>
<div class="packt_infobox">Information regarding existing profiles can be found in the <kbd>~/.minikube/profiles</kbd> folder.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Working with Kubernetes CLI, kubectl</h1>
                </header>
            
            <article>
                
<p><kbd>kubectl</kbd> is the Kubernetes CLI tool. Once a cluster has been set up, this is usually the only tool you need to manage the cluster!</p>
<p>For managing the API objects we described earlier in this chapter, the <kbd>kubectl apply</kbd> command is the only command you need to know about. It is a declarative command; that is, as an operator, we ask Kubernetes to apply the object definition we give to the command. It is then up to Kubernetes to figure out what actually needs to be done.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<div class="packt_infobox">Another example of a declarative command that's hopefully familiar to many readers of this book is a <kbd>SQL SELECT</kbd> statement that joins information from several database tables. We only declare the expected result in the SQL query, and it is up to the database query optimizer to figure out in what order the tables shall be accessed and what indexes to use to retrieve the data in the most efficient way.</div>
<p>In some cases, imperative statements that explicitly tell Kubernetes what to do are preferred. One example is the <kbd>kubectl delete</kbd> command, where we explicitly tell Kubernetes to delete some API objects. Creating a namespace object can also be conveniently done with an explicit <kbd>kubectl create namespace</kbd> command. </p>
<p>Repetitive usage of the imperative statements will make them fail, for example, deleting the same API object twice using <kbd>kubectl delete</kbd> or creating the same namespace twice using <kbd>kubectl create</kbd>. A declarative command, that is, using <kbd>kubectl apply</kbd>, will not fail on repetitive usageâit will simply state that there is no change and exit without taking any action.</p>
<p>Some commonly used commands for retrieving information about a Kubernetes cluster are as follows:</p>
<ul>
<li><kbd>kubectl get</kbd> shows information about the specified API object.</li>
<li><kbd>kubectl describe</kbd> gives more detail about the specified API object.</li>
<li><kbd>kubectl logs</kbd> display log output from containers.</li>
</ul>
<p>We will see a lot of examples of these and other <kbd>kubectl</kbd> commands in this and the upcoming chapters!</p>
<p>If in doubt about how to use the <kbd>kubectl</kbd> tool, the <kbd>kubectl help</kbd> and <kbd>kubectl &lt;command&gt; --help</kbd> commands are always available and provide very useful information on how to use the <kbd>kubectl</kbd> tool.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Working with kubectl contexts</h1>
                </header>
            
            <article>
                
<p>To be able to work with more than one Kubernetes cluster, <span>using </span>either Minikube locally or Kubernetes clusters set up on-premises servers or in the cloud, <kbd>kubectl</kbd> comes with the concept of contexts. A context is a combination of the following:</p>
<ul>
<li>A Kubernetes cluster</li>
<li>Authentication information for a user</li>
<li>A default namespace</li>
</ul>
<p>By default, contexts are saved in the <kbd>~/.kube/config</kbd> file, but the file can be changed using the <kbd>KUBECONFIG</kbd> environment variable. In this book, we will use the default location, so we will unset <span><kbd>KUBECONFIG</kbd> using the <kbd>unset KUBECONFIG</kbd> command.</span></p>
<p>When a Kubernetes cluster is created in Minikube, a context is created with the same name as the Minikube profile and is then set as the current context. So, <kbd>kubectl</kbd> commands that are issued after the cluster is created in Minikube will be sent to that cluster.</p>
<p>To list the available contexts, run the following command:</p>
<pre><strong>kubectl config get-contexts</strong></pre>
<p>The following is a sample response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9f432d73-d57a-4227-bdf6-880fe1b7a39a.png" width="1099" height="138" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9f432d73-d57a-4227-bdf6-880fe1b7a39a.png"></p>
<p>The wildcard, <kbd>*</kbd>, in the first column, mark the current context.</p>
<div class="packt_infobox">
<p>You will only see the <kbd>handson-spring-boot-cloud</kbd> context in the preceding response once the cluster has been created, which we will describe here.</p>
</div>
<p>If you want to switch the current context to another context, that is, work with another Kubernetes cluster, run the following command:</p>
<pre><strong>kubectl config use-context my-cluster</strong></pre>
<p>In the preceding example, the current context will be changed to <kbd>my-cluster</kbd>.</p>
<p><span>To update a context, for example, switching the default namespace used by</span> <kbd>kubectl</kbd><span>, use the </span><kbd>kubectl config set-context</kbd> <span>command.</span></p>
<p><span>For example, to change the default namespace of the current context to <kbd>my-namespace</kbd>, use the following command:</span></p>
<pre><strong>kubectl config set-context $(kubectl config current-context) --namespace my-namespace</strong></pre>
<p>In the preceding command, <kbd>kubectl config current-context</kbd><span>&nbsp;</span>is used to get the name of the current context.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Creating a Kubernetes cluster</h1>
                </header>
            
            <article>
                
<p>To create a Kubernetes cluster using Minikube, we need to run a few commands:</p>
<ul>
<li>Unset <span>the </span><span><kbd>KUBECONFIG</kbd> environment variable to ensure that the <kbd>kubectl</kbd> context is created in the default config file, <kbd>~/.kube/config</kbd>.</span></li>
<li>Specify the Minikube profile to be used for the cluster. <span>We will use </span><kbd>handson-spring-boot-cloud</kbd><span> as the profile name.</span></li>
<li><span>Create the cluster using the </span><kbd>minikube start</kbd><span> command, where we can also specify how much hardware resources we want to allocate to the cluster. To be able to complete the examples in the remaining chapters of this book, allocate at least 10 GB of memory, that is, 10,240 MB, to the cluster.</span></li>
<li>After the cluster has been created, we will use the add-on manager in Minikube to enable an Ingress controller and a metrics server that comes out of the box with Minikube. The Ingress controller and the metrics will be used in the next two chapters.</li>
</ul>
<div class="packt_infobox"><span>Before you create a Kubernetes cluster using Minikube, it might be a good idea to shut down Docker for macOS to avoid running out of memory.</span></div>
<p>Run the following commands to create the Kubernetes cluster:</p>
<pre><strong>unset <span>KUBECONFIG<br></span></strong><br><strong>minikube profile handson-spring-boot-cloud</strong><br><br><strong>minikube start \</strong><br><strong> --memory=10240 \</strong><br><strong> --cpus=4 \</strong><br><strong> --disk-size=30g \</strong><br><strong> --kubernetes-version=v1.15.0 \</strong><br><strong> --vm-driver=virtualbox</strong><br><br><strong>minikube addons enable Ingress</strong><br><strong>minikube addons enable metrics-server</strong></pre>
<p>After the preceding commands complete, you should be able to communicate with the cluster. Try the <kbd>kubectl get nodes</kbd> command. It should respond with something that looks similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/da7ecf41-9129-4e4a-8eb1-c37936e83aa6.png" style="width:22.83em;height:5.67em;" width="459" height="114" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/da7ecf41-9129-4e4a-8eb1-c37936e83aa6.png"></p>
<p>Once created, the cluster will initialize itself in the background, starting up a number of system pods in the <kbd>kube-system</kbd> namespace. We can monitor its progress by issuing the following command:</p>
<pre><strong>kubectl get pods --namespace=kube-system</strong></pre>
<p class="mce-root">Once the startup is complete, the preceding command should report the status for all pods as <kbd>Running</kbd> and the <span class="packt_screen">READY</span> count should be <kbd>1/1</kbd>, meaning that a single container in each pod is up and running:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/91deb3a4-1969-4c58-a0bf-6c7bab49fd86.png" style="width:50.75em;height:22.00em;" width="793" height="344" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/91deb3a4-1969-4c58-a0bf-6c7bab49fd86.png"></p>
<p>We are now ready for some action!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out a sample deployment</h1>
                </header>
            
            <article>
                
<p>Let's see how we can do the following:</p>
<ul>
<li>Deploy a simple web server based on NGINX in our Kubernetes cluster.</li>
<li>Apply some changes to the deployment:
<ul>
<li>Delete a pod and verify that the ReplicaSet creates a new one.</li>
<li>Scale the web server to three pods to <span>verify that the ReplicaSet fills the gap.</span></li>
</ul>
</li>
<li>Route external traffic to it using a service with a node port.</li>
</ul>
<p>First, create a namespace, <kbd>first-attempts</kbd>, and update the <kbd>kubectl</kbd> context to use this namespace by default:</p>
<pre><strong>kubectl create namespace first-attempts</strong><br><strong>kubectl config set-context $(kubectl config current-context) --namespace=first-attempts</strong></pre>
<p>We can now create a deployment of NGINX in the namespace using the <kbd>kubernetes/first-attempts/nginx-deployment.yaml</kbd> file. This file looks as follows:</p>
<pre>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: nginx-deploy<br>spec:<br>  replicas: 1<br>  selector:<br>    matchLabels:<br>    app: nginx-app<br>  template:<br>    metadata:<br>      labels:<br>        app: nginx-app<br>    spec:<br>      containers:<br>      - name: nginx-container<br>        image: nginx:latest<br>        ports:<br>        - containerPort: 80</pre>
<p>Let's explain the preceding source code in more detail:</p>
<ul>
<li>The <kbd>kind</kbd> and <kbd>apiVersion</kbd> attributes are used to specify that we are declaring a deployment object.</li>
<li>The <kbd>metadata</kbd> section is used to describe the d<span><span>eployment object, for example, when we give it a name of <kbd>nginx-deploy</kbd>.</span></span></li>
<li><span><span>Next comes a <kbd>spec</kbd> section that defines our desired state of the deployment object:<br></span></span>
<ul>
<li><kbd>replicas: 1</kbd> specifies we want to have one pod up and running.</li>
<li>A <kbd>selector</kbd> section that specifies how the deployment will find the pods it manages. In this case, the deployment will look for pods that have the <kbd>app</kbd> label set to <kbd>nginx-app</kbd>.</li>
<li>The <kbd>template</kbd> section is used to specify how pods shall be created:
<ul>
<li>The <kbd>metadata</kbd> section specifies the <kbd>label</kbd>, <kbd>app: nginx-app</kbd>, which is used to identify the pods, thereby matching the selector.</li>
<li>The <kbd>spec</kbd> section specifies details for the creation of the single container in the pod, that is, <kbd>name</kbd> and <kbd>image</kbd> and what <kbd>ports</kbd> it uses.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Create the deployment with the following commands:</p>
<pre><strong>cd $BOOK_HOME<span>/Chapter15<br></span>kubectl apply -f kubernetes/first-attempts/nginx-deployment.yaml</strong></pre>
<p>Let's see what we got with the <kbd>kubectl get all</kbd> command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b598f9f8-8904-434f-8927-9e2ff0860e89.png" style="width:37.92em;height:12.92em;" width="739" height="252" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b598f9f8-8904-434f-8927-9e2ff0860e89.png"></p>
<p>As expected, we got a deployment, ReplicaSet, and pod object. After a short while, which mainly depends on the time it takes to download the NGINX Docker image, the pod will be up and running, and the desired state will be equal to the current state!</p>
<p>Change the current state by deleting the pod with the following command:</p>
<pre><strong>kubectl delete pod --selector app=nginx-app</strong></pre>
<div class="packt_infobox">Since the pod has a random name (<kbd>nginx-deploy-59b8c5f7cd-mt6pg</kbd> in the preceding example), the pod is selected <span>based on the</span> <kbd>app</kbd><span> label, which is set to </span><kbd>nginx-app</kbd><span> in the pod.</span></div>
<p>Running a subsequent <kbd>kubectl get all</kbd><span> command will reveal that the difference between the desired and current state was detected and handled by the ReplicaSet in just a few seconds, that is, a new pod was launched almost immediately.</span></p>
<p>Change the desired state by setting the number of desired pods to three replicas in the <kbd>kubernetes/first-attempts/nginx-deployment.yaml</kbd> deployment file. Apply the change in the desired state by simply repeating the <kbd>kubectl apply</kbd> command, as we mentioned previously.</p>
<p>Quickly run the<span>&nbsp;</span><kbd>kubectl get all</kbd><span> command a couple of times to monitor how Kubernetes takes action to ensure that the current state meets the new desired state. After a few seconds, two new NGINX pods will be up and running. The desired state is, again, equal to the current state with three running NGINX pods. Expect a response that looks similar to the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/45d803cd-745b-4d3f-9d25-79df6eadfeb1.png" style="width:38.17em;height:15.25em;" width="748" height="299" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/45d803cd-745b-4d3f-9d25-79df6eadfeb1.png"></p>
<p>To enable external communication with the web servers, create a service using the <kbd>kubernetes/first-attempts/nginx-service.yaml</kbd> file:</p>
<pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: nginx-service<br>spec:<br>  type: NodePort<br>  selector:<br>    app: nginx-app<br>  ports:<br>    - targetPort: 80<br>      port: 80<br>      nodePort: 30080</pre>
<p>Let's explain the preceding source code in more detail:</p>
<ul>
<li>The<span>&nbsp;</span><kbd>kind</kbd><span>&nbsp;</span>and<span>&nbsp;</span><kbd>apiVersion</kbd><span>&nbsp;</span>attributes are used to specify that we are declaring a <kbd>Service</kbd> object.</li>
<li>The<span>&nbsp;</span><kbd>metadata</kbd><span>&nbsp;</span>section is used to describe the <kbd>Service</kbd> <span>object, for example, to give it a name: <kbd>nginx-service</kbd>.</span></li>
<li><span>Next comes a <kbd>spec</kbd> section, which defines</span> the desired state of the <kbd>Service</kbd> object:
<ul>
<li>With the <kbd>type</kbd> field, we specify that we want <kbd>NodePort</kbd>, that is, something that's accessible externally on a dedicated port on each node in the cluster. This means that an external caller can reach the pods using this port on any of the nodes in the cluster, independent of which nodes the pods actually run on.</li>
<li>The selector is used by the service to find available pods, which, in our case, is pods labeled with <kbd>app: nginx-app</kbd>.</li>
<li>Finally, <kbd>ports</kbd> are declared as follows:
<ul>
<li><kbd>port: 80</kbd> specifies on which port the services will be accessible on, that is, internally in the cluster.</li>
<li><kbd>nodePort: 30080</kbd> specifies on what port the service will be externally accessible on using any of the nodes in the cluster. By default, a node port must be in the range of <kbd>30000</kbd> to <kbd>32767</kbd>.</li>
<li><kbd>targetPort: 80</kbd> specifies the port in the pod where the requests shall be forwarded to.</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="packt_infobox">This port range is used to minimize the risk of colliding with other ports in use. In a production system, a load balancer is typically placed in front of the Kubernetes cluster, shielding the external users both from the knowledge of these ports and the IP numbers of the nodes in the Kubernetes cluster. See <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>, the <em>Setting up port forwarding required by Istio</em> section, for more on the usage of a <kbd>LoadBalanced</kbd> Kubernetes service.</div>
<p><span>Create the service with the following command:</span></p>
<pre><strong>kubectl apply -f kubernetes/first-attempts/nginx-service.yaml</strong></pre>
<p class="CDPAlignLeft CDPAlign"><span>To see what we got, run the <kbd>kubectl get svc</kbd> command. Expect a response such as the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/987abb3e-e317-4cb1-8169-63a163c2ac1c.png" style="width:37.00em;height:5.58em;" width="776" height="116" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/987abb3e-e317-4cb1-8169-63a163c2ac1c.png"></p>
<div class="packt_infobox"><kbd>kubectl</kbd> supports short names for many of the API objects as an alternative to their full name. For example, <kbd>svc</kbd> was used in the preceding command instead of the full name, <kbd>service</kbd>.</div>
<p>To try this out, we need to know the IP address of the single node in our cluster. We can get that by issuing the <kbd>minikube ip</kbd> command. In my case, it is <span><kbd>192.168.99.116</kbd>. With this IP address and the node port <kbd>30080</kbd>, we can direct our web browser to the deployed web server. In my case, the address is <kbd>http://192.168.99.116:30080</kbd></span>. Expect a response such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2f710472-8b21-4b04-ba91-1496388ec643.png" style="width:28.67em;height:15.17em;" width="573" height="303" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2f710472-8b21-4b04-ba91-1496388ec643.png"></p>
<p>Great! But what about the internal cluster IP address and port?</p>
<p>One way to verify this is to launch a small pod inside the cluster that we can use to run <kbd>curl</kbd> from the inside, that is, we are able to use the internal cluster IP address and port. We don't need to use the IP address; instead, we can use a DNS name that is created for the service in the internal DNS server. The short name of the DNS name is the same as the name of the service, that is, <kbd>nginx-service</kbd>.</p>
<p>Run the following command:</p>
<pre><strong>kubectl run -i --rm --restart=Never curl-client --image=tutum/curl:alpine --command -- curl -s 'http://nginx-service:80'</strong></pre>
<p>The preceding command looks a bit complex, but it will only do the following:</p>
<ol>
<li>Create a pod with a small container based on the <kbd>tutum/curl:alpine</kbd> Docker image, which contains the <kbd>curl</kbd> command.</li>
<li>Run the <kbd>curl -s 'http://nginx-service:80'</kbd> command inside the container and redirect the output to the Terminal using the <kbd>-i</kbd> option.</li>
<li>Delete the pod using the <kbd>--rm</kbd> option.</li>
</ol>
<p>Expect the output from the preceding command to contain the following information (we are only showing parts of the response here):</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/da13fcf8-bed7-49e3-ac2a-5013a5d3621f.png" width="1232" height="186" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/da13fcf8-bed7-49e3-ac2a-5013a5d3621f.png"></p>
<p>This means that the web server is also accessible internally in the cluster!</p>
<p class="mce-root">This is basically all we need to know to be able to deploy our system landscape.</p>
<p>Wrap this up by removing the namespace containing the <kbd>nginx</kbd> deployment:</p>
<pre><strong>kubectl delete namespace first-attempts</strong></pre>
<p>Before we end this introductory chapter on Kubernetes, we need to learn how to manage our Kubernetes cluster.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Managing a Kubernetes cluster</h1>
                </header>
            
            <article>
                
<p>A running Kubernetes cluster consumes a lot of resources, mostly memory. So, when we are done working with a Kubernetes cluster in Minikube, we must be able to hibernate it in order to release the resources allocated to it. We also need to know how to resume the cluster when we want to continue working with it. Eventually, we must also be able to permanently remove the cluster when we don't want to keep it on disk anymore.</p>
<p>Minikube comes with a <kbd>stop</kbd> command that can be used to hibernate a Kubernetes cluster. The <kbd>start</kbd> command we used to initially create the Kubernetes cluster can also be used to resume the cluster from its hibernated state. To permanently remove a cluster, we can use the <kbd>delete</kbd> command from Minikube.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Hibernating and resuming a Kubernetes cluster</h1>
                </header>
            
            <article>
                
<p>Run the following command to hibernate (that is, <kbd>stop</kbd>) the Kubernetes cluster:</p>
<pre><strong>minikube stop</strong></pre>
<p>Run the following command to resume (that is, <kbd>start</kbd>) the Kubernetes cluster again:</p>
<pre><strong>minikube start</strong></pre>
<div class="packt_infobox"><span>When resuming an already existing cluster, the <kbd>start</kbd> command ignores switches that were used when you were creating the cluster.</span></div>
<p class="mce-root">After resuming the <span>Kubernetes cluster, the <kbd>kubectl</kbd> context will be updated to use this cluster with the currently used</span> namespace <span>set to <kbd>default</kbd>. If you are working with another</span> namespace<span>, for example, the <kbd>hands-on</kbd> namespace that we will use in the upcoming chapter, that is, <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>,&nbsp;<em>Deploying Our Microservices to Kubernetes</em>, you can update the <kbd>kubectl</kbd> context with the following command:</span></p>
<pre><strong>kubectl config set-context $(kubectl config current-context) --namespace=<span>hands-on</span></strong><span><br></span></pre>
<p>Subsequent <kbd>kubectl</kbd> commands will be applied to the <kbd>hands-on</kbd><span> namespace when applicable.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have been introduced to Kubernetes as a container orchestrator. Kubernetes makes a cluster of servers that run containers appear as one big logical server. As an operator, we declare a desired state to the cluster and Kubernetes continuously compares the desired state with the current state. If it detects differences, it takes actions to ensure that the current state is the same as the desired state.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>The desired state is declared by creating resources using the Kubernetes API server. The controller manager in Kubernetes and its controllers react to the various resources that were created by the API server and takes actions to ensure that the current state meets the new desired state. The scheduler assigns nodes to newly created containers, that is, pods that contain one or more containers. On each node, an agent, <kbd>kubelet</kbd>, runs and ensures that the pods that were scheduled to its node are up and running. <kbd>kube-proxy</kbd> acts as a network proxy, enabling a service abstraction by forwarding requests that are sent to the service to available pods in the cluster. External requests can be handled either by a service that specifies a node port that's available on all of the nodes in the cluster or through a dedicated Ingress resource.</p>
<p>We have also tried out Kubernetes by creating a local single-node cluster using Minikube and VirtualBox. Using the Kubernetes CLI tool known as&nbsp;<kbd>kubectl</kbd>, we deployed a simple web server based on NGINX. We tried out resilience capabilities by deleting the web server, and we observed it being recreated automatically and scaled it by requesting three pods running on the web server. Finally, we created a service with a node port and verified that we could access it both externally and from the inside of the cluster.</p>
<p>Finally, we learned how to manage a Kubernetes cluster running in Minikube on VirtualBox in terms of how to hibernate, resume, and terminate a Kubernetes cluster.</p>
<p><span>We are now ready to deploy our system landscape from the earlier chapters in Kubernetes. H</span>ead over to the next chapter to find out how to do this!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have been introduced to Kubernetes as a container orchestrator. Kubernetes makes a cluster of servers that run containers appear as one big logical server. As an operator, we declare a desired state to the cluster and Kubernetes continuously compares the desired state with the current state. If it detects differences, it takes actions to ensure that the current state is the same as the desired state.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>The desired state is declared by creating resources using the Kubernetes API server. The controller manager in Kubernetes and its controllers react to the various resources that were created by the API server and takes actions to ensure that the current state meets the new desired state. The scheduler assigns nodes to newly created containers, that is, pods that contain one or more containers. On each node, an agent, <kbd>kubelet</kbd>, runs and ensures that the pods that were scheduled to its node are up and running. <kbd>kube-proxy</kbd> acts as a network proxy, enabling a service abstraction by forwarding requests that are sent to the service to available pods in the cluster. External requests can be handled either by a service that specifies a node port that's available on all of the nodes in the cluster or through a dedicated Ingress resource.</p>
<p>We have also tried out Kubernetes by creating a local single-node cluster using Minikube and VirtualBox. Using the Kubernetes CLI tool known as <kbd>kubectl</kbd>, we deployed a simple web server based on NGINX. We tried out resilience capabilities by deleting the web server, and we observed it being recreated automatically and scaled it by requesting three pods running on the web server. Finally, we created a service with a node port and verified that we could access it both externally and from the inside of the cluster.</p>
<p>Finally, we learned how to manage a Kubernetes cluster running in Minikube on VirtualBox in terms of how to hibernate, resume, and terminate a Kubernetes cluster.</p>
<p><span>We are now ready to deploy our system landscape from the earlier chapters in Kubernetes. H</span>ead over to the next chapter to find out how to do this!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What happens if you run the same <kbd>kubectl create</kbd> command twice?</li>
<li>What happens if <span>you run the same <kbd>kubectl apply</kbd> command twice?</span></li>
<li>In terms of questions <em>1</em> and <em>2</em>, why do they act differently the second time they are run?</li>
<li>What is the purpose of a ReplicaSet, and what other resource creates a ReplicaSet?</li>
<li>What is the purpose of <kbd>etcd</kbd> in a Kubernetes cluster?</li>
<li>How can a container find out the IP address of another container that runs in the same pod?</li>
<li>What happens if you create two deployments with the same name but in different namespaces?</li>
<li>What can you make the creation of two services with the same name fail if they are created in two different namespaces?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All the commands that are described in this book<span>&nbsp;are run on a MacBook Pro using macOS Mojave but should be straightforward to modify if you want to run them on another platform such as Linux or Windows.</span></p>
<p>The only new tool that's required for this chapter is the&nbsp;<kbd>siege</kbd>&nbsp;command-line tool, which is used for HTTP-based load testing and&nbsp;<span>benchmarking.&nbsp;</span>We will use <kbd>siege</kbd> to put some load on the Kubernetes cluster while performing rolling upgrades. The tool can be installed using Homebrew with the following commands:</p>
<div>
<pre><strong>brew install siege</strong></pre></div>
<p>The source code for this chapter&nbsp;can be found&nbsp;<span>in this book's GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter16">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter16</a>.</p>
<p>To be able to run the commands that are described in this book, you need to download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter16</span></strong></pre>
<p><span>All the source code examples in this chapter come from the source code in&nbsp;<kbd>$BOOK_HOME/Chapter16</kbd>&nbsp;and&nbsp;have been tested using Kubernetes 1.15.</span></p>
<p>If you want to see the changes that were applied to the source code in this chapter,&nbsp;that is, see the changes that are required to be able to deploy the microservices on Kubernetes<span>,&nbsp;</span>you can compare it with the source code for <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>, <em>Introduction to Kubernetes</em>.<span>&nbsp;You can use your favorite <kbd>diff</kbd> tool and compare the two folders,&nbsp;<kbd>$BOOK_HOME/Chapter15</kbd> and <kbd>$BOOK_HOME/Chapter16</kbd>.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All the commands that are described in this book<span> are run on a MacBook Pro using macOS Mojave but should be straightforward to modify if you want to run them on another platform such as Linux or Windows.</span></p>
<p>The only new tool that's required for this chapter is the <kbd>siege</kbd> command-line tool, which is used for HTTP-based load testing and <span>benchmarking. </span>We will use <kbd>siege</kbd> to put some load on the Kubernetes cluster while performing rolling upgrades. The tool can be installed using Homebrew with the following commands:</p>
<div>
<pre><strong>brew install siege</strong></pre></div>
<p>The source code for this chapter can be found <span>in this book's GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter16">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter16</a>.</p>
<p>To be able to run the commands that are described in this book, you need to download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter16</span></strong></pre>
<p><span>All the source code examples in this chapter come from the source code in <kbd>$BOOK_HOME/Chapter16</kbd> and have been tested using Kubernetes 1.15.</span></p>
<p>If you want to see the changes that were applied to the source code in this chapter, that is, see the changes that are required to be able to deploy the microservices on Kubernetes<span>,&nbsp;</span>you can compare it with the source code for <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>, <em>Introduction to Kubernetes</em>.<span> You can use your favorite <kbd>diff</kbd> tool and compare the two folders, <kbd>$BOOK_HOME/Chapter15</kbd> and <kbd>$BOOK_HOME/Chapter16</kbd>.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Replacing Netflix Eureka with Kubernetes services</h1>
                </header>
            
            <article>
                
<p>As shown in the previous chapter, <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>, <em>Introduction to Kubernetes</em>, Kubernetes comes with a <span>built-in discovery service based on Kubernetes</span> <kbd>Service</kbd><span> objects and the </span><kbd>kube-proxy</kbd><span> runtime component. This makes it unnecessary to deploy a separate discovery service such as Netflix Eureka, which we used in the previous chapters. An advantage of using Kubernetes discovery service is that it doesn't require a client library such as Netflix Ribbon, which we have been using together with Netflix Eureka. This makes the Kubernetes discovery service easy to use, independent of which language or framework a microservice is based on. A drawback of using the Kubernetes discovery service is that it only works in a Kubernetes environment. However, since the discovery service is based on <kbd>kube-proxy</kbd>, which accepts requests to the DNS name or IP address of a service object, it should be fairly simple to replace it with a similar discovery service, for example, one that comes bundled with another container orchestrator.</span></p>
<p>To summarize this, we will remove the discovery server based on Netflix Eureka from our microservice landscape, as illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8b8eed5f-e4b6-4d3a-85a1-e0df126462ee.png" width="1918" height="957" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8b8eed5f-e4b6-4d3a-85a1-e0df126462ee.png"></p>
<p class="mce-root"></p>
<p>To replace the d<span>iscovery server based on Netflix Eureka with the Kubernetes built-in discovery service, the following changes have been applied to the source code:</span></p>
<ul>
<li>Netflix Eureka and the Ribbon-specific configuration (client and server) have been removed from the configuration repository, <kbd>config-repo</kbd>.</li>
<li>Routing rules in the gateway service to the Eureka server have been removed from the <kbd>config-repo/gateway.yml</kbd><span> file.</span></li>
<li><span>We've removed the Eureka server project, that is, we've removed the <kbd>spring-cloud/eureka-server</kbd> folder.</span></li>
<li>We've removed the Eureka server from the Docker Compose files and the <kbd>settings.gradle</kbd> Gradle file.</li>
<li>We've removed the dependency to <kbd>spring-cloud-starter-netflix-eureka-client</kbd> in all of Eureka's client build files, that is, <kbd>build.gradle</kbd>.</li>
<li>We've removed the no-longer-required <kbd>eureka.client.enabled=false</kbd> property setting from all of Eureka's client integration tests.</li>
<li>The gateway service no longer uses routing based on the client-side load balancer in Spring Cloud using the<span>&nbsp;</span><kbd>lb</kbd><span>&nbsp;</span>protocol. For example,  the <kbd>lb://product-composite</kbd> routing destination has been replaced by<span> the </span><kbd>http://product-composite</kbd> in t<span>he </span><kbd>config-repo/gateway.yml</kbd><span> file.</span></li>
<li class="mce-root"><span>The HTTP port used by the microservices and the authorization server has been changed from port</span> the <kbd>8080</kbd> port <span>(</span><kbd>9999</kbd> <span>in the case of the authorization server) to the default HTTP port</span> <kbd>80</kbd><span>. This has been configured in <kbd>config-repo</kbd> for each affected service like so:</span></li>
</ul>
<pre style="color: black;padding-left: 60px">spring.profiles: docker<br>server.port: 80</pre>
<p>None of the HTTP addresses that we are using are affected by the replacement of Netflix Eureka with Kubernetes services. For example<span>, addresses used by the composite service are unaffected</span>:</p>
<div>
<pre><span>private</span><span> </span><span>final</span><span> </span><span>String</span><span> </span><span>productServiceUrl</span><span> </span><span>=</span><span> </span><span>"http://product"</span><span>;<br></span><span>private</span><span> </span><span>final</span><span> </span><span>String</span><span> </span><span>recommendationServiceUrl</span><span> </span><span>=</span><span> </span><span>"http://recommendation"</span><span>;<br></span><span>private</span><span> </span><span>final</span><span> </span><span>String</span><span> </span><span>reviewServiceUrl</span><span> </span><span>=</span><span> </span><span>"http://review"</span><span>;</span></pre></div>
<p>This is achieved by changing the HTTP port <span>used by the microservices and the</span> <span>authorization server</span> to the default HTTP port, <kbd>80</kbd>, as described previously. </p>
<p class="mce-root"></p>
<div class="packt_infobox">Using Docker Compose still works, even though Netflix Eureka has been removed. This can be used for running functional <span>tests of the microservices without deploying them to Kubernetes, for example, running <kbd>test-em-all.bash</kbd> together with Docker for macOS in the same way as in the previous chapters. Removing Netflix Eureka, however, means that we no longer have a discovery service in place when using plain Docker and Docker Compose. Therefore, scaling microservices will only work when deploying to Kubernetes.</span></div>
<p>Now that we've familiarized ourselves with Kubernetes services, let's move on to Kustomize, a tool that's used for customizing Kubernetes objects.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing Kustomize</h1>
                </header>
            
            <article>
                
<p><strong>Kustomize</strong> is a tool that's used for creating environment-specific customizations of the Kubernetes definitions files, that is, the YAML files, for example, for development, test, staging, and production environments. Common definition files are stored in a <kbd>base</kbd> folder, while environment-specific additions are kept in environment-specific <kbd>overlay</kbd> folders. E<span>nvironment-specific information can, for example, be any of the following:</span></p>
<ul>
<li><span>What version of the Docker images to use</span></li>
<li><span>Number of replicas to run </span></li>
<li><span>Resource quotas in terms of CPU and memory</span></li>
</ul>
<p>Each folder contains a <kbd>kustomization.yml</kbd> file that describes its content for Kustomize. When deploying to a specific environment, <span>Kustomize will take the content from the <kbd>base</kbd> folder and the environment-specific <kbd>overlay</kbd> folder and send the combined result to <kbd>kubectl</kbd>. Properties from the files in the <kbd>overlay</kbd> folder will override the corresponding properties in the <kbd>base</kbd> folder, if any.  </span></p>
<p class="mce-root">In this chapter, we will set up customizations for two sample environments: development and production.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root">The folder structure under <span><kbd>$BOOK_HOME/Chapter16</kbd> looks as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a62901e6-d94d-49e1-be8f-22ba37d810c2.png" style="width:10.00em;height:8.08em;" width="409" height="326" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a62901e6-d94d-49e1-be8f-22ba37d810c2.png"></p>
<p><span>Since Kubernetes 1.14, </span><kbd>kubectl</kbd><span> comes with built-in support for Kustomize using the </span><kbd>-k</kbd><span> flag. As we will see as we proceed, deploying to the development environment using Kustomize will be done with the</span>&nbsp;<kbd>kubectl apply -k kubernetes/services/overlays/dev</kbd> command.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying to Kubernetes for development and test</h1>
                </header>
            
            <article>
                
<p>In this section, we will deploy the microservices in an environment to be used for development and test activities, for example, system integration tests. This type of environment is used primarily for functional tests and is therefore configured to use minimal system resources.</p>
<p>Since the deployment objects in the <kbd>base</kbd> folder are configured for a development environment, they don't need any further refinement in the overlay for development. We only have to add deployment and service objects for the three resource managers for&nbsp;<span>RabbitMQ, MySQL, and MongoDB</span>&nbsp;in the same way as when using Docker Compose. We will deploy the resource managers&nbsp;in the same Kubernetes namespace as the microservices. This is illustrated by the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c5ebe638-fb4c-4344-ab1d-2cd3b1698480.png" width="1914" height="628" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c5ebe638-fb4c-4344-ab1d-2cd3b1698480.png"></p>
<p><span>The definition files for the resource managers can be found in the&nbsp;</span><kbd>kubernetes/services/overlays/dev</kbd><span>&nbsp;folder.</span></p>
<p><span>The <kbd>kustomization.yml</kbd> file looks like this:</span></p>
<pre>bases:<br>- ../../base<br>resources:<br>- mongodb-dev.yml<br>- rabbitmq-dev.yml<br>- mysql-dev.yml</pre>
<p>It defines that the <kbd>base</kbd> folder shall be used as the base and adds the three resources we mentioned previously.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying to Kubernetes for development and test</h1>
                </header>
            
            <article>
                
<p>In this section, we will deploy the microservices in an environment to be used for development and test activities, for example, system integration tests. This type of environment is used primarily for functional tests and is therefore configured to use minimal system resources.</p>
<p>Since the deployment objects in the <kbd>base</kbd> folder are configured for a development environment, they don't need any further refinement in the overlay for development. We only have to add deployment and service objects for the three resource managers for <span>RabbitMQ, MySQL, and MongoDB</span> in the same way as when using Docker Compose. We will deploy the resource managers in the same Kubernetes namespace as the microservices. This is illustrated by the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c5ebe638-fb4c-4344-ab1d-2cd3b1698480.png" width="1914" height="628" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c5ebe638-fb4c-4344-ab1d-2cd3b1698480.png"></p>
<p><span>The definition files for the resource managers can be found in the </span><kbd>kubernetes/services/overlays/dev</kbd><span> folder.</span></p>
<p><span>The <kbd>kustomization.yml</kbd> file looks like this:</span></p>
<pre>bases:<br>- ../../base<br>resources:<br>- mongodb-dev.yml<br>- rabbitmq-dev.yml<br>- mysql-dev.yml</pre>
<p>It defines that the <kbd>base</kbd> folder shall be used as the base and adds the three resources we mentioned previously.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building Docker images</h1>
                </header>
            
            <article>
                
<p>Normally, we have to push images to a Docker registry and configure Kubernetes to pull images from the registry. In our case, where we have a local single node cluster, we can shortcut this process by pointing our Docker client to the Docker engine in Minikube and then run the <kbd>docker-compose build</kbd> command. This will result in the Docker images being immediately available to Kubernetes. For development, we will be using <kbd>latest</kbd> as the Docker image version for the microservices.</p>
<div class="packt_tip">You might be wondering how we can update a pod that uses the <kbd>latest</kbd> Docker image.<br>
<br>
From Kubernetes 1.15, this is very simple. Just change the code and rebuild the Docker image, for example, using the <kbd>build</kbd> command that's described here. Then, update a pod with the <kbd>kubectl rollout restart</kbd> command.<br>
<br>
For example, if the <kbd>product</kbd> service has been updated, run the <kbd>kubectl rollout restart deploy product</kbd> command.</div>
<p>You can build Docker images from source as follows:</p>
<div>
<pre><strong>cd $BOOK_HOME<span>/Chapter16</span><span><br>eval</span><span> </span><span>$(minikube docker-env)</span><span><br></span><span>./gradlew build &amp;&amp;</span> <span>docker-compose build</span></strong></pre></div>
<p>The <span><kbd>eval $(minikube docker-env)</kbd> command directs the local Docker client to communicate with the Docker engine in Minikube, for example, when building the Docker images.</span></p>
<p>The <kbd>docker-compose.yml</kbd> file has been updated to specify a name for the Docker images it builds. For example, for the <kbd>product</kbd> service, we have the following:</p>
<pre>  product:<br>    build: microservices/product-service<br>    image: hands-on/product-service</pre>
<div class="packt_infobox"><kbd>latest</kbd> is the default tag for a Docker image name, so it is not specified.</div>
<p>With the Docker images built, we can start creating the Kubernetes resource objects!</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying to Kubernetes</h1>
                </header>
            
            <article>
                
<p>Before we can deploy the microservices to Kubernetes, we need to create a namespace, the required config maps, and secrets. After the deployment is performed, we will wait for the deployments to be up and running, and also verify that we got the expected result in terms of deployed pods and Docker images that were used per pod.</p>
<p>Create a namespace, <span><kbd>hands-on</kbd>,&nbsp;</span>and set it as the default namespace for <kbd>kubectl</kbd>:</p>
<div>
<pre><strong><span>kubectl create namespace hands-on<br></span>kubectl config set-context $(kubectl config current-context) --namespace=<span>hands-on</span></strong></pre></div>
<p>All application configuration is kept in the configuration repository that's managed by the configuration server. The only configuration information that needs to be stored outside of the <span>configuration repository</span> is the credentials for connecting to the <span>configuration </span>server and an encryption key. <span>The encryption key</span> is used by the <span>configuration</span> server to keep sensitive information in the <span>configuration repository</span> encrypted at rest, that is, on disk.</p>
<p>We will store the <span>configuration repository </span>in a config map with all the sensitive information encrypted; see <a href="a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml">C<span>hapter</span> 12</a>, <em>Centralized Configuration</em>, for details. The <span>credentials for connecting to the configuration server and the encryption key will be stored in </span>two secrets, one for the <span>configuration</span> server and one for its clients.</p>
<p>To check this, perform the following steps:</p>
<ol>
<li>Create the config map for the <span>configuration repository</span> based on the files in the <kbd>config-repo</kbd> folder with the following command:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>kubectl create configmap config-repo --from-file=config-repo/ --save-config</span></strong></pre></div>
<ol start="2">
<li>Create the <span>secr</span><span>et for the configuration server with the following command:</span></li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>kubectl create secret generic config-server-secrets \<br></span><span>  --from-literal=ENCRYPT_KEY=my-very-secure-encrypt-key \<br></span><span>  --from-literal=SPRING_SECURITY_USER_NAME=dev-usr \<br></span><span>  --from-literal=SPRING_SECURITY_USER_PASSWORD=dev-pwd \<br></span><span>  --save-config</span></strong></pre></div>
<ol start="3">
<li>Create the <span>s</span><span>ecret for the clients of the configuration server with the following command:</span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl create secret generic config-client-credentials <span>\<br></span>--from-literal=<span>CONFIG_SERVER_USR</span>=dev-usr <span>\<br></span>--from-literal=<span>CONFIG_SERVER_PWD</span>=dev-pwd --save-config</strong></pre>
<div class="mce-root packt_infobox"><span><span>Since we have just entered commands that contain sensitive information in clear text, for example, passwords and an encryption key, it is a good idea to clear the <kbd>history</kbd> command. To clear the <kbd>history</kbd> command both in memory and on disk, run the <kbd>history -c; history -w</kbd></span></span> command.<span><span><br>
<br></span></span> <span>See the discussion at <a href="https://unix.stackexchange.com/a/416831">https://unix.stackexchange.com/a/416831</a> for details on the <kbd>history</kbd> command.</span></div>
<ol start="4">
<li class="mce-root">To avoid a slow deployment due to Kubernetes downloading Docker images (potentially causing the liveness probes we described previously to restart our pods), ru<span>n the following</span> <kbd>docker pull</kbd> <span>commands to download the images:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>docker pull mysql:5.7</strong><br><strong>docker pull mongo:3.6.9</strong><br><strong>docker pull rabbitmq:3.7.8-management</strong><br><strong>docker pull openzipkin/zipkin:2.12.9</strong></pre>
<ol start="5">
<li>Deploy the microservices for the development environment, based on the <kbd>dev</kbd> overlay, using the <kbd>-k</kbd> switch to activate Kustomize, as described previously:</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -k kubernetes/services/overlays/dev</strong></pre>
<ol start="6">
<li>Wait for the deployments and their pods to be up and running by running the following command:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl wait --timeout=600s --for=condition=ready pod --all</strong></pre>
<p style="padding-left: 60px">Expect each command to respond with <kbd>deployment.extensions/... condition met</kbd>. <kbd>...</kbd> will be replaced with the name of the actual deployment.</p>
<ol start="7">
<li>To see the Docker images that <span>are </span>used for development, run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl get pods -o json | jq .items[].spec.containers[].image</strong></pre>
<p class="mce-root"></p>
<div>
<p>The response should look similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/42300572-7769-4a8b-9b4d-5efd9d361dab.png" style="width:39.08em;height:19.25em;" width="1324" height="652" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/42300572-7769-4a8b-9b4d-5efd9d361dab.png"></p>
<p>We are now ready to test our deployment!</p>
<p>But before we can do that, we need to go through changes that are required in the test script for use with Kubernetes.</p>
</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the test script for use with Kubernetes</h1>
                </header>
            
            <article>
                
<p>To test the deployment we will, as usual, run the test script, that is, <kbd>test-em-all.bash</kbd>. To work with Kubernetes, the circuit breaker tests have been slightly modified. Take a look at the <kbd>testCircuitBreaker()</kbd> function for more details. T<span>he circuit breaker tests call the </span><kbd>actuator</kbd> endpoints on the <kbd>product-composite</kbd> service to check their health state and get access to circuit breaker events. The <kbd>actuator</kbd> endpoints are not exposed externally, so the test script needs to use different techniques to access the internal endpoints when using Docker Compose and Kubernetes:</p>
<ul>
<li>When using Docker Compose, the test script will launch a Docker container using a plain <kbd>docker run</kbd> command that calls the <kbd>actuator</kbd> endpoints from the inside of the network created by Docker Compose.</li>
<li>When using Kubernetes, <span>the test script will</span> launch a Kubernetes pod that it can use to run the corresponding commands inside Kubernetes.</li>
</ul>
<p class="mce-root"></p>
<p>Let's see how this is done when using Docker Compose and Kubernetes.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Reaching the internal actuator endpoint using Docker Compose</h1>
                </header>
            
            <article>
                
<p><span>The base command that's defined for Docker Compose is as follows:</span></p>
<pre><strong>EXEC="docker run --rm -it --network=my-network alpine"</strong></pre>
<p><span>Note that t</span>he container will be killed using the <kbd>--rm</kbd> switch after each execution of a test command.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Choosing between Docker Compose and Kubernetes</h1>
                </header>
            
            <article>
                
<p>To make the test script work with both Docker Compose and Kubernetes, it assumes that Docker Compose will be used if the <kbd>HOST</kbd> environment variable is set to <kbd>localhost</kbd>; otherwise, it assumes that Kubernetes will be used. See the following code:<span>&nbsp;</span></p>
<pre>if [ "$HOST" = "localhost" ]<br>then<br>    EXEC="docker run --rm -it --network=my-network alpine"<br>else<br>    echo "Restarting alpine-client..."<br>    ...<br>    EXEC="kubectl -n $ns exec alpine-client --"<br>fi</pre>
<p><span>The default value for the </span><kbd>HOST</kbd><span> environment variable in the test script is <kbd>localhost</kbd>.</span></p>
<p>Once the <kbd>EXEC</kbd> variable has been set up, depending on whether the tests are running on Docker Compose or on Kubernetes, it is used in the <kbd>testCircuitBreaker()</kbd> test function. The test starts by verifying that the circuit breaker is closed with the following statement:</p>
<pre><strong>assertEqual "CLOSED" "$($EXEC wget product-composite:${MGM_PORT}/actuator/health -qO - | jq -r .details.productCircuitBreaker.details.state)"</strong></pre>
<p>A final change in the test script occurs because our services are now reachable on the <kbd>80</kbd> port inside the cluster; that is, they are no longer on the <kbd>8080</kbd> port.</p>
<div class="packt_tip"><span>If the various ports that we've used seem confusing, r</span>eview the definitions of the services in the <em>Setting up common definitions in the base folder</em> section.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing the deployment</h1>
                </header>
            
            <article>
                
<p>When launching the test script, we have to give it the address of the host that runs Kubernetes, that is, our Minikube instance, and the external port where our gateway service listens for external requests. The <kbd>minikube ip</kbd> command can be used to find the IP address of the Minikube instance and, as mentioned in the <span><em>Setting up common definitions in the base folder</em> section, we have assigned the external <kbd>NodePort 31443</kbd> to the gateway service.</span></p>
<p>Start the tests with the following command:</p>
<pre><strong>HOST=$(minikube ip) PORT=31443 ./test-em-all.bash</strong></pre>
<p>In the output from the script we will see how the IP address of the Minikube instance is used and also how the <kbd><span>alpine-client</span></kbd> pod is created and destroyed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/97c12d92-177b-46e9-9f35-57cef2157850.png" style="width:45.50em;height:25.00em;" width="862" height="474" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/97c12d92-177b-46e9-9f35-57cef2157850.png"></p>
<p><span>Before we move on and look at how to set up a corresponding environment for staging and production use, let's clean up what we have installed in the development environment to preserve resources in the Kubernetes cluster. We can do this by simply deleting the namespace. Deleting the namespace will recursively delete the resources that exist in the namespace.</span></p>
<p>Delete the namespace with the following command:</p>
<pre><strong><span>kubectl delete namespace hands-on</span></strong></pre>
<p>With the development environment removed, we can move on and set up an environment targeting <span>staging</span> and production.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying to Kubernetes for staging and production</h1>
                </header>
            
            <article>
                
<p>In this section, we will deploy the microservices in an environment for staging and production usage. A staging environment is used for performing <strong>quality</strong> <strong>assurance</strong> (<strong>QA</strong>) and <strong>user acceptance tests</strong> (<strong>UAT</strong>) as the last step before taking a new release into production. To be able to verify that the new release not only meets functional requirements but also non-functional requirements, for example, in terms of performance, robustness, scalability, and resilience, a staging environment is configured to be as similar as possible to the production environment.</p>
<p class="p1">When deploying to an environment for <span>staging</span> or production, there are a number of changes required compared to when deploying for development or tests:</p>
<ul>
<li><strong>Resource managers should run outside of the Kubernetes cluster</strong>: It is technically feasible to run databases and queue managers for production use on Kubernetes as stateful containers using <kbd>StatefulSets</kbd> and <kbd>PersistentVolumes</kbd>. At the time of writing this chapter, I recommend against it, mainly because the support for stateful containers is relatively new and unproven in Kubernetes. Instead, I recommend <span>using the existing database and queue manager services on premises or managed services in the cloud, leaving Kubernetes to do what it is best for, that is, running stateless containers.</span><span> For the scope of this book, to simulate a production environment, we will run MySQL, MongoDB, and RabbitMQ as plain Docker containers outside of Kubernetes using the already existing Docker Compose files.</span></li>
<li><strong>Lockdown:</strong>
<ul>
<li>For security reasons, things like <kbd>actuator</kbd> endpoints and log levels need to be constrained in a production environment.</li>
</ul>
<ul>
<li>Externally exposed endpoints should also be reviewed from a security perspective. For example, access to the configuration server should most probably be locked down in a production environment, but we will keep it exposed in this book for convenience.</li>
<li>Docker image tags must be specified to be able to track which versions of the microservices have been deployed.</li>
</ul>
</li>
<li><strong>Scale up available resources</strong>: To meet the requirements of both high availability and higher load, we need to run at least two pods per deployment. We might also need to increase the amount of memory and CPU that are allowed to be used per pod. To avoid running out of memory in the Minikube instance, we will keep one pod per deployment but increase the maximum memory allowed in the production environment.</li>
<li><strong>Set up a production-ready Kubernetes cluster</strong>:<strong>&nbsp;</strong>This is outside the scope of this book, but, if feasible, I recommend using one of the managed Kubernetes services provided by the leading cloud providers. For the scope of this book, we will deploy to our local Minikube instance.</li>
</ul>
<div class="packt_infobox">This is not meant to be an exhaustive list of things that have to be considered when setting up an environment for production, but it's a good start.</div>
<p>Our simulated production environment will look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3ddfcf4d-89dd-4007-a19c-4568e6cb2220.png" width="1950" height="853" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3ddfcf4d-89dd-4007-a19c-4568e6cb2220.png"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the source code</h1>
                </header>
            
            <article>
                
<p>The following changes have been applied to the source code to prepare for deployment in an environment that's used for production:</p>
<ul>
<li>A&nbsp;<span>Spring profile named</span> <kbd>prod</kbd>  has been added to the configuration files in the <kbd>config-repo</kbd> configuration repository:</li>
</ul>
<pre style="padding-left: 60px">spring.profiles: prod</pre>
<ul>
<li>In the <kbd>prod</kbd> profiles, the following has been added:</li>
<li style="padding-left: 60px"><span>URLs to the resource managers that run as plain Docker containers:</span></li>
</ul>
<pre style="color: black;padding-left: 120px">spring.rabbitmq.host: 172.17.0.1<br>spring.data.mongodb.host: 172.17.0.1<br>spring.datasource.url: jdbc:mysql://172.17.0.1:3306/review-db</pre>
<div class="packt_infobox">We are using the <kbd>172.17.0.1</kbd> <span>IP address </span>to address the Docker engine in the Minikube instance. This is the default IP address for the <span>Docker engine when</span> <span>creating it with Minikube, at least for Minikube up to version 1.2.<br>
There is work ongoing for establishing a standard DNS name for containers to use if they need to access the Docker host they are running on, but at the time of writing this chapter, this work effort hasn't been completed. <br></span></div>
<ul>
<li style="padding-left: 60px">Log levels have been set to warning or higher, that is, error or fatal. For example:</li>
</ul>
<pre style="padding-left: 120px"><span>logging.level.root: WARN</span></pre>
<ul>
<li style="padding-left: 60px"><span>The only <kbd>actuator</kbd> endpoints that are exposed over HTTP are the </span><span><kbd>info</kbd> and </span><kbd>health</kbd><span> endpoints that are used by the liveness and readiness probes in Kubernetes, as well as the </span><span><kbd>circuitbreakerevents</kbd> endpoint that's used by the test script, <kbd>test-em-all.bash</kbd>:</span></li>
</ul>
<pre style="color: black;padding-left: 120px">management.endpoints.web.exposure.include: health,info,circuitbreakerevents</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<ul>
<li class="mce-root">In the production <kbd>overlay</kbd> folder, <kbd>kubernetes/services/overlays/prod</kbd>, one deployment object for each microservice has been added with the following content so that it can be merged with the base definition:</li>
<li style="padding-left: 60px"><span>For all microservices,</span> <kbd>v1</kbd> <span>is specified as the Docker <kbd>image</kbd> tag, and the</span> <kbd>prod</kbd> <span>profile is added to the active Spring profiles. For example, we have the following for the</span> <kbd>product</kbd> <span>service:</span></li>
</ul>
<pre style="color: black;padding-left: 120px">image: hands-on/product-service:v1<br>env:<br>- name: SPRING_PROFILES_ACTIVE<br>  value: "docker,prod"</pre>
<ul>
<li style="padding-left: 60px"><span>For the Zipkin and </span><span>configuration </span><span>server</span><span>, which don't keep their configuration in the configuration repository, environment variables have been added in their deployment definitions with the corresponding configuration:</span></li>
</ul>
<pre style="padding-left: 120px"><span>env:</span><br><span>- name: LOGGING_LEVEL_ROOT</span><br><span>  value: WARN</span><br><span>- name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE</span><br><span>  value: "health,info"<br>- name: RABBIT_ADDRESSES<br>  value: 172.17.0.1</span></pre>
<ul>
<li style="padding-left: 60px" class="mce-root"><span>Finally, a </span><span><kbd>kustomization.yml</kbd> file defines that the files in the <kbd>prod overlay</kbd> folder shall be merged by specifying the <kbd>patchesStrategicMerge</kbd></span><span> patch mechanism with the corresponding definition in the <kbd>base</kbd> folder:</span></li>
</ul>
<pre style="color: black;padding-left: 120px">bases:<br>- ../../base<br>patchesStrategicMerge:<br>- auth-server-prod.yml<br>- ...</pre>
<div class="packt_infobox"><span>In a real-world production environment, we should have also changed the</span> <kbd>imagePullPolicy: Never</kbd><span> setting to <kbd>IfNotPresent</kbd>, that is, to download Docker images from a Docker registry. But since we will be deploying the production setup to the Minikube instance where we manually build and tag the Docker images, we will not update this setting.</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying to Kubernetes</h1>
                </header>
            
            <article>
                
<div>
<p>To simulate production-grade <span>resource managers,</span><span>&nbsp;M</span><span>ySQL, MongoDB, and RabbitMQ will run outside of Kubernetes using Docker Compose. We start them up as we did in the previous chapters:</span></p>
<div>
<pre><strong><span>eval $(minikube docker-env)<br></span><span>docker-compose up -d mongodb mysql rabbitmq</span></strong></pre></div>
</div>
<p>We also need to tag the existing Docker images with <kbd>v1</kbd> using the following commands:</p>
<pre><strong>docker tag hands-on/auth-server hands-on/auth-server:v1</strong><br><strong>docker tag hands-on/config-server hands-on/config-server:v1</strong><br><strong>docker tag hands-on/gateway hands-on/gateway:v1 </strong><br><strong>docker tag hands-on/product-composite-service hands-on/product-composite-service:v1 </strong><br><strong>docker tag hands-on/product-service hands-on/product-service:v1</strong><br><strong>docker tag hands-on/recommendation-service hands-on/recommendation-service:v1</strong><br><strong>docker tag hands-on/review-service hands-on/review-service:v1</strong></pre>
<p>From here, the commands are very similar to how we deployed to the development environment.</p>
<p>We will use another Kustomize overlay and use different credentials for the configuration server, but, otherwise, it will be the same (which, of course, is a good thing!). We will use the same configuration repository but configure the pods to use the <kbd>prod</kbd> Spring profile, as described previously. Follow these steps to do so:</p>
<ol>
<li>Create a namespace, <span><kbd>hands-on</kbd>,&nbsp;</span>and set this as the default namespace for<span>&nbsp;</span><kbd>kubectl</kbd>:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>kubectl create namespace hands-on<br></span>kubectl config set-context $(kubectl config current-context) --namespace=<span>hands-on</span></strong></pre></div>
<ol start="2">
<li>Create the config map for the configuration repository based on the files in the<span>&nbsp;</span><kbd>config-repo</kbd><span>&nbsp;</span>folder with the following command:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>kubectl create configmap config-repo --from-file=config-repo/ --save-config</span></strong></pre></div>
<ol start="3">
<li>
<p>Create the <span>secret for the configuration server with the following command:</span></p>
</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>kubectl create secret generic config-server-secrets \<br></span><span>  --from-literal=ENCRYPT_KEY=my-very-secure-encrypt-key \<br></span><span>  --from-literal=SPRING_SECURITY_USER_NAME=prod-usr \<br></span><span>  --from-literal=SPRING_SECURITY_USER_PASSWORD=prod-pwd \<br></span><span>  --save-config</span></strong></pre></div>
<ol start="4">
<li>
<p>Create the <span>secret for the clients of the configuration server with the following command:</span></p>
</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl create secret generic config-client-credentials <span>\<br></span>--from-literal=<span>CONFIG_SERVER_USR</span>=prod-usr <span>\<br></span>--from-literal=<span>CONFIG_SERVER_PWD</span>=prod-pwd --save-config</strong></pre>
<ol start="5">
<li>Remove the clear text encryption key and passwords from the command history:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>history -c; history -w</span></strong></pre>
<ol start="6">
<li>
<p>Deploy the microse<span>rvices for the de</span><span>velopment environment, based on the </span><kbd>prod</kbd><span> overlay, using the </span><kbd>-k</kbd><span> switch to activate Kustomize, as described previously:</span></p>
</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -k kubernetes/services/overlays/prod</strong></pre>
<ol start="7">
<li>Wait for the deployments to be up and running:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl wait --timeout=600s --for=condition=ready pod --all</strong></pre>
<ol start="8">
<li>
<p>To see the Docker images that are currently being used for production, run the following command:</p>
</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl get pods -o json | jq .items[].spec.containers[].image</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>The response should look something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3f3b0b67-78f2-4815-8c42-3f67a8931783.png" style="width:38.00em;height:15.25em;" width="1309" height="525" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3f3b0b67-78f2-4815-8c42-3f67a8931783.png"></p>
<p>Note the <kbd>v1</kbd> version of the Docker images!</p>
<p>Also note that the resource manager pods for MySQL, MongoDB, and RabbitMQ are gone; these can be found with the <kbd>docker-compose ps</kbd> command.</p>
<p>Run the test script, <kbd>thest-em-all.bash</kbd>, to verify the simulated production environment:</p>
<pre><strong>HOST=$(minikube ip) PORT=31443 ./test-em-all.bash</strong></pre>
<p>Expect the same type of output that we got when the test script was run against the development environment.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Performing a rolling upgrade</h1>
                </header>
            
            <article>
                
<p><span>Historically, updates often result in some downtime of the component that is updated. In a system landscape with an increasing number of autonomous microservices that are updated independently of each other, recurring downtimes due to frequent updates of the microservices is not acceptable. Being able to deploy an update without downtime becomes crucial.</span></p>
<p class="mce-root"></p>
<p>In this section, we will see how we can perform a rolling upgrade, updating a microservice to a new version of its Docker image without requiring any downtime. Performing a rolling upgrade means that Kubernetes first starts the new version of the microservice in a new pod, and when it reports as being healthy, Kubernetes will terminate the old one. This ensures that there is always a pod up and running, ready to serve incoming requests during the upgrade. A prerequisite for a rolling upgrade to work is that the upgrade is backward compatible, both in terms of APIs and message formats that are used to communicate with other services and <span>database structures</span>. If the new version of the microservice requires changes to either the external APIs, message formats, or database structures that the old version can't handle, a rolling upgrade can't be applied. A deployment object is configured to perform any updates as a rolling upgrade by default.</p>
<p>To try this out, we will create a v2 version of the Docker image for the <kbd>product</kbd> service and then start up a test client, <kbd>siege</kbd>, that will submit one request per second during the rolling upgrade. The assumption is that the test client will report 200 (OK) for all the requests that it sends during the upgrade.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Preparing the rolling upgrade</h1>
                </header>
            
            <article>
                
<p class="mce-root">To prepare for the rolling upgrade, first, verify that we have the <kbd>v1</kbd> version of the product pod deployed:</p>
<pre class="mce-root"><strong>kubectl get pod -l app=product -o jsonpath='{.items[*].spec.containers[*].image} '</strong></pre>
<p>The expected output should reveal that <kbd>v1</kbd> of the Docker image is in use:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8ba120f8-86f6-4b8e-9f5c-88e9e66e05cf.png" width="854" height="77" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8ba120f8-86f6-4b8e-9f5c-88e9e66e05cf.png"></p>
<p class="CDPAlignLeft CDPAlign">Create a <kbd>v2</kbd> tag on the Docker image for the <kbd>product</kbd> service with the following command:</p>
<pre class="mce-root"><strong>docker tag hands-on/product-service:v1 hands-on/product-service:v2</strong></pre>
<div class="packt_infobox">To try out a rolling upgrade from a Kubernetes perspective, we don't need to change any code in the <kbd>product</kbd> service. Deploying a Docker image with another tag than the existing one will start up a rolling upgrade.</div>
<p>To be able to observe whether any downtime occurs during the upgrade, we will start a low volume load test using <kbd>siege</kbd>. The following command starts a load test that simulates one user (<kbd>-c1</kbd>) that submits one request per second on average (<kbd>-d1</kbd>):</p>
<pre class="mce-root"><strong>siege https://$(minikube ip):31443/actuator/health -c1 -d1</strong></pre>
<p>Since the test calls the gateways health endpoint, it verifies that all the services are healthy.</p>
<p>You should receive an output that looks similar to the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2a9f905e-809b-4146-9b64-208c924649cb.png" style="width:38.17em;height:9.42em;" width="691" height="171" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2a9f905e-809b-4146-9b64-208c924649cb.png"></p>
<p>The interesting part in the response is the HTTP status code, which we expect to be <kbd>200</kbd> at all times.</p>
<p class="mce-root">Also, monitor changes to the state of the product pods with the following command:</p>
<pre class="mce-root"><strong>kubectl get pod -l app=product -w</strong></pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Upgrading the product service from v1 to v2</h1>
                </header>
            
            <article>
                
<p class="mce-root">To upgrade the <kbd>product</kbd> service, edit the <kbd>kubernetes/services/overlays/prod/product-prod.yml</kbd> file and change <kbd><span>image</span><span>:</span> <span>hands-on/product-service:v1</span></kbd> to <span><kbd>image: hands-on/product-service:v2</kbd>.</span></p>
<p class="mce-root">Apply the update with the following command:</p>
<pre class="mce-root"><strong>kubectl apply -k kubernetes/services/overlays/prod</strong></pre>
<p>Expect a response from the command that reports that most of the objects are left unchanged, except for the product deployment that should be reported to be updated to <span><kbd>deployment.apps/product configured</kbd>.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<div class="packt_infobox"><span>Kubernetes comes with some shorthand commands. For example, </span><kbd>kubectl set image deployment/product pro=hands-on/product-service:v2</kbd><span> can be used to perform the same update that we did by updating the definitions file and running the </span><kbd>kubectl apply</kbd><span> command. A major benefit of using the </span><span><kbd>kubectl apply</kbd> command is that we can keep track of the changes by pushing the changes in the source code to a version control system such as Git. This is very important if we want to be able to handle our infrastructure as code. When playing around with a Kubernetes cluster, only use it to test shorthand commands, as this can be very useful.</span></div>
<p><span>In the output from</span> the <kbd><span>kubectl get pod -l app=product -w</span></kbd> command we launched<span> in the <em>Preparing the rolling upgrade</em> section, we will </span>see some action occurring. Take a look at the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e395b5c0-fc40-43f1-b8ec-f85a9ce86628.png" style="width:40.00em;height:16.75em;" width="721" height="302" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e395b5c0-fc40-43f1-b8ec-f85a9ce86628.png"></p>
<p>Here, we can see how the existing pod (<kbd>ffrdh</kbd>) initially reported that it was up and running and also reported to be healthy when a new pod was launched (<kbd>t8mcl</kbd>). After a while (<kbd>16s</kbd>, in my case), it is reported as up and running as well. During a certain time period, both pods will be up and running and processing requests. After a while, the first pod is terminated (2 minutes, in my case).</p>
<p class="mce-root"></p>
<p>When looking at the <kbd>siege</kbd> output, we can sometimes find a few errors being reported in terms of the <kbd>503</kbd> service unavailable errors:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3caab39b-4d02-4f93-ace5-5402b72aac09.png" style="width:41.75em;height:7.42em;" width="687" height="122" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3caab39b-4d02-4f93-ace5-5402b72aac09.png"></p>
<p class="mce-root"><span>This typically happens when the old pod is terminated. Before the old pod is reported unhealthy by the readiness probe, it can receive a few requests during its termination, that is, when it is no longer capable of serving any requests. </span></p>
<div class="packt_infobox">In <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>, we will see how we can set up routing rules that move traffic in a smoother way from an old pod to a newer one without causing 503 errors. We will also see how we can apply retry mechanisms to stop temporary failures from reaching an end user.</div>
<p class="mce-root"><span>Wrap this up by v</span><span>erifying that the pod is using the new <kbd>v2</kbd> version of the Docker image:</span></p>
<pre class="mce-root"><strong>kubectl get pod -l app=product -o jsonpath='{.items[*].spec.containers[*].image} '</strong></pre>
<p><span>The expected output reveals that </span><kbd>v2</kbd><span> of the Docker image is in use:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bd38e1cd-b695-49b8-991b-63e395ee577d.png" width="851" height="74" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bd38e1cd-b695-49b8-991b-63e395ee577d.png"></p>
<p>After performing this upgrade, we can move on to learning what happens when things fail. In the next section, we will see how we can roll back a failed deployment.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Rolling back a failed deployment</h1>
                </header>
            
            <article>
                
<p>From time to time, things don't go according to plan, for example, an upgrade of deployments and pods can fail for various reasons. To demonstrate how to roll back a failed upgrade, let's try to upgrade to <kbd>v3</kbd> without creating a <kbd>v3</kbd> tag on the <span>Docker image!</span></p>
<p>Let's try out the following shorthand command to perform the update:</p>
<pre class="mce-root"><strong><span>kubectl set image deployment/product pro=hands-on/product-service:v3</span></strong></pre>
<p>Expect to see the following changes reported by the <kbd><span>kubectl get pod -l app=product -w</span></kbd> command we launched <span>in the <em>Preparing the rolling upgrade</em></span><em>&nbsp;</em>section:</p>
<p class="CDPAlignCenter CDPAlign"><q><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/290f3cd4-de45-4a31-8abb-495b8f20c15e.png" style="width:39.42em;height:6.92em;" width="690" height="121" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/290f3cd4-de45-4a31-8abb-495b8f20c15e.png"></q></p>
<p>We can clearly see that the new pod (ending with <kbd>m2dtn</kbd>, in my case) has failed to start because of a problem finding its Docker image (as expected). If we look at the output from the <kbd>siege</kbd> test tool, no errors are reported, only 200 (OK)! Here, the deployment hangs since it can't find the requested Docker image, but no errors are affecting end users since the new pod couldn't even start.</p>
<p>Let's see what history Kubernetes has regarding the product's deployment. Run the following command:</p>
<pre class="mce-root"><strong>kubectl rollout history deployment product</strong></pre>
<p>You will receive output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f5e3c16a-efab-43df-a5a5-04dda9c5c4c9.png" style="width:26.92em;height:12.50em;" width="453" height="211" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f5e3c16a-efab-43df-a5a5-04dda9c5c4c9.png"></p>
<p>We can guess that revision 2 is the one with the latest successful deployment, that is, <kbd>v2</kbd> of the Docker image. Let's check this with the following command:</p>
<pre class="mce-root"><strong>kubectl rollout history deployment product --revision=2</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<p>In the response, we can see that <kbd>revision #2</kbd> is the one with Docker image <kbd>v2</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4e3d11c8-a112-40c0-be6b-29631e196c19.png" style="width:32.08em;height:14.00em;" width="586" height="256" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4e3d11c8-a112-40c0-be6b-29631e196c19.png"></p>
<p>Let's roll back our deployment to <kbd>revision=2</kbd> with the following command:</p>
<pre class="mce-root"><strong>kubectl rollout undo deployment product --to-revision=2</strong></pre>
<p>Expect a response that confirms the rollback, like so:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/936308fe-f3a2-45aa-949b-2fe8fe52cbe5.png" style="width:32.08em;height:5.33em;" width="583" height="97" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/936308fe-f3a2-45aa-949b-2fe8fe52cbe5.png"></p>
<p>The <kbd><span>kubectl get pod -l app=product -w</span></kbd> command we launched in the <em>Preparing the rolling upgrade</em> section will report that the new (not working) pod has been removed by the <kbd>rollback</kbd> command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0f0d45ea-cb69-4f32-a2fa-55edca821532.png" style="width:30.50em;height:6.17em;" width="613" height="124" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0f0d45ea-cb69-4f32-a2fa-55edca821532.png"></p>
<p class="mce-root">We can wrap this up by verifying that the current image version is still <kbd>v2</kbd>:</p>
<pre class="mce-root"><strong>kubectl get pod -l app=product -o jsonpath='{.items[*].spec.containers[*].image} '</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Cleaning up</h1>
                </header>
            
            <article>
                
<p>To delete the resources that we used, run the following commands:</p>
<ol>
<li>Stop the watch command, <span><kbd>kubectl get pod -l app=product -w</kbd>, and the </span>load test program, <kbd>siege</kbd>, with <em>Ctrl</em> <em>+</em> <em>C</em>.</li>
<li>Delete the namespace:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl delete namespace hands-on</strong></pre>
<ol start="3">
<li class="mce-root"><span>Shut down the resource managers that run outside of Kubernetes:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>eval $(minikube docker-env)<br>docker-compose down</strong></pre>
<p>The <kbd>kubectl delete namespace</kbd> command will recursively delete all Kubernetes resources that existed in the namespace, and the <kbd>docker-compose down</kbd> command will stop MySQL, MongoDB, and RabbitMQ. With the production environment removed, we have reached the end of this chapter.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to deploy the microservices in this book on Kubernetes. We also introduced some core features in Kubernetes, such as using<span>&nbsp;</span>Kustomize<span>&nbsp;</span>to configure deployments for different runtime environments, using Kubernetes deployment objects for rolling upgrades, and how to roll back a failed update if required. <span>To help Kubernetes understand when the microservices need to be restarted and if they are ready to accept requests, we implemented liveness and readiness probes.</span></p>
<p><span>Finally, t</span>o be able to deploy our microservices, we had to replace Netflix Eureka with the built-in discovery service in Kubernetes. Changing the discovery service was done without any code changes <span>â all we had to do was</span> apply changes to the build dependencies and some of the configuration. </p>
<p>In the next chapter, we will see how we can further utilize Kubernetes to reduce the number of supporting services we need to deploy in Kubernetes. Head over to the next chapter to see how we can eliminate the need for the configuration server and how our edge server can be replaced by a Kubernetes ingress controller. </p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Why did we remove the Eureka server from the microservices landscape when deploying it on Kubernetes?</li>
<li>What did we replace <span>the Eureka server with and how was the source code of the microservices affected by this change?</span></li>
<li>How are base and overlay folders used with Kustomize?</li>
<li>How can we get a running pod updated with changes in a config map or secret?</li>
<li>If we are using the latest tag on a Docker image, how can we get running pods using a new build of the Docker image?</li>
<li>What commands can we use to roll back a failed deployment?</li>
<li>What's the purpose of liveness and readiness probes?</li>
<li>What are the different ports that are being used in the following service definition?</li>
</ol>
<pre style="padding-left: 90px">apiVersion: v1<br>kind: Service<br>spec:<br>  type: NodePort<br>  ports:<br>    - port: 80<br>      nodePort: 30080<br>      targetPort: 8080</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Implementing Kubernetes Features as an Alternative</h1>
                </header>
            
            <article>
                
<p><span>The current microservice landscape contains a number of supportive services that implement important design patterns required in a large-scale microservice landscape; for example an edge, config, and authorization server, and a service for distributed</span> tracing. <span>For details, see <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml" target="_blank">Chapter 1</a>, <em>Introduction to Microservices</em>, and refer to the <em>Design patterns for microservices</em> section. In the previous chapter, we replaced the implementation of the design pattern for service discovery, based on Netflix Eureka, with the built-in discovery service in Kubernetes. In this chapter, we will further simplify the microservice landscape by reducing the number of supportive services required to be deployed. Instead, the corresponding design patterns will be handled by built-in capabilities in Kubernetes. The Spring Cloud Config Server will be replaced with Kubernetes config maps and secrets. The Spring Cloud Gatewa</span>y will be replaced <span>by a Kubernetes ingress resource, which can act as an edge server in the same way as Spring Cloud Gateway.</span></p>
<p>In <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml" target="_blank">Chapter 11</a>, S<em>ecuring Access to APIs</em>, refer to the <em>Protecting the external communication with HTTPS</em> section, where we use certificates to protect the external API. Manual handling of certificates is both time-consuming and error-prone. As an alternative, we will be introduced to the Cert Manager, which can automatically provide new <span>certificates </span>and replace expired ones for the external HTTPS endpoint exposed by the ingress. We will<span> configure </span><kbd>cert-manager</kbd> to use <strong>Let's Encrypt</strong> to issue the certificates. Let's Encrypt is a free Certificate Authority that can be used to automatically issue certificates. Let's Encrypt must be able to verify that we own the DNS name that the certificate will be issued for. Since our Kubernetes cluster runs locally in Minikube, we have to<span> make it possible for </span>Let's Encrypt to access our<span> cluster during the provisioning</span>. We will use <kbd>ngrok</kbd> to create a temporary HTTP tunnel from the internet to our local Kubernetes cluster to be used by Let's Encrypt.</p>
<p>When using more and more features in a platform such as Kubernetes, it is important to ensure that the source code of the microservices isn't dependent on the platform; that is, should ensure that the microservices can still be used without Kubernetes. To ensure that w<span>e can still use the microservices outside Kubernetes, the chapter will conclude by deploying the microservice landscape using Docker Compose and executing the</span>&nbsp;<kbd>test-em-all.bash</kbd><span> test script to verify that the microservices still work without using Kubernetes.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Replacing <span>Spring Cloud Config Server with Kubernetes config maps and secrets</span></li>
<li><span>Replacing Spring Cloud Gateway with a Kubernetes ingress resource</span></li>
<li>Adding the Cert Manager to<span> automatically provide certificates issued by Let's Encrypt</span></li>
<li>Using <kbd>ngrok</kbd> to establish an HTTP tunnel from the internet to our local Kubernetes cluster</li>
<li>Deploying and testing the microservice landscape using Docker Compose to ensure that the source code in the microservices isn't locked into Kubernetes</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All commands described in this book<span> are run on a MacBook Pro using macOS Mojave but modifying this so that it can run on other platforms, such as Linux or Windows, should be straightforward.</span></p>
<p>The only new tool required for this chapter is the command-line <span><kbd>ngrok</kbd> tool used for establishing an HTTP tunnel from the internet to our local environment. It can be installed </span>using Homebrew with the following command:</p>
<div>
<pre><strong><span>brew cask install ngrok</span></strong></pre></div>
<p>To use <kbd>ngrok</kbd>, a <span>free</span> account has to be created and an authorization token also has to be registered by taking the following steps:</p>
<ol>
<li>Sign up here: <a href="https://dashboard.ngrok.com/signup">https://dashboard.ngrok.com/signup</a>.</li>
<li>After the account is created, run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>ngrok authtoken &lt;YOUR_AUTH_TOKEN&gt;</span></strong></pre>
<p style="padding-left: 60px">Here, <span><kbd>&lt;YOUR_AUTH_TOKEN&gt;</kbd> is replaced with the authorization token found on the following pageâ<a href="https://dashboard.ngrok.com/auth">https://dashboard.ngrok.com/auth</a>.</span></p>
<p>The source code for this chapter can be found <span>on GitHub</span>:&nbsp;<a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter17">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter17</a>.</p>
<p>To be able to run the commands as described in the book, you need to download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter17</span></strong></pre>
<p><span>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1, SR2 (also known as the <strong>Greenwich</strong> release), Spring Boot 2.1.6, and Spring 5.1.8âthe latest available versions of the Spring components at the time of writing this chapter. The source code has been tested using Kubernetes v1.15.</span></p>
<p><span>All source code examples in this chapter come from the source code in </span><span><kbd>$BOOK_HOME/Chapter17</kbd> but have been edited in several cases to remove non-relevant parts of the source code, such as comments, imports, and log statements.</span></p>
<p>If you want to see the changes applied to the source code in <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 17</a>,&nbsp;<em>Implementing Kubernetes Features as an Alternative</em>, that is, the changes required to replace the Spring Cloud Config Server and <span>Spring Cloud </span>Gateway with corresponding features in Kubernetes<span>,&nbsp;</span>you can compare it with the source code for <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml" target="_blank">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em>.&nbsp;<span> You can use your favorite <kbd>diff</kbd> tool and compare the <kbd>$BOOK_HOME/Chapter16</kbd> and <kbd>$BOOK_HOME/Chapter17</kbd> folders. </span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Replacing the Spring Cloud Config Server</h1>
                </header>
            
            <article>
                
<p>As we have seen in the previous chapter, <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em>, in the <em>Deploying to Kubernetes</em> section, config maps and secrets can be used to hold configuration information for our microservices. The Spring Cloud Config Server adds values such as keeping all configuration in one place, optional version control using Git, and the ability to encrypt sensitive information on the disk. But it also consumes a non-negligible amount of memory (as with any Java and Spring-based application) and adds significant overhead during startup. For example, when running automated integration tests such as the test script we are using in this book, <kbd>test-em-all.bash</kbd>, all microservices are started up at the same time, including the configuration server. Since the other microservices must get their configuration from the configuration server before they can start up, they all have to wait for the configuration server to be up and running before they can start up. This leads to a significant delay when running integration tests. If we use Kubernetes config maps and secrets instead, this delay is eliminated, making automated integration tests run faster. <span>To me, it makes sense to use </span><span>the Spring Cloud Config Server where the underlying platform doesn't provide a similar capability, but when deploying to Kubernetes, it is better to use config maps and secrets. </span></p>
<p><span>Using Kubernetes config maps and secrets instead of the </span><span>Spring Cloud Config Server will </span>make the <span>microservice landscape start up faster and it will require less memory. It </span>will also simplify the microservice landscape by eliminating one supportive service, the <span>configuration</span> server. This is illustrated by the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7ab391b2-00bd-4930-8d8f-371cd083215a.png" style="width:49.17em;height:23.42em;" width="1909" height="908" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/7ab391b2-00bd-4930-8d8f-371cd083215a.png"></p>
<p>Let's see what is required to replace the <span>Spring Cloud Config Server with Kubernetes config maps and secrets!</span></p>
<div class="packt_tip">Note especially that we only change the configuration; that is, no changes are required in the Java source code! </div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the source code to replace the Spring Cloud Config Server</h1>
                </header>
            
            <article>
                
<p>The following changes have been applied in the configuration of the source code to replace the <span>Spring Cloud Config Server with Kubernetes config maps and secrets</span>:</p>
<ul>
<li><span>Removed the project </span><kbd>spring-cloud/config-server</kbd><span>, also including:</span>
<ul>
<li>Removed the project in the <kbd>settings.gradle</kbd> build file.</li>
<li>Removed the <span>configuration</span> server YAML files and its declaration in the <kbd>kustomization.yml</kbd> <span>files in the </span><kbd>kubernetes/services/base</kbd> <span>and </span><kbd>kubernetes/services/overlays/prod</kbd><span> folders.</span></li>
</ul>
</li>
<li><span>Removed configuration from all microservices:</span><br>
<ul>
<li>Removed the <kbd>spring-cloud-starter-config</kbd> dependency in the <span><kbd>build.gradle</kbd> build files</span></li>
</ul>
<ul>
<li>Removed the<span>&nbsp;</span><kbd>bootstrap.yml</kbd><span>&nbsp;</span>files in the<span>&nbsp;</span><kbd>src/main/resource</kbd> folders in each project</li>
<li>Removed the  <kbd>spring.clod.config.enabled=false</kbd><span> property setting </span>in integration tests</li>
</ul>
</li>
<li>Changes in the configuration files in the <kbd>config-repo</kbd> folder:
<ul>
<li>Removed properties with sensitive information, that is, credentials for M<span>ongoDB, MySQL, RabbiMQ, and the password for the TLS certificate used by the edge server. They will be replaced by Kubernetes secrets.</span></li>
<li>The route to the<span>&nbsp;</span><span>configuration</span> server API is removed in the configuration of the edge server</li>
</ul>
</li>
</ul>
<ul>
<li>Changes in the Kubernetes definition files for deployment resources in the <kbd>kubernetes/services/base</kbd> folder:
<ul>
<li>Config maps are mounted as volumes, that is, as folders in the filesystem of the container.  Each microservice gets its own <span>config maps, which contain the configuration files applicable for the specific microservice.<br></span></li>
<li>Define the <kbd>SPRING_CONFIG_LOCATION</kbd> <span>environment variable </span>to point out the configuration files in the volume.</li>
<li>Define credentials for access to resource managers using secrets.</li>
</ul>
</li>
</ul>
<p>Most changes are in the Kubernetes definitions files for the deployment resources. Let's look at the definition of the deployment resource for the <kbd>product</kbd> microservice as an example:</p>
<pre>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: product<br>spec:<br>  template:<br>    spec:<br>      containers:<br>      - name: pro<br>        env:<br>        - name: SPRING_PROFILES_ACTIVE<br>          value: "docker"<br>        - name: SPRING_CONFIG_LOCATION<br>          value: file:/config-repo/application.yml,file:/config-<br>           repo/product.yml<br>        envFrom:<br>        - secretRef:<br>            name: rabbitmq-credentials<br>        - secretRef:<br>            name: mongodb-credentials<br>        volumeMounts:<br>        - name: config-repo-volume<br>          mountPath: /config-repo<br>      volumes:<br>      - name: config-repo-volume<br>        configMap:<br>          name: config-repo-product</pre>
<p>Note that parts of the definition that have not been affected by the changes are left out for improved readability. See <kbd>kubernetes/services/base/product.yml</kbd> for the full source code.</p>
<p>The following explains the preceding source code:</p>
<ul>
<li>The<span>&nbsp;</span><kbd>config-repo-product</kbd><span> config map </span>is mapped in a volume named<span>&nbsp;</span><kbd>config-repo-volume</kbd>.</li>
<li>The <span><span><kbd>config-repo-volume</kbd><span> volume </span>is mounted in the filesystem at </span></span><kbd>/config-repo</kbd>.</li>
<li>The <kbd>SPRING_CONFIG_LOCATION</kbd> <span>environment variable tells</span> Spring where to find the property files, in this case, the <kbd>/config-repo/application.yml</kbd> and <kbd>/config-repo/product.yml</kbd><span> files.</span></li>
<li>Credentials for accessing RabbitMQ and MongoDB are set up as environment variables based on the content in the <kbd>rabbitmq-credentials</kbd> and <kbd>mongodb-credentials</kbd>&nbsp;<span>secrets.</span></li>
</ul>
<p><span>The Kubernetes config maps and secrets will be created in the <em>Testing with ConfigMaps, secrets, and ingress</em> section.</span></p>
<p>That is what is required to replace the<span>&nbsp;</span><span>configuration</span> server with <span>Kubernetes config maps and secrets. In the next section, we will learn about how we can r</span>eplace the Spring Cloud Gateway with a Kubernetes ingress resource.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Replacing the Spring Cloud Gateway</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will further simplify the microservice landscape by replacing the </span>Spring Cloud Gateway with the built-in ingress resource in Kubernetes, <span>reducing the number of supportive services required to be deployed.</span></p>
<p>As introduced in <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>, <em>Introduction to Kubernetes</em>, an ingress resource can be used in Kubernetes to act as an edge server in the same way as a Spring Cloud Gateway. The<span> Spring Cloud Gateway comes with a richer routing functionality compared to an ingress resource. But the ingress feature is part of the Kubernetes platform and can also be extended using the Cert Manager to automatically provide certificates, as we will see later on in this chapter.</span></p>
<p><span>We have also used the Spring Cloud Gateway to protect our microservices from unauthenticated requests; that is, the microservices require a valid OAuth 2.0/OIDC access token from a trusted OAuth Authorization Server or OIDC Provider. See <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml">Chapter 11</a>, <em>Securing Access to APIs</em></span>,<span> if a recap is required. Generally, Kubernetes ingress resources do not have support for that. Specific implementations of the ingress controller might, however, support it. </span></p>
<p><span>Finally, the composite health check we added to the gateway in <a href="a3383211-405d-4319-b142-ddb8cf3674fd.xhtml">Chapter 10</a>, <em>Using Spring Cloud Gateway to Hide Microservices Behind an Edge Server</em> can be replaced by the Kubernetes liveness and readiness probes defined in each microservices deployment resource. To me, it makes sense to use the </span><span>Spring Cloud Gateway </span><span>where the underlying platform doesn't provide a similar capability, but when deploying to Kubernetes, it is better to use ingress resources.</span></p>
<p><span>In this chapter, we will delegate the responsibility for validating that the request contains a valid access token to the</span><span>&nbsp;</span><kbd>product-composite</kbd><span> microservice. The next chapter will introduce the concept of a Service Mesh, where we will see an alternate implementation of an ingress that fully supports validating JWT-encoded access tokens, that is, the type of access tokens that we are using in this book.</span></p>
<div class="packt_infobox">In the <em>Verifying that the microservices work without Kubernetes</em> section, we will still use the <span>Spring Cloud Gateway together with Docker Compose, so we will not remove the project.</span></div>
<p>The following diagram shows how to remove the Spring Cloud Gateway from the microservice landscape when deploying to Kubernetes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d19dde2e-6974-428d-a6e3-ba564833e00a.png" width="1915" height="717" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d19dde2e-6974-428d-a6e3-ba564833e00a.png"></p>
<p>Let's see what is required to replace the <span>Spring Cloud Gateway with a Kubernetes ingress resource!</span></p>
<div class="packt_tip">Note especially that we only change the configuration; that is, no changes are required in the Java source code! </div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the source code for Spring Cloud Gateway</h1>
                </header>
            
            <article>
                
<p><span>The following changes have been applied in the configuration of the source code to replace the </span><span>Spring Cloud Gateway with a Kubernetes ingress resource</span><span>:</span></p>
<ul>
<li><span>Changes in the Kubernetes definition files for deployment resources in the</span> <kbd>kubernetes/services</kbd><span> folder.</span>
<ul>
<li>Removed the gateway YAML files and its declaration in the<span>&nbsp;</span><kbd>kustomization.yml</kbd><span>&nbsp;</span><span>files in the </span><kbd>base</kbd><span>&nbsp;</span><span>and </span><kbd>overlays/prod</kbd><span> folders</span></li>
<li>Added the ingress resource in <span><kbd>base/ingress-edge-server.yml</kbd> and added a reference to it in <kbd>base/kustomization.yml</kbd></span></li>
</ul>
</li>
</ul>
<p>The definition of the ingress resource<span>&nbsp;</span>looks like the following code:</p>
<pre>apiVersion: extensions/v1beta1<br>kind: Ingress<br>metadata:<br>  name: edge<br>spec:<br>  tls:<br>    - hosts:<br>      - minikube.me<br>      secretName: tls-certificate<br>  rules:<br>  - host: minikube.me<br>    http:<br>      paths:<br>      - path: /oauth<br>        backend:<br>          serviceName: auth-server<br>          servicePort: 80<br>      - path: /product-composite<br>        backend:<br>          serviceName: product-composite<br>          servicePort: 80 <br>      - path: /actuator/health<br>        backend:<br>          serviceName: product-composite<br>          servicePort: 80</pre>
<p><span>The following are the explanations for the preceding source code:</span></p>
<ul>
<li>The ingress resource is named <kbd>edge</kbd>.</li>
<li>The <kbd>tls</kbd> section specifies that the ingress will require the use of HTTPS and that it will use a certificate issued for the <kbd>minikube.me</kbd> hostname.</li>
<li>The certificate is stored in a secret named <kbd>tls-certificate</kbd><br>
<span>The </span><span><kbd>tls-certificate</kbd> secret will be created in <em>step 4</em> in the <em>Testing with Kubernetes ConfigMaps, secrets, and ingress resource</em> section.</span></li>
<li>Routing rules are defined for requests to the <kbd>minikube.me</kbd> hostname.<span><br>
The DNS name <kbd>minikube.me</kbd> will be mapped to the IP address of the Minikube instance in the next topic.<br></span></li>
<li>Routes are defined for the following:
<ul>
<li>The <kbd>auth-server</kbd> on the <kbd>/oauth</kbd> path</li>
<li>The <kbd>product-composite</kbd> microservice on the <kbd>/<span>product-composite</span></kbd><span> path</span></li>
<li>The <kbd>health</kbd> endpoint in t<span><span>he <kbd>product-composite</kbd> microservice on the </span></span><kbd>/actuator/health</kbd><span> path</span></li>
</ul>
</li>
</ul>
<p>The Kubernetes ingress resource will be created in the next section where we will test the microservice landscape together with Kubernetes config maps, secrets, and an ingress resource.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing with Kubernetes ConfigMaps, secrets, and ingress resource</h1>
                </header>
            
            <article>
                
<p>With the preceding changes described, we are ready to test the system landscape with the Spring Cloud Config Server and the Spring Cloud<span>&nbsp;G</span>ateway replaced by Kubernetes config maps, secrets, and an ingress resource. <span>&nbsp;</span><span>As before, when we used</span><span> the Spring Cloud Gateway as the edge server, the external API will be protected by HTTPS. With this deployment, we will configure the ingress resource to reuse the self-signed certificate we used with the Spring Cloud Gateway for HTTPS.</span> This is illustrated by the following diagram: </p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/5eb38462-0a35-4944-a8e6-0e6ff5e338b1.png" width="1950" height="619" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/5eb38462-0a35-4944-a8e6-0e6ff5e338b1.png"></p>
<div class="packt_infobox"><span>In the next section, we will enhance the certificate usage and replace the self-signed certificate with certificates issued by Let's Encrypt.</span></div>
<p>The ingress will be exposed on the default HTTPS port, <kbd>443</kbd>, on the Minikube instance. This is handled by the ingress controller that was installed when we performed the <kbd>minikube addons enable ingress</kbd> command; see <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>, <em>Introduction to Kubernetes </em>and <span>refer to the </span><em>Creating a Kubernetes cluster</em> section for a recap. The ingress controller consists of a deployment, <kbd>nginx-ingress-controller</kbd>, in the <kbd>kube-system</kbd> namespace. The deployment configures its pod using a <kbd>hostPort</kbd> to map port <kbd>443</kbd> in the host, that is, the Minikube instance, to port <kbd>443</kbd> in the container that runs in the pod. The central parts in the definition of the deployment look like the following:</p>
<pre>apiVersion: extensions/v1beta1<br>kind: Deployment<br>metadata:<br>  name: nginx-ingress-controller<br>spec:<br>  template:<br>    spec:<br>      containers:<br>        image: quay.io/kubernetes-ingress-controller/nginx-ingress-<br>         controller:0.23.0<br>        ports:<br>        - containerPort: 443<br>          hostPort: 443</pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<div class="packt_infobox"><span>This setup works for a single-node Kubernetes cluster used for development and testing. </span>In a multi-node <span>Kubernetes cluster, external load balancers are used to expose an ingress controller for high availability and scalability.</span></div>
<p><span>The deployment uses the same type of commands as we used in</span>&nbsp;<a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em>; refer to the <em>Deploying to Kubernetes for development and test</em> section.  </p>
<p>The major differences are that this deployment will:</p>
<ul>
<li>Create one config map for each microservice instead of one config map for the<span>&nbsp;</span><span>configuration</span> server</li>
<li>Create secrets for credentials to the resource managers and a secret for the TLS certificate used by the ingress instead of creating secrets for the <span>credentials to the configuration server </span></li>
<li>Create one ingress instead of using Spring Cloud Gateway</li>
</ul>
<p>To simplify the deployment, deploy scripts for the development and production environments have been added to the source code. Let's go through the deploy script for the <span>development </span>environment that we will use in this section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Walking through the deploy script</h1>
                </header>
            
            <article>
                
<p>The <kbd><span>kubernetes/scripts/deploy-dev-env.bash</span></kbd> script, contains the necessary commands for performing the deployment. <span>The script will perform the following steps:</span></p>
<ol>
<li class="mce-root"><span>It will create config maps, one per microservice. For example, for the</span> <kbd>product</kbd> <span>microservice, we have the following:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl create configmap config-repo-product --from-file=config-repo/application.yml --from-file=config-repo/product.yml --save-config</strong></pre>
<ol start="2">
<li><span>Then, it will create the required secrets. For example, credentials for accessing RabbitMQ are created with the following command:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl create secret generic rabbitmq-credentials \</strong><br><strong>    --from-literal=SPRING_RABBITMQ_USERNAME=rabbit-user-dev \</strong><br><strong>    --from-literal=SPRING_RABBITMQ_PASSWORD=rabbit-pwd-dev \</strong><br><strong>    --save-config</strong></pre>
<ol start="3">
<li class="mce-root"><span>Secrets are also created for the resource managers; their names are suffixed with</span><span>&nbsp;</span><kbd>server-credentials</kbd><span>. They are used in the Kubernetes definitions files in the </span><kbd>kubernetes/services/overlays/dev</kbd><span> folder. For example, credentials used by RabbitMQ are created with the following command:<br></span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl create secret generic rabbitmq-server-credentials \</strong><br><strong>    --from-literal=RABBITMQ_DEFAULT_USER=rabbit-user-dev \</strong><br><strong>    --from-literal=RABBITMQ_DEFAULT_PASS=rabbit-pwd-dev \</strong><br><strong>    --save-config</strong></pre>
<ol start="4">
<li><span>The secret that contains the TLS certificate for the ingress, </span><kbd>tls-certificate</kbd><span>, is based on the already existing self-signed certificate in the </span><kbd>kubernetes/cert</kbd><span> folder. It is created with the following command:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl create secret tls tls-certificate --key kubernetes/cert/tls.key --cert kubernetes/cert/tls.crt</strong></pre>
<div class="packt_infobox"><span>The self-signed certificate has been created with the following command: <br></span><kbd><span>openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout kubernetes/cert/tls.key -out kubernetes/cert/tls.crt -subj "/CN=minikube.me/O=minikube.me"</span></kbd></div>
<ol start="5">
<li><span>Deploy the microservices for the development environment, based on the</span><span>&nbsp;</span><kbd>dev</kbd><span>&nbsp;</span><span>overlay, using the</span><span>&nbsp;</span><kbd>-k</kbd><span>&nbsp;</span><span>switch to activate Kustomize:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -k kubernetes/services/overlays/dev</strong></pre>
<ol start="6">
<li><span>Wait for the deployment and its pods to be up and running:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl wait --timeout=600s --for=condition=ready pod --all</strong></pre>
<p>After this walkthrough, we are ready to run the commands required for deploying and testing!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running commands for deploying and testing</h1>
                </header>
            
            <article>
                
<p>Before we can perform the deployment, we need to make the following preparations:</p>
<ul>
<li>Map the DNS name <span>used by the ingress,</span><span>&nbsp;</span><kbd>minikube.me</kbd>, to the IP address of the Minikube instance</li>
<li><span>Build Docker images from source</span></li>
<li>Create a namespace in Kubernetes</li>
</ul>
<p>Run the following commands to prepare, deploy, and test:</p>
<ol>
<li class="mce-root"><span>Map</span><span>&nbsp;</span><kbd>minikube.me</kbd><span> to the IP address of the Minikube instance </span><span>by adding a line to the file</span> <kbd>/etc/hosts</kbd> <span>with the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px" class="mce-root"><strong>sudo bash -c "echo $(minikube ip) minikube.me | tee -a /etc/hosts"<br></strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>Verify the result with the </span><span><kbd>cat /etc/hosts</kbd></span><span> command. Expec</span><span>t a line that contains the IP address of your Minikube instance followed by </span><kbd>minikube.me</kbd><span> as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e620ccae-5f94-4d2f-a0cc-9c5b94ffb268.png" style="width:16.25em;height:7.08em;" width="548" height="238" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e620ccae-5f94-4d2f-a0cc-9c5b94ffb268.png"></p>
<ol start="2">
<li>Build Docker images from source with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd $BOOK_HOME/Chapter17</strong><br><strong>eval $(minikube docker-env)</strong><br><strong>./gradlew build &amp;&amp; docker-compose build</strong></pre>
<ol start="3">
<li>Recreate the namespace, <kbd>hands-on</kbd>, and set it as the default namespace:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl delete namespace hands-on</strong><br><strong>kubectl create namespace hands-on</strong><br><strong>kubectl config set-context $(kubectl config current-context) --namespace=hands-on </strong></pre>
<ol start="4">
<li>Execute the deployment by running the script with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>./kubernetes/scripts/deploy-dev-env.bash </strong></pre>
<ol start="5">
<li>Once the deployment is complete, start the tests with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>HOST=minikube.me PORT=443 ./test-em-all.bash</strong></pre>
<p style="color: black;padding-left: 60px">Expect the normal output that we have seen from the previous chapters as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2029adbf-aead-45bf-b8a5-78f258baa0f6.png" style="width:40.42em;height:12.00em;" width="1601" height="472" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/2029adbf-aead-45bf-b8a5-78f258baa0f6.png"></p>
<ol start="6">
<li>You can try out the APIs manually by performing the same steps as in the earlier chapters: just replace the host and port with <kbd>minikube.me</kbd>. Get an OAuth/OIDC access token and use it to call an API with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong><br><br><strong>curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" | jq .productId</strong></pre>
<p>Expect the requested product ID, <kbd>2</kbd>, in the response.</p>
<div class="packt_infobox">
<p>The deployment we have set up in this section is based on an environment aimed at developing and testing. If you want to set up an environment aimed at staging and production, such as the one described in <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em>, refer to the <em>Deploying to Kubernetes for staging and production</em> section. For this, you can use the <kbd>./kubernetes/scripts/deploy-prod-env.bash</kbd> script. Use it in <em>step 4</em> as previously outlined instead of the <kbd>deploy-dev-env.bash</kbd><span> script.</span><br>
Note that the <kbd>deploy-prod-env.bash</kbd> script will launch the three resource managers for MySQL, MongoDB, and RabbitMQ using Docker Compose; that is, they will run as Docker containers outside Kubernetes (in the same way as in <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em>).</p>
</div>
<div>
<p><span>This deployment uses a self-signed certificate that was exposed by the Kubernetes ingress and that requires manual provisioning. </span><span>Manual handling of certificates is both time-consuming and error-prone. It is, for example, very easy to forget to renew a certificate in time. In the next section, we will learn how to use the Cert Manager and Let's Encrypt to automate this provisioning process!</span></p>
</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Automating the provision of certificates </h1>
                </header>
            
            <article>
                
<p>As mentioned in the introduction to this chapter, <span>we will use the Cert Manager t</span>o automate the provision of certificates used by the <span>external HTTPS endpoint exposed by the ingress. </span>The Cert Manager will run as an add-on in Kubernetes and will be configured to request the issuing of certificates from Let's Encrypt with a free <span>Certificate Authority that can be used to</span> automate the issuing of certificates. To be able to <span>verify that we own the DNS name that the certificate shall be issued for, Let's Encrypt requires access to the endpoint we want to issue the certificate for. Since our Kubernetes cluster runs locally in Minikube, we must make it possible for Let's Encrypt to access our cluster during the provisioning. We will use the ngrok tool to create a temporary HTTP tunnel from the internet to our local Kubernetes cluster to be used by Let's Encrypt.</span></p>
<p>For more information on each product, see the following:</p>
<ul>
<li>Cert Manager: <a href="http://docs.cert-manager.io/en/latest/index.html">http://docs.cert-manager.io/en/latest/index.html</a></li>
<li>Let's Encrypt: <a href="https://letsencrypt.org/docs/">https://letsencrypt.org/docs/</a></li>
<li>ngrok: <a href="https://ngrok.com/docs">https://ngrok.com/docs</a></li>
</ul>
<p><span>All this together</span><span> might seem a bit overwhelming, so let's take it step by step:</span></p>
<ol>
<li>Deploy the Cert Manager and define issuers in Kubernetes based on Let's Encrypt.</li>
<li>Create an HTTP tunnel using ngrok.</li>
<li>P<span>rovision certificates with the Cert Manager and Let's Encrypt.</span></li>
<li>Verify that we got certificates from <span>Let's Encrypt.</span></li>
<li>Clean up.</li>
</ol>
<div class="packt_infobox">The HTTP tunnel is only required if your Kubernetes cluster isn't reachable on the internet. If its ingress resource can be accessed directly from the internet, the use of ngrok can be skipped.</div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying the Cert Manager and defining Let's Encrypt issuers</h1>
                </header>
            
            <article>
                
<p>To deploy the Cert Manager, we can execute a single Kubernetes definition file that will create a namespace, <kbd>cert-manager</kbd>, and then deploy the Cert Manager into the namespace. We will install version 0.8.1, the latest available version when writing this chapter. Run the following command:</p>
<pre class="mce-root"><strong>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.8.1/cert-manager.yaml</strong></pre>
<p>If you get an error message such as <kbd>unable to recognize "https://github.com/jetstack/cert-manager/releases/download/v0.8.1/cert-manager.yaml": no matches for kind "Issuer" in version "certmanager.k8s.io/v1alpha1"</kbd>, then simply rerun the command again.</p>
<p>Wait for the deployment and its pods to be available:</p>
<pre><strong>kubectl wait --timeout=600s --for=condition=ready pod --all -n cert-manager</strong></pre>
<p>Expect output similar to the following from the command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/196e1fc3-83ec-4d71-84dc-18e5f84a59ed.png" style="width:40.75em;height:7.42em;" width="786" height="143" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/196e1fc3-83ec-4d71-84dc-18e5f84a59ed.png"></p>
<p>With the Cert Manager in place, we can define issuers in Kubernetes that are based on Let's Encrypt.</p>
<p>Let's Encrypt exposes the following issuers:</p>
<ul>
<li><strong>Staging environment</strong>, to be used during development and test phases where it can be expected that a lot of short-lived certificates are requested. The staging environment allows for the creation of many certificates but the root <strong>CA</strong> <em>(</em>short for <strong>Certificate Authority</strong>) in the certificate is not trusted. This means that certificates from the staging environment can't be used to protect web pages or APIs used by a web browser. A web browser won't trust its root CA and will complain when a user opens a web page protected by<span> certificates from the staging environment.</span>&nbsp;</li>
<li><strong>Production environment</strong>, it uses a trusted root CA to issue certificates. It can, therefore, be used to issue certificates that are trusted by web browsers. The production environment limits the number of certificates that can be issued. For example, only 50 new certificates per week can be issued per registered domain, for instance, in case <kbd>ngrok.io</kbd>.</li>
</ul>
<p>We will register two issuers in Kubernetes, one for the staging environment and one for the production environment<span>. Issuers can be registered either globally in the cluster or locally in a namespace. To keep things together, we will use namespace local issuers.</span></p>
<p>Communication between the Cert Manager and Let's Encrypt during the provision of certificates is based on a standard protocol, <strong>Automated Certificate Management Environment v2</strong>, or <strong>ACME v2</strong> for short. Let's Encrypt will act as a CA and the Cert Manager will act as an ACME client. To validate the ownership of a DNS name, the ACME protocol specifies two types of challenge that a CA can use:</p>
<ul>
<li><kbd>http-01</kbd>: The CA asks the ACME client for a randomly named file to be made available under the following URL: <kbd>http://&lt;domainname&gt;/.well-known/acme-challenge/&lt;randomfilename&gt;</kbd>. If the CA succeeds in accessing the file using this URL, the ownership of the domain is validated.</li>
<li><kbd>dns-01</kbd>: The CA asks <span>the ACME client </span>for a specified value to be placed in a TXT record, <kbd><span>_acme-challenge.&lt;YOUR_DOMAIN&gt;</span></kbd>, under the domain in the DNS server. This is typically achieved by using an API of the DNS provider. <span>If the CA succeeds in accessing the specified content in the TXT record in the DNS server, the ownership of the domain is validated.</span></li>
</ul>
<p>Automating a <kbd>dns-01</kbd> based challenge is harder to achieve than automating an <kbd>http-01</kbd> challenge in most cases; however, it is preferred, for example, if the HTTP endpoint isn't available on the internet. A <kbd>dns-01</kbd> challenge also supports issuing wildcard certificates, which an <kbd>http-01</kbd> challenge can't be used for. In this chapter, we will configure the Cert Manager to use an <kbd>http-01</kbd>âbased challenge.</p>
<p>The definition of the issuer for the Let's Encrypt staging environment looks like the following:</p>
<pre>apiVersion: certmanager.k8s.io/v1alpha1<br>kind: Issuer<br>metadata:<br>  name: letsencrypt-issuer-staging<br>spec:<br>  acme:<br>    email: &lt;your email address&gt;<br>    server: https://acme-staging-v02.api.letsencrypt.org/directory<br>    privateKeySecretRef:<br>      name: letsencrypt-issuer-staging-account-key<br>    solvers:<br>    - http01:<br>        ingress:<br>          class: nginx</pre>
<p>The following explains the preceding source code:</p>
<ul>
<li>The <kbd>name</kbd> of the issuer, <kbd>letsencrypt-issuer-staging</kbd>, will be used in the ingress when referring to the issuer to be used when provisioning certificates for the ingress.</li>
<li>The <kbd>email</kbd> must be filled in with your email address. <span>Let's Encrypt will use the email address to contact you about expiring </span><span>certificates and issues, if any, related to your account.</span></li>
<li>The <kbd>server</kbd> field points out the URL for the Let's Encrypt staging environment.</li>
<li>The <kbd>privateKeySecretRef</kbd> field contains the name of a secret. This secret will be created by the Cert Manager and will contain an ACME/Let's Encrypt <kbd>account private key</kbd>. This key identifies you (or your company) as a user of the ACME service, that is, Let's Encrypt. It is used to sign requests sent to Let's Encrypt to validate your identity.</li>
<li>The <kbd>solver</kbd> definition declares that an <kbd>http-01</kbd> challenge shall be used to verify the ownership of the domain name.</li>
</ul>
<p>The <span>definition of the issuer for the Let's Encrypt production environment looks the same, the major difference is the ACME server URL used: <a href="https://acme-v02.api.letsencrypt.org/directory">https://acme-v02.api.letsencrypt.org/directory</a>.</span></p>
<p>Edit the following files and replace <kbd>&lt;your email address&gt;</kbd> with your email address:</p>
<ul>
<li><kbd>kubernetes/services/base/letsencrypt-issuer-staging.yaml</kbd></li>
<li><kbd>kubernetes/services/base/letsencrypt-issuer-prod.yaml</kbd></li>
</ul>
<div>
<p><span>Apply the definitions with the following commands: </span></p>
</div>
<pre class="mce-root"><strong>kubectl apply -f kubernetes/services/base/letsencrypt-issuer-staging.yaml</strong><br><strong>kubectl apply -f kubernetes/services/base/letsencrypt-issuer-prod.yaml</strong></pre>
<p>We now have the Cert Manager in place and have registered issuers for the Let's Encrypt staging and production environment. The next step is to create an HTTP tunnel using <kbd>ngrok</kbd>.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Creating an HTTP tunnel using ngrok</h1>
                </header>
            
            <article>
                
<p><span>The free subscription to <kbd>ngrok</kbd> can be used to create an HTTP tunnel where <kbd>ngrok</kbd> terminates the HTTPS traffic using its own wildcard certificate for <kbd><em>*</em>.ngrok.io</kbd>, that is, before the HTTP requests reach the ingress resource in Kubernetes. The client that sends the HTTPS request will only see the <kbd>ngrok</kbd> certificate and not the certificate exposed by the ingress resource in Kubernetes. This means that we can't use the HTTP tunnel to test a certificate that has been issued by Let's Encrypt and is used by the ingress resource in Kubernetes. This is illustrated in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a5583460-7ef4-4f1a-a566-507395d3be49.png" width="1769" height="723" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a5583460-7ef4-4f1a-a566-507395d3be49.png"></p>
<p><span>But the HTTP tunnel can be used during the provisioning phase where Let's Encrypt needs to verify that the ACME client owns the DNS name it is requested to issue a certificate for. The DNS name will be the hostname that <kbd>ngrok</kbd> assigns to the HTTP tunnel, for example, </span><kbd>6cc09528.ngrok.io</kbd><span>. Once the provisioning is performed, we can shut down the HTTP tunnel and redirect the hostname to the IP address of the Minikube instance (using the local <kbd>/etc/hosts</kbd> file). This is illustrated in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d42306ac-0d80-43ca-bbc8-f783f35353ed.png" style="width:39.25em;height:23.58em;" width="1230" height="743" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d42306ac-0d80-43ca-bbc8-f783f35353ed.png"></p>
<div class="packt_infobox"><span>For paying customers, <kbd>ngrok</kbd> provides a TLS tunnel that passes through HTTPS traffic instead of terminating it; that is, a client that sends an HTTPS request will be able to see and verify the certificate exposed by the ingress resource in Kubernetes. Using a TLS tunnel instead of the HTTP tunnel should make this extra step <span>unnecessary</span>.</span></div>
<p>Perform the following steps to create the HTTP tunnel:</p>
<ol>
<li>Create the HTTP tunnel with the following command: </li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>ngrok http https://minikube.me:443</strong></pre>
<ol start="2">
<li class="mce-root"><span>Expect output similar to the following screenshot:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cc23906d-f9c7-44e8-aa0d-da848bd6e750.png" style="width:40.25em;height:13.58em;" width="1798" height="606" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cc23906d-f9c7-44e8-aa0d-da848bd6e750.png"></p>
<ol start="3">
<li class="mce-root"><span>P</span><span>ick up</span> <span>the hostname for the</span> <span>HTTP tunnel, </span><kbd>6cc09528.ngrok.io</kbd><span> in the preceding example</span><span>, and save it in an environment variable such as the following:</span></li>
</ol>
<pre style="color: black;padding-left: 60px" class="mce-root"><strong>NGROK_HOST=6cc09528.ngrok.io</strong></pre>
<p>With the HTTP tunnel in place, we can prepare the definition of the ingress resource for automatic provisioning of its certificate using the Cert Manager and Let's Encrypt!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Provisioning certificates with the Cert Manager and Let's Encrypt</h1>
                </header>
            
            <article>
                
<p>Before configuring the <span>ingress resource, it might be good to have a high level understanding of how the provisioning is performed. </span>The automated provisioning <span>of a certificate </span>using the Cert Manager and Let's Encrypt looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ed4a016d-32e6-4381-af09-0f5d93641c42.png" width="1920" height="880" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ed4a016d-32e6-4381-af09-0f5d93641c42.png"></p>
<p>The following steps will be taken during the provisioning:</p>
<ol>
<li>An ingress is created annotated with <kbd>certmanager.k8s.io/issuer: "name of a Let's Encrypt issuer"</kbd>.</li>
<li>This annotation will trigger the Cert Manager to start to provide a certificate for the ingress using Let's Encrypt.</li>
</ol>
<ol start="3">
<li>During the provisioning process, Let's Encrypt will perform an <kbd>http-01</kbd> challenge and use the HTTP tunnel to verify that the Cert Manager owns the DNS name.</li>
<li>Once the provisioning is complete, the Cert Manager will store the certificate in Kubernetes and create a secret with the name specified by the ingress.</li>
</ol>
<p class="mce-root">We will add a new ingress, <kbd>edge-ngrok</kbd>, defined in the <kbd>ingress-edge-server-ngrok.yml</kbd><span> file,</span> which will route requests to the hostname of the HTTP tunnel. This ingress will have the same routing rules as the existing ingress. The part that differs looks like the following:</p>
<pre>apiVersion: extensions/v1beta1<br>kind: Ingress<br>metadata:<br>  name: edge-ngrok<br>  annotations:<br>    certmanager.k8s.io/issuer: "letsencrypt-issuer-staging"<br>spec:<br>  tls:<br>  - hosts:<br>    - xxxxxxxx.ngrok.io<br>    secretName: tls-ngrok-letsencrypt-certificate<br>  rules:<br>  - host: xxxxxxxx.ngrok.io</pre>
<p>Here is an explanation for the preceding source code:</p>
<ul>
<li>Using the <kbd>certmanager.k8s.io/issuer: "letsencrypt-issuer-staging"</kbd>&nbsp;<span>annotation, </span> we ask the Cert Manager to provision a certificate for this ingress using the issuer named <kbd>letsencrypt-issuer-staging</kbd>.</li>
<li>The <kbd>xxxxxxxx.ngrok.io</kbd> hostname in the <kbd>tls</kbd> and <kbd>rules</kbd> declarations must be replaced with the actual hostname of your HTTP tunnel.</li>
<li>The secret with the name <kbd>tls-ngrok-letsencrypt-certificate</kbd> is where the certificate will be stored once the provisioning is complete.</li>
</ul>
<p>With this high level of understanding of the provisioning process and an ingress resource prepared for using it in place, we can start to provision certificates using the two environments that Let's Encrypt supports. Let's start with the staging environment, suitable for development and test activities.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Let's Encrypt's staging environment</h1>
                </header>
            
            <article>
                
<p>Perform the following steps to provision a certificate from Let's Encrypt staging environment and verify that it works:</p>
<ol>
<li class="mce-root">Edit the <kbd>kubernetes/services/base/ingress-edge-server-ngrok.yml</kbd><span> file </span>and replace <kbd>xxxxxxxx.ngrok.io</kbd><span>&nbsp;</span>with the hostname of your HTTP tunnel in two places!<br>
(<kbd><span>6cc09528</span>.ngrok.io</kbd><span>&nbsp;</span>in the preceding example.)</li>
<li>Before starting up the provisioning, run a watch command in a separate Terminal window to monitor the provisioning of the certificate. Run the following command:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl get cert --watch</strong></pre>
<ol start="3">
<li>Initiate the provisioning by applying the new ingress definition with the following command:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl apply -f kubernetes/services/base/ingress-edge-server-ngrok.yml</strong></pre>
<ol start="4">
<li>The Cert Manager will now detect the new ingress and start to provide a certificate with Let's Encrypt staging environment as the issuer using the ACME v2 protocol via the HTTP tunnel set up by <kbd>ngrok</kbd>.</li>
<li>After a while, you should notice the <kbd>http-01</kbd> challenge in the Terminal window where the HTTP tunnel runs. Expect a request like the following in the output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/48532285-e42c-41b8-87c5-953c433986d2.png" style="width:33.50em;height:7.17em;" width="1167" height="249" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/48532285-e42c-41b8-87c5-953c433986d2.png"></p>
<ol start="6">
<li>A&nbsp;<kbd>tls-ngrok-letsencrypt-certificate</kbd><span> certificate will be created and it will be stored in the </span><kbd>tls-ngrok-letsencrypt-certificate</kbd><span> secret, as specified in the ingress. Expect output from the</span> <kbd>kubectl get cert --watch</kbd> <span>command similar to the following:<br></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/75548aa6-b6b1-4b86-b7f3-24a40a679869.png" style="width:42.58em;height:6.00em;" width="1711" height="242" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/75548aa6-b6b1-4b86-b7f3-24a40a679869.png"></p>
<ol start="7">
<li>After a while the <kbd>READY</kbd> <span>state of the certificate will be changed to</span> <kbd>True</kbd><span>, meaning that the certificate is provisioned and we are ready to try it out!</span></li>
<li>To try out the certificate provisioned by Let's Encrypt, we need to redirect the <kbd>ngrok</kbd> hostname to point directly to the Minikube IP address. We will add the hostname of the HTTP tunnel to the <kbd>/etc/hosts</kbd> <span>file resolved to the IP address of the Minikube instance. This will result in local requests sent to the </span><span>hostname of the HTTP tunnel being directed to the Minikube instance as illustrated by the following diagram:<br></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e82140ea-df2b-41c7-9abd-a64f465ec373.png" style="width:36.17em;height:22.17em;" width="1217" height="744" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e82140ea-df2b-41c7-9abd-a64f465ec373.png"></p>
<ol start="9">
<li><span>Edit the <kbd>/etc/hosts</kbd><span> file </span><span>and add the hostname of your HTTP tunnel </span><span>after</span> <span><kbd>minikube.me</kbd> in the line we added earlier in the chapter. </span></span>After the edit, the line should look similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bc8718bb-49a9-4a2d-835a-7a07a6b48936.png" style="width:23.75em;height:6.17em;" width="913" height="237" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bc8718bb-49a9-4a2d-835a-7a07a6b48936.png"></p>
<ol start="10">
<li>Use the <kbd>keytool</kbd> <span>command to see what certificate the hostname of the HTTP tunnel exposes:<br></span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>keytool -printcert -sslserver $NGROK_HOST:443 | grep -E "Owner:|Issuer:"</strong></pre>
<ol start="11">
<li>Expect a response such as the following: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/02d1d310-1b40-4966-ab53-2a8b2a6dd43a.png" style="width:40.75em;height:9.00em;" width="1518" height="335" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/02d1d310-1b40-4966-ab53-2a8b2a6dd43a.png"></p>
<div class="packt_infobox">If your <kbd>keytool</kbd> is localized, that is, it prints its output in another language rather than English, you will need to change the <kbd>Owner:|Issuer:</kbd>&nbsp;<span>string </span>used by the preceding <kbd>grep</kbd> command, to the localized version.</div>
<ol start="12">
<li>The certificate is issued for the hostname of th<span>e HTTP tunnel (</span><kbd>6cc09528.ngrok.io</kbd><span> in the preceding example</span><span>) and it is issued by </span><kbd>Fake LE Intermediate X1</kbd><span> using </span><kbd>Fake LE Root X1</kbd><span> as its Root CA. This verifies that the ingress uses the Let's Encrypt staging certificate!<br></span></li>
<li class="mce-root"><span>Wrap up by running the</span> <kbd>test-em-all.bash</kbd><span> test script using the same command:<br></span></li>
</ol>
<pre style="color: black;padding-left: 60px" class="mce-root"><strong>HOST=$NGROK_HOST PORT=443 ./test-em-all.bash</strong></pre>
<p class="mce-root"><span>Expect the usual output from the test script; check that it concludes with the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b922e10e-eac0-41a4-8ca6-c714f34b08e5.png" style="width:26.83em;height:3.92em;" width="993" height="147" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b922e10e-eac0-41a4-8ca6-c714f34b08e5.png"></p>
<p>Certificates provisioned by Let's Encrypt staging environment are, as mentioned previously, good for development and test activities. But since its root CA is not trusted by web browsers, they can't be used in production scenarios. Let's also try out Let's Encrypt's production environment, which is capable of provisioning trusted certificates, albeit in limited numbers.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Let's Encrypt's staging environment</h1>
                </header>
            
            <article>
                
<p>Perform the following steps to provision a certificate from Let's Encrypt staging environment and verify that it works:</p>
<ol>
<li class="mce-root">Edit the <kbd>kubernetes/services/base/ingress-edge-server-ngrok.yml</kbd><span> file </span>and replace <kbd>xxxxxxxx.ngrok.io</kbd><span>&nbsp;</span>with the hostname of your HTTP tunnel in two places!<br>
(<kbd><span>6cc09528</span>.ngrok.io</kbd><span>&nbsp;</span>in the preceding example.)</li>
<li>Before starting up the provisioning, run a watch command in a separate Terminal window to monitor the provisioning of the certificate. Run the following command:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl get cert --watch</strong></pre>
<ol start="3">
<li>Initiate the provisioning by applying the new ingress definition with the following command:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl apply -f kubernetes/services/base/ingress-edge-server-ngrok.yml</strong></pre>
<ol start="4">
<li>The Cert Manager will now detect the new ingress and start to provide a certificate with Let's Encrypt staging environment as the issuer using the ACME v2 protocol via the HTTP tunnel set up by <kbd>ngrok</kbd>.</li>
<li>After a while, you should notice the <kbd>http-01</kbd> challenge in the Terminal window where the HTTP tunnel runs. Expect a request like the following in the output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/48532285-e42c-41b8-87c5-953c433986d2.png" style="width:33.50em;height:7.17em;" width="1167" height="249" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/48532285-e42c-41b8-87c5-953c433986d2.png"></p>
<ol start="6">
<li>A&nbsp;<kbd>tls-ngrok-letsencrypt-certificate</kbd><span> certificate will be created and it will be stored in the </span><kbd>tls-ngrok-letsencrypt-certificate</kbd><span> secret, as specified in the ingress. Expect output from the</span> <kbd>kubectl get cert --watch</kbd> <span>command similar to the following:<br></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/75548aa6-b6b1-4b86-b7f3-24a40a679869.png" style="width:42.58em;height:6.00em;" width="1711" height="242" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/75548aa6-b6b1-4b86-b7f3-24a40a679869.png"></p>
<ol start="7">
<li>After a while the <kbd>READY</kbd> <span>state of the certificate will be changed to</span> <kbd>True</kbd><span>, meaning that the certificate is provisioned and we are ready to try it out!</span></li>
<li>To try out the certificate provisioned by Let's Encrypt, we need to redirect the <kbd>ngrok</kbd> hostname to point directly to the Minikube IP address. We will add the hostname of the HTTP tunnel to the <kbd>/etc/hosts</kbd> <span>file resolved to the IP address of the Minikube instance. This will result in local requests sent to the </span><span>hostname of the HTTP tunnel being directed to the Minikube instance as illustrated by the following diagram:<br></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e82140ea-df2b-41c7-9abd-a64f465ec373.png" style="width:36.17em;height:22.17em;" width="1217" height="744" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e82140ea-df2b-41c7-9abd-a64f465ec373.png"></p>
<ol start="9">
<li><span>Edit the <kbd>/etc/hosts</kbd><span> file </span><span>and add the hostname of your HTTP tunnel </span><span>after</span> <span><kbd>minikube.me</kbd> in the line we added earlier in the chapter. </span></span>After the edit, the line should look similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bc8718bb-49a9-4a2d-835a-7a07a6b48936.png" style="width:23.75em;height:6.17em;" width="913" height="237" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bc8718bb-49a9-4a2d-835a-7a07a6b48936.png"></p>
<ol start="10">
<li>Use the <kbd>keytool</kbd> <span>command to see what certificate the hostname of the HTTP tunnel exposes:<br></span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>keytool -printcert -sslserver $NGROK_HOST:443 | grep -E "Owner:|Issuer:"</strong></pre>
<ol start="11">
<li>Expect a response such as the following: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/02d1d310-1b40-4966-ab53-2a8b2a6dd43a.png" style="width:40.75em;height:9.00em;" width="1518" height="335" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/02d1d310-1b40-4966-ab53-2a8b2a6dd43a.png"></p>
<div class="packt_infobox">If your <kbd>keytool</kbd> is localized, that is, it prints its output in another language rather than English, you will need to change the <kbd>Owner:|Issuer:</kbd>&nbsp;<span>string </span>used by the preceding <kbd>grep</kbd> command, to the localized version.</div>
<ol start="12">
<li>The certificate is issued for the hostname of th<span>e HTTP tunnel (</span><kbd>6cc09528.ngrok.io</kbd><span> in the preceding example</span><span>) and it is issued by </span><kbd>Fake LE Intermediate X1</kbd><span> using </span><kbd>Fake LE Root X1</kbd><span> as its Root CA. This verifies that the ingress uses the Let's Encrypt staging certificate!<br></span></li>
<li class="mce-root"><span>Wrap up by running the</span> <kbd>test-em-all.bash</kbd><span> test script using the same command:<br></span></li>
</ol>
<pre style="color: black;padding-left: 60px" class="mce-root"><strong>HOST=$NGROK_HOST PORT=443 ./test-em-all.bash</strong></pre>
<p class="mce-root"><span>Expect the usual output from the test script; check that it concludes with the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b922e10e-eac0-41a4-8ca6-c714f34b08e5.png" style="width:26.83em;height:3.92em;" width="993" height="147" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b922e10e-eac0-41a4-8ca6-c714f34b08e5.png"></p>
<p>Certificates provisioned by Let's Encrypt staging environment are, as mentioned previously, good for development and test activities. But since its root CA is not trusted by web browsers, they can't be used in production scenarios. Let's also try out Let's Encrypt's production environment, which is capable of provisioning trusted certificates, albeit in limited numbers.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Let's Encrypt's production environment</h1>
                </header>
            
            <article>
                
<p>To provision a certificate from Let's Encrypt production environment, instead of the staging environment, we have to change the issuer in the ingress definition and then apply the updated definition. Perform the following steps:</p>
<ol>
<li><span>Edit the</span><span>&nbsp;</span><kbd>kubernetes/services/base/ingress-edge-server-ngrok.yml</kbd><span>&nbsp;</span>&nbsp;<span>file </span>and change the following code:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>certmanager.k8s.io/issuer: "letsencrypt-issuer-staging"</strong></pre>
<p style="padding-left: 60px">The preceding code should now be as follows:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>certmanager.k8s.io/issuer: "letsencrypt-issuer-prod"</strong></pre>
<ol start="2">
<li>Apply the change by running the following command:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl apply -f kubernetes/services/base/ingress-edge-server-ngrok.yml</strong></pre>
<ol start="3">
<li class="mce-root">Monitor the output from the <kbd>kubectl get cert --watch</kbd> command and wait for the new certificate to be provisioned. Its ready state will change to <kbd>False</kbd> immediately after the apply command, and after a short while it will go back to <kbd>True</kbd>. This means that the Cert Manager has provisioned a certificate issued by Let's Encrypt production environment!</li>
<li>Check the certificate with the following <kbd>keytool</kbd> <span><span>command:<br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong><span>keytool -printcert -sslserver $NGROK_HOST:443 | grep -E "Owner:|Issuer:"</span></strong></pre>
<p style="padding-left: 60px"><span>Expect output such as the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/5d0e1392-a7d9-469e-9b9e-6c49555ebdcb.png" style="width:41.17em;height:9.33em;" width="1517" height="344" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/5d0e1392-a7d9-469e-9b9e-6c49555ebdcb.png"></p>
<ol start="5">
<li class="mce-root">The new certificate is like the one previously issued for the hostname of the HTTP tunnel (<kbd>6cc09528.ngrok.io</kbd><span> in the preceding example</span><span>), but this time the issuer and Root CA are from the production environment. This means that the certificate should be trusted by a web browser. <br></span></li>
</ol>
<ol start="6">
<li><span>Open the </span><kbd>https://6cc09528.ngrok.io/actuator/health</kbd><span> URL (replace </span><span><kbd>6cc09528.ngrok.io</kbd></span> with the hostname of your HTTP tunnel) in a local web browser. If you use Google Chrome and click on the certificate icon (the padlock in front of the URL) you should see something like the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/152c06f8-df1e-4c52-b633-84ac22f7717e.png" style="width:44.33em;height:29.83em;" width="1167" height="784" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/152c06f8-df1e-4c52-b633-84ac22f7717e.png"><span><br></span></p>
<p style="padding-left: 60px">As seen in the preceding screenshot Chrome reports: <span class="packt_screen">This certificate is valid</span>!<span><br></span></p>
<ol start="7">
<li>Wrap up by verifying that the <kbd>test-em-all.bash</kbd><span><span> test script also works with this certificate as follows:<br></span></span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>HOST=$NGROK_HOST PORT=443 ./test-em-all.bash</strong></pre>
<p style="padding-left: 60px">Expect the usual output from the test script; check that it concludes with the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6d63d35e-1728-4d39-a779-d1f4424d0918.png" style="width:26.00em;height:3.75em;" width="991" height="145" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6d63d35e-1728-4d39-a779-d1f4424d0918.png"></p>
<p class="mce-root">You can switch back to the staging issuer by following the same procedure but also change back to the staging issuer in the ingress definition.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Cleaning up</h1>
                </header>
            
            <article>
                
<p>When you are done, clean up the resources created in Kubernetes (and optionally in Docker) using Docker Compose by running the following commands:</p>
<ol>
<li>Stop the <span><kbd>kubectl get cert --watch</kbd> command with <em>Ctrl + C.</em></span></li>
<li>Stop the HTTP tunnel with <em>Ctrl</em> <em>+</em> <em>C.</em></li>
<li class="mce-root"><span>Delete the namespace in Kubernetes with the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl delete namespace hands-on</strong></pre>
<ol start="4">
<li><span>If you tried out the production environment deployment using the <kbd>./kubernetes/scripts/deploy-prod-env.bash</kbd> script, you also need to s</span>top the resource managers that were launched as Docker containers using Docker Compose. Run the following command to stop them:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker-compose down mongodb mysql rabbitmq</strong></pre>
<p>Now that we are done automating the certificates to provision them, let's see how to verify that microservices work without Kubernetes. Let's see how this is done.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Verifying that microservices work without Kubernetes</h1>
                </header>
            
            <article>
                
<p><span>In this chapter and the previous one, we have seen how features in the Kubernetes platform, such as config maps, secrets, services, and ingress resources, can simplify the effort of developing a landscape of cooperating microservices. But it is </span><span>important to ensure that the source code of the microservices doesn't get dependent on the platform from a functional perspective. Avoiding such a lock-in makes it possible to change to another platform in the future, if required, with minimal effort. Changing the platform should not require changes in the source code but only in the configuration of the microservices. </span></p>
<p><span>Testing the microservices using Docker Compose and the <kbd>test-em-all.bash</kbd> test script will ensure that they work from a functional perspective, meaning that they will verify that the functionality in the microservice source code still works without Kubernetes. When running microservices without Kubernetes, we will lack the non-functional features that Kubernetes provides us with, for example, monitoring, scaling, and restarting containers. </span></p>
<p>When using Docker Compose, we will map the following Kubernetes features:</p>
<ul>
<li>Instead of config maps, we use volumes that map the configuration files directly from the host filesystem.</li>
<li>Instead of using secrets, we keep sensitive information such as credentials in the <kbd>.env</kbd> file.</li>
<li>Instead of an ingress, we will use the Spring Cloud Gateway.</li>
<li>Instead of services, we will map hostnames used by the clients directly to the hostnames of the containers, meaning that we will not have any service discovery in place and will not be able to scale containers.</li>
</ul>
<p>Using Docker Compose this way will result in significant disadvantages from a non-functional perspective compared to using Kubernetes. But it is acceptable, given that Docker Compose will only be used to run functional tests. </p>
<p>Let's go through the code changes in the <kbd>docker-compose*.yml</kbd><span> files before we run the tests using Docker Compose.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the source code for Docker Compose</h1>
                </header>
            
            <article>
                
<p><span>To run microservices outside Kubernetes, using Docker Compose, the following changes have been applied to the</span><span>&nbsp;</span><kbd>docker-compose*.yml</kbd> files:</p>
<ul>
<li>Removed the<span>&nbsp;</span><span>configuration</span> server definition</li>
<li>Removed the use of<span> the following </span><span>configuration</span> server <span>environment variables:</span><span>&nbsp;</span><kbd>CONFIG_SERVER_USR</kbd><span>&nbsp;</span>and<span>&nbsp;</span><kbd>CONFIG_SERVER_PWD</kbd></li>
<li>Mapped the<span>&nbsp;</span><kbd>config-repo</kbd><span> folder </span>as a volume in each container that needs to read configuration files from the configuration repository</li>
<li>Defined the<span>&nbsp;</span><kbd>SPRING_CONFIG_LOCATION</kbd><span> environment variable </span>to point to the configuration files in the <span><span>configuration repository</span></span></li>
<li>Stored sensitive information such as credentials and passwords in TLS certificates in the Docker Compose<span>&nbsp;</span><kbd>.env</kbd><span>&nbsp;</span>file</li>
<li>Defined environment variables with credentials for access to resource managers using the variables defined in the<span>&nbsp;</span><kbd>.env</kbd><span>&nbsp;</span>file</li>
</ul>
<p>For example, the configuration of the <kbd>product</kbd> microservice looks like the following in<span>&nbsp;</span><kbd>docker-compose.yml</kbd>:</p>
<pre><span>product</span>:<br>  <span>build</span>: microservices/product-service<br>  <span>image</span>: hands-on/product-service<br>  <span>environment</span>:<br>    - SPRING_PROFILES_ACTIVE=docker<br>    - SPRING_CONFIG_LOCATION=file:/config-repo/application.yml,file:/config-repo/product.yml<br>    - SPRING_RABBITMQ_USERNAME=${RABBITMQ_USR}<br>    - SPRING_RABBITMQ_PASSWORD=${RABBITMQ_PWD}<br>    - SPRING_DATA_MONGODB_AUTHENTICATION_DATABASE=admin<br>    - SPRING_DATA_MONGODB_USERNAME=${MONGODB_USR}<br>    - SPRING_DATA_MONGODB_PASSWORD=${MONGODB_PWD}<br>  <span>volumes</span>:<br>    - $PWD/config-repo:/config-repo</pre>
<p>Here is an explanation for the preceding source code:</p>
<ul>
<li>The<span>&nbsp;</span><kbd>config-repo</kbd><span> folder </span>is mapped as a volume into the container at<span>&nbsp;</span><kbd>/config-repo</kbd>.</li>
<li>The <kbd>SPRING_CONFIG_LOCATION</kbd><span> environment variable </span>tells Spring where to find the property files, in this case, the<span>&nbsp;</span><kbd>/config-repo/application.yml</kbd><span>&nbsp;</span>and<span>&nbsp;</span><kbd>/config-repo/product.yml</kbd><span> files.</span></li>
<li>Credentials for accessing RabbitMQ and MongoDB are set up as environment variables based on the content in the <kbd>.env</kbd><span> file.</span><span>&nbsp;</span></li>
</ul>
<p>The credentials referred to in the preceding source code are defined in the<span>&nbsp;</span><kbd>.env</kbd><span>&nbsp;</span>file as:</p>
<pre>RABBITMQ_USR=rabbit-user-prod<br>RABBITMQ_PWD=rabbit-pwd-prod<br>MONGODB_USR=mongodb-user-prod<br>MONGODB_PWD=mongodb-pwd-prod</pre>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing with Docker Compose</h1>
                </header>
            
            <article>
                
<p>To test with Docker<span> Compose, </span>we will use Docker Desktop (earlier named Docker for macOS) instead of Minikube. Perform the following steps:</p>
<ol>
<li class="mce-root">To direct the Docker client to use Docker Desktop instead of Minikube run the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>eval $(minikube docker-env --unset)</strong></pre>
<ol start="2">
<li>To save memory, you might want to s<span>top the Minikube instance:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>minikube stop</strong> </pre>
<ol start="3">
<li>Start Docker Desktop (if not already running).</li>
<li>Build the Docker images in Docker Desktop with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong><span>docker-compose build</span></strong></pre>
<ol start="5">
<li>Run the tests using RabbitMQ (with one partition per topic):</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>COMPOSE_FILE=docker-compose.yml ./test-em-all.bash start stop</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The tests should begin by starting all the containers, run the tests, and finally stop all the containers. Expect output like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/949cd3b9-d551-4336-a25d-a256cae5ad6e.png" style="width:44.50em;height:25.50em;" width="1912" height="1095" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/949cd3b9-d551-4336-a25d-a256cae5ad6e.png"></p>
<ol start="6">
<li>Optionally, run the tests using <span>RabbitMQ </span><span>with multiple partitions per topic:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>COMPOSE_FILE=docker-compose-partitions.yml ./test-em-all.bash start stop</strong></pre>
<p style="padding-left: 60px"><span>Expect output that's similar to the preceding test.</span></p>
<ol start="7">
<li>Alternatively, run the test using <span>Kafka</span> <span><span>with multiple partitions per topic:<br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong>COMPOSE_FILE=docker-compose-kafka.yml ./test-em-all.bash start stop</strong></pre>
<p style="padding-left: 60px"><span>Expect output that's similar to the preceding test.</span></p>
<ol start="8">
<li>Stop Docker Desktop to save memory.</li>
<li>Start the Minikube instance, if it was stopped previously, and set the default namespace to <kbd>hands-on</kbd><span>:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>minikube start</strong><br><strong>kubectl config set-context $(kubectl config current-context) --namespace=hands-on</strong></pre>
<ol start="10">
<li>Point the Docker client back to the Kubernetes cluster in the Minikube instance:</li>
</ol>
<pre style="padding-left: 60px"><strong>eval $(minikube docker-env)</strong></pre>
<p>With the successful execution of these tests, we have verified that the microservices work without Kubernetes.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how capabilities in Kubernetes can be used to simplify a microservice landscape, meaning that we reduce the number of support services to be developed and deployed together with the microservices. We have seen how Kubernetes config maps and secrets can be used to replace the Spring Cloud Config Server and how a Kubernetes ingress can replace an edge service based on Spring Cloud Gateway.</p>
<p><span>Using the Cert Manager together with Let's Encrypt allowed us to automatically provision certificates for HTTPS endpoints exposed by the ingress, eliminating the need for manual and cumbersome work. Since our Kubernetes cluster running in a local Minikube instance isn't available from the internet, we used <kbd>ngrok</kbd> to establish an HTTP tunnel from the internet to the </span><span>Minikube instance. The HTTP tunnel was </span><span>used by Let's Encrypt to verify that we are the owner of the DNS name we requested a certificate for.</span></p>
<p>To verify that the source code of the microservices can run on other platforms, that is, isn't locked into Kubernetes, we deployed the microservices using Docker Compose and ran the <kbd>test-em-all.bash</kbd><span> test script.</span></p>
<p>In the next chapter, we will be introduced to the concept of a service mesh and learn how a service mesh product, <strong>Istio</strong>, can be used to improve observability, security, resilience, and routing in a landscape of cooperating microservices that are deployed on Kubernetes.</p>
<p>Head over to the next chapter!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>How was the Spring Cloud Config Server replaced by Kubernetes resources?</li>
<li>How was the Spring Cloud Gateway replaced by Kubernetes resources?</li>
<li>What does ACME stand for and what is it used for?</li>
<li>What role does the Cert Manager and <span>Let's Encrypt play </span>in automating the provision of certificates?</li>
<li>What Kubernetes resources are involved in<span> automating the provision of certificates?</span><span>&nbsp;</span></li>
<li>Why did we use <kbd>ngrok</kbd> and what is required to be added to remove the use of ngrok?</li>
<li>Why did we run the tests using Docker Compose?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using a Service Mesh to Improve Observability and Management</h1>
                </header>
            
            <article>
                
<p>In this chapter, you will be introduced to the concept of a service mesh and see how its capabilities can be used to handle challenges in a system landscape of microservices in areas including security, policy enforcement, resilience, and traffic management. A service mesh can also be used to provide observability, that is, the capability to visualize how traffic flows between microservices in a service mesh. </p>
<p><span>A service mesh overlaps partly with the capabilities of Spring Cloud and Kubernetes we learned about earlier in this book. But most of the functionality in a service mesh complements Spring Cloud and Kubernetes, as we will see in this chapter.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>An introduction to the service mesh concept and Istio, a popular open source implementation </li>
<li>You will also learn how to do the following:
<ul>
<li>Deploy Istio in Kubernetes</li>
<li>Create a service mesh</li>
<li>Observe a service mesh</li>
<li>Secure a service mesh</li>
<li>Ensure that a service mesh is resilient</li>
<li>Perform zero downtime deployments using a service mesh</li>
<li>Test the microservice landscape using Docker Compose to ensure that the source code in the microservices is not locked into either Kubernetes or Istio</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All commands described in this book<span> are run on a MacBook Pro using macOS Mojave, but modifying these commands should be sufficiently straightforward to run them on another platform such as Linux or Windows.</span></p>
<p>The only new tool required for this chapter is Istio's command-line tool, <kbd>istioctl</kbd><span>. This can be installed </span>using Homebrew with the following command:</p>
<div>
<pre><strong>brew install istioctl</strong></pre></div>
<p>The source code for this chapter can be found <span>on GitHub at</span><span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter18">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter18</a>.</p>
<p>To be able to run the commands as described in the book, you need to download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Examples of sample commands include the following:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter18</span></strong></pre>
<p>The Java source code is written for Java 8 and tested on Java 12. This chapter uses Spring Cloud 2.1, SR2 (also known as the <strong>Greenwich</strong> release), Spring Boot 2.1.6, and Spring 5.1.8, that is, the latest available version of the Spring components at the time of writing this chapter. The source code has been tested using Kubernetes V1.15.</p>
<p><span>All source code examples in this chapter come from the source code in </span><span><kbd>$BOOK_HOME/Chapter18</kbd>, but are, in several cases, edited to remove non-relevant parts of the source code, such as comments, and import and log statements.</span></p>
<p>If you want to see the changes applied to the source code in <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml" target="_blank">Chapter 18</a>,&nbsp;<em>Using a Service Mesh to Improve Observability and Management</em>, that is, the changes required to create a service mesh using Istio<span>,&nbsp;</span>you can compare it with the source code for <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 17</a>, <em>Implementing Kubernetes Features as an Alternative</em>.&nbsp;<span>You can use your favorite diff tool and compare the two folders, <kbd>$BOOK_HOME/Chapter17</kbd> and <kbd>$BOOK_HOME/Chapter18</kbd>.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to service mesh using Istio</h1>
                </header>
            
            <article>
                
<p>A service mesh is an infrastructure layer that controls and observes the communication between services, for example, microservices. The capabilities in a service mesh, for example, observability, security<span>,&nbsp;</span>policy enforcement<span>,&nbsp;</span>resilience,<span> and </span>traffic management,<em>&nbsp;</em>are implemented by controlling and monitoring all internal communication inside the service mesh, that is, between the microservices in the service mesh. One of the core components in a service mesh is a lightweight<span>&nbsp;</span><strong>proxy<span>&nbsp;</span></strong>component that is injected into all microservices that will be part of the service mesh. All traffic in and out of a microservice is configured to go through its proxy component. The proxy components are configured in runtime by a<span>&nbsp;</span><strong>control plane</strong><span>&nbsp;</span>in the service mesh using API's exposed by the proxy. The control plane also collects telemetry data through these APIs from the proxies to <span>visualize how the traffic flows in the service mesh. </span></p>
<p>A service mesh also contains a<strong><span>&nbsp;</span>data plane</strong>, consisting of the proxy components in all microservices in the service mesh together with separate components for handling external incoming and outgoing traffic to and from the service mesh. This is illustrated by the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6f4234c5-40c7-405d-856e-9e11009c242d.png" width="1881" height="775" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6f4234c5-40c7-405d-856e-9e11009c242d.png"></p>
<p class="mce-root">The first publicly available implementation of a service mesh was the open source project<span>&nbsp;</span>Linkerd,<span>&nbsp;</span>managed by Buoyant (<a href="https://linkerd.io">https://linkerd.io</a>), <span>having its origins in</span> Twitter's Finagle <span>project </span>(<a href="http://twitter.github.io/finagle">http://twitter.github.io/finagle</a>). It was launched during 2016 and, one year later, in 2017, IBM, Google, and Lyft launched the open source project,<span>&nbsp;</span>Istio<span>&nbsp;</span>(<a href="https://istio.io">https://istio.io</a>).</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root">One of the core components in Istio, the proxy component, is based on Lyft's<span>&nbsp;</span>Envoy proxy<span>&nbsp;</span>(<a href="https://www.envoyproxy.io">https://www.envoyproxy.io</a>). Linkerd and Istio are, at the time of writing this chapter, the two most popular and widely used service mesh implementations. In this chapter, we will use Istio. </p>
<p>Istio can be deployed<span> in various environments, including Kubernet</span><span>es (see <a href="https://istio.io/docs/setup">https://istio.io/docs/setup</a>). </span>When deploying Istio on Kubernetes, its runtime components are deployed into a separate Kubernetes namespace, <kbd>istio-system</kbd>. Istio also comes with a set of Kubernetes <strong>Custom Resources Definitions</strong>&nbsp;(<strong>CRDs</strong>). CRDs are used in Kubernetes to extend its API, that is, to add new objects to its API. The Istio objects <span>added </span>are used to configure how Istio will be used. Finally, Istio comes with a CLI tool, <kbd>istioctl</kbd>, which will be<span> used to inject Istio proxies</span><span> into the microservices that participate in the service mesh. </span></p>
<p><span>Istio is, as explained previously, divided into a control plane and a data plane. </span>As an operator, we will define a desired state by creating <span>Istio objects in the Kubernetes API server, for example, declaring routing rules. The control plane will read these objects and send commands to the proxies in the data plane to take actions according to the desired state, for example, configuring routing rules. The proxies handle the actual communication between the microservices and report back telemetry data to the control plane. The telemetry data is used by various components in a control plane to visualize what's going on in the service mesh.</span></p>
<p>In the following subsections, we will cover the following topics:</p>
<ul>
<li>How Istio proxies are injected into microservices</li>
<li>The Istio API objects that we will use in this chapter</li>
<li>The runtime components in Istio that constitute the control plane and the data plane</li>
<li>Changes in the microservice landscape as a result of the introduction of Istio</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Injecting Istio proxies into existing microservices</h1>
                </header>
            
            <article>
                
<p>The microservices we have deployed in Kubernetes in the previous chapters run as a single container in a Kubernetes pod (refer to the <em>Introducing Kubernetes API objects</em> section in<span>&nbsp;</span><a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a><span>,&nbsp;</span><em>Introduction to Kubernetes</em><span>,</span> for a recap). To make a microservice join an Istio-based service mesh, <span>an Istio proxy is injected into</span> each microservice. This is done by adding an extra container to the pod that runs the Istio proxy.</p>
<div class="packt_infobox">Containers added to a pod with the aim of supporting the main container, such as an Istio proxy, are referred to as a <em>sideca</em><em>r</em>.</div>
<p>The following diagram shows how an Istio proxy has been injected into a sample pod, <strong>Pod A</strong>, as a sidecar:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a824d18b-8f01-4c08-97ef-d697d7154fc8.png" style="width:18.83em;height:20.50em;" width="762" height="828" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a824d18b-8f01-4c08-97ef-d697d7154fc8.png"></p>
<p><span>The main container in the pod, </span><strong>Container A</strong><span>, is configured to route all its traffic through the Istio proxy.</span></p>
<p><span>Istio proxies can be injected either automatically when a deployment object is created or manually using the </span><span><kbd>istioctl</kbd> tool.</span></p>
<p>In this chapter, we will inject the Istio proxies manually. The reason for this is that Istio proxies do not support the protocols used by MySQL, MongoDB, and RabbitMQ, so we will only inject Istio proxies into pods where the HTTP protocol is used. An <span>Istio proxy can be injected into the pods of an existing deployment object by means of the following command:</span></p>
<pre><strong><span>kubectl get deployment sample-deployment -o yaml | istioctl kube-inject -f - | kubectl apply -f -</span></strong></pre>
<p class="mce-root"></p>
<p>This command may, at first glance, appear somewhat daunting, but it is actually just three separate commands. The previous command sends its output to the next command using pipes, that is, the <span><kbd>|</kbd> character. Let's go through each command:</span></p>
<ol>
<li>The <kbd>kubectl get deployment</kbd> command gets the current definition of a deployment named <kbd>sample-deployment</kbd> from the Kubernetes API server and returns its definition in the YAML format.</li>
<li>The <kbd>istioctl kube-inject</kbd> command reads the definition from the <kbd>kubectl get deployment</kbd><span> command </span>and adds an extra container for an Istio proxy in pods that the deployment handles. The configuration for the existing container in the deployment object is updated so that incoming and outgoing traffic goes through the Istio proxy.<br>
The <span><kbd>istioctl</kbd> command returns the new definition of the deployment object, including a container for the Istio proxy.</span></li>
<li>The <kbd>kubectl apply</kbd> command reads the updated configuration from the <span><kbd>istioctl kube-inject</kbd> command </span>and applies the updated configuration. A rolling upgrade of the pods belonging to the deployment will start up in the same way as we have seen before (refer to the <em>Performing a rolling upgrade</em><span> section in </span><a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a><span>,&nbsp;</span><em>Deploying Our Microservices to Kubernetes)</em>.</li>
</ol>
<p>The deployment scripts in the <kbd>kubernetes/scripts</kbd>&nbsp;<span>folder </span>have been extended to use <kbd><span>istioctl</span></kbd> to inject the Istio proxies. Refer to the upcoming <em>Creating the service mesh</em>&nbsp;<span>section </span>for details.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing Istio API objects</h1>
                </header>
            
            <article>
                
<p>Istio extends the Kubernetes API with a number of objects using its CRDs. Refer to the <em>Introducing Kubernetes API objects</em> section in <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a><span>,&nbsp;</span><em>Introduction to Kubernetes</em><span>,&nbsp;</span>for a recap of the Kubernetes API. In this chapter we will use the following Istio objects:</p>
<ul>
<li><kbd>Gateway</kbd> is used to configure how to handle incoming traffic to, and outgoing traffic from, the service mesh. A gateway depends on a virtual service routing the incoming traffic to Kubernetes services. We will use a gateway object to accept incoming traffic to the DNS name, <kbd>minikube.me</kbd>, using HTTPS. Refer to the <em>Kubernetes Ingress resource replaced with Istio Ingress Gateway as an edge server</em> section for details.</li>
<li><kbd>VirtualService</kbd> is used to define routing rules in the service mesh. We will use v<span>irtual services t</span>o describe how to route incoming traffic from an Istio gateway to the Kubernetes services and between services. We will also use <span>v</span><span>irtual services t</span>o inject faults and delays in order to test the reliability and resilience capabilities of the service mesh.</li>
<li><kbd>DestinationRule</kbd> is used to define policies and rules for traffic that is routed (using a virtual service) to a specific service (that is, a destination). We will use destination rules to set up encryption policies to encrypt internal HTTP traffic and define service subsets that describe available versions of the services. We will use service subsets when performing zero downtime (blue/green) deployments from an existing version of a microservice to a new version.</li>
<li><kbd>Policy</kbd> is used to define how requests will be authenticated. We will use policies to require incoming requests to the service mesh to be authenticated using a JWT-based OAuth 2.0/OIDC access token. Refer to the <em>Authenticating external requests using OAuth 2.0/OIDC access tokens</em> section of this chapter. A policy can also be used to define how to secure parts of the internal communication in the service mesh. For example, a policy can require that internal requests are encrypted using HTTPS or allow plain text requests. Finally, a <kbd>MeshPolicy</kbd> object can be used to define global policies that apply to the whole service mesh.</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing runtime components in Istio </h1>
                </header>
            
            <article>
                
<p>Istio contains a number of runtime components, is highly configurable in terms of what components to use, and provides fine-grained control over the configuration of each component. Refer to the <em>Deploying Istio in a Kubernetes cluster</em> section of this chapter for information on the configuration we will use in this chapter.</p>
<p><span>In the configuration used in this chapter, the Istio control plane consists of the following r</span>untime components:</p>
<ul>
<li><strong>Pilot</strong>:<strong>&nbsp;</strong><span>responsible for supplying all sidecars with updates of the service mesh configuration.</span></li>
<li><strong>Mixer</strong>: consists of two different runtime components:
<ul>
<li><strong>Policy</strong>&nbsp;â&nbsp;<span>enforces network policies such as authentication, authorization, rate limits, and quotas.</span></li>
<li><strong>Telemetry</strong><em><strong>&nbsp;</strong>â<strong>&nbsp;</strong></em>collects<span><span> telemetry information and sends it to Prometheus, for example. </span></span></li>
</ul>
</li>
<li><strong>Galley</strong>:<em><strong>&nbsp;</strong></em>responsible for collecting and validating configuration information and distribution to the other Istio components in the control plane.</li>
<li><strong>Citadel</strong>: responsible for issuing and rotating internally used certificates.</li>
<li><span><strong>Kiali</strong></span><span>: provides observability to the service mesh, visualizing what is going on in the mesh. Kiali is a separate open source project (see <a href="https://www.kiali.io">https://www.kiali.io</a>)</span></li>
<li><span><strong>Prometheus</strong></span>: performs data ingestion and storage for time series-based data, for example, performance metrics.<br>
Prometheus is a separate open source project (refer to <a href="https://prometheus.io">https://prometheus.io</a>).</li>
<li><span><strong>Grafana</strong></span>:&nbsp;<span>visualizes performance metrics and other time series-related data collected in Prometheus. Grafana is a separate open source project (see <a href="https://grafana.com">https://grafana.com</a>).</span></li>
<li><span><strong>Tracing</strong></span>:&nbsp;<span>handles and visualizes distributed tracing information. Based on Jaeger, it is an open source project for distributed tracing (refer to <a href="https://www.jaegertracing.io">https://www.jaegertracing.io</a>). Jaeger provides the same type of functionality as </span><span>Zipkin, which we used in</span> <a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml">Chapter 14</a><span>, <em>Understanding Distributed Tracing</em>.</span></li>
</ul>
<p>Kiali is accessed using a web browser and integrates Grafana for viewing <span>performance metrics and Jaeger for visualizing distributed tracing information.</span></p>
<p><span>The Istio data plane consists of the following r</span><span>untime components:</span></p>
<ul>
<li><strong>Ingress</strong> <strong>Gateway</strong>: handles incoming traffic to the service mesh</li>
<li><strong>E</strong><strong>gress</strong> <strong>Gateway</strong>: handles outgoing traffic from the service mesh</li>
<li>All pods with an Istio proxy are injected as a sidecar</li>
</ul>
<p>The runtime components in Istio's control plane and data plane are summarized in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bc1d5d4f-c2d3-4504-8d01-506c8fec94c4.png" style="width:34.08em;height:18.00em;" width="3616" height="1905" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/bc1d5d4f-c2d3-4504-8d01-506c8fec94c4.png"></p>
<p>In the next section, we will go through changes applied to the <span>mic</span><span>roservice landscape arising from the introduction of Istio.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in the microservice landscape </h1>
                </header>
            
            <article>
                
<p>As we have seen in the preceding section, Istio comes with components that overlap with components currently used in the microservice landscape <span>in terms of functionality</span>:</p>
<ul>
<li><span>The Istio Ingress Gateway can act as an edge server, an alternative to the </span>Kubernetes Ingress resources.</li>
<li>The Jaeger <span>component </span><span>that comes bundled with</span> Istio can be used for distributed tracing instead of Zipkin.</li>
</ul>
<p>In the following two subsections, we will learn why and how <span>Kubernetes Ingress resources are replaced with Istio Ingress Gateway, and how and why Zipkin is replaced with Jaeger.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Kubernetes Ingress resources are replaced with Istio Ingress Gateway as an edge server</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we introduced Kubernetes Ingress resources as edge servers (refer to the <em>Replacing the Spring Cloud Gateway</em> section in <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 17</a><span>,&nbsp;</span><em>Implementing Kubernetes Features as an Alternative)</em>. Unfortunately, ingress resources cannot be configured to handle the fine-grained routing rules that come with Istio. Instead, Istio has its own edge server, the Istio ingress Gateway, introduced previously in the <em>Introducing runtime components in Istio</em> section. The Istio Ingress Gateway is used by creating <kbd>Gateway</kbd> and <kbd>VisualService</kbd> resources described previously in the <em>Introducing Istio API objects</em> <span>section</span>.</p>
<p>The definition files for the following <span>Kubernetes Ingre</span><span>ss resources</span><span>,&nbsp;<kbd>kubernetes/services/base/ingress-edge-server.yml</kbd> and <kbd>kubernetes/services/base/ingress-edge-server-ngrok.yml</kbd>, have therefore been removed. Definition files for Istio <kbd>Gateway</kbd> and <kbd>VirtualService</kbd> resources will be added in the</span> <em>Creating the service mesh</em><span> section.</span></p>
<p>The Istio Ingress Gateway is reached using a different IP address than the IP address used to access Kubernetes Ingress resources, so we also need to update the IP address mapped to the hostname, <kbd>minikube.me</kbd>, which we use when running tests. This is handled in the <em>Setting up access to Istio services</em> section in this chapter.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Simplifying the system landscape and replacing Zipkin with Jaeger</h1>
                </header>
            
            <article>
                
<p>As mentioned in the <span><em>Introducing runtime components in Istio</em> section, Istio comes with built-in support for distributed tracing using Jaeger<em>.</em> Using Jaeger, we can offload and simplify the microservice landscape by removing the Zipkin server we introduced in <a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml">Chapter 14</a>, <em>Understanding Distributed Tracing</em>.</span></p>
<p>The following changes have been applied to the source code to remove the Zipkin server:</p>
<ul>
<li>The dependency to <span><kbd>org.springframework.cloud:spring-cloud-starter-zipkin</kbd> in all microservice build files, <kbd>build.gradle</kbd>, has been removed.</span></li>
<li>The definition of the Zipkin server in the three Docker Compose files, <span><kbd>docker-compose.yml</kbd>,&nbsp;<kbd>docker-compose-partitions.yml</kbd>, and </span><kbd>docker-compose-kafka.yml</kbd><span>, has been removed.</span></li>
<li>The following Kubernetes definition files for Zipkin have been removed:
<ul>
<li><kbd><span>kubernetes/services/base/zipkin-server.yml</span></kbd></li>
<li><kbd><span>kubernetes/services/overlays/prod/zipkin-server-prod.yml</span></kbd></li>
</ul>
</li>
</ul>
<p>Jaeger will be installed in the <em>Creating the service mesh</em> section.</p>
<p>The changes were made to the <span>microservice land</span><span>scape due to the introduction of Istio</span>. We are now ready to deploy Istio in the Kubernetes cluster.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying Istio in a Kubernetes cluster</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to deploy Istio in a Kubernetes cluster and how to access Istio services in it.</p>
<p>We will use v1.2.4 of Istio, the latest release available when this chapter was written.</p>
<p>We will be using a demo configuration of Istio that is suitable for testing Istio in a development environment, that is, with most features enabled but configured for minimalistic resource usage.</p>
<div class="packt_infobox">This configuration is unsuitable for production usage and for performance testing.</div>
<p>For other installation options, see <a href="https://istio.io/docs/setup/kubernetes/install">https://istio.io/docs/setup/kubernetes/install</a>.</p>
<p><span>To deploy Istio, perform the following steps:</span></p>
<ol>
<li class="mce-root"><span>Download Istio as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>cd $BOOK_HOME/Chapter18<br><span class="token function">curl</span> -L https://git.io/getLatestIstio <span class="token operator">|</span> ISTIO_VERSION<span class="token operator">=</span>1.2.4 sh -</strong></pre>
<ol start="2">
<li class="mce-root"><span>Ensure that your Minikube instance is up-and-running with the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>minikube status</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>Expect a response along the lines of the following, provided it is up and running:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b73dc4d8-a721-40eb-87be-a3395fc8c84f.png" style="width:40.00em;height:8.83em;" width="742" height="164" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b73dc4d8-a721-40eb-87be-a3395fc8c84f.png"></p>
<ol start="3">
<li>Install Istio-specific custom resource definitions (CRDs) in Kubernetes:</li>
</ol>
<pre style="padding-left: 60px"><strong>for i in istio-1.2.4/install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done</strong></pre>
<ol start="4">
<li><span><span>Install Istio demo configurations in Kubernetes as follows:<br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -f istio-1.2.4/install/kubernetes/istio-demo.yaml</strong></pre>
<ol start="5">
<li><span><span><span>Wait for the Istio deployments to become available:<br></span></span></span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>kubectl -n istio-system wait --timeout=600s --for=condition=available deployment --all</strong></pre>
<p style="padding-left: 60px">The command will report deployment resources in Istio as available, one after another. Expect 12 messages such as <kbd>deployment.extensions/NNN condition met</kbd><span> before the command ends. I</span>t can take a couple of minutes (or more) depending on your hardware and internet connectivity.</p>
<ol start="6">
<li>Update Kiali's config map with URLs to Jaeger and Grafana with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl -n istio-system apply -f kubernetes/istio/setup/kiali-configmap.yml &amp;&amp; \</strong><br><strong>kubectl -n istio-system delete pod -l app=kiali &amp;&amp; \</strong><br><strong>kubectl -n istio-system wait --timeout=60s --for=condition=ready pod -l app=kiali</strong></pre>
<p>The config map, <kbd>kubernetes/istio/setup/kiali-configmap.yml</kbd>, contains URLs to <span>Jaeger and Grafana that utilize the DNS names set up by the <kbd>minikube tunnel</kbd> command used in the next section.</span></p>
<p>Istio is now deployed in Kubernetes, but before we move on and create the service mesh, we need to learn a bit about how to access Istio services in a Minikube environment.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up access to Istio services</h1>
                </header>
            
            <article>
                
<p>The <span>demo configuration used in the previous section to install </span>Istio comes with a few connectivity-related issues that we need to resolve. The Istio Ingress Gateway is configured as a load-balanced Kubernetes service; that is, its type is <kbd>LoadBalancer</kbd>.</p>
<p><span>It can also be reached using its </span>node port<span>, in the port range </span><kbd>30000</kbd><span>-</span><kbd>32767</kbd><span>, on the IP address of the Minikube instance. Unfortunately, HTTPS-based routing in Istio can't include port numbers; that is, Istio's Ingress Gateway must be reached </span>over the default port for HTTPS (<kbd>443</kbd>). Therefore, a node port can't be used. Instead, a load balancer must be used to be able to use Istio's routing rules with HTTPS.</p>
<p><span>Minikube contains a command that can be used to simulate a local load balancer,</span> <kbd>minikube tunnel</kbd><span>. This command assigns an external IP address to each load-balanced Kubernetes service, including the</span><span>&nbsp;</span><span>Istio Ingress Gateway.</span><span> This gives you what we need to update the translation of the hostname</span> <kbd>minikube.me</kbd><span>, which we use in our tests. The hostname, <kbd>minikube.me</kbd>, now needs to be </span>translated<span> to the external IP address of the </span><span>Istio Ingress Gateway, instead of to the IP address of the Minikube instance that we used in the previous chapters.</span></p>
<p>The <kbd>minikube tunnel</kbd> command also makes cluster-local <span>Kubernetes services accessible using their DNS name. The DNS name is based on the naming convention: <kbd>{service-name}.{namespace}.svc.cluster.local</kbd>. For example, Istio's Kiali service can be reached from a local web browser using the DNS name, <kbd>kiali.istio-system.svc.cluster.local</kbd>, when the tunnel is up and running. </span></p>
<p>The following diagram summarizes how Istio services are accessed:  </p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6df8153e-3139-4cc7-9cb2-0f832f9e5129.png" width="1902" height="794" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6df8153e-3139-4cc7-9cb2-0f832f9e5129.png"></p>
<p>Perform the following steps to set up the Minikube tunnel:</p>
<ol>
<li class="mce-root"><span>Make Kubernetes services available locally. Run the following command in a separate terminal window (the command locks the terminal window when the tunnel is up and running):</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>minikube tunnel</strong></pre>
<p style="color: black;padding-left: 60px">Note that this command requires that your user has <kbd>sudo</kbd> privileges and that you enter your password during startup and shutdown. It takes a couple of seconds before the command asks for the password, so it is easy to miss!</p>
<ol start="2">
<li>Config <kbd>minikube.me</kbd> to be resolved to the IP address of the <span>Istio Ingress</span> <span>Gateway as follows:</span>
<ol start="1">
<li class="mce-root"><span>Get the IP address exposed by the</span> <kbd>minikube tunnel</kbd> <span>command for the </span><span>Istio Ingress Gateway and save it in an environment variable named <kbd>INGRESS_HOST</kbd>:</span></li>
</ol>
</li>
</ol>
<pre style="color: black;padding-left: 120px" class="mce-root"><strong>INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')</strong></pre>
<ul>
<li style="list-style-type: none">
<ol start="2">
<li>Update <kbd>/etc/hosts</kbd> so that <kbd>minikube.me</kbd> points to the Istio Ingress Gateway:</li>
</ol>
</li>
</ul>
<pre style="padding-left: 120px" class="mce-root"><strong>echo "$INGRESS_HOST minikube.me" | sudo tee -a /etc/hosts</strong></pre>
<ul>
<li style="list-style-type: none">
<ol start="3">
<li>Remove the line in <kbd>/etc/hosts</kbd> where <kbd>minikube.me</kbd> that points to the IP address of the Minikube instance (<kbd>minikube ip</kbd>). Verify that <kbd>/etc/hosts</kbd> only contains one line that translates <kbd>minikube.me</kbd> and that it points to the IP address of the Istio Ingress Gateway; for example, the value of <kbd>$INGRESS_HOST</kbd>:</li>
</ol>
</li>
</ul>
<p style="padding-left: 30px" class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/466a71aa-3959-4801-8017-ce9c50266daf.png" style="width:15.17em;height:7.08em;" width="508" height="237" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/466a71aa-3959-4801-8017-ce9c50266daf.png"></p>
<ol start="3">
<li><span>Verify that Kiali, Jaeger, and Grafana can be reached through the tunnel with the following commands:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>curl -o /dev/null -s -L -w "%{http_code}" http://kiali.istio-system.svc.cluster.local:20001/kiali/</strong><br><strong>curl -o /dev/null -s -L -w "%{http_code}" http://grafana.istio-system.svc.cluster.local:3000</strong><br><strong>curl -o /dev/null -s -L -w "%{http_code}" http://jaeger-query.istio-system.svc.cluster.local:16686</strong></pre>
<p>Each command should return <kbd>200</kbd> (OK).</p>
<div class="packt_infobox">The <kbd>minikube tunnel</kbd> command can stop running if, for example, your computer or the Minikube instance in the virtual machine is <span>paused or restarted. The command </span>needs to be restarted manually in these cases. So, if you fail to call API's on the <kbd>https://minikube.me</kbd>&nbsp;<span>URL </span>or i<span>f Kiali's web UI can't reach Jaeger to visualize distributed tracing, or Grafana to visualize performance metrics</span>, always check whether the Minikube tunnel is running and restart it if required.</div>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">An added bonus from using the minikube tunnel command</h1>
                </header>
            
            <article>
                
<p>Running the <kbd>minikube tunnel</kbd> command also makes it possible to access some other cluster-internal Kubernetes services that may be of interest. Once the environment is up and running as described in the <em>Running commands to create the service mesh</em>&nbsp;<span>section, </span>the following can be achieved: </p>
<ul>
<li>The <kbd>health</kbd> endpoint of the <kbd>product-composite</kbd> microservice can be checked with the following command:</li>
</ul>
<pre style="padding-left: 60px"><strong>curl -k http://product-composite.hands-on.svc.cluster.local:4004/actuator/health</strong></pre>
<p style="padding-left: 60px">Refer to the <em>Observing the service mesh</em><span> section for an explanation of the use of port <kbd>4004</kbd>.</span></p>
<ul>
<li>MySQL tables in the review database can be accessed with the following command:</li>
</ul>
<pre style="padding-left: 60px"><strong>mysql -umysql-user-dev -pmysql-pwd-dev review-db -e "select * from reviews" -h mysql.hands-on.svc.cluster.local</strong></pre>
<ul>
<li>MongoDB collections in the <kbd>product</kbd> and <kbd>recommendations</kbd> databases can <span>be accessed with the following commands:</span></li>
</ul>
<pre style="padding-left: 60px"><strong>mongo --host mongodb.hands-on.svc.cluster.local -u mongodb-user-dev</strong> <strong>-p mongodb-pwd-dev --authenticationDatabase admin product-db --eval "db.products.find()"</strong><br><br><strong>mongo --host mongodb.hands-on.svc.cluster.local -u mongodb-user-dev -p mongodb-pwd-dev --authenticationDatabase admin recommendation-db --eval "db.recommendations.find()"</strong></pre>
<ul>
<li>RabbitMQ's web UI can be accessed using the following URL: <kbd>http://rabbitmq.hands-on.svc.cluster.local:15672</kbd>. Log in using the credentials <kbd>rabbit-user-dev</kbd> and <span><kbd>rabbit-pwd-dev</kbd>.</span></li>
</ul>
<p>With the Minikube tunnel in place, we are now ready to create the service mesh.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Creating the service mesh</h1>
                </header>
            
            <article>
                
<p>With Istio deployed, we are ready to create the service mesh. We will use the <kbd>kubernetes/scripts/deploy-dev-env.bash</kbd><span> script to set up an environment for </span><span>development and testing.</span></p>
<p><span>The steps required to create the service mesh are basically the same as those we used in <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 17</a>, <em>Implementing Kubernetes Features as an Alternative</em> (refer to the <em>Testing with Kubernetes ConfigMaps, secrets, and ingress</em> section). Let's first see what additions have been made to the Kubernetes definition files to set up the service mesh before we run the commands to create the service mesh.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Source code changes</h1>
                </header>
            
            <article>
                
<p><span>To be able to run the microservices in a service mesh managed by Istio, the following changes have been applied to the Kubernetes definition files:</span></p>
<ul>
<li>The deployment scripts have been updated to inject Istio proxies</li>
<li>The file structure of the Kubernetes definition files has been changed</li>
<li>Kubernetes definition files for Istio have been added</li>
</ul>
<p>Let's go through them one by one.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Updating the deployment scripts to inject Istio proxies</h1>
                </header>
            
            <article>
                
<p><span>The scripts used to deploy microservices in Kubernetes, </span><span><kbd>deploy-dev-env.bash</kbd></span><span> and </span><span><kbd>deploy-prod-env.bash</kbd>, both in the <kbd>kubernetes/scripts</kbd> folder, have been updated to inject Istio proxies into the five microservices, that is, the <kbd>auth-server</kbd>, <kbd>product-composite</kbd>, <kbd>product</kbd></span>,&nbsp;<span><kbd>recommendation</kbd></span>,<span> and <kbd>review</kbd> services.</span></p>
<div class="packt_infobox">
<p><span>The</span> <kbd>deploy-prod-env.bash</kbd><span> script will be used in the <em>Performing zero downtime deploys </em>section<em>.&nbsp;</em></span></p>
</div>
<p>The <kbd>istioctl kube-inject</kbd> command previously described in the <em>Injecting Istio proxies in existing microservices</em> <span>section </span>has been added to both deployment scripts as follows:</p>
<div>
<pre><strong><span>kubectl get deployment auth-server product product-composite recommendation review -o yaml | istioctl kube-inject -f - | kubectl apply -f -</span></strong></pre>
<p><span>Since the <kbd>kubectl apply</kbd> command will start a rolling upgrade, the following command has been added to wait for the upgrade to be completed:</span></p>
<pre><strong><span>waitForPods 5 'version=&lt;version&gt;'</span></strong></pre>
<p><span>During the rolling upgrade, we will have two pods running for each microservice: an old one without an Istio proxy and a new one with the Istio proxy injected. The</span> <span><kbd>waitForPods</kbd> function will wait until the old pods are terminated; that is, the rolling upgrade is complete, and only the five new pods are running. To identify what pods to wait for, a label named <kbd>version</kbd> is used. In a development environment, all microservice pods are labeled with <kbd>version=latest</kbd>.</span></p>
<p>For example, the deployment file for the product microservice, <span><kbd>kubernetes/services/base/deployments/product-deployment.yml</kbd>,&nbsp;</span>has the following definition of the<span>&nbsp;</span><kbd>version</kbd> label:</p>
<pre>metadata:<br>  labels:<br>    version: latest</pre></div>
<div class="packt_infobox"><span>In the</span> <em><span>Performing zero downtime deployments</span></em><span> section, where we will upgrade microservices from version <kbd>v1</kbd> to <kbd>v2</kbd>, the version label will be set to <kbd>v1</kbd> and <kbd>v2</kbd>.</span></div>
<p><span>Finally, the following command has been added to the scripts to make them wait until the deployments and their pods are ready: </span></p>
<pre><strong><span>kubectl wait --timeout=120s --for=condition=Ready pod --all</span></strong></pre>
<p><span>After reviewing the updates in the deployment scripts, let's see how the file structure of the Kubernetes definition files has been affected as a result of the introduction of Istio.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changing the file structure of the Kubernetes definition files</h1>
                </header>
            
            <article>
                
<p>The file structure for the Kubernetes definition files in <span><kbd>kubernetes/services</kbd> has been expanded a bit since <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em> (refer to the <em>Introduction to Kustomize</em> section) and this now appears as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cc35c1d8-4834-4161-9759-347150bb845d.png" style="width:15.00em;height:17.58em;" width="497" height="582" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cc35c1d8-4834-4161-9759-347150bb845d.png"></p>
<p>The<span>&nbsp;</span><kbd>base</kbd><span>&nbsp;</span>folder consists of three subfolders. The reason for this is that we will <span>run two versions of the microservices concurrently in the <em>Performing zero downtime deploys</em> section, that is, one pod per microservice version. </span>Since pods are managed by the deployment object, we also need two deployment objects per microservice. To be able to achieve this, the base version of the deployment objects has been placed in a separate folder,<span>&nbsp;</span><kbd>deployments</kbd>. The service objects and the Istio definitions are placed in their own base folders:<span>&nbsp;</span><kbd>services</kbd><span>&nbsp;</span>and<span>&nbsp;</span><kbd>istio</kbd><span>,&nbsp;</span>respectively.</p>
<p>In the development environment, we will only run one version per microservice. Its kustomization file, <span><kbd>kubernetes/services/overlays/dev/kustomization.yml</kbd>, has been updated to include all three folders as base folders:</span></p>
<pre>bases:<br>- ../../base/deployments<br>- ../../base/services<br>- ../../base/istio</pre>
<p>Refer to the following <span><em>Performing zero downtime deploys</em> section for details on how two concurrent versions of the microservices will be deployed using the setup for the production environment.</span></p>
<p><span>For now, let's also go through the new files in the Istio folder.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Adding Kubernetes definition files for Istio</h1>
                </header>
            
            <article>
                
<p>Istio definitions have been added to the <kbd>istio</kbd> folder. The Istio files of interest in this section are the gateway definition and its corresponding virtual services. The other Istio files will be explained in the <em>Authenticating external requests using OAuth 2.0/OIDC access tokens</em> and <em>Protecting internal communication using mutual authentication (mTLS)</em>&nbsp;<span>sections.</span></p>
<p>The Istio gateway is declared in the <kbd>kubernetes/services/base/istio/gateway.yml</kbd>&nbsp;<span>file </span>and appears as follows:</p>
<pre>apiVersion: networking.istio.io/v1alpha3<br>kind: Gateway<br>metadata:<br>  name: hands-on-gw<br>spec:<br>  selector:<br>    istio: ingressgateway<br>  servers:<br>  - hosts:<br>    - "minikube.me"<br>    port:<br>      number: 443<br>      name: https<br>      protocol: HTTPS<br>    tls:<br>      mode: SIMPLE<br>      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt<br>      privateKey: /etc/istio/ingressgateway-certs/tls.key</pre>
<p>The following are some explanations of the preceding source code:</p>
<ul>
<li>The gateway is named <kbd>hands-on-gw</kbd>; this name is used by the virtual services underneath.</li>
<li>The <kbd>selector</kbd> field specifies that the gateway resource will be handled by the built-in Istio Ingress Gateway.</li>
<li>The <kbd>hosts</kbd> and <kbd>port</kbd> fields specify that the gateway will handle incoming requests for the <kbd>minikube.me</kbd> <span>hostname </span>using HTTPS over port <kbd>443</kbd>.</li>
<li>The <kbd>tls</kbd> field specifies where the <span><span>Istio Ingress Gateway can find the certificate and private key used for HTTPS communication. Refer to the <em>Protecting external endpoints with HTTPS and certificates</em> section</span></span><span> for details on how these certificate files are created.</span></li>
</ul>
<p>The virtual service object for routing requests from the gateway to the <kbd>product-composite</kbd> service, <kbd><span>kubernetes/services/base/istio/</span>product-composite-virtual-service.yml</kbd>, appears as follows:</p>
<pre>apiVersion: networking.istio.io/v1alpha3<br>kind: VirtualService<br>metadata:<br>  name: product-composite-vs<br>spec:<br>  hosts:<br>  - "minikube.me"<br>  gateways:<br>  - hands-on-gw<br>  http:<br>  - match:<br>    - uri:<br>        prefix: /product-composite<br>    route:<br>    - destination:<br>        port:<br>          number: 80<br>        host: product-composite</pre>
<p>Explanations for the preceding source code are as follows:</p>
<ul>
<li>The <kbd>hosts</kbd> field specifies that the virtual service will route requests sent to the host, <kbd>minikube.me</kbd>.&nbsp;</li>
<li>The <kbd>match</kbd> and <kbd>route</kbd> blocks specify that requests that contain a URI starting with <kbd>/product-composite</kbd> will be forwarded to the Kubernetes service named <kbd>product-composite</kbd>.</li>
</ul>
<div class="packt_infobox">In the preceding source code, <span>the destination host is specified using its </span>short name, in other words, <kbd>product-composite</kbd>. This works since the example in this chapter keeps all Kubernetes definitions in one and the same namespace, <kbd>hands-on</kbd>. If that is not the case, it is recommended in the Istio documentation using the host's <strong>fully qualified domain name </strong>(<strong>FQDN</strong>) instead, that is, <kbd><span>product-composite.hands-on.svc.cluster.local</span></kbd>.</div>
<p>Finally, the<span> virtual service object for routing requests from the gateway to the auth server, </span><kbd><span>kubernetes/services/base/istio/auth-server-virtual-service</span>.yml</kbd><span>, looks very similar, the difference being that it routes requests that start with </span><span><kbd>/oauth</kbd> to the Kubernetes service, </span><span><kbd>auth-server</kbd>.</span></p>
<p>With these changes in the source code in place, we are now ready to create the service mesh.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running commands to create the service mesh</h1>
                </header>
            
            <article>
                
<p>Create the service mesh by running the following commands:</p>
<ol>
<li class="mce-root"><span>Build Docker images from source with the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>cd $BOOK_HOME/Chapter18</strong><br><strong>eval $(minikube docker-env)</strong><br><strong>./gradlew build &amp;&amp; docker-compose build</strong></pre>
<ol start="2">
<li>Recreate the <kbd>hands-on</kbd> namespace, and set it as the default namespace:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl delete namespace hands-on</strong><br><strong>kubectl create namespace hands-on</strong><br><strong>kubectl config set-context $(kubectl config current-context) --namespace=hands-on</strong> </pre>
<ol start="3">
<li>Execute the deployment by running the <kbd>deploy-dev-env.bash</kbd> <span>script </span>with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>./kubernetes/scripts/deploy-dev-env.bash </strong></pre>
<ol start="4">
<li>Once the deployment is complete, verify that we have two containers in each of the microservice pods:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl get pods</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>Expect a response along the lines of the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f0040d84-3544-45f4-9dd2-fd7a30e5ea31.png" style="width:38.33em;height:14.67em;" width="1468" height="559" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f0040d84-3544-45f4-9dd2-fd7a30e5ea31.png"></p>
<p style="padding-left: 60px">Note that the pods that run our microservices report two containers per pod; that is, they have the Istio proxy injected as a sidecar!</p>
<ol start="5">
<li>Run the usual tests with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>./test-em-all.bash</strong></pre>
<div class="packt_infobox">The default values for <kbd>script test-em-all.bash</kbd> have been updated from previous chapters to accommodate Kubernetes running in Minikube.</div>
<p style="color: black">Expect the output to be similar to what we have seen in previous chapters:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f18d0079-84bb-4a36-96db-00e71ec593df.png" width="1882" height="418" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f18d0079-84bb-4a36-96db-00e71ec593df.png"></p>
<ol start="6">
<li><span>You can try out the APIs manually by running the following commands:</span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong><br><br><strong>curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" | jq .productId</strong></pre>
<p><span>Expect the requested product ID, </span><kbd>2</kbd><span>, in the response.</span></p>
<p>With the service mesh up and running, let's see how we can observe what's going on in the service mesh using Kiali!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Observing the service mesh</h1>
                </header>
            
            <article>
                
<p>In this section, we will use Kiali <span>together with Jaeger to observe what's going on in the service mesh. For performance monitoring using Grafana, refer to <a href="5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml" target="_blank">Chapter 20</a>,&nbsp;<em>Monitoring Microservices</em>.&nbsp;</span></p>
<p class="mce-root">Before we do that, we need to get rid of some noise created by the health checks performed by Kubernetes' liveness and readiness probes. In the previous chapters, they have been using the same port as the API requests. This means that Istio will collect telematics data for both <span>health checks and requests sent to the API. This will cause the graphs shown by Kiali to become unnecessarily cluttered.</span> <span>Kiali can filter out traffic that we are not interested in, but a simpler solution is to use a different port for the health checks.</span></p>
<p>Microservices can be configured to use a separate port for requests sent to the actuator endpoints, for example, health checks sent to the <kbd>/actuator/health</kbd>&nbsp;<span>endpoint. </span>The following line has been added to the common configuration file for all microservices, <kbd>config-repo/application.yml</kbd>:</p>
<pre>management.server.port: 4004</pre>
<p>This will make all microservices use port <kbd>4004</kbd> to expose the health endpoints. All deployment files in the <kbd>kubernetes/services/base/deployments</kbd> <span>folder </span>have been updated to use port <kbd>4004</kbd> in their <span>liveness and readiness probes.</span></p>
<p>The Spring Cloud Gateway (this is retained so we can run tests in Docker Compose) will continue to use the same port for requests to the API and the <kbd>health</kbd> endpoint. In the <span><kbd>config-repo/gateway.yml</kbd> configuration file, the management port is reverted to the port used for the API</span>:</p>
<pre>management.server.port: 8443</pre>
<p>With the requests sent to the health endpoint out of the way, we can start to send some requests through the service mesh. </p>
<p>We will start a low-volume load test using <kbd>siege</kbd>, which we got to know in <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em> (refer to the <em>Performing a rolling upgrade</em> section). After that, we will go through some of the most important parts of Kiali to see how Kiali can be used to observe a service mesh. We will also explore Kiali's integration with Jaeger and how Jaeger is used for distributed tracing:</p>
<p class="mce-root"><span>Start the test with the following commands:</span></p>
<pre style="color: black"><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong><br><br><strong>siege https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" -c1 -d1</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"><span>The first command will get an OAuth 2.0/OIDC access token that will be used in the next command, where</span> <kbd>siege</kbd> <span>is used to submit one HTTP request per second to the product-composite API.</span></p>
<p class="mce-root"><span>Expect output from the</span> <kbd>siege</kbd> <span>command as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/124bc0e4-2fda-4b01-be2f-81c0653eaa0a.png" width="1969" height="332" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/124bc0e4-2fda-4b01-be2f-81c0653eaa0a.png"></p>
<ol>
<li>Open Kiali's web UI using the <kbd>http://kiali.istio-system.svc.cluster.local:20001/kiali</kbd>&nbsp;<span>URL </span>in a web browser and log in with the following username and password: <kbd>admin</kbd> and <kbd>admin</kbd>. Expect a web page similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/621bc3a8-d0ad-4254-9de3-fea188d12428.png" width="2081" height="1156" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/621bc3a8-d0ad-4254-9de3-fea188d12428.png"><strong><br></strong></p>
<ol start="2">
<li>Click on the <span class="packt_screen">Overview</span> tab, if not already active.</li>
<li>Click on the graph icon in the <span class="packt_screen">hand</span><span class="packt_screen"><span class="packt_screen">s</span>-on</span> namespace. Expect a graph to be shown, representing the current traffic flow through the service mesh, along the lines of the following:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8e6dcb80-2f70-4714-acb9-b8abcf8e49cd.png" width="3313" height="1733" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8e6dcb80-2f70-4714-acb9-b8abcf8e49cd.png"></p>
<ol start="4">
<li>Click on the <span class="packt_screen">Display</span>-button, unselect <span class="packt_screen">S</span><span class="packt_screen">ervice Nodes</span>, and select <span class="packt_screen">Traffic Animation</span>.<br>
Kiali displays a graph representing requests that are currently sent through the service mesh, where active requests are represented by small moving circles along with arrows.<br>
This gives a pretty good initial overview of what's going on in the service mesh!</li>
</ol>
<ol start="5">
<li>Let's now look at some distributed tracing using Jaeger:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/36f57f31-b217-474d-8bd6-99ddb55b7370.png" width="2625" height="1544" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/36f57f31-b217-474d-8bd6-99ddb55b7370.png"></p>
<ol start="6">
<li>Click on the <span class="packt_screen">product</span> node.</li>
</ol>
<ol start="7">
<li>Click on the <span class="packt_screen">Service: product</span> link. <span>On the web page for the service, c</span>lick on the <span class="packt_screen">Traces</span> tab in the menu and Kiali will use Jaeger to show an embedded view of traces that the product service is involved in. Expect a web page such as the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4e530bf8-fba4-4321-9f45-450dfb3fe9dc.png" style="width:58.00em;height:41.25em;" width="2626" height="1865" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4e530bf8-fba4-4321-9f45-450dfb3fe9dc.png"></p>
<ol start="8">
<li>Click on one of the traces to examine it. Expect a web page such as the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/402e5bb3-c03c-48dc-a5aa-ed14e971ecb0.png" width="2624" height="1597" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/402e5bb3-c03c-48dc-a5aa-ed14e971ecb0.png"></p>
<p class="mce-root">This is basically the same tracing information as Zipkin, made available in <a href="42f456c5-d911-494a-a1ba-4631863068b6.xhtml" target="_blank">Chapter 14</a>,&nbsp;<em>Understanding Distributed Tracing</em>.</p>
<p><span>There is much more to explore, but this is enough by way of an introduction. Feel free to explore the web UI in Kiali, Jaeger, and Grafana on your own.</span></p>
<div class="packt_infobox">In <a href="5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml">Chapter 20</a>, <em>Monitoring Microservices</em>, we will explore performance monitoring capabilities further.</div>
<p><span>Let's move on and learn how Istio can be used to improve security in the service mesh!</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Securing a service mesh</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to use Istio to improve the security of a service mesh. We will cover the following topics:</p>
<ul>
<li>How to protect external endpoints with HTTPS and certificates</li>
<li>How to require that external requests are authenticated using OAuth 2.0/OIDC access tokens</li>
<li>How to protect internal communication using mutual authentication (mTLS)</li>
</ul>
<p>Let's now understand each of these in the following sections.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Protecting external endpoints with HTTPS and certificates</h1>
                </header>
            
            <article>
                
<p>In the <em>Creating the service mesh</em>&nbsp;<span>section,</span> we saw how the Istio Ingress Gateway is configured to use the following certificate files to protect external requests sent to <kbd>minikube.me</kbd> using HTTPS. The <span>Istio Ingress Gateway is configured as follows</span>:</p>
<pre>spec:<br>  servers:<br>  - hosts:<br>    - "minikube.me"<br>    ...<br>    tls:<br>      mode: SIMPLE<br>      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt<br>      privateKey: /etc/istio/ingressgateway-certs/tls.key </pre>
<p>But where did these files come from, you may ask?</p>
<p>We can see how the Istio Ingress Gateway is configured by running the following command:</p>
<pre><strong><span>kubectl -n istio-system get deploy istio-ingressgateway -o json</span></strong></pre>
<p>We will find that it is prepared to mount an optional secret named <span><kbd>istio-ingressgateway-certs</kbd> and that it will be mapped to the folder, </span><kbd>/etc/istio/ingressgateway-certs/</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/206f48a3-7bfa-42df-97ad-e1be388adda9.png" style="width:20.00em;height:12.00em;" width="930" height="558" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/206f48a3-7bfa-42df-97ad-e1be388adda9.png"></p>
<p>This results in certificate files, <kbd>tls.crt</kbd> and <kbd>tls.key</kbd>, from a secret named <span><kbd>istio-ingressgateway-certs</kbd> being made available to the Istio Ingress Gateway on the <kbd>/etc/istio/ingressgateway-certs/tls.crt</kbd> and <kbd>/etc/istio/ingressgateway-certs/tls.key</kbd> file paths.</span></p>
<p><span>Creating of this secret is handled by means of the <kbd>deploy-dev-env.bash</kbd></span> and <span><kbd>deploy-prod-env.bash</kbd> deployment scripts, found</span> in the <span><kbd>kubernetes/scripts</kbd> folder, by means of the following command:</span></p>
<pre><strong>kubectl create -n istio-system secret tls istio-ingressgateway-certs \</strong><br><strong>--key kubernetes/cert/tls.key --cert kubernetes/cert/tls.crt</strong></pre>
<p>The certificate files were created in <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 17</a>, <em>Implementing Kubernetes Features as an Alternative</em> (refer to the <em>Testing with Kubernetes ConfigMaps, secrets, and ingress</em> section).</p>
<p>To verify that it is these certificates that are used by <span>the Istio Ingress Gateway, we can run the following command:</span></p>
<pre><strong><span>keytool -printcert -sslserver minikube.me:443 | grep -E "Owner:|Issuer:"</span></strong></pre>
<p class="CDPAlignLeft CDPAlign">Expect the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a5d99e95-d5ab-49fc-b2c3-62afb06295d1.png" style="width:29.50em;height:4.75em;" width="1502" height="243" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a5d99e95-d5ab-49fc-b2c3-62afb06295d1.png"></p>
<p>The output shows that the certificate is issued for <kbd>minikube.se</kbd> and that it is self-signed; that is, the issuer is also <kbd>minikube.me</kbd>.</p>
<p>This self-signed certificate can be replaced with a certificate bought by a trusted certificate authority (CA) for production use cases. Istio has recently added support for the automated provisioning of trusted certificates using, for example, the cert manager and Let's Encrypt, as we did in <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 17</a>, <em>Implementing Kubernetes Features as an Alternative</em> (refer to the <em>Provisioning certificates with the cert manager and Let's Encrypt</em> section). This support is currently a bit too complex to fit into this chapter. </p>
<p>With the certificate configuration verified, let's now move on to see how the Istio Ingress Gateway can protect microservices from unauthenticated requests<strong>.</strong></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Authenticating external requests using OAuth 2.0/OIDC access tokens</h1>
                </header>
            
            <article>
                
<p>Istio Ingress Gateway is capable of <span>requiring and validating JWT-based OAuth 2.0/OIDC access tokens, in other words, protecting the microservices in the service mesh from external unauthenticated requests. For a recap on JWT, OAuth 2.0, and OIDC, refer to <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 11</a>, <em>Secure Access to APIs</em> (see the <em>Authenticating and authorizing API access using OAuth 2.0 and OpenID Connect</em> section).</span></p>
<p>To enable authentication, we need to create an Istio <kbd>Policy</kbd> object that specifies which targets should be protected and which access token issuers, that is, OAuth 2.0/OIDC providers, should be trusted. This is done in the <kbd>kubernetes/services/base/istio/jwt-authentication-policy.yml</kbd> <span>file </span>and appears as follows:</p>
<pre>apiVersion: "authentication.istio.io/v1alpha1"<br>kind: "Policy"<br>metadata:<br>  name: "jwt-authentication-policy"<br>spec:<br>  targets:<br>  - name: product-composite<br>  peers:<br>  - mtls:<br>      mode: PERMISSIVE<br>  origins:<br>  - jwt:<br>      issuer: "http://auth-server.local"<br>      jwksUri: "http://auth-server.hands-on.svc.cluster.local/.well-known/jwks.json"<br>  principalBinding: USE_ORIGIN</pre>
<p><span>Explanations for the preceding source code are as follows</span>:</p>
<ul>
<li>The <kbd>targets</kbd> list specifies that the authentication check will be performed for requests sent to the <kbd>product-composite</kbd> microservice.</li>
<li>The <kbd>origins</kbd> list specifies the OAuth 2.0<span>/OIDC providers we rely on. For each issuer, the name of the issuer and the URL for its JSON web key set are specified. For a recap, see <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml">Chapter 11</a>, <em>Securing Access to APIs</em> (refer to the <em>Introducing OpenId Connect</em> section). We have specified the local auth server, <kbd>http://auth-server.local</kbd>.</span></li>
</ul>
<p>The policy file was applied by the <kbd>kubernetes/scripts/deploy-dev-env.bash</kbd>&nbsp;<span>deployment script, </span>when it was used in the <em>Running commands to create the service mesh</em>&nbsp;<span>section.</span></p>
<p>The easiest way to verify that an invalid request is rejected by the Istio Ingress Gateway and not the <kbd>product-composite</kbd> microservice is to make a request without an access token and observe the error message that is returned. The <span>Istio Ingress Gateway returns the following error message, <kbd>Origin authentication failed.</kbd>, in the event of a failed authentication, while the <kbd>product-composite</kbd> microservice returns an empty string. Both return the HTTP code <kbd>401</kbd> (Unauthorized).</span></p>
<p>Try it out with the following commands:</p>
<ol>
<li>Make a request without an access token along the lines of the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl https://minikube.me/product-composite/2 -kw " HTTP Code: %{http_code}\n"</strong></pre>
<p style="padding-left: 60px">Expect a response saying <kbd>Origin authentication failed. HTTP Code: 401</kbd><span>.&nbsp;</span></p>
<ol start="2">
<li><span><span>Temporarily delete the policy with the following command:<br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl delete -f kubernetes/services/base/istio/jwt-authentication-policy.yml</strong></pre>
<p style="padding-left: 60px"><span>Wait a minute to allow that policy change to be propagated to the </span><span>Istio Ingress Gateway and then retry the request without an access token. The response should now only contain the HTTP code: <kbd>HTTP Code: 401</kbd>.</span></p>
<ol start="3">
<li><span>Enable the policy again with the following command:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -f kubernetes/services/base/istio/jwt-authentication-policy.yml</strong></pre>
<div class="packt_infobox"><strong>Suggested additional exercise:</strong> Try out the Auth0 OIDC provider, as described in <a href="bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml">Chapter 11</a>, <em>Securing Access to APIs</em> (refer to the T<em>esting with an OpenID Connect provider, Auth0,</em> section. Add your Auth0 provider to <span><kbd>jwt-authentication-policy.yml</kbd>. I</span>n my case, it appears as follows:<br>
<pre>  - jwt:<br>      issuer: "https://dev-magnus.eu.auth0.com/" <br>      jwksUri: "https://dev-magnus.eu.auth0.com/.well-known/jwks.json"</pre></div>
<p><span>Now, let's move on to the last security mechanism that we will cover in Istio â the automatic protection of internal communication in the service mesh using mutual authentication, mTLS.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Protecting internal communication using mutual authentication (mTLS)</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how Istio can be configured to automatically protect internal communication within the service mesh using <em>mutual</em> <em>authentication</em>, mTLS. When using mutual authentication, not only does the service side prove its identity by exposing a certificate, but also the clients prove their identity to the servers by exposing a client-side certificate. This provides a higher level of security compared to normal TLS/HTTPS usage, where only the identity of the server is proven. Setting up and maintaining <span>mutual</span> <span>authentication;</span> that is, the provision of new, and the rotating of outdated, certificates, is known to be complex and is therefore seldom used. Istio fully automates the provisioning and rotation of certificates for mutual authentication used for internal communication inside the service mesh. This makes it much easier to use <span>mutual</span> <span>authentication compared to setting it up manually.</span></p>
<p>So, why should we use <span>mutual</span> <span>authentication</span><span>? Isn't it sufficient to protect external APIs with HTTPS and OAuth 2.0/OIDC access tokens?</span></p>
<p>As long as the attacks come through the external API, it might be sufficient. But what if a pod inside the Kubernetes cluster becomes compromised? For example, if an attacker gains control over a pod, then the attacker can start listening to traffic between other pods in the Kubernetes cluster. If the internal communication is sent as plain text, it will be very easy for the attacker to gain access to sensitive information sent between pods in the cluster. To minimize the damage caused by such an intrusion, mutual authentication can be used to prevent an attacker from eavesdropping on internal network traffic.</p>
<p>To enable the use of <span>mutual</span><span>&nbsp;</span><span>authentication managed by </span>Istio, Istio needs to be configured both on the server side, using a policy, and on the client side, using a destination rule.</p>
<p>When using the demo configuration of Istio, as we did in the <em>Deploying Istio in a Kubernetes cluster</em>&nbsp;<span>section, </span>we got a global mesh policy created that configures the server side to use a permissive mode, meaning the Istio proxies will allow both plain text and encrypted requests. This can be verified with the following command:</p>
<pre><strong>kubectl get meshpolicy default -o yaml</strong></pre>
<p>Expect a response similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/adef0bf8-61fd-484e-854c-a07e50609603.png" style="width:22.50em;height:12.75em;" width="909" height="513" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/adef0bf8-61fd-484e-854c-a07e50609603.png"></p>
<p>To configure microservices to use mutual authentication when sending requests internally to other microservices, a destination rule is created for each microservice. This is done in the <kbd>kubernetes/services/base/istio/internal_mtls_destination_rules.yml</kbd>&nbsp;<span>file. </span>The <span>destination rules all look the same; for example, for the </span><kbd>product-composite</kbd> service, they appear as follows:</p>
<pre>apiVersion: networking.istio.io/v1alpha3<br>kind: DestinationRule<br>metadata:<br>  name: product-composite-dr<br>spec:<br>  host: product-composite<br>  trafficPolicy:<br>    tls:<br>      mode: ISTIO_MUTUAL</pre>
<p><kbd>trafficPolicy</kbd> is set to use <kbd>tls</kbd> with <kbd>ISTIO_MUTUAL</kbd>, meaning mutual authentication is managed by Istio.</p>
<p><span>The destination rules were applied by the </span><kbd>kubernetes/scripts/deploy-dev-env.bash</kbd><span> deployment script, when it was used in the preceding </span><em><span>Running commands to create the service mesh</span></em><span> section.</span></p>
<p>To verify that the internal communication is protected, perform the following steps:</p>
<ol>
<li>Ensure that the load tests started in the preceding <em>Observing the service mesh</em> section are still running.</li>
<li>Go to the Kiali graph in a web browser (<a href="http://kiali.istio-system.svc.cluster.local:20001/kiali">http://kiali.istio-system.svc.cluster.local:20001/kiali</a>).</li>
<li>Click on the <span class="packt_screen">Display</span> button to enable the <span class="packt_screen">S</span><span class="packt_screen">ecurity</span> label. The graph will show a padlock on all communication links that are protected by Istio's automated mutual authentication, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cc3e1088-449c-4140-8da3-9d90d416f1ab.png" width="2172" height="1395" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/cc3e1088-449c-4140-8da3-9d90d416f1ab.png"></p>
<p>Expect a padlock on all links except for those to resource managers â RabbitMQ, MySQL, and MongoDB.</p>
<div class="packt_infobox"><span>Calls to </span><span>RabbitMQ, MySQL, and MongoDB are not handled by Istio proxies, and therefore require manual configuration to be protected using TLS.</span></div>
<p>With this, we have seen all three security mechanisms in Istio in action, and it's now time to see how Istio can help us to verify that a service mesh is resilient.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Ensuring that a service mesh is resilient</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will learn how to use Istio to ensure that a service mesh is resilient; that is, it can handle temporary faults in a service mesh. Istio comes with mechanisms similar to what the Spring Framework offers in terms of timeouts, retries, and a type of circuit breaker called <strong>outlier detection</strong> to handle temporary faults. When it comes to deciding whether language-native mechanisms should be used to handle temporary faults, or whether this should be delegated to a service mesh such as Istio, I tend to favor using language-native mechanisms, as in the examples in <a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4J</em>.</span> In many cases, it is important to keep the logic for handling errors, for example, handling fallback alternatives for a circuit breaker, together with other business logic for a microservice. </p>
<p><span>There are cases when the corresponding mechanisms in Istio could be of great help. For example, if a microservice is deployed and it is determined that it can't handle temporary faults that occur in production from time to time, then it can be very convenient to add a timeout or a retry mechanism using Istio instead of waiting for a new release of the microservice with corresponding error handling features put in place.</span></p>
<p>Another capability in the area of resilience that comes with Istio is the capability to inject faults and delays into an existing service mesh. Why would anyone want to do that?</p>
<p>Injecting faults and delays in a controlled way is very useful for verifying that the resilient capabilities in the microservices work as expected! We will try them out in this section, verifying that the retry, timeout, and circuit breaker in the <kbd>product-composite</kbd> microservice work as expected.</p>
<div class="packt_infobox">In <a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em> (refer to the <em>Adding programmable delays and random errors</em> section), we added support for injecting faults and delays in the <span>microservices </span>source code. That source code can preferably be replaced by using Istio's capabilities for injecting faults and delays at runtime, as demonstrated in the following.</div>
<p>We will begin by injecting faults to see whether the retry mechanisms in the <kbd>product-composite</kbd> microservice work as expected. After that, we will delay the responses from the product service and verify that the circuit breaker handles the delay as expected. </p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing resilience by injecting faults</h1>
                </header>
            
            <article>
                
<p>Let's make the product service throw random errors and verify that the microservice landscape handles this correctly. We expect the retry mechanism in the <kbd>product-composite</kbd> microservice to kick in and retry the request until it succeeds or its limit of max numbers of retries is reached. This will ensure that a shortlived fault does not affect the end user more than the delay introduced by the retry attempts. Refer to the <em>Adding a retry mechanism</em><span> section in </span><a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em>, for a recap on the retry mechanism <span>in the <kbd>product-composite</kbd> microservice</span>.</p>
<p>Faults can be injected using <kbd><span>kubernetes/resilience-tests/product-virtual-service-with-faults.yml</span></kbd><span>. This appears as follows:</span></p>
<pre>apiVersion: networking.istio.io/v1alpha3<br>kind: VirtualService<br>metadata:<br>  name: product-vs<br>spec:<br>  hosts:<br>    - product<br>  http:<br>  - route:<br>    - destination:<br>        host: product<br>    fault:<br> abort:<br>        httpStatus: 500<br>        percent: 20</pre>
<p>The definition says that 20% of the requests sent to the product service shall be aborted with the HTTP status code 500 (Internal Server Error).</p>
<p>Perform the following steps to test this:</p>
<ol>
<li><span>Ensure that the load tests using <kbd>siege</kbd>, as started in the</span> <em><span>Observing the</span> <span>ser</span><span>vice</span> <span>mesh</span></em><span><em>&nbsp;</em>section, are running.<br></span></li>
<li>Apply fault injection with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>kubectl apply -f kubernetes/resilience-tests/product-virtual-service-with-faults.yml</span></strong></pre>
<ol start="3">
<li>Monitor the output from the <span><kbd>siege</kbd>&nbsp;</span>load tests tool. Expect output similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0bc01f04-88dd-4507-8e09-21be25135194.png" style="width:38.92em;height:8.75em;" width="1458" height="328" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/0bc01f04-88dd-4507-8e09-21be25135194.png"></p>
<p style="padding-left: 60px">From the sample output, we can see that all requests are still successful, in other words, status 200 (OK) is returned; however, some of them (20%) take an extra second to complete. This indicates that the retry mechanism in the <kbd>product-composite</kbd> microservice has kicked in and has retried a failed request to the product service.</p>
<ol start="4">
<li><span>Kiali will also indicate that something is wrong with requests sent to the product service, as follows:</span>
<ol>
<li><span>Go to the call graph in Kiali's web UI that we used earlier to observe the traffic in our namespace, <kbd>hands-on</kbd></span>.</li>
<li><span>Click on the <span class="packt_screen">Display</span> menu button and select <span class="packt_screen">Service Nodes</span>.</span></li>
<li><span><span><span><span>Click on the menu button to the left of the <span class="packt_screen">Display</span> button, named <span class="packt_screen">No edge labels</span>, and select the <span class="packt_screen">Response time</span> option.</span></span></span></span></li>
</ol>
</li>
</ol>
<ul>
<li style="list-style-type: none">
<ol start="4">
<li>The gr<span>aph</span> <span>will show something like the following:</span></li>
</ol>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ee4ac35f-e8a5-437f-b1de-261228e0ed60.png" width="2506" height="1434" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ee4ac35f-e8a5-437f-b1de-261228e0ed60.png"></p>
<p>The arrow to the Service Node <span>product </span>will be shown in red to indicate that failed requests are detected. If we click on the arrow, we can see fault statistics to the right.</p>
<p>In the preceding sample screenshot, an error rate of 19.4% is reported, which corresponds well with the 20% we asked for. Note that the arrow from the Istio Gateway to the <kbd>product-composite</kbd> service is still green. This means that the retry mechanism in the <kbd>product-composite</kbd> service protects the end user; in other words, the faults do not propagate to the end user.</p>
<p>Conclude the removal of the fault injection with the following command<span>:</span></p>
<pre style="padding-left: 60px"><strong><span>kubectl delete -f </span><span>kubernetes/resilience-tests/product-virtual-service-with-faults.yml</span></strong></pre>
<p><span>Let's now move on to the next section, where we will inject delays to trigger the circuit breaker.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing resilience by injecting delays</h1>
                </header>
            
            <article>
                
<p>From <a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml" target="_blank"></a><a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em> (refer to the <em>Introducing the circuit breaker</em> section), we know that a circuit breaker can be used to prevent problems due to the slow response of services, or the fact that the services do not respond at all.</p>
<p>Let's verify that the circuit breaker in the <span><kbd>product-composite</kbd> service works as expected by injecting a delay into the</span> <span>product service using Istio. A delay can be injected using a virtual service. </span></p>
<p>Refer to <span><kbd>kubernetes/resilience-tests/product-virtual-service-with-delay.yml</kbd>. Its code appears as follows:</span></p>
<pre>apiVersion: networking.istio.io/v1alpha3<br>kind: VirtualService<br>metadata:<br>  name: product-vs<br>spec:<br>  hosts:<br>    - product<br>  http:<br>  - route:<br>    - destination:<br>        host: product<br>    fault:<br>      delay:<br>        fixedDelay: 3s<br>        percent: 100</pre>
<p><span>The preceding definition says that all requests sent to the product service shall be delayed by 3 seconds.</span></p>
<p>Requests sent to the product service from the <kbd>product-composite</kbd> service are configured to timeout after 2 seconds. The circuit breaker is configured to open its circuit if 3 consecutive requests fail. When the circuit is open, it will fast-fail; in other words, it will immediately throw an exception, not attempting to call the underlying service. The business logic in the <kbd>product-composite</kbd> microservice will catch this exception and apply fallback logic. For a recap, see  <a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a><span>, <em>Improving Resilience Using Resilience4j</em> (refer to the <em>Adding a circuit breaker</em> section).</span></p>
<p><span>Perform the following steps to test the circuit breaker by injecting a delay:</span></p>
<ol>
<li><span>Stop the load test run by means of the </span><kbd>siege</kbd><span> command by pressing </span><em>Ctrl + C</em>&nbsp;<span>in the terminal window where </span><kbd>siege</kbd><span> is running. </span></li>
<li><span><span>Create a temporary delay in the product service with the following command:<br></span></span></li>
</ol>
<pre style="padding-left: 60px"><span><strong>kubectl apply -f kubernetes/resilience-tests/product-virtual-service-with-delay.yml</strong></span></pre>
<ol start="3">
<li>Acquire an access token as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong></pre>
<ol start="4">
<li>Send six requests in a row. Expect the circuit to open up after the first three failed calls, that the <span>circuit breaker </span>applies fast-fail logic for the three last calls, and that a fallback response is returned, as follows:<span><span><br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong><span>for</span> i <span>in</span> <span>{</span><span>1</span>..6<span>}</span>; <span>do</span> time curl -k https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN"; <span>done</span></strong></pre>
<p style="padding-left: 60px"><span>The responses from the first 3 calls are expected to be a timeout-related error message, and a response time of 2 seconds, in other words, the timeout time. Expect responses for the first 3 calls along the lines of the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><strong><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c61d561d-e023-488b-9e75-1dbb32bd2f5c.png" style="width:36.42em;height:4.33em;" width="1643" height="195" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c61d561d-e023-488b-9e75-1dbb32bd2f5c.png"></strong></p>
<p>The responses from the last 3 calls are expected to be a response from the fallback logic with a short response time. Expect responses for the last 3 calls as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/468fa43c-0a38-47fa-8baf-80de72b55711.png" style="width:19.25em;height:3.08em;" width="898" height="144" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/468fa43c-0a38-47fa-8baf-80de72b55711.png"></p>
<ol>
<li><span>Simulate the fact that the delay problem is fixed by removing the temporary delay with the following command:</span></li>
</ol>
<pre style="padding-left: 60px"><strong><span>kubectl delete -f kubernetes/resilience-tests/product-virtual-service-with-delay.yml</span></strong></pre>
<ol start="2">
<li>Verify that correct answers are returned again and without any delay by sending a new request using the preceding command.</li>
</ol>
<div class="packt_infobox">If you want to check the state of the circuit breaker, you can do it with the following command:<br>
<br>
<strong><kbd>curl product-composite.hands-on.svc.cluster.local:4004/actuator/health -s | jq -r .details.productCircuitBreaker.details.state</kbd></strong>
<p>It should report <kbd>CLOSED</kbd>, <kbd>OPEN</kbd>, or <kbd>HALF_OPEN</kbd>, depending on its state.</p>
</div>
<p>This proves that the circuit breaker reacts as expected when we inject a delay using Istio. This concludes testing features in Istio that can be used to verify that the microservice landscape is resilient. The final feature we will explore in Istio is its support for traffic management; we will establish how it can be used to enable deployments with zero downtime.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Performing zero-downtime deployments</h1>
                </header>
            
            <article>
                
<p>As already mentioned in <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a><span>, <em>Deploying Our Microservices to Kubernetes</em> (refer to the <em>Performing a rolling upgrade</em> section), being able to deploy an update without downtime becomes crucial with a growing number of</span> autonomous microservices that are updated independently of one another.</p>
<p>In this section, we will learn about Istio's <span>traffic management and routing capabilities and how they can be used to</span> perform deployments of new versions of microservices without requiring any downtime. In <a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a>, <em>Deploying Our Microservices to Kubernetes</em> (refer to the <em>Performing a rolling upgrade</em> section), we saw how Kubernetes can be used to perform a rolling upgrade without requiring any downtime. Using the Kubernetes rolling upgrade mechanism automates the entire process, but unfortunately provides no option to test the new version before all users are routed to the new version.</p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>Using Istio, we can deploy the new version, but initially route all users to the existing version<span> (called the old version in this chapter). After that, we can use Istio's fine-grained routing mechanism to control how users are routed to the new and the old versions.</span> We will see how two popular upgrade strategies can be implemented using Istio:</p>
<ul>
<li><strong>Canary deploys<em>:&nbsp;</em></strong>When using canary deploys, most users are routed to the old version, except for a group of selected test users who are routed to the new version. When the test users have approved the new version, regular users can be routed to the new version using a blue/green deploy. </li>
<li><strong>Blue/</strong><strong>green</strong> <strong>deploys</strong><em><strong>:&nbsp;</strong></em>Traditionally, a blue/green deploy means that all users are switched to either the blue or the green version, one being the new version and the other being the old version. If something goes wrong when switching over to the new version, it is very simple to switch back to the old version. Using Istio, this strategy can be refined by gradually shifting users over to the new version, for example, starting with 20% of the users and then slowly increasing the percentage of users who are routed to the new version. At all times, it is very easy to route all users back to the old version if a fatal error is revealed in the new version.</li>
</ul>
<p>As already stated in<a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml" class="totri-footnote">&nbsp;</a><a href="ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml">Chapter 16</a><span>, <em>Deploying Our Microservices to Kubernetes</em> (refer to the <em>Performing a rolling upgrade</em> section),</span> it is important to remember that a<span> prerequisite for these types of upgrade strategies is that the upgrade is backward-compatible. Such an upgrade is compatible both in terms of APIs and message formats, which are used to communicate with other services and </span><span>database structures. If the new version of the microservice requires changes to external APIs, message formats, or database structures that the old version can't handle, these upgrade strategies can't be applied.</span></p>
<p>We will go through the following deploy scenario in the following subsections:</p>
<ul>
<li>We will start by deploying <span>the <kbd>v1</kbd> and <kbd>v2</kbd> versions of the microservices, with routing configured to send all requests to the <kbd>v1</kbd> version of the microservices.</span></li>
<li>Next, we will allow a test group to run canary tests; that is, we'll verify the new <kbd>v2</kbd> versions of the<span> microservices.</span> <span>To simplify the tests somewhat, we will only route test users to the new versions of the core microservices, that is, the <kbd>product</kbd>, <kbd>recommendation</kbd></span>, a<span>nd <kbd>review</kbd> microservices.</span></li>
<li>Finally, we will start to move regular users over to the new versions using a blue/green deploy: initially, a small percentage of users and then, over time, more and more users until, eventually, they are all routed to the new version. We will also see how we can quickly <span><span>switch back to the <kbd>v1</kbd> versions if a fatal error is detected in the new v2 version.</span></span></li>
</ul>
<p>Let's first see what changes have been applied to the source code to deploy two concurrent versions,<span>&nbsp;</span><kbd>v1</kbd><span>&nbsp;</span>and<span>&nbsp;</span><kbd>v2</kbd>, of the microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Source code changes</h1>
                </header>
            
            <article>
                
<p>As already mentioned in the <em>Changing the file structure of the Kubernetes definition files </em><span>section,</span> the<span> file structure for the Kubernetes definition files in </span><span><kbd>kubernetes/services</kbd> has been expanded in this chapter to support the deployment of concurrent versions of the microservices in the production environment. The expanded file structure appears as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/26f33b74-c0c1-46ea-80f3-259b536c3bf8.png" style="width:9.33em;height:10.33em;" width="481" height="534" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/26f33b74-c0c1-46ea-80f3-259b536c3bf8.png"></p>
<div class="packt_infobox"><span>Details regarding the development environment have been removed from the preceding diagram.</span></div>
<p>Let's first see how service and deployment objects for the <kbd>v1</kbd> and <kbd>v2</kbd> versions of the microservices are configured and created. After that, we will go through additional definition files for Istio objects used to control the routing.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Service and deployment objects for concurrent versions of microservices</h1>
                </header>
            
            <article>
                
<p>To be able to run multiple versions of a microservice concurrently, the deployment objects and their corresponding pods must have different names, for example, <kbd>product-v1</kbd> and <kbd>product-v2</kbd>. There must, however, be only one Kubernetes service object per microservice. All traffic to a specific microservice always goes through one and the same service object, irrespective of what version of the pod the request will be routed to in the end. This is achieved using Kustomize by splitting up deployment objects and service objects into different folders.</p>
<p>To give deployment objects and their pods version-dependent names, the <kbd>kustomization.yml</kbd> file can use the <kbd>nameSuffix</kbd> directive to tell Kustomize to add the given suffix to all Kubernetes objects it creates. For example, the <kbd>kustomization.yml</kbd><span> file used for the</span> <kbd>v1</kbd>&nbsp;<span>version of the microservices in the </span><kbd>kubernetes/services/overlays/prod/v1</kbd> folder appears as follows:</p>
<pre>nameSuffix: -v1<br>bases:<br>- ../../../base/deployments<br>patchesStrategicMerge:<br>- ...</pre>
<p>The <kbd>nameSuffix: -v1</kbd> <span>setting </span>will result in all objects created using this <kbd>kustomization.yml</kbd><span> file being named with the </span><kbd>-v1</kbd>&nbsp;<span>suffix.</span></p>
<p>To create the objects without a version suffix, and the deployment objects and their pods with the <kbd>v1</kbd> and <kbd>v2</kbd> version suffixes, the <kbd>kubernetes/scripts/deploy-prod-env.bash</kbd>&nbsp;<span>deployment script </span>executes separate <kbd>kubectl apply</kbd> commands as follows:</p>
<div>
<pre><strong>kubectl apply -k kubernetes/services/base/services</strong><br><strong>kubectl apply -k kubernetes/services/overlays/prod/v1</strong><br><strong>kubectl apply -k kubernetes/services/overlays/prod/v2</strong></pre></div>
<p>Let's also see what Istio definition files we have added to configure routing rules.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Added Kubernetes definition files for Istio</h1>
                </header>
            
            <article>
                
<p>To configure routing rules, we will add Istio objects to<span> the </span><kbd>kubernetes/services/overlays/prod/istio</kbd><span> folder. Each microservice has a virtual service object that defines the weight distribution for the routing between the old and the new versions. Initially, it is set to route 100% of the traffic to the old version. For example, the routing rule for the product microservice in <kbd>product-routing-virtual-service.yml</kbd> appears as follows:</span></p>
<pre>  http:<br>  - route:<br>    - destination:<br>        host: product<br>        subset: old<br>      weight: 100<br>    - destination:<br>        host: product<br>        subset: new<br>      weight: 0</pre>
<p>The virtual service<span> defines subsets for the old and the new versions. To define what actual versions the old and new versions are, </span>each microservice also has a destination rule defined. The destination rule details how the old subset and the new subset shall be identified, for example, in the case of the product microservice in <kbd>old_new_subsets_destination_rules.yml</kbd>:</p>
<pre>apiVersion: networking.istio.io/v1alpha3<br>kind: DestinationRule<br>metadata:<br>  name: product-dr<br>spec:<br>  host: product<br>  subsets:<br>  - name: old<br>    labels:<br>      version: v1<br>  - name: new<br>    labels:<br>      version: v2</pre>
<p>The subset named <kbd>old</kbd> points to product pods that have the <kbd>version</kbd> label set to <kbd>v1</kbd>, while the subset named <kbd>new</kbd> points to pods with the <span><kbd>version</kbd> label set to</span>&nbsp;<kbd>v2</kbd>.&nbsp;</p>
<div class="packt_infobox"><span>To route traffic to a specific version, Istio documentation recommends that pods are labeled with a label named </span><kbd>version</kbd><span> to identify its version. Refer to </span><a href="https://istio.io/docs/setup/kubernetes/additional-setup/requirements/">https://istio.io/docs/setup/kubernetes/additional-setup/requirements/</a><span> for details.</span></div>
<p>Finally, to support canary testers, an extra routing rule has been added to the virtual services for the three core microservices: <span>product, recommendation, and review. This routing rule states that any incoming request that has an HTTP header named <kbd>X-group</kbd> set to the value <kbd>test</kbd> will always be routed to the new version of the service. This appears as follows:</span></p>
<pre>  http:<br>  - match:<br>    - headers:<br>        X-group:<br>          exact: test<br>    route:<br>    - destination:<br>        host: product<br>        subset: new</pre>
<p>The <kbd>match</kbd> and <kbd>route</kbd> sections specify that requests with the HTTP header, <kbd>X-group</kbd>, set to the value, <kbd>test</kbd>, shall be routed to the subset named <kbd>new</kbd>.</p>
<p><span>To create these Istio objects, the <kbd>kubernetes/scripts/deploy-prod-env.bash</kbd> deployment script executes the following command:</span></p>
<pre><strong>kubectl apply -k kubernetes/services/overlays/prod/istio</strong></pre>
<p>Finally, to be able to route canary testers to the new version based on header-based routing, the <kbd>product-composite</kbd> microservice has been updated to forward the HTTP header <kbd>X-group</kbd>. Refer to the <span><kbd>getCompositeProduct()</kbd> method in the</span> <span><kbd>se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl</kbd> class  for details.</span></p>
<p>Now, we have seen all the changes to the source code and we are ready to deploy v1 and v2 versions of the microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying v1 and v2 versions of the microservices with routing to the v1 version</h1>
                </header>
            
            <article>
                
<p>To be able to test the <kbd>v1</kbd> and <kbd>v2</kbd> versions of the microservices, we need to remove the development environment we have been using earlier in this chapter and create a production environment where we can deploy the v1 and v2 versions of the <span>microservices.</span></p>
<p>To achieve this, run the following commands:</p>
<ol start="1">
<li>Recreate the <kbd>hands-on</kbd>&nbsp;<span>namespace:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl delete namespace hands-on<br>kubectl create namespace hands-on</strong></pre>
<ol start="2">
<li>Execute the deployment by running the script with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>./kubernetes/scripts/deploy-prod-env.bash </strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The command takes a couple of minutes and should eventually list all the v1 or v2 versions of the pods as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3ac160f5-b56c-46fd-8d26-d0cad647e4d5.png" style="width:34.17em;height:19.08em;" width="1247" height="696" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/3ac160f5-b56c-46fd-8d26-d0cad647e4d5.png"></p>
<ol start="3">
<li>Run the usual tests to verify that everything works:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>SKIP_CB_TESTS=true ./test-em-all.bash</span></strong></pre>
<p>If this command is executed immediately after the <kbd>deploy</kbd> command, it sometimes fails. Simply rerun the command and it should run fine! </p>
<div class="packt_infobox">Since we now have two pods (version V1 and V2) running for each microservice, the circuit breaker tests no longer work. The reason for this is that the test script can't control which pod it talks to, through the Kubernetes service. The test script asks about the state of the circuit breaker in the <kbd>product-composite</kbd> microservice using the actuator endpoint on port <kbd>4004</kbd>. This port is not managed by Istio, so its routing rules do no apply. The test script will therefore not know whether it is checking the state of the circuit breaker in V1 or V2 of the <span><kbd>product-composite</kbd> microservice. </span>We can skip <span>circuit breaker tests</span> by using the <span><span><kbd>SKIP_CB_TESTS=true</kbd> flag.</span></span></div>
<p><span>Expect output that is similar to what we have seen from the previous chapters, but excluding the circuit breaker tests:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4ebb9d21-ccfc-4210-92a0-8be37b37fd74.png" width="1885" height="425" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4ebb9d21-ccfc-4210-92a0-8be37b37fd74.png"></p>
<p>We are now ready to run some <em>zero-downtime deploy</em> tests. Let's begin by verifying that all traffic goes to the v1 version of the microservices!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Verifying that all traffic initially goes to the v1 version of the microservices</h1>
                </header>
            
            <article>
                
<p>To verify that all requests are routed to the v1 version of the microservices, we will start up the load test tool, <kbd>siege</kbd>, and then observe the traffic that flows through the service mesh using Kiali.</p>
<p>Perform the following steps:</p>
<ol>
<li>Get a new access token and start the <kbd>siege</kbd>&nbsp;<span>load test tool, </span>with the following commands:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong><br><br><strong>siege https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" -c1 -d1</strong></pre>
<p class="mce-root"></p>
<ol start="2">
<li>Go to the graph view in Kiali's web UI (<a href="http://kiali.istio-system.svc.cluster.local:20001/kiali">http://kiali.istio-system.svc.cluster.local:20001/kiali</a>):
<ol>
<li>Click on the D<span class="packt_screen">isplay</span> menu button and deselect <span class="packt_screen">Service Nodes</span>.</li>
<li>After a minute or two, expect only traffic to the <span class="packt_screen">v1</span> <span>version of the microservices </span>as follows:</li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e2170279-e949-462e-b545-b0b5c6c69d54.png" style="width:48.75em;height:38.17em;" width="1778" height="1388" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e2170279-e949-462e-b545-b0b5c6c69d54.png"></p>
<p>Good! This means that, even though the v2 versions of the microservices are deployed, they do not get any traffic routed to them. Let's now try out canary tests where selected test users are allowed to try out the v2 versions of the microservices!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running canary tests</h1>
                </header>
            
            <article>
                
<p>To run a canary test, in other words, in order to be routed to the new versions while all other users are still routed to the old versions of the deployed microservices, we need to add the<span>&nbsp;</span><kbd>X-group</kbd><span> HTTP header set to the value </span><kbd>test</kbd><span> in our requests sent to the external API.</span></p>
<p><span>To see which version of a microservice served a request, the</span> <kbd>serviceAddresses</kbd> <span>field in the response can be inspected. The</span> <kbd>serviceAddresses</kbd> <span>field contains the hostname of each service that took part in creating the response. The hostname is equal to the name of the pod, so we can find the version in the hostname; for example,</span> <kbd>product-v1-...</kbd> <span>for a product service of version V1, and </span><kbd>product-v2-...</kbd><span> for a product service of version V2.</span></p>
<p>Let's begin by sending a normal request and verify that it is the v1 versions of the microservices that respond to our request. Next, send a request with the<span>&nbsp;</span><kbd>X-group</kbd><span> HTTP header set to the value </span><kbd>test</kbd><span>, and verify that the new v2 versions are responding.</span></p>
<p><span>To do this, perform the following steps:</span></p>
<ol>
<li class="mce-root"><span>Perform a normal request to verify that the request is routed to the v1 version of the microservices by using</span> <kbd>jq</kbd> <span>to filter out the </span><span><kbd>serviceAddresses</kbd> field in the response</span><span>:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" | jq .serviceAddresses</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>Expect a response along the lines of the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/661a5a65-6610-40c6-aaf4-5a55f2e57b81.png" width="2326" height="445" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/661a5a65-6610-40c6-aaf4-5a55f2e57b81.png"></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">As expected, all three core services are v1 <span>versions of the micro</span>services.</p>
<ol start="2">
<li class="mce-root">If we add the <kbd>X-group=test</kbd> header, we expect the request to be served by v2 versions of the core microservices. Run the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" -H "X-group: test" | jq .serviceAddresses</strong></pre>
<p class="mce-root"><span>Expect</span><span> a response similar to the following</span><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1f63113a-ae33-446b-bdde-05fa4cf621c6.png" style="width:51.08em;height:14.00em;" width="1712" height="468" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1f63113a-ae33-446b-bdde-05fa4cf621c6.png"></p>
<p>As expected, all three core microservices that respond are now v2 versions; that is, as a canary tester, we are routed to the new v2 versions!</p>
<p>Given that the canary tests returned the expected results, we are ready to allow normal users to be routed to the new v2 versions using blue/green deployment.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running blue/green tests</h1>
                </header>
            
            <article>
                
<p>To route parts of the normal users to the new v2 versions of the microservices, we have to modify the weight distribution in the virtual services. They are currently 100/0; in other words, all traffic is routed to the old v1 versions. We can achieve this as we did before, that is, by editing the definition files of the virtual services in the <span><kbd>kubernetes/services/overlays/prod/istio</kbd> folder and then running a <kbd>kubectl apply</kbd> command to make the change take effect. As an alternative, we can use the <kbd>kubectl patch</kbd> command to change the weight distribution directly on the virtual service objects in the Kubernetes API server. </span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>I find the patch command useful when making a number of changes to the same objects to try something out, for example, to change the <span>weight </span><span>distribution in the routing rules.</span> In this section, we will use the <kbd><span>kubectl patch</span></kbd><span>&nbsp;</span>command to quickly change the <span>weight </span><span>distribution in the routing rules</span><span>&nbsp;</span>between the v1 and v2 versions of the microservices. To get the state of a virtual service after a number of <kbd>kubectl patch</kbd> commands have been executed, a command such as<span>&nbsp;</span><kbd>kubectl get vs NNN -o yaml</kbd><span>&nbsp;</span>can be issued. For example, to get the state of the virtual service of the product microservice, issue the following command: <kbd>kubectl get vs product-vs -o yaml</kbd>.</p>
<p>Since we haven't used the <kbd>kubectl patch</kbd> command before and it can be a bit involved to start with, let's undertake a short introduction to how it works before we perform the green/blue deploy.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">A short introduction to the kubectl patch command</h1>
                </header>
            
            <article>
                
<p><span>The </span><kbd>kubectl patch</kbd><span> command can be used to update specific fields in an existing object in the Kubernetes API server. We will try the patch command on the virtual service for the review microservice, named <kbd>review-vs</kbd>.&nbsp;</span>The relevant parts of the definition for <span>the virtual service, </span><kbd>review-vs</kbd><span>, appear as follows:</span></p>
<pre>spec:<br>  http:<br>  - match:<br>    ...<br>  - route:<br>    - destination:<br>        host: review<br>        subset: old<br>      weight: 100<br>    - destination:<br>        host: review<br>        subset: new<br>      weight: 0 </pre>
<p>For the full source code, refer to <span><kbd>kubernetes/services/overlays/prod/istio/review-routing-virtual-service.yml</kbd>.</span></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>A sample<span> patch </span>command that changes the weight distribution of the routing to the v1 and v2 pods in the <kbd>review</kbd> microservice appears as follows:</p>
<pre><strong>kubectl patch virtualservice review-vs --type=json -p='[</strong><br><strong>  {"op": "add", "path": "/spec/http/1/route/0/weight", "value": "80"},</strong><br><strong>  {"op": "add", "path": "/spec/http/1/route/1/weight", "value": "20"}</strong><br><strong>]'</strong></pre>
<p>The command will configure the routing rules of the review microservice to route 80% of the requests to the old version, and 20% of the requests to the new version.</p>
<p><span>To specify that the <kbd>weight</kbd> value shall be changed in the <kbd>review-vs</kbd> virtual service, the <kbd>/spec/http/1/route/0/weight</kbd> path is given for the old version and <kbd>/spec/http/1/route/1/weight</kbd> for the new version. </span></p>
<p><span>The <kbd>0</kbd> and <kbd>1</kbd> in the path are used to specify the index of array elements in the definition of the virtual service. For example, <kbd>http/1</kbd> means the second element in the array under the <kbd>http</kbd> element. See the definition of the preceding <kbd>review-vs</kbd> virtual service.</span></p>
<p><span>From the preceding definition we can see that the second element is the <kbd>route</kbd> element. The first element with index <kbd>0</kbd> being the match element.</span></p>
<p>Now that we know a bit more about the <kbd>kubectl patch</kbd> command, we are ready to test a blue/green deployment.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Performing the blue/green deployment</h1>
                </header>
            
            <article>
                
<p>Now, it is time to gradually move more and more users to the new versions using blue/green deployment. To perform the deployment, <span>run the following steps:</span></p>
<ol>
<li>Ensure that the load test tool, <kbd>Siege</kbd>, is still running.<br>
It was started in the preceding <em>Verifying that all traffic initially goes to the v1 version of the microservices</em> section.</li>
<li>To allow 20% of the users to be routed to the new v2 <span>version of the review micro</span><span>service, we can patch the virtual service and change weights with the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl patch virtualservice review-vs --type=json -p='[</strong><br><strong>  {"op": "add", "path": "/spec/http/1/route/0/weight", "value": <br>  "80"},</strong><br><strong>  {"op": "add", "path": "/spec/http/1/route/1/weight", "value":  <br>  "20"}</strong><br><strong>  ]'</strong></pre>
<ol start="3">
<li>To observe the change in the routing rule, go to the Kiali web UI (<a href="http://kiali.istio-system.svc.cluster.local:20001/kiali">http://kiali.istio-system.svc.cluster.local:20001/kiali</a>) and select the graph view.</li>
<li>Change the edge label to <kbd>Requests percentage</kbd>.</li>
<li>Wait for a minute before the statics are updated in Kiali so that we can observe the change. Expect the graph in Kiali to show something like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a610a4da-ad35-4bbd-ab61-f30f72241355.png" style="width:54.75em;height:44.33em;" width="2406" height="1950" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/a610a4da-ad35-4bbd-ab61-f30f72241355.png"></p>
<p>Depending on how long you have waited, the graph might look a bit different!<br>
In the screenshot, we can see that Istio now routes traffic to both the v1 and v2 versions of the <kbd>review</kbd> microservice.</p>
<p>Of the 33% of the traffic that is sent to the <kbd>review</kbd> microservice from the <kbd>product-composite</kbd> microservice, 7% are routed to the new v2 pod, and 26% to the old v1 pod. This means that 7/33 (= 21%) of the requests are routed to the v2 pod, and 26/33 (= 79%) to the v1 pod. This is in line with the 20/80 distribution we have requested:</p>
<ol start="1">
<li>Please feel free to try out the preceding <kbd>kubectl patch</kbd> command to affect the routing rules for the other core microservices: <kbd>product</kbd> and <kbd>recommendation</kbd>.</li>
<li>If you want to route all traffic to the v2 version of all microservices, you can run the following script:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>./kubernetes/scripts/route-all-traffic-to-v2-services.bash</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>You have to give Kiali a minute or two to collect metrics before it can visualize the changes in routing between the v1 and v2 versions of the microservices, but remember that the change in the actual routing is immediate!</span><br>
<br>
<span>Expect only </span><span>v2 versions of the microservices to show up in the graph after a while</span><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/fafbc4a4-4e90-47d5-8a5f-b933a548edf3.png" style="width:35.75em;height:31.83em;" width="1789" height="1588" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/fafbc4a4-4e90-47d5-8a5f-b933a548edf3.png"></p>
<p style="padding-left: 60px">Depending on how long you have waited, the graph might look a bit different!</p>
<ol start="3">
<li>If something goes terribly wrong after the upgrade to v2, the following command can be executed to revert all traffic back to the v1 version of all microservices:</li>
</ol>
<pre style="padding-left: 60px"><strong>./kubernetes/scripts/route-all-traffic-to-v1-services.bash</strong></pre>
<p>After a short while, the graph in Kiali should look like the screenshot in the previous <em>Verifying that all traffic initially goes to the v1 version of the microservices</em> section; that is, visualize that all requests go to the v1 version of all microservices again.</p>
<p>This concludes the introduction to the service mesh concept and Istio as an implementation of the concept.</p>
<p>Before we wrap up the chapter, let's recap how we can run tests in Docker Compose to ensure that the source code of our microservices does not rely on deployment in Kubernetes.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running tests with Docker Compose</h1>
                </header>
            
            <article>
                
<p>As mentioned in <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml" target="_blank">Chapter 17</a>, <em>Implementing Kubernetes Features as an Alternative</em> (refer to the <em>Verifying that the microservices work without Kubernetes</em> section), it is <span>important to ensure that the source code of the microservices doesn't become dependent on a platform such as Kubernetes or Istio from a functional perspective.</span></p>
<p><span>To verify that the microservices work as expected without the presence of Kubernetes and Istio, run the tests as described in <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 1<em>7</em></a><em>, Implementing Kubernetes Features as an Alternative</em> (refer to the T<em>esting with Docker Compose</em> section). Since the default values of the test script, <kbd>test-em-all.bash</kbd>, are changed, as described previously in the <em>Running commands to create the service mesh</em> section, the following parameters must be set when using Docker Compose: </span><span><kbd>HOST=localhost PORT=8443 HEALTH_URL=https://localhost:8443</kbd>. For example, to run the tests using the default Docker Compose file, </span><span><kbd>docker-compose.yml</kbd>, run the following command:</span></p>
<pre><strong><span>HOST=localhost PORT=8443 HEALTH_URL=https://localhost:8443 ./test-em-all.bash start stop</span></strong></pre>
<p><span>The tests should, as before, begin by starting all containers; it should then run the tests, and finally stop all containers. </span>For details of the expected output, see <a href="a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml">Chapter 17</a><span>, <em>Implementing Kubernetes Features as an Alternative</em> (refer to the <em>Verifying that the microservices work without Kubernetes</em> section). </span></p>
<p>After successfully executing the tests using Docker Compose, we have verified that the microservices are dependent neither on Kubernetes nor Istio from a functional perspective. These tests conclude the chapter on using Istio as a service mesh. </p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about the service mesh concept and Istio, an open source project that implements it. A service mesh provides capabilities for handling challenges in a system landscape of microservices in areas such as security, policy enforcement, resilience, and traffic management. A service mesh can also be used to make a system landscape of microservices observable by visualizing the traffic that flows through the microservices.</p>
<p>For observability, Istio uses Kiali, Jaeger, and Grafana (more on Grafana in <a href="5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml">Chapter 20</a>, <em>Monitoring Microservices</em>). <span>When it comes to security, Istio can be configured to use a certificate to protect external APIs with HTTPS and require that external requests contain valid JWT-based OAuth 2.0/OIDC access tokens. Finally, Istio can be configured to automatically protect internal communication using <strong>mutual authentication</strong> (<strong>mTLS</strong>).</span></p>
<p class="mce-root">For resilience and robustness, Istio comes with mechanisms for handling retries, timeouts, and an outlier detection mechanism similar to a circuit breaker. In many cases, it is preferable to implement these resilience capabilities in the source code of the microservices, if possible. The ability in Istio to inject faults and delays is very useful for verifying that the microservices in the service mesh work together as a resilient and robust system landscape. Istio can also be used to handle zero-downtime deployments. Using its fine-grained routing rules, both canary and blue/green deployments can be performed.</p>
<p class="mce-root">One important area that we haven't covered yet is how to collect and analyze log files created by all microservice instances. In the next chapter, we will see how this can be done using a popular stack of tools, known as the EFK stack, based on Elasticsearch, Fluentd, and Kibana.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the purpose of a proxy component in a service mesh?</li>
<li>What's the difference between a control plane and a data plane <span>in a service mesh?</span></li>
<li>What is the <kbd>istioctl kube-inject</kbd> <span>command </span>used for?</li>
<li>What is the <kbd>minikube tunnel</kbd> <span> command </span>used for?</li>
<li>What tools are used in Istio for observability?</li>
<li>What configuration is required to make Istio protect communication within the service mesh using mutual authentication?</li>
<li>What can the <kbd>abort</kbd> and <kbd>delay</kbd> elements in a virtual service be used for?</li>
<li>What configuration is required to set up a blue/green deploy scenario?</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Centralized Logging with the EFK Stack</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this chapter, we will learn how to collect and store log records from microservice instances, as well as how to search and analyze log records. As we mentioned in</span> <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a><span>,</span> <em>Introduction to Microservices</em> <span>(refer to the</span> <em>Centralized log analysis</em> <span>section), it is difficult to get an overview of what is going on in a system landscape of microservices when each microservice instance writes log records to its local filesystem. We need a component that can collect the log records from the microservice's local filesystem and store them in a central database for analysis, search, and visualization. A popular open source-based solution for this builds on the following tools:</span></p>
<ul>
<li><strong>Elasticsearch</strong><em>,</em> a distributed database with great capabilities for the search and analysis of large datasets</li>
<li><strong>Fluentd</strong>, a data collector that can be used to collect log records from various sources, filter and transform the collected information, and finally send it to various consumers, for example, Elasticsearch</li>
<li><strong>Kibana</strong>, a graphical frontend to Elasticsearch that can be used to visualize search results and run analyses of the collected log records</li>
</ul>
<p>Together, these tools are called the <strong>EFK stack</strong>, named after the initials of each tool.</p>
<p class="mce-root">The following topics will be covered in this chapter:</p>
<ul>
<li>Configuring Fluentd</li>
<li class="mce-root"><span>Deploying the EFK stack on Kubernetes for development and test usage</span></li>
<li>Trying out the EFK stack by:
<ul>
<li>Analyzing the collected log records</li>
<li>Discovering log records from the microservices and finding related log records</li>
<li>Performing root cause analysis</li>
</ul>
</li>
</ul>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All of the commands that are described in this book<span> have been run on a MacBook Pro using macOS Mojave but should be straightforward to modify so that they can be run on another platform, such as Linux or Windows.</span></p>
<p>No new tools need to be installed in this chapter.</p>
<p>The source code for this chapter can be found in this book's<span> GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19</a>.</p>
<p>To be able to run the commands that are described in this book, you need to download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, which points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter19</span></strong></pre>
<p><span>All of the source code examples in this chapter come from the source code in <kbd>$BOOK_HOME/Chapter19</kbd> and have been tested using Kubernetes 1.15.</span></p>
<p>If you want to take a look at the changes we applied to the source code in this chapter, that is, look at the changes we made so that we can use the EFK stack for centralized log analyses<span>,&nbsp;</span>you can compare it with the source code for <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>.&nbsp;<span>You can use your favorite <kbd>diff</kbd> tool and compare the two folders, <kbd>$BOOK_HOME/Chapter18</kbd> and <kbd>$BOOK_HOME/Chapter19</kbd>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Configuring Fluentd</h1>
                </header>
            
            <article>
                
<p>In this section we will learn the basics for how to configure Fluentd. Before we do that, let's learn a bit about the background of Fluentd and how it works on a high level.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introducing Fluentd</h1>
                </header>
            
            <article>
                
<p>Historically, one of the most<span>&nbsp;</span>popular<span>&nbsp;</span>open source stacks for handling log records has been the ELK stack from Elastic (<a href="https://www.elastic.co">https://www.elastic.co</a>), based on Elasticsearch, Logstash (used for log collection and transformation), and Kibana. Since Logstash runs on a Java VM, it requires a relatively large amount of memory. Over the years, a number of open source alternatives have been developed that require significantly less memory than Logstash, one of them being Fluentd (<a href="https://www.fluentd.org">https://www.fluentd.org</a>).</p>
<p>Fluentd is managed by the <strong>Cloud Native Computing Foundation</strong>&nbsp;(<strong>CNCF</strong>) (<a href="https://www.cncf.io">https://www.cncf.io</a>), that is, the same organization that manages the<span>&nbsp;</span>Kubernetes<span>&nbsp;</span>project. Therefore, Fluentd has become a natural choice as an open source-based log collector that runs in Kubernetes. Together with <span>Elastic and Kibana, it forms the EFK stack.</span></p>
<p>Fluentd is written in a mix of C and Ruby, using C for the performance-critical parts and Ruby where flexibility is of more importance, for example, allowing the simple installation of third-party plugins using Ruby's<span>&nbsp;</span><kbd>gem install</kbd><span>&nbsp;</span>command.</p>
<p>A log record is processed as an event in<span>&nbsp;</span>Fluentd<span>&nbsp;</span>and consists of the following information:</p>
<ul>
<li>A<span>&nbsp;</span><kbd>time</kbd><span>&nbsp;</span>field describing when the log record was created</li>
<li>A&nbsp;<kbd>tag</kbd><span>&nbsp;</span>field that identifies what type of log record it isâthe tag is used by Fluentd's routing engine to determine how a log record shall be processed</li>
<li>A<span>&nbsp;</span><strong>record </strong>that contains the<span>&nbsp;</span>actual<span>&nbsp;</span>log information, which is stored as a JSON object</li>
</ul>
<p>A Fluentd configuration file is used to tell Fluentd how to collect, process, and finally send log records to various targets, such as Elasticsearch. A configuration file consists of the following types of core elements:</p>
<ul>
<li><span><kbd>&lt;source&gt;</kbd>: S</span>ource elements describe where<span>&nbsp;</span>Fluentd<span>&nbsp;</span>will collect log records. For example, tailing log files that have been written to by Docker containers. Source elements typically tag the log records, describing the type of log record. It could, for example, be used to tag log records to state that they come from containers running in Kubernetes.</li>
<li><span><kbd>&lt;filter&gt;</kbd>: F</span>ilter elements are used to process the log records, for example, a filter element can parse log records that come from Spring Boot-based microservices and extract interesting parts of the log message into separate fields in the log record. Extracting information into separate fields in the log record makes the information searchable by Elasticsearch. A filter element selects what log records to process based on their tags. </li>
<li><span><kbd>&lt;match&gt;</kbd>: O</span>utput elements are used to perform two main tasks:
<ul>
<li>Send processed log records to targets such as Elasticsearch.</li>
<li>Routing is to decide how to process log records. A routing rule can rewrite the tag and reemit the log record into the Fluentd routing engine <span>for further processing.</span> A routing rule is <span>expressed as an embedded </span><kbd>&lt;rule&gt;</kbd><span> element inside the <kbd>&lt;match&gt;</kbd> element. </span>Output elements <span>decide what log records to process, in the same way as a filter: based on the tag of the log records. </span></li>
</ul>
</li>
</ul>
<p>Fluentd comes with a number of built-in and external third-party plugins that are used by the source, filter, and output elements. We will see some of them in action when we walk through the configuration file in the next section. For more information on the available plugins, see Fluentd's documentation, which is<span>&nbsp;</span>available<span>&nbsp;</span>at <a href="https://docs.fluentd.org">https://docs.fluentd.org</a>.</p>
<p>With this introduction to Fluentd out of the way, we are ready to see how<span>&nbsp;</span>Fluentd<span>&nbsp;</span>can be configured to process the log records from our microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Configuring Fluentd</h1>
                </header>
            
            <article>
                
<p>The configuration of Fluentd is based on the <span>configuration files from a Fluentd project on GitHub, <kbd>fluentd-kubernetes-daemonset</kbd>. The project contains Fluentd configuration files for how to collect log records from containers that run in Kubernetes and how to send them to Elasticsearch once they have been processed. We can reuse this configuration without changes and it will simplify our own configuration to a great extent. The Fluentd configuration files can be found at <a href="https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf">https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf</a>.&nbsp;</span></p>
<p><span>The configuration files that provide this functionality are <kbd>kubernetes.conf</kbd> and <kbd>fluent.conf</kbd>.&nbsp;</span><span>The </span><kbd>kubernetes.conf</kbd> configuration file <span>contains the following information:</span></p>
<ul>
<li><span>Source elements that tail container log files and log files from processes that run outside of Kubernetes, for example, the <kbd>kubelet</kbd> and the Docker daemon. </span>The source elements also tag the log records from Kubernetes with<span> the full name of the log file with <kbd>/</kbd> replaced by <kbd>.</kbd> and prefixed with </span><kbd>kubernetes</kbd>. Since the tag is based on the full filename, the name contains the name of the namespace, pod, and container, among other things. So, the tag is very useful for finding log records of interest by matching the tag. For example, the tag from the <kbd><span>product-composite</span></kbd><span>&nbsp;</span>microservice could be something like <kbd><span>kubernetes.var.log.containers.product-composite-7...s_hands-on_comp-e...b.log</span></kbd><span>, while t</span>he tag for the corresponding <kbd>istio-proxy</kbd> in the same pod could be something like <kbd><span>kubernetes.var.log.containers.product-composite-7...s_hands-on_istio-proxy-1...3.log</span></kbd><span>.</span></li>
<li>A filter element that enriches the log records that come from containers running inside Kubernetes, along with Kubernetes-specific fields that contain information such as the names of the <span>containers and the </span>namespace they run in.</li>
</ul>
<p><span>The main configuration file, <kbd>fluent.conf</kbd>, contains the following information:</span></p>
<ul>
<li><span><kbd>@include</kbd> statements for other configuration files, for example, the <kbd>kubernetes.conf</kbd> file we described previously. It also includes custom configuration files that are placed in a specific folder, making it very easy for us to reuse these configuration files without any changes and provide our own configuration file that only handles processing related to our own log records. We simply need to place our own configuration file in the folder specified by the <kbd>fluent.conf</kbd></span> file.</li>
<li><span>An output element that sends log records to Elasticsearch.</span></li>
</ul>
<p>As we described in the <em>Deploying Fluentd</em> section, these two configuration files will be packaged into the Docker image we will build for Fluentd<span>.&nbsp;</span></p>
<p><span>What's left to cover in our own configuration file is the following:</span></p>
<ul>
<li>Detect and parse Spring Boot formatted log records <span>from our microservices.</span></li>
<li>Handling multiline stack traces. Stack traces, for example, are written to log files using multiple lines. This makes it hard for Fluentd to handle a stack trace as a single log record. </li>
<li>Separating log records from the <kbd>istio-proxy</kbd> sidecars from the log records that were created by the microservices running in the same pod. The log records that are created by <kbd>istio-proxy</kbd> don't follow the same pattern as the log patterns that are created by our Spring Boot-based microservices. Therefore, they must be handled separately so that Fluentd doesn't try to parse them as Spring Boot formatted log records.</li>
</ul>
<p>To achieve this, the configuration is, to a large extent, based on using the <kbd>rewrite_tag_flter</kbd> plugin. This plugin can be used for routing log records based on the concept of changing the name of a tag and then reemitting the log record to the Fluentd routing engine. </p>
<p><span>This processing is summarized by the following UML activity diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4aa9c779-4a07-422d-af6c-00539062d30a.png" style="width:42.33em;height:38.42em;" width="1191" height="1079" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4aa9c779-4a07-422d-af6c-00539062d30a.png"></p>
<p><span>At a high level, the design of the configuration file looks as follows:</span></p>
<ul>
<li>The tags of all of the log records from Istio, including <kbd>istio-proxy</kbd>, are prefixed with<span>&nbsp;</span><kbd>istio</kbd><span>&nbsp;</span>so that they can be separated from the Spring Boot-based log records.</li>
<li>The tags of all of the log records from the<span>&nbsp;</span><kbd>hands-on</kbd><span>&nbsp;</span>namespace (except for the log records from <kbd>istio-proxy</kbd>) are prefixed with<span>&nbsp;</span><kbd>spring-boot</kbd>.</li>
<li>The log records from Spring Boot are checked for the presence of multiline stack traces. If the log record is part of a <span>multiline stack trace,</span><span>&nbsp;</span>it is processed by the third-party <span><kbd>detect-exceptions</kbd> plugin to recreate the stack trace. Otherwise, it is</span> parsed using a regular expression to extract information of interest. See the <em>Deploying Fluentd</em> section for details on this third-<span>party plugin.</span></li>
</ul>
<p>The <span><kbd>fluentd-hands-on.conf</kbd> configuration file </span>follows this activity diagram closely. The configuration file is placed inside a Kubernetes config map (see <kbd>kubernetes/efk/fluentd-hands-on-configmap.yml</kbd>). <span>Let's go through this step by step, as follows:</span></p>
<ol>
<li>First comes the definition of the config map and the filename of the configuration file, <span><kbd>fluentd-hands-on.conf</kbd>. It looks as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">apiVersion: v1<br>kind: ConfigMap<br>metadata:<br>  name: fluentd-hands-on-config<br>  namespace: kube-system<br>data:<br>  fluentd-hands-on.conf: |</pre>
<p style="padding-left: 60px">From the preceding source code, we understand that the <kbd>data</kbd> element will contain the configuration of Fluentd. It starts with the filename and uses a vertical bar <kbd>|</kbd><span> to mark the beginning of the embedded configuration file for Fluentd.</span></p>
<ol start="2">
<li>The first <kbd>&lt;match&gt;</kbd> element matches the log records from Istio, that is, tags that are prefixed with <kbd>kubernetes</kbd> and contain either <kbd>istio</kbd> as part of its namespace or part of its container name. It looks like this:</li>
</ol>
<pre style="padding-left: 60px">    &lt;match kubernetes.**istio**&gt;<br>      @type rewrite_tag_filter<br>      &lt;rule&gt;<br>        key log<br>        pattern ^(.*)$<br>        tag istio.${tag}<br>      &lt;/rule&gt;<br>    &lt;/match&gt;</pre>
<p style="padding-left: 60px"><span>Let's explain the preceding source code in more detail:</span></p>
<ul>
<li style="list-style-type: none">
<ul>
<li>The <kbd>&lt;match&gt;</kbd><span><span> element matches any tags that follow the <kbd>kubernetes.**istio**</kbd> pattern, that is, it starts with <kbd>kubernetes</kbd> and then contains the word <kbd>istio</kbd> somewhere in the tag name. <kbd>istio</kbd> can either come from the name of the namespace or the container; both are part of the tag.<br></span></span></li>
<li><span>The <kbd>&lt;match&gt;</kbd> element contains only one <kbd>&lt;rule&gt;</kbd> element, which prefixes the tag with <kbd>istio</kbd>. The <kbd>${tag}</kbd> variable holds the current value of the tag.</span></li>
<li><span><span>Since this is the only <kbd>&lt;rule&gt;</kbd> element in the <kbd>&lt;match&gt;</kbd> element, it is configured to match all of the log records like so:</span></span>
<ul>
<li><span><span>Since all of the log records that come from Kubernetes have a <kbd>log</kbd> field, the <kbd>key</kbd> field is set to <kbd>log</kbd>, that is, the rule looks for a <kbd>log</kbd> field in the log records. </span></span></li>
<li><span><span>To match any string in the <kbd>log</kbd> field, the <kbd>pattern</kbd> field is set to the </span></span><kbd>^(.*)$</kbd> regular expression. <span><kbd>^</kbd> marks the beginning of a string, while <kbd>$</kbd> marks the end of a string. <kbd>(.*)</kbd> matches any number of characters, except for line breaks.</span></li>
<li>The log records are <span>reemitted to the Fluentd routing engine. Since no other elements in the configuration file match tags starting with</span> <kbd>istio</kbd>,&nbsp;<span>they will be sent directly to the output element for Elasticsearch, which is defined in the</span> <kbd>fluent.conf</kbd><span> file we described previously</span><span>.</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="3">
<li>The second <kbd>&lt;match&gt;</kbd> element matches all of the log records from the <kbd>hands-on</kbd> namespace, that is, the log records that are emitted by our microservices. It looks like this:</li>
</ol>
<pre style="padding-left: 60px">    &lt;match kubernetes.**hands-on**&gt;<br>      @type rewrite_tag_filter<br>      &lt;rule&gt;<br>        key log<br>        pattern ^(.*)$<br>        tag spring-boot.${tag}<br>      &lt;/rule&gt;<br>    &lt;/match&gt;</pre>
<p style="padding-left: 60px"><span>From the preceding source code we can see that:</span></p>
<ul>
<li style="list-style-type: none">
<ul>
<li><span>The log records emitted by our microservices use formatting rules for the log message defined by Spring Boot so their tags are prefixed with </span><kbd>spring-boot</kbd><span>. Then, they are re-emitted for further processing.</span></li>
<li>The <kbd>&lt;match&gt;</kbd> element is configured in the same way as the <kbd>&lt;match kubernetes.**istio**&gt;</kbd> element we looked at previously.</li>
</ul>
</li>
</ul>
<ol start="4">
<li>The third <kbd>&lt;match&gt;</kbd> element matches <kbd>spring-boot</kbd> log records and determines whether they are ordinary Spring Boot log records or are part of a multiline stack trace. <span>It looks like this</span>:</li>
</ol>
<pre style="padding-left: 60px">    &lt;match spring-boot.**&gt;<br>      @type rewrite_tag_filter<br>      &lt;rule&gt;<br>        key log<br>        pattern /^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}\.\d{3}.*/<br>        tag parse.${tag}<br>      &lt;/rule&gt;<br>      &lt;rule&gt;<br>        key log<br>        pattern /^.*/<br>        tag check.exception.${tag}<br>      &lt;/rule&gt;<br>    &lt;/match&gt;</pre>
<p style="padding-left: 60px"><span>As seen in the preceding source code, t</span><span>his is determined by using two </span><kbd>&lt;rule&gt;</kbd><span> elements:</span></p>
<ul>
<li style="list-style-type: none">
<ul>
<li><span>The first </span><span>uses a regular expression to check whether the </span><kbd>log</kbd><span> field in the log element starts with a timestamp or not.</span></li>
<li><span>If the <kbd>log</kbd> field starts with a timestamp, the log record is treated as an ordinary Spring Boot log record and its tag is prefixed with </span><kbd>parse</kbd><span>.</span></li>
<li><span>Otherwise, the second <kbd>&lt;rule&gt;</kbd> element will match and the log record is handled as a multiline log record. Its tag is prefixed with </span><kbd>check.exception</kbd><span>.</span></li>
<li><span>The log record is re-emitted in either case and its tag will either start with</span> <kbd>check.exception.spring-boot.kubernetes</kbd> <span>or</span> <kbd>parse.spring-boot.kubernetes</kbd><span><span> after this process.</span></span></li>
</ul>
</li>
</ul>
<ol start="5">
<li>In the fourth <kbd>&lt;match&gt;</kbd> element, the selected log records have a tag that starts with <kbd>check.exception.spring-boot</kbd><span>, that is, log records that are part of a multiline stack trace. It looks like this</span><span>:</span></li>
</ol>
<pre style="padding-left: 60px">    &lt;match check.exception.spring-boot.**&gt;<br>      @type detect_exceptions<br>      languages java<br>      remove_tag_prefix check<br>      message log<br>      multiline_flush_interval 5<br>    &lt;/match&gt;</pre>
<p style="padding-left: 60px"><span>The source code for the <kbd>detect_exceptions</kbd> plugin works like:</span></p>
<ul>
<li style="list-style-type: none">
<ul>
<li><span>The</span><span>&nbsp;</span><kbd>detect_exceptions</kbd><span>&nbsp;</span><span>plugin is used to combine multiple one-line log records into a single log record that contains a complete stack trace.</span></li>
<li><span>Before a multiline log record is reemitted into the routing engine, the</span><span>&nbsp;</span><kbd>check</kbd><span>&nbsp;</span><span>prefix is removed from the tag to prevent a never-ending processing loop of the log record.</span></li>
</ul>
</li>
</ul>
<ol start="6">
<li>Finally, the configuration file consists of a filter element that parses Spring Boot log messages using a regular expression, extracting information of interest. It looks like this:</li>
</ol>
<pre style="padding-left: 60px">    &lt;filter parse.spring-boot.**&gt;<br>      @type parser<br>      key_name log<br>      time_key time<br>      time_format %Y-%m-%d %H:%M:%S.%N<br>      reserve_data true<br>      format /^(?&lt;time&gt;\d{4}-\d{2}-<br>      \d{2}\s\d{2}:\d{2}:\d{2}\.\d{3})\s+<br>      (?&lt;spring.level&gt;[^\s]+)\s+<br>      (\[(?&lt;spring.service&gt;[^,]*),(?&lt;spring.trace&gt;[^,]*),(?<br>      &lt;spring.span&gt;[^,]*),[^\]]*\])\s+<br>      (?&lt;spring.pid&gt;\d+)\s+---\s+\[\s*(?&lt;spring.thread&gt;[^\]]+)\]\s+<br>      (?&lt;spring.class&gt;[^\s]+)\s*:\s+<br>      (?&lt;log&gt;.*)$/<br>    &lt;/filter&gt;</pre>
<p style="padding-left: 60px"><span>Let's explain the preceding source code in more detail</span><span>:</span></p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Note that filter elements don't re-emit log records; instead, they just pass them on to the next element in the configuration file that matches the log record's tag.</li>
<li>The following fields are extracted from the Spring Boot log message that's stored in the <kbd>log</kbd> <span>field in the log record:</span>
<ul>
<li><kbd>&lt;time&gt;</kbd>: The timestamp for when the log record was created</li>
<li><kbd>&lt;spring.level&gt;</kbd>: The log level of the log record, for example, <kbd>FATAL</kbd>, <kbd>ERROR</kbd>, <kbd>WARN</kbd>, <kbd>INFO</kbd>, <kbd>DEBUG</kbd>, or <kbd>TRACE</kbd></li>
<li><span><kbd>&lt;spring.service&gt;</kbd>: The </span>name of the microservice</li>
<li><kbd>&lt;spring.trace&gt;</kbd>: The trace ID used to perform distributed tracing</li>
<li><kbd>&lt;spring.span&gt;</kbd>: The span ID, the ID of the part of the distributed processing that this microservice executed</li>
<li><kbd>&lt;spring.pid&gt;</kbd>: The process ID</li>
<li><kbd>&lt;spring.thread&gt;</kbd>: The thread ID</li>
<li><kbd>&lt;spring.class&gt;</kbd>: The name of the Java class </li>
<li><kbd>&lt;log&gt;</kbd>: The actual log message</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="packt_infobox">The names of Spring Boot-based microservices are specified using the <kbd>spring.application.name</kbd> property. This property has been added to each microservice-specific property file in the config repository,<span>&nbsp;</span><span>in the</span> <kbd>config-repo</kbd><span> folder.</span></div>
<p>Getting regular expressions right can be challenging, to say the least. Thankfully, there are several websites that can help. When it comes to using regular expressions together with Fluentd, I recommend using the following site: <a href="https://fluentular.herokuapp.com/">https://fluentular.herokuapp.com/</a>.</p>
<p>Now that you've been introduced to how Fluentd works and how the configuration file is constructed, we are ready to deploy the EKF stack.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying the EFK stack on Kubernetes</h1>
                </header>
            
            <article>
                
<p>Deploying the EFK stack on Kubernetes will be done in the same way as we have deployed our own microservices: using Kubernetes definition files for objects such as deployments, services, and configuration maps.</p>
<p>The deployment of the EFK stack is divided into two parts:</p>
<ul>
<li>One part where we deploy Elasticsearch and Kibana </li>
<li>One part where we deploy Fluentd</li>
</ul>
<p>But first, we need to build and deploy our own microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building and deploying our microservices</h1>
                </header>
            
            <article>
                
<p>Building, deploying, and verifying the deployment using the <kbd>test-em-all.bash</kbd> test script is done in the same way as it was done in <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>,&nbsp;<em>Using a Service Mesh to Improve Observability and Management</em>, in the <em>Running commands to create the service mesh</em> section. Run the following commands to get started:</p>
<ol>
<li class="mce-root"><span>First, build the Docker images from the source with the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>cd $BOOK_HOME/Chapter19</strong><br><strong>eval $(minikube docker-env)</strong><br><strong>./gradlew build &amp;&amp; docker-compose build</strong></pre>
<ol start="2">
<li>Recreate the namespace, <kbd>hands-on</kbd>, and set it as the default namespace:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl delete namespace hands-on</strong><br><strong>kubectl create namespace hands-on</strong><br><strong>kubectl config set-context $(kubectl config current-context) --namespace=hands-on </strong></pre>
<ol start="3">
<li>Execute the deployment by running the <kbd>deploy-dev-env.bash</kbd> script with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>./kubernetes/scripts/deploy-dev-env.bash</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<p class="mceNonEditable"></p>
<ol start="4">
<li>Start the Minikube tunnel, if it's not already running (see <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml" target="_blank">Chapter 18</a>,&nbsp;<span><em>Using a Service Mesh to Improve Observability and Management</em>,</span> the <em>Setting up access to Istio services</em> section for a recap, if required<span>):</span></li>
</ol>
<pre style="padding-left: 60px"><strong>minikube tunnel</strong></pre>
<p style="padding-left: 60px">Remember that this command requires that your user has <kbd>sudo</kbd> privileges and that you enter your password during startup and shutdown. It takes a couple of seconds before the command asks for the password, so it is easy to miss!</p>
<ol start="5">
<li>Run the normal tests to verify the deployment with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>./test-em-all.bash</strong></pre>
<p style="color: black;padding-left: 60px">Expect the output to be similar to what we have seen from the previous chapters:</p>
<p style="color: black" class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/042de643-01de-4776-8b29-ab05be9fe519.png" width="1883" height="420" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/042de643-01de-4776-8b29-ab05be9fe519.png"></p>
<ol start="6">
<li>You <span>can also try out the APIs manually by running the following commands:</span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong><br><br><strong>curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" | jq .productId</strong></pre>
<p style="padding-left: 60px"><span>Expect the requested</span> product ID<span>,&nbsp;</span><kbd>2</kbd><span>, in the response.</span></p>
<p>With the microservices deployed, we can move on and deploy Elasticsearch and Kibana!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying Elasticsearch and Kibana</h1>
                </header>
            
            <article>
                
<p>We will deploy Elasticsearch and Kibana to its own namespace, <kbd>logging</kbd>. Both Elasticsearch and Kibana will be deployed for development and test usage using a Kubernetes deployment object. This will be done with a single pod and a Kubernetes node port service. The<span>&nbsp;</span>services will expose the standard ports for <span>Elasticsearch and Kibana </span>internally in the Kubernetes cluster, that is, port <kbd>9200</kbd> for Elasticserach and port <kbd>5601</kbd> for Kibana. Thanks to the <kbd>minikube tunnel</kbd> command, we will be able to access these services locally using the following URLs:</p>
<ul>
<li><kbd>elasticsearch.logging.svc.cluster.local:9200</kbd> for Elasticserch</li>
<li><kbd>kibana.logging.svc.cluster.local:5601</kbd> <span>for Kibana</span></li>
</ul>
<div class="packt_infobox">For the recommended deployment in a production environment on Kubernetes, see <a href="https://www.elastic.co/elasticsearch-kubernetes">https://www.elastic.co/elasticsearch-kubernetes</a>.&nbsp;</div>
<p>We will use the versions that were available when this chapter was written:</p>
<ul>
<li><span>Elasticsearch version 7.3.0</span></li>
<li>Kibana <span>version 7.3.0</span></li>
</ul>
<p>Before we perform the deployments, let's look at the most interesting parts of the definition files.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">A walkthrough of the definition files</h1>
                </header>
            
            <article>
                
<p>The definition file for Elasticsearch, <kbd>kubernetes/efk/elasticsearch.yml</kbd>, contains a standard Kubernetes deployment and service object that we have seen multiples times before, for example, in <a href="87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml">Chapter 15</a>, <em>Introduction to Kubernetes</em>, in the <em>Trying out a sample deployment</em> section. The most interesting part, as we explained previously, of the <span>definition </span>file is the following:</p>
<pre>apiVersion: extensions/v1beta1<br>kind: Deployment<br>...<br>      containers:<br>      - name: elasticsearch<br>        image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.3.0<br>        resources:<br>          limits:<br>            cpu: 500m<br>            memory: 2Gi<br>          requests:<br>            cpu: 500m<br>            memory: 2Gi</pre>
<p>Let's explain the preceding source code in detail:</p>
<ul>
<li>We use an official Docker image from Elastic that's available at <kbd>docker.elastic.co</kbd> with a package that only contains open source components. This is ensured by using the <kbd>-oss</kbd> suffix on the name of the Docker image, <kbd>elasticsearch-oss</kbd>. The version is set to <kbd>7.3.0</kbd>.</li>
<li>The Elasticsearch container is allowed to allocate a relatively large amount of memoryâ2 GBâto be able to perform queries with good performance. The more memory, the better the performance.</li>
</ul>
<p><span>The definition file for Kibana, </span><kbd>kubernetes/efk/kibana.yml</kbd><span>, also contains a standard Kubernetes deployment and service object. </span>The most interesting parts in the definition file are as follows:</p>
<pre>apiVersion: extensions/v1beta1<br>kind: Deployment<br>...<br>      containers:<br>      - name: kibana<br>        image: docker.elastic.co/kibana/kibana-oss:7.3.0<br>        env:<br>        - name: ELASTICSEARCH_URL<br>          value: http://elasticsearch:9200</pre>
<p><span>Let's explain the preceding source code in detail:</span></p>
<ul>
<li>For Kibana, we also use an official Docker image <span>from Elastic that's available at</span><span>&nbsp;</span><kbd>docker.elastic.co</kbd>, along with a package that only contains open source components,<span>&nbsp;</span><kbd>kibana-oss</kbd>.&nbsp;<span>The version is set to </span><kbd>7.3.0</kbd><span>.</span></li>
<li>To connect Kibana with the Elasticsearch pod, an environment variable, <kbd>ELASTICSEARCH_URL</kbd>, is defined to specify the address to the <span>Elasticsearch service, <kbd>http://elasticsearch:9200</kbd>.</span></li>
</ul>
<p>With these insights, we are ready to perform the deployment of <span>Elasticsearch and Kibana.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running the deploy commands</h1>
                </header>
            
            <article>
                
<p>Deploy Elasticsearch and Kibana by performing the following steps:</p>
<ol>
<li>Create a namespace for <span><span>Elasticsearch and Kibana with the following command:</span></span><span><span><br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl create namespace logging</strong></pre>
<ol start="2">
<li><span>To make the deploy steps run faster, prefetch the Docker images for Elasticsearch and Kibana with the following commands:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>eval $(minikube docker-env)</strong><br><strong>docker pull docker.elastic.co/elasticsearch/elasticsearch-oss:7.3.0</strong><br><strong>docker pull docker.elastic.co/kibana/kibana-oss:7.3.0</strong></pre>
<ol start="3">
<li class="mce-root"><span>Deploy Elasticsearch and wait for its pod to be ready with the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl apply -f kubernetes/efk/elasticsearch.yml -n logging</strong><br><strong>kubectl wait --timeout=120s --for=condition=Ready pod -n logging --all</strong> </pre>
<ol start="4">
<li class="mce-root"><span>Verify that </span><span>Elasticsearch is up and running with the following command:</span><span><br></span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>curl http://elasticsearch.logging.svc.cluster.local:9200 -s | jq -r .tagline</strong></pre>
<p class="mce-root"><span>Expect <kbd>You Know, for Search</kbd> as a response.<br></span></p>
<div class="packt_infobox">Depending on your hardware, you might need to wait for a minute or two before Elasticsearch responds with this message.</div>
<ol start="5">
<li><span><span>Deploy Kibana and wait for its pod to be ready with the following commands:<br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl apply -f kubernetes/efk/kibana.yml -n logging</strong><br><strong>kubectl wait --timeout=120s --for=condition=Ready pod -n logging --all</strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p class="mceNonEditable"></p>
<ol start="6">
<li><span>Verify that </span><span><span>Kibana is up and running with the following command:<br></span></span></li>
</ol>
<pre style="padding-left: 60px"><strong>curl -o /dev/null -s -L -w "%{http_code}\n" http://kibana.logging.svc.cluster.local:5601</strong></pre>
<p style="padding-left: 60px"><span>Expect <kbd>200</kbd> as the response.</span></p>
<p>With <span>Elasticsearch and Kibana deployed, we can start to deploy Fluentd.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Deploying Fluentd</h1>
                </header>
            
            <article>
                
<p class="mce-root">Deploying Fluentd is a bit more complex compared to deploying <span>Elasticsearch and Kibana. To deploy Fluentd, we will use a Docker image that's been published by the Fluentd project on Docker Hub, </span><kbd>fluent/fluentd-kubernetes-daemonset</kbd><span>, and sample the Kubernetes definition files from a Fluentd project on GitHub, <kbd>fluentd-kubernetes-daemonset</kbd>. It is located at <a href="https://github.com/fluent/fluentd-kubernetes-daemonset">https://github.com/fluent/fluentd-kubernetes-daemonset</a>. As it's implied by the name of the project, Fluentd will be deployed as a daemon set, running one pod per node in the Kubernetes cluster. Each Fluentd pod is responsible for collecting log output from processes and containers that run on the same node as the pod. Since we are using Minikube, that is, a single node cluster, we will only have one Fluentd pod. </span></p>
<p class="mce-root"><span>To handle multiline log records that contain stack traces from exceptions, we will use a third-party Fluentd plugin provided by Google, <kbd>fluent-plugin-detect-exceptions</kbd>, which is available at <a href="https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions">https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions</a>.&nbsp;</span>To be able to use this<span>&nbsp;</span>plugin, we will build our own Docker image <span>where the <kbd>fluent-plugin-detect-exceptions</kbd> plugin will be installed. Fluentd's Docker image, <kbd>fluentd-kubernetes-daemonset</kbd>, will be used as the base image.</span></p>
<p><span>We will use the versions that were available when this chapter was written:</span></p>
<ul>
<li><span>Fluentd version</span><span> 1.4.2</span></li>
<li><span>fluent-plugin-detect-exceptions version 0.0.12 </span></li>
</ul>
<p><span>Before we perform the deployments, let's look at the most interesting parts of the definition files.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">A walkthrough of the definition files</h1>
                </header>
            
            <article>
                
<p><span>The Dockerfile that's used to build the Docker image, <kbd>kubernetes/efk/Dockerfile</kbd>, looks as follows:</span></p>
<pre>FROM fluent/fluentd-kubernetes-daemonset:v1.4.2-debian-elasticsearch-1.1<br><br>RUN gem install fluent-plugin-detect-exceptions -v 0.0.12 \<br> &amp;&amp; gem sources --clear-all \<br> &amp;&amp; rm -rf /var/lib/apt/lists/* \<br>           /home/fluent/.gem/ruby/2.3.0/cache/*.gem</pre>
<p>Let's explain the preceding source code in detail:</p>
<ul>
<li><span><span><span>The base image is Fluentd's Docker image, <kbd>fluentd-kubernetes-daemonset</kbd>. The <kbd>v1.4.2-debian-elasticsearch-1.1</kbd> tag specifies that version v1.4.2 shall be used with a package that contains</span></span></span><span><span> built-in support for sending log records to Elasticsearch. The base Docker image contains the Fluentd configuration files that were mentioned in the <em>Configuring Fluentd</em> section.</span></span></li>
<li>The Google plugin, <span><kbd>fluent-plugin-detect-exceptions</kbd>, is installed using Ruby's package manager, <kbd>gem</kbd>.</span></li>
</ul>
<p class="mce-root">The definition file of the daemon set, <kbd>kubernetes/efk/fluentd-ds.yml</kbd>, is based on a sample definition file in the <kbd><span>fluentd-kubernetes-daemonset</span></kbd> project, which can be found at <a href="https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml">https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml</a>. This file is a bit complex, so let's go through the most interesting parts separately:</p>
<ol>
<li class="mce-root">First, here's the declaration of the daemon set:</li>
</ol>
<pre style="padding-left: 60px">apiVersion: extensions/v1beta1<br>kind: DaemonSet<br>metadata:<br>  name: fluentd<br>  namespace: kube-system</pre>
<p style="padding-left: 60px">Let's explain the preceding source code in detail:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>The <kbd>kind</kbd> key specifies that this is a daemon set.</li>
<li>The <kbd>namespace</kbd> key specifies that the daemon set shall be created in the <kbd>kube-system</kbd> namespace and not in the <kbd>logging</kbd> namespace where Elasticseach and Kibana are deployed.</li>
</ul>
</li>
</ul>
<ol start="2">
<li>The next part specifies the template for the pods that are created by the daemon set. T<span>he most interesting parts are as follows:</span></li>
</ol>
<pre style="padding-left: 60px">spec:<br>  template:<br>    spec:<br>      containers:<br>      - name: fluentd<br>        image: hands-on/fluentd:v1<br>        env:<br>          - name: FLUENT_ELASTICSEARCH_HOST<br>            value: "elasticsearch.logging"<br>          - name: FLUENT_ELASTICSEARCH_PORT<br>            value: "9200"</pre>
<p style="padding-left: 60px"><span>Let's explain the preceding source code in detail:</span></p>
<ul>
<li style="list-style-type: none">
<ul>
<li>The Docker image that's used for the pods is <kbd>hands-on/fluentd:v1</kbd>. We will build this Docker image after walking through the definition files using the Dockerfile we described previously.</li>
<li>A number of environment variables are supported by the Docker image and are used to customize it. The two most important ones are as follows:
<ul>
<li><kbd>FLUENT_ELASTICSEARCH_HOST</kbd>, which specifies the hostname of the Elasticsearch service, that is, <kbd>elasticsearch.logging</kbd></li>
<li><kbd>FLUENT_ELASTICSEARCH_PORT</kbd><span>, which specifies the port that's used to communicate with </span><span>Elasticsearch, that is, </span><kbd>9200</kbd></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="packt_infobox">Since the Fluentd pod runs in another namespace than <span>Elasticsearch, the hostname cannot be specified using its short name, that is, <kbd>elasticsearch</kbd>.</span> <span>Instead, the namespace part of the DNS name must also be specified, that is, <kbd>elasticsearch.logging</kbd>. As an alternative, the <strong>fully qualified domain name</strong> (<strong>FQDN</strong>), <kbd>elasticsearch.logging.svc.cluster.local</kbd>, can also be used. But since the last part of the DNS name, <kbd>svc.cluster.local</kbd>, is shared by all DNS names inside a Kubernetes cluster, it does not need to be specified.<br></span></div>
<ol start="3">
<li>Finally, a number of volumes, that is, filesystems, are mapped into the pod, as follows:</li>
</ol>
<pre style="padding-left: 60px">        volumeMounts:<br>        - name: varlog<br>          mountPath: /var/log<br>        - name: varlibdockercontainers<br>          mountPath: /var/lib/docker/containers<br>          readOnly: true<br>        - name: journal<br>          mountPath: /var/log/journal<br>          readOnly: true<br>        - name: fluentd-extra-config<br>          mountPath: /fluentd/etc/conf.d<br>      volumes:<br>      - name: varlog<br>        hostPath:<br>          path: /var/log<br>      - name: varlibdockercontainers<br>        hostPath:<br>          path: /var/lib/docker/containers<br>      - name: journal<br>        hostPath:<br>          path: /run/log/journal<br>      - name: fluentd-extra-config<br>        configMap:<br>          name: "fluentd-hands-on-config"</pre>
<p style="padding-left: 60px"><span>Let's explain the preceding source code in detail:</span></p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Three folders on the host (that is, the node) are mapped into the Fluentd pod. These folders contain the log files that Fluentd will tail and collect log records from. The folders are: <kbd>/var/log</kbd>, <kbd>/var/lib/docker/containers</kbd> and <kbd>/run/log/journal</kbd>.</li>
<li>Our own configuration file that specifies how Fluentd shall process log records from our microservices is mapped using a config map called <kbd>fluentd-hands-on-config</kbd> to the <kbd>/fluentd/etc/conf.d</kbd> folder. The base Docker image that's used for preceding Fluentd, <kbd>fluentd-kubernetes-daemonset</kbd>, configures Fluentd to include any configuration file that's found in the <span><kbd>/fluentd/etc/conf.d</kbd> folder. See the <em>Configuring Fluentd</em> section for details.</span></li>
</ul>
</li>
</ul>
<p>For the full source code of the definition file for the daemon set, see the <span><kbd>kubernetes/efk/fluentd-ds.yml</kbd> file.</span></p>
<p><span>Now that we've walked through everything, we are ready to perform the deployment of </span><span>Fluentd.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Running the deploy commands</h1>
                </header>
            
            <article>
                
<p>To deploy Fluentd, we have to build the Docker image, create the config map, and finally deploy the daemon set. Run the following commands to perform these steps:</p>
<ol>
<li class="mce-root"><span>Build the Docker image and tag it with</span> <kbd>hands-on/fluentd:v1</kbd> <span>using the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>eval $(minikube docker-env)</strong><br><strong>docker build -f kubernetes/efk/Dockerfile -t hands-on/fluentd:v1 kubernetes/efk/</strong></pre>
<ol start="2">
<li class="mce-root"><span>Create the config map, deploy Fluentd's daemon set, and wait for the pod to be ready with the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl apply -f kubernetes/efk/fluentd-hands-on-configmap.yml </strong><br><strong>kubectl apply -f kubernetes/efk/fluentd-ds.yml</strong><br><strong>kubectl wait --timeout=120s --for=condition=Ready pod -l app=fluentd -n kube-system</strong></pre>
<ol start="3">
<li class="mce-root"><span>Verify that the Fluentd pod is healthy with the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl logs -n kube-system $(kubectl get pod -l app=fluentd -n kube-system -o jsonpath={.items..metadata.name}) | grep "fluentd worker is now running worker"</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>Expect a response of</span>&nbsp;<kbd>2019-08-16 15:11:33 +0000 [info]: #0 fluentd worker is now running worker=0</kbd><span>.</span></p>
<ol start="4">
<li>Fluentd will start to collect a considerable amount of log records from the various processes and containers in the Minkube instance. After a minute or so, you can ask Elasticsearch how many log records have been collected with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>curl http://elasticsearch.logging.svc.cluster.local:9200/_all/_count</span></strong></pre>
<p class="mce-root"></p>
<p class="mce-root"></p>
<p>The command can be a bit slow the first time it is executed, but should return a response similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/14a27b89-c4e5-4d29-86c2-4652b7fbe575.png" width="770" height="93" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/14a27b89-c4e5-4d29-86c2-4652b7fbe575.png"></p>
<p><span>In this example, Elasticsearch contains <kbd>144750</kbd> log records.</span></p>
<p>This completes the deployment of the EFK stack. Now, it's time to try it out and find out what all of the collected log records are about!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the EFK stack</h1>
                </header>
            
            <article>
                
<p>The first thing we need to do before we can try out the EFK stack is initialize Kibana so it knows what search indices to use in Elasticsearch. Once that is done, we will try out the following, in my experience, common tasks:</p>
<ol>
<li>We will start by analyzing of what types of log records Fluentd has collected and stored in Elasticsearch. Kibana has a very useful visualization capability that can be used for this.</li>
<li>Next, we will learn how to discover log records from different microservices that belong to one and the same <span>processing of an external request to the API. </span>We will use the <strong>trace ID</strong> in the log records as a correlation ID to find related log records.</li>
<li>Thirdly, we will learn how to use Kibana to perform<span>&nbsp;<strong>root cause analysis</strong>, that is, find the actual reason for an</span> error.</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Initializing Kibana</h1>
                </header>
            
            <article>
                
<p>Before we start to use Kibana, we must specify what search indices to use in Elasticsearch and what field in the indices holds the timestamps for the log records.</p>
<p>Perform the following steps to initialize Kibana:</p>
<ol>
<li><span>Open Kibana's web UI using the <kbd>http://kibana.logging.svc.cluster.local:5601</kbd></span><span> URL in a web browser.</span></li>
<li><span>On the welcome pag</span>e, <span class="packt_screen">Welcome to Kibana</span><span>, click on the </span><span class="packt_screen">Explore on my own</span><span> button.</span></li>
</ol>
<ol start="3">
<li>Click on the <span class="packt_screen">Expand</span> button in the lower-left corner to view the names of the menu choices. These will be shown on the left-hand side.</li>
<li><span>Click on</span> <span class="packt_screen">Discover </span>in <span>the menu to the left. You will be asked to define a pattern that's used by Kibana to identify what Elasticsearch indices it shall retrieve log records from.<strong><br></strong></span></li>
<li>Enter the <kbd>logstash-*</kbd> index pattern and click on <span class="packt_screen">Next Step</span>.</li>
<li>On the next page, you will be asked to specify the name of the field that contains the timestamp for the log records. Click on the drop-down list for the <span class="packt_screen">Time Filter</span> field name and select the only available field, <span class="packt_screen">@timestamp</span>.</li>
<li>Click on the <span class="packt_screen">Create index pattern</span> button.</li>
<li>Kibana will show a page that summarizes the fields that are available in the selected indices.</li>
</ol>
<div class="packt_infobox">Indices are, by default, named <kbd>logstash</kbd> for historical reasons, even though it is Flutentd that is used for log collection.</div>
<p>With Kibana initialized, we are ready to examine the log records we have collected. </p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Analyzing the log records</h1>
                </header>
            
            <article>
                
<p>From the deployment of Fluentd, we know that it immediately started to collect a significant number of log records. So, the first thing we need to do is get an understanding of what types of log records Fluentd has collected and stored in Elasticsearch.</p>
<p>We will use Kibana's visualization feature to divide the log records per Kubernetes namespace and then ask Kibana to show us how the log records are divided per type of container within each namespace. A p<span>ie chart is a </span>suitable chart type for this type of analysis. <span>Perform the following steps to create a pie chart:</span></p>
<ol>
<li>In <span>Kibana's web UI, c</span>lick on <span class="packt_screen">Visualize</span> in the menu to the left.</li>
<li>Click on the <span class="packt_screen">Create new visualization</span> button.</li>
<li>Select <span class="packt_screen">Pie</span> as the visualization type.</li>
<li>Select <span class="packt_screen">logstash-*</span> as the source.</li>
<li><span>In the time picker (a date interval selector) above the pie chart, set a date interval of your choice (set to the last 7 days in the following screenshot). Click on its calendar icon to adjust the time interval.</span></li>
</ol>
<ol start="6">
<li>Click on <span class="packt_screen">Add</span> to create the first bucket, as follows:
<ol>
<li>Select the bucket type, that is, <span class="packt_screen">Split slices.</span></li>
<li>For the aggregation type, select<span>&nbsp;</span><span class="packt_screen">Terms </span>from the drop-down list.</li>
<li>As the field, select <span class="packt_screen">kubernetes.namespace_name.keyword.</span></li>
<li>For the size, select <span class="packt_screen">10.</span></li>
<li>Enable <span class="packt_screen">Group other values in separate bucket.</span></li>
<li>Enable <span class="packt_screen">Show missing values.</span></li>
<li>Press the <span class="packt_screen">Apply changes</span> button (the blue play icon above the <span class="packt_screen">Bucket</span> definition). Expect a pie chart that looks similar to the following: </li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1d79e158-3af1-495b-8415-e2b769b59454.png" style="width:44.08em;height:31.58em;" width="2361" height="1693" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1d79e158-3af1-495b-8415-e2b769b59454.png"></p>
<p style="padding-left: 60px"><span>We can see that the log records are divided over the namespaces we have been working with in the previous chapters: </span><kbd>kube-system</kbd><span>,&nbsp;</span><kbd>istio-system</kbd><span>,&nbsp;</span><kbd>logging</kbd><span>,&nbsp;</span><kbd>cert-manager</kbd><span>, and our own </span><kbd>hands-on</kbd><span> namespace. To see what containers have created the log records divided per namespace, we need to create a second bucket.</span></p>
<ol start="7">
<li>Click on <span class="packt_screen">Add</span> again to create a second bucket:
<ol>
<li>Select the bucket type, that is,<span>&nbsp;</span><span class="packt_screen">Split slices.</span></li>
<li>As the sub-aggregation type, select<span>&nbsp;</span><span class="packt_screen">Terms </span>from the drop-down list.</li>
<li>As the field, select<span>&nbsp;</span><span class="packt_screen">kubernetes.container_name.keyword.</span></li>
<li>For the size, select<span>&nbsp;</span><span class="packt_screen">10.</span></li>
<li>Enable <span class="packt_screen">Group other values in separate bucket.</span></li>
<li>Enable<span>&nbsp;</span><span class="packt_screen">Show missing values</span>.</li>
<li>Press the<span>&nbsp;</span><span class="packt_screen">Apply changes</span><span>&nbsp;</span><span>button</span><span> again. </span><span>E</span><span>xpect a pie chart that looks similar to the following: </span></li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><br>
<img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/660ceb82-205e-401d-8595-5a23cc328e26.png" width="2355" height="1698" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/660ceb82-205e-401d-8595-5a23cc328e26.png"></p>
<p style="padding-left: 60px">Here, we can find the log records from our microservices. Most of the log records come from the <kbd>product-composite</kbd> microservice.</p>
<ol start="8">
<li>At the top of the pie chart, we have a group of log records labeled <kbd>missing</kbd>, that is, they neither have a Kubernetes namespace nor a container name specified. What's behind these missing log records? These log records come from processes running outside of the Kubernetes cluster in the Minikube instance and they are stored using Syslog. They can be analyzed using Syslog-specific fields, specifically the <em>identifier field.</em> Let's create a third bucket that divides log records based on their Syslog identifier field, if any. </li>
<li><span>Click on <kbd>Add</kbd> again to create a third bucket:</span>
<ol>
<li>Select the bucket type, that is, <span class="packt_screen">Split slices.</span></li>
<li>As the sub-aggregation type, select<span>&nbsp;</span><span class="packt_screen">Terms </span>from the drop-down list.</li>
<li>As the field, select <span class="packt_screen">SYSLOG_IDENTIFIER.keyword.</span></li>
<li>Enable<span>&nbsp;</span><span class="packt_screen">Group other values in separate bucket.</span></li>
<li>Enable<span>&nbsp;</span><span class="packt_screen">Show missing values.</span></li>
<li>Press the<span>&nbsp;</span><span class="packt_screen">Apply changes</span><span>&nbsp;</span>button and e<span>xpect a pie chart that looks similar to the following: </span></li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/56a1d5be-61bf-49e6-87c2-a18290f65e38.png" style="width:44.75em;height:32.17em;" width="2360" height="1694" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/56a1d5be-61bf-49e6-87c2-a18290f65e38.png"></span></p>
<p><span>The </span><kbd>missing</kbd><span> log records turn out to come from the </span><kbd>kubelet</kbd><span> process, which manages the node from a Kubernetes perspective, and </span><kbd>dockerd</kbd><span>, the Docker daemon that manages all of the containers.</span></p>
<p>Now that we have found out where the log records come from, we can start to locate the actual log records from our microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Discovering the log records from microservices</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will learn how to utilize one of the main features of centralized logging, that is,</span>&nbsp;<span>finding log records from our microservices.</span>&nbsp;<span>We will also learn how to use the </span>trace ID<span> in the log records to find log records from other microservices that belong to one and the same process, for example, a request to the API.</span></p>
<p>Let's start by creating some log records that we can look up with the help of Kibana. We will use the API to create a product with a unique product ID and then retrieve information about the product. After that, we can try to find the log records that were created when retrieving the product information.</p>
<p>The creation of log records in the microservices has updated a bit from the previous chapter so that the product composite and the three core microservices, <kbd>product</kbd>, <kbd>recommendation</kbd>, and <kbd>review</kbd>, all write a <span>log record with the log level set to</span>&nbsp;<kbd>INFO</kbd> when they begin processing a get request. Let's go over the source code that's been added to each microservice:</p>
<ul>
<li><span>Product c</span>omposite microservice log creation:</li>
</ul>
<pre style="padding-left: 60px">LOG.info("Will get composite product info for product.id={}", productId);</pre>
<ul>
<li>Product <span>microservice log creation</span>:</li>
</ul>
<pre style="padding-left: 60px"><span>LOG</span>.info(<span>"Will get product info for id={}"</span>, productId);</pre>
<ul>
<li>Recommendation <span>microservice log creation</span>:</li>
</ul>
<pre style="padding-left: 60px"><span>LOG</span>.info(<span>"Will get recommendations for product with id={}"</span>, productId)</pre>
<ul>
<li>Review <span>microservice log creation</span>:</li>
</ul>
<pre style="padding-left: 60px"><span>LOG</span>.info(<span>"Will get reviews for product with id={}"</span>, productId);</pre>
<p>For more details, see the source code in the <kbd>microservices</kbd> folder.</p>
<p>Perform the following steps to use the API to create log records and then use Kibana to look up the log records:</p>
<ol>
<li>Get an access token with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong></pre>
<ol start="2">
<li>As mentioned in the introduction to this section we will start by creating a product with a unique product ID. Create a minimalistic product (without recommendations and reviews) for <kbd>"productId" :1234</kbd> by executing the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl -X POST -k https://minikube.me/product-composite \</strong><br><strong>  -H "Content-Type: application/json" \</strong><br><strong>  -H "Authorization: Bearer $ACCESS_TOKEN" \</strong><br><strong>  --data '{"productId":1234,"name":"product name 1234","weight":1234}'</strong></pre>
<ol start="3">
<li>Read the product with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" -k 'https://minikube.me/product-composite/1234'</strong></pre>
<p style="padding-left: 60px">Expect a response similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/258fd470-f7ae-432b-933e-0098656e7199.png" width="1946" height="291" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/258fd470-f7ae-432b-933e-0098656e7199.png"></p>
<p style="padding-left: 60px">Hopefully, we got some log records created by these API calls. Let's jump over to Kibana and find out!</p>
<ol start="4">
<li>On the Kibana web page, click on the <kbd>Discover</kbd> menu on the left. You will see something like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/75f6dd32-2743-454f-a301-007444bb2cfd.png" width="2537" height="1576" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/75f6dd32-2743-454f-a301-007444bb2cfd.png"></p>
<p style="padding-left: 60px"><span>On the left-top corner, we can see that Kibaba has found <span class="packt_screen">326,642</span> log records. The </span>time picker<span> shows that they are from the last 7 days. In the </span>histogram<span>, we can see how the log records are spread out over time. Following that is a </span>table<span> showing the most recent log events that were found by the query.</span></p>
<ol start="5">
<li>If you want to change the time interval, you can use the time picker. Click on its calendar icon to adjust the time interval.</li>
<li>To get a better view of the content in the log records, add some fields from the log records to the table under the histogram. Select the fields from the list of available fields to the left. Scroll down until the field is found. Hold the cursor over the field and an add button will appear; click on it to add the field as a column in the table. Select the following fields, in order:
<ol>
<li><span class="packt_screen">spring.level</span>, the log level</li>
<li><span class="packt_screen">kubernetes.container_name</span>, the name of the container</li>
<li><span class="packt_screen">spring.trace</span>, the trace ID used for distributed tracing</li>
</ol>
</li>
</ol>
<ul>
<li style="list-style-type: none">
<ol start="4">
<li><span class="packt_screen">log</span>, the actual log message. The web page should look something similar to the following:</li>
</ol>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e42ce2ba-2d4c-4436-95d7-cde3621c572f.png" width="2355" height="1697" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e42ce2ba-2d4c-4436-95d7-cde3621c572f.png"></p>
<p style="padding-left: 60px">The table now contains information that is of interest regarding the log records!</p>
<ol start="7">
<li>To find log records from the call to the <kbd>GET</kbd> API, we can ask Kibana to find log records where the log field contains the text <span><span class="packt_screen">product.id=1234</span></span><span>. This</span><span> matches the log output from the product composite microservice that was shown previously.</span><em><span>&nbsp;</span></em><span>This can be done by</span> entering<span>&nbsp;</span><kbd>log:"product.id=1234"</kbd> in the <span class="packt_screen">Search</span> field and clicking on the <span class="packt_screen">Update</span> button (this button can also be labeled <span class="packt_screen">Refresh</span>)<span><span>. Expect one log record to be found:</span></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c5370730-05a8-4dc0-a414-3020057d3bb6.png" width="2999" height="1333" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c5370730-05a8-4dc0-a414-3020057d3bb6.png"><br></span></p>
<ol start="8">
<li>Verify that the timestamp is from when you called the <kbd>GET</kbd> API and verify that the name of the container that created the log record is <span class="packt_screen">comp</span>, that is, verify that the log record was sent by the product composite microservice.</li>
<li>Now, we want to see the related log records from the other microservices that participated in the process of returning information about the product with <span class="packt_screen">productId 1234</span>, that is, finding log records with the same trace ID as that of the log record we found. To do that, place the cursor over the <kbd>spring.trace</kbd> field for the log record. Two small magnifying glasses will be shown to the right of the field, one with a <kbd>+</kbd> sign and one with a <kbd>-</kbd> sign. Click on the magnifying glass with the <kbd>+</kbd> sign to filter on the trace ID.</li>
</ol>
<ol start="10">
<li>Clean the <span class="packt_screen">Search</span> field so that the only search criteria is the filter of the trace field. Then, click on the <span class="packt_screen">Update</span> button to see the result. Expect a response similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/909125b4-7868-4ed9-b33c-3a5197408304.png" width="2835" height="1410" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/909125b4-7868-4ed9-b33c-3a5197408304.png"></p>
<p style="padding-left: 60px"><span>We can see a lot of detailed </span>debug<span> and </span>trace<span> messages that clutter the view; let's get rid of them!</span></p>
<ol start="11">
<li>Place the cursor over a <span class="packt_screen">TRACE</span> value and click on the <span>magnifying glass with the</span> <span class="packt_screen">-</span> <span>sign to filter out log records with the log level set to <span class="packt_screen">TRACE</span>.</span></li>
<li>Repeat the preceding step for the <span class="packt_screen">DEBUG</span> log record.</li>
<li>We should now be able to see the four expected log records, one for each microservice involved in the lookup of product information for the product with product ID 1234:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/437104c3-c1c1-463a-a9d9-9fe2e57f6461.png" width="2928" height="1365" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/437104c3-c1c1-463a-a9d9-9fe2e57f6461.png"></p>
<p><span>Also, note the filters that were applied included the trace ID but excluded log records with the log level set to <span class="packt_screen">DEBUG</span> or <span class="packt_screen">TRACE</span>.</span></p>
<p>Now that we know how to find the expected log records, we are ready to take the next step. This will be to learn how to find unexpected log records, that is, error messages, and how to perform root cause analysis, that is, find the reason for these error messages.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Performing root cause analyses</h1>
                </header>
            
            <article>
                
<p class="mce-root">One of the most important features of centralized logging is that it makes it possible to analyze errors using log records from many sources and, based on that, perform root cause analysis, that is, find the actual reason for the error message.</p>
<p class="mce-root">In this section, we will simulate an error and see how we can find information about it, all of the way down to the line of source code that caused the error in one of the microservices in the system landscape. To simulate an error, we will reuse the fault parameter we introduced in <a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em>, in the <em>Adding programmable delays and random errors</em> section. We can use this to force the product microservice to throw an exception. Perform the following steps:</p>
<ol>
<li>Run the following command to generate a fault in the product microservice while searching for product information on the product with product ID <kbd>666</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://minikube.me/product-composite/666?faultPercent=100</strong></pre>
<p style="padding-left: 60px">Expect the following error in response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8d0a558f-7fc3-4f83-bb92-320d419859ab.png" width="2230" height="235" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8d0a558f-7fc3-4f83-bb92-320d419859ab.png"></p>
<p style="padding-left: 60px">Now, we have to pretend that we have no clue about the reason for this error! Otherwise, the root cause analysis wouldn't be very exciting, right? <span>Let's assume that we work in a support organization and have been asked to investigate some problems that just occurred while an end user tried to look up information regarding a product with product ID <kbd>666</kbd>.</span></p>
<ol start="2">
<li>Before we start to analyze the problem, let's delete the previous search filters in the Kibana web UI so that we can start from scratch. For each filter we defined in the previous section, click on their close icon (an <span class="packt_screen">x</span>) to remove them. After all of the filters have been removed, the web page should look similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/82e6c766-b5d4-4bf8-900d-081dfff40319.png" width="2450" height="1510" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/82e6c766-b5d4-4bf8-900d-081dfff40319.png"></p>
<ol start="3">
<li>Start by selecting a time interval that includes the point in time when the problem occurred using the time picker. For example, search the last seven days if you know that the problem occurred within the last seven days.</li>
<li>Next, search for log records with the log level set to <span class="packt_screen">ERROR</span> within this timeframe. This can be done by clicking on the <span class="packt_screen">spring.level</span> field in the list of selected fields. When you click on this field, its most commonly used values will be displayed under it. Filter on the <span class="packt_screen">ERROR</span> value by clicking on its magnifier, shown with the <span class="packt_screen">+</span> sign. Kibana will now show log records within the selected time frame with its log level set to <span class="packt_screen">ERROR</span>, like so: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/70b40e75-4168-4207-90fb-e528fad127f7.png" width="2949" height="1356" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/70b40e75-4168-4207-90fb-e528fad127f7.png"></p>
<ol start="5">
<li>We can see a number of error messages related to product ID <kbd>666</kbd>. The top four have the same trace ID, so this seems like a trace ID of interest to use for further investigation.</li>
<li>We can also see more error messages below the top four that seem to be related to the same error but with different trace IDs. Those are caused by the retry mechanism in the product composite microservice, that is, it retries the request a couple of times before giving up and returning an error message to the caller.</li>
<li>Filter on the trace ID of the first log record in the same way we did in the previous section.</li>
</ol>
<ol start="8">
<li>Remove the filter of the <span class="packt_screen">ERROR</span> log level to be able to see all of the records belonging to this trace ID. Expect Kibana to respond with a lot of log records. Look to the oldest log record, that is, the one that occurred first, that looks suspicious. For example, it may have a <span class="packt_screen">WARN</span> or <span class="packt_screen">ERROR</span> log level or a strange log message. The default sort order is showing the latest log record at the top, so scroll down to the end and search backward (you can also change the sort order to show the oldest log record first by clicking on the small up/down arrow next to the <kbd>Time</kbd> column header). The <span class="packt_screen">WARN</span> log message that says <kbd>Bad luck, and error occurred</kbd>&nbsp;<span>looks like it could be the root cause of the problem. Let's investigate it further:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/47ac8d1a-707a-410f-869f-d0923b4392b2.png" width="3154" height="1267" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/47ac8d1a-707a-410f-869f-d0923b4392b2.png"></p>
<ol start="9">
<li>Once a <span>log record has been found that might be the root cause of the problem, it is of</span> <span>great interest to be able to find the nearby stack trace describing where exceptions were thrown in the source code<strong>.&nbsp;</strong></span>Unfortunately, the Fluentd plugin we use for collecting multiline exceptions, <span><kbd>fluent-plugin-detect-exceptions</kbd>, is </span>unable to relate stack traces to the trace ID that was used. Therefore, stack traces will not show up in Kibana when we filter on a trace ID. Instead, we can use a feature in Kibana for finding surrounding log records that show log records that have occurred in near time to a specific log record.</li>
<li>Expand the log record that says bad luck using the arrow to the left of the log record. Detailed information about this specific log record will be revealed. There is also a link named <span class="packt_screen">View surrounding documents</span>; click on it to see nearby log records. Expect a web page similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d533fe87-485e-4fe5-95c6-8de6c2ae04c6.png" width="3163" height="1264" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/d533fe87-485e-4fe5-95c6-8de6c2ae04c6.png"></p>
<ol start="11">
<li><span>The log record above the bad luck log record with the stack trace for the error message <span class="packt_screen">Something went wrong...</span> looks interesting and was logged by the product microservice just two milliseconds after it logged the <em>bad luck</em> log record. They seem to be related! The</span> <span>stack trace in that log record points to line 96 in </span><span><kbd>ProductServiceImpl.java</kbd>. Looking in the source code (see <kbd>microservices/product-service/src/main/java/se/magnus/microservices/core/product/services/ProductServiceImpl.java</kbd>), line 96 looks as follows:<br></span></li>
</ol>
<pre style="color: black;padding-left: 60px">throw new RuntimeException("Something went wrong...");</pre>
<p>This is the root cause of the error. We did know this in advance, but now we have seen how we can navigate to it as well.</p>
<div class="packt_infobox">In this case, the problem is quite simple to resolve: simply omit the <span><kbd>faultPercent</kbd> parameter in the request to the API. In other cases, the resolution of the root cause can be much harder to figure out!</span></div>
<p><span>This concludes this chapter on using the EFK stack for centralized logging.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about the importance of collecting log records from microservices in a system landscape into a common centralized database where analysis and searches among the stored log records can be performed. We used the EFK stack, that is, Elasticsearch, Fluentd, and Kibana, to collect, process, store, analyze, and search for log records.</p>
<p>Fluentd was used to collect log records not only from our microservices but also from the various supporting containers and processes in the Kubernetes cluster. Elasticsearch was used as a text search engine. Together with Kibana, we saw how easy it is to get an understanding of what types of log records we have collected. </p>
<p>We also learned how to use Kibana to perform important tasks such as finding related log records from cooperating microservices and how to perform root cause analysis, that is, finding the real problem for an error message. Finally, we learned how to update the configuration of Fluentd and how to get the change reflected by the executing Fluentd pod.</p>
<p>Being able to collect and analyze log records in this way is an important capability in a production environment, but these types of activities are always done afterward, once the log record has been collected. Another important capability is to be able to monitor the current health of the microservices, that is, collect and visualize runtime metrics in terms of the use of hardware resources, response times, and so on. We touched on this subject in the previous chapter, <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>, and in the next chapter, <a href="5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml">Chapter 20</a>, <em>Monitoring Microservices</em>, we will learn more about monitoring microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>A user searched for <span class="packt_screen">ERROR</span> log messages in the <kbd>hands-on</kbd> namespace for the last 30 days using the search criteria shown in the following screenshot, but none were found. Why?</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6d818708-6286-4a4c-b9f5-1ad9d67581cd.png" style="width:31.75em;height:10.00em;" width="633" height="198" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6d818708-6286-4a4c-b9f5-1ad9d67581cd.png"></p>
<ol start="2">
<li>A user has found a log record of interest. How can the user find related log records from this and other microservices, for example, that come from processing an external API request?</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/292c8785-1180-40d6-8ad2-2b7e564cc8ef.png" style="width:53.83em;height:49.42em;" width="1080" height="992" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/292c8785-1180-40d6-8ad2-2b7e564cc8ef.png"></p>
<p class="mce-root"></p>
<p class="mce-root"></p>
<ol start="3">
<li>A user has found a log record that seems to indicate the root cause of a problem that was reported by an end user. How can the user find the stack trace that shows wherein the source code the error occurred?</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6e935782-d0e3-46aa-8d97-141af6289a9c.png" width="864" height="261" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6e935782-d0e3-46aa-8d97-141af6289a9c.png"></p>
<ol start="4">
<li class="mce-root"><span>Why doesn't the following Fluentd configuration element work?</span></li>
</ol>
<pre style="color: black;padding-left: 60px">&lt;match kubernetes.**hands-on**&gt;<br>  @type rewrite_tag_filter<br>  &lt;rule&gt;<br>    key log<br>    pattern ^(.*)$<br>    tag spring-boot.${tag}<br>  &lt;/rule&gt;<br>&lt;/match&gt;</pre>
<ol start="5">
<li>
<p class="mce-root">How can you determine whether Elasticsearch is up and running?</p>
</li>
<li>
<p class="mce-root">Suddenly, you lose connection to Kibana from your web browser. What caused this problem?</p>
</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Monitoring Microservices</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to use Prometheus and Grafana to collect, monitor, and alert about performance metrics. As we mentioned in <a href="282e7b49-42b8-4649-af81-b4b6830d391d.xhtml">Chapter 1</a>, <em>Introduction to Microservices</em>, in the <em>Centralized monitoring and alarms</em> section, in a production environment, it is crucial to be able to collect metrics for application performance and hardware resource usage. Monitoring these metrics is required in order to avoid long response times or outages for API requests and other processes.</p>
<p>To be able to monitor a system landscape of microservices in a cost-efficient and proactive way, we need to define alarms that are triggered automatically if the metrics exceed the configured limits.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Introduction to performance monitoring using Prometheus and Grafana</li>
<li>Changes in source code for collecting application metrics</li>
<li>Building and deploying microservices</li>
<li>Monitoring microservices using Grafana dashboards</li>
<li>Setting up alarms in Grafana</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>All of the commands that are described in this book<span> have been run on a MacBook Pro using macOS Mojave but should be straightforward to modify so that they can be run on another platform, such as Linux or Windows.</span></p>
<p>The source code for this chapter can be found <span>in this book's GitHub repository</span>:<span>&nbsp;</span><a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter20">https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter20</a>.</p>
<p>To be able to run the commands as described in this book, you need to download the source code to a folder and set up an environment variable,<span>&nbsp;</span><kbd>$BOOK_HOME</kbd>, that points to that folder. Some sample commands are as follows:</p>
<pre><strong>export BOOK_HOME=~/Documents/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud</strong><br><strong>git clone https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud $BOOK_HOME</strong><br><strong>cd $BOOK_HOME<span>/Chapter20</span></strong></pre>
<p><span>All of the source code examples in this chapter come from the source code in <kbd>$BOOK_HOME/Chapter20</kbd> and have been tested using Kubernetes 1.15.</span></p>
<p>If you want to see the changes that we applied to the source code for this chapter so that you can use Prometheus and Grafana to monitor<span> an alert on performance metrics</span><span>,</span> you can compare it with the source code for <a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml"></a><a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml"></a><a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml"></a><a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml">Chapter 19</a>, <em>Centralized Logging with the EFK Stack</em>. <span>You can use your favorite differentiating tool and compare the two folders <kbd>$BOOK_HOME/Chapter19</kbd> and <kbd>$BOOK_HOME/Chapter20</kbd>.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Introduction to performance monitoring using Prometheus and Grafana</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will reuse the deployment of Prometheus and Grafana that we created in <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>, in the <em>Deploying Istio in the Kubernetes cluster</em> section. Also in that chapter,<span> in the <em>Introducing the runtime components in Istio</em> section, we were briefly </span><span>introduced to Prometheus, a popular open source database for time series such as performance metrics. We were also introduced to Grafana as an open source tool for visualizing performance metrics. Istio's console for observability, Kiali, is integrated with Grafana. A user can navigate from a service of interest in Kiali to its corresponding performance metrics in Grafana. Kiali can also render some performance-related graphs without the use of Grafana. In this chapter, we will get some hands-on experience with this integration by using these tools together. </span></p>
<p>The Istio configuration we deployed in <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>,&nbsp;<em>Using a Service Mesh to Improve Observability and Management</em>, includes a configuration of Prometheus, where it automatically collects metrics from pods in Kubernetes. All we need to do is set up an endpoint in our microservice that produces metrics in a format Prometheus can consume. We also need to add annotations to the Kubernetes pods so that Prometheus can find the address of the endpoints. See the <em>Changes in source code for collecting application metrics</em> section of this chapter for details on how to set this up.</p>
<p>The following diagram illustrates the relationship between the runtime components we just discussed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/53cbe67e-e1ec-419c-8113-92bb9bd15231.png" style="width:48.00em;height:25.00em;" width="1708" height="889" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/53cbe67e-e1ec-419c-8113-92bb9bd15231.png"></p>
<p>Here, we can see how Prometheus uses the annotations in the definitions of the Kubernetes pods to be able to collect metrics from our microservices. It then stores these metrics in its database. A user can access the web UIs of Kiali and Grafana to monitor these metrics in a <strong>web browser</strong>. The <strong>web browser</strong> uses the <strong>minikube tunnel</strong> that was introduced in <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>, in the <em>Setting up access to Istio services</em> section, to access <span>Kiali and Grafana</span>.</p>
<div class="packt_infobox">Please remember that the configuration that was used for deploying Istio from <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>,&nbsp;<em>Using a Service Mesh to Improve Observability and Management</em>, is only intended for development and test, not production. For example, performance metrics stored in the Prometheus database will not survive the Prometheus pod being restarted!</div>
<p>In the next section, we will look at what changes have been applied to the source code to make the microservices produce performance metrics that Prometheus can collect.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Changes in source code for collecting application metrics</h1>
                </header>
            
            <article>
                
<p><span>Spring Boot 2 supports producing performance metrics in a Prometheus format using the</span> <span>Micrometer library (<a href="https://micrometer.io">https://micrometer.io</a>). There's only one change we need to make to the source code: we need to add a dependency to the Micrometer library, </span><kbd>micrometer-registry-prometheus</kbd><span>, in the Gradle build files, <kbd>build.gradle</kbd>, for each microservice. Here, t</span>he following dependency has been added:</p>
<pre>implementation("io.micrometer:micrometer-registry-prometheus")</pre>
<p><span>This will make the microservices produce Prometheus metrics on port <kbd>4004</kbd> using the</span> <kbd>"/actuator/prometheus"</kbd> URI. To let Prometheus know about these endpoints, each microservice's pod is annotated with the following code:</p>
<pre>annotations:<br>  prometheus.io/scrape: "true"<br>  prometheus.io/port: "4004"<br>  prometheus.io/scheme: http<br>  prometheus.io/path: "/actuator/prometheus"</pre>
<div class="packt_infobox">View the deployment definitions in the <kbd>kubernetes/services/base/deployments</kbd> folder for more details.</div>
<p>To make it easier to identify the source of the metrics once they have been collected by Prometheus, they are tagged with the name of the microservice that produced the metric. This is achieved by adding the following configuration to the common configuration file<span>,&nbsp;</span><kbd>config-repo/application.yml</kbd>:</p>
<pre>management.metrics.tags.application: ${spring.application.name}</pre>
<p>This will result in each metric that's produced having an extra label named <kbd>application</kbd>. It will contain the value of the standard Spring property for the name of a microservice, <kbd>spring.application.name</kbd>.&nbsp;</p>
<p>These are all the changes that are required to prepare the microservices to produce performance metrics and to make Prometheus aware of what endpoints to use to start collecting these metrics. In the next section, we will build and deploy the microservices.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Building and deploying the microservices</h1>
                </header>
            
            <article>
                
<p>Building, deploying, and verifying the deployment using the<span>&nbsp;</span><kbd>test-em-all.bash</kbd><span> test script </span>is done in the same way it was done in <a href="7a733f89-e54e-48d2-9a03-d7d2f72157ac.xhtml">Chapter 19</a>, <em>Centralized Logging with the EFK Stack</em>, in the <em>Building and deploying the microservices</em> section. Run the following commands:</p>
<ol>
<li class="mce-root"><span>Build the Docker images from the source with the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>cd $BOOK_HOME/Chapter20</strong><br><strong>eval $(minikube docker-env)</strong><br><strong>./gradlew build &amp;&amp; docker-compose build</strong></pre>
<ol start="2">
<li>Recreate the namespace, <kbd>hands-on</kbd>, and set it as the default namespace:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>kubectl delete namespace hands-on</strong><br><strong>kubectl create namespace hands-on</strong><br><strong>kubectl config set-context $(kubectl config current-context) --namespace=hands-on </strong></pre>
<ol start="3">
<li>Execute the deployment by running the <kbd>deploy-dev-env.bash</kbd> script with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>./kubernetes/scripts/deploy-dev-env.bash </strong></pre>
<ol start="4">
<li>Start the Minikube tunnel, if it's not already running, as follows (see <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>,&nbsp;<span><em>Using a Service Mesh to Improve Observability and Management</em>,</span> the <em>Setting up access to Istio services</em> section, for a recap if you need one<span>):</span></li>
</ol>
<pre style="padding-left: 60px"><strong>minikube tunnel</strong></pre>
<ol start="5">
<li><span>Run the normal tests to verify the deployment with the following command:<br></span></li>
</ol>
<pre style="padding-left: 60px"><strong>./test-em-all.bash</strong></pre>
<p style="padding-left: 60px"><span>Expect the output to be similar to what we've seen in the previous chapters:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c0e56736-a6d9-4bad-9624-d1daf3195629.png" style="width:35.58em;height:8.08em;" width="1889" height="427" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c0e56736-a6d9-4bad-9624-d1daf3195629.png"><br></span></p>
<p>With the microservices deployed, we can move on and start monitoring our microservices using Grafana!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Monitoring microservices using Grafana dashboards</h1>
                </header>
            
            <article>
                
<p>As we already mentioned in the introduction, Kiali is integrated with Grafana and provides some very useful dashboards out of the box. In general, they are focused on application-level performance metrics such as requests per second, response times, and fault percentages for processing requests. They are, as we will see shortly, very useful on an application level. But if we want to understand the usage of the underlying hardware resources, we need more detailed metrics, for example, Java VM-related metrics. </p>
<p><span>Grafana has an active community that</span><span>, among other things,</span><span> shares reusable dashboards.</span> <span>We will try out a dashboard from the community that's tailored for getting a lot of valuable Java VM-related metrics from a Spring Boot 2 application such as our microservices. Finally, we will see how we can build our own dashboards in Grafana. But let's start by exploring the integration between Kiali and Grafana.</span></p>
<p><span>Before we do that, we need to make two preparations:</span></p>
<ol>
<li>Install a local mail - server for tests and configure Grafana to be able to send emails to it.<br>
We will use the mail server in the section "Setting up alarms in Grafana".</li>
<li><span>To be able to monitor some metrics, we will start the load test tool we used in previous chapters.</span></li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Installing a local mail server for tests</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will set up a local test mail server and configure Grafana to send alert emails to the mail server. </span></p>
<p class="mce-root">Grafana can send emails to any SMPT mail server, but to<span>&nbsp;</span>keep<span>&nbsp;</span>the tests local, we will deploy a test mail server named<span>&nbsp;</span><kbd>maildev</kbd>. Consider the following steps:</p>
<ol>
<li class="mce-root">Install the test mail server with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl create deployment mail-server --image djfarrelly/maildev:1.1.0</strong><br><strong>kubectl expose deployment mail-server --port=80,25 --type=ClusterIP</strong><br><strong>kubectl wait --timeout=60s --for=condition=ready pod -l app=mail-server</strong></pre>
<ol start="2">
<li>Verify that the test mail server is up and running by visiting its web page at <a href="http://mail-server.hands-on.svc.cluster.local">http://mail-server.hands-on.svc.cluster.local</a>. Expect a web page such as the following to be rendered:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c3dc6129-9eed-4130-af97-33e94b21a429.png" style="width:40.42em;height:9.50em;" width="815" height="191" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c3dc6129-9eed-4130-af97-33e94b21a429.png"></p>
<ol start="3">
<li>Configure Grafana to send emails to the test mail server by setting up a number of environment variables. Run the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>kubectl -n istio-system set env deployment/grafana \</strong><br><strong>    GF_SMTP_ENABLED=true \</strong><br><strong>    GF_SMTP_SKIP_VERIFY=true \</strong><br><strong>    GF_SMTP_HOST=mail-server.hands-on.svc.cluster.local:25 \</strong><br><strong>    GF_SMTP_FROM_ADDRESS=grafana@minikube.me</strong><br><strong>kubectl -n istio-system wait --timeout=60s --for=condition=ready pod -l app=grafana</strong></pre>
<div class="packt_tip"><span>For more details, see </span><a href="https://hub.docker.com/r/djfarrelly/maildev">https://hub.docker.com/r/djfarrelly/maildev</a><span>.<br></span></div>
<div><span>Now, we have a test mail server up and running and Grafana has been configured to send emails to it. In the next section we will start the load test tool.</span></div>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Starting up the load test</h1>
                </header>
            
            <article>
                
<p>To have something to monitor, let's start up the load test using Siege, which we used in previous chapters. Run the following commands:</p>
<pre><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong><br><strong>siege https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" -c1 -d1</strong></pre>
<p>Now, we are ready to learn about the integration between Kiali and Grafana and explore the Grafana dashboards that come with Istio.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Using Kiali's built-in Grafana dashboards</h1>
                </header>
            
            <article>
                
<p>In <a href="422649a4-94bc-48ae-b92b-e3894c014962.xhtml">Chapter 18</a>, <em>Using a Service Mesh to Improve Observability and Management</em>, in the <em>Observing the service mesh</em> section, we learned about Kiali, but we skipped the part where Kiali shows performance metrics. Now, it's time to get back to that subject!</p>
<p>Execute the following steps to learn about Kiali's integration with Grafana:</p>
<ol>
<li>Open the Kiali web UI in a web browser using the <kbd>http://kiali.istio-system.svc.cluster.local:20001</kbd> URL. Log in with <kbd>admin</kbd>/<kbd>admin</kbd> if required.</li>
<li>Go to the service page by clicking on the <span class="packt_screen">Services</span> tab from the menu on the left-hand side.</li>
<li>Select the <span class="packt_screen">Product</span> service page by clicking on it.</li>
<li>On the <span class="packt_screen">Service:product</span> page, select the <span class="packt_screen">Inbound Metrics</span> tab. You will see the following page:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/457f3485-6af5-4f6f-a6ef-c7b0adf28a24.png" style="width:42.00em;height:25.58em;" width="2561" height="1560" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/457f3485-6af5-4f6f-a6ef-c7b0adf28a24.png"></p>
<ol start="5">
<li>Kiali will visualize some overall performance graphs. However, far more detailed performance metrics are available in Grafana. Click on the <span class="packt_screen">View in Grafana</span> link and Grafana will open up in a new tab. Expect a web page like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e4795b3f-5a78-41f3-8401-5f3de515cd62.png" style="width:74.25em;height:38.33em;" width="2507" height="1295" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e4795b3f-5a78-41f3-8401-5f3de515cd62.png"></p>
<ol start="6">
<li>There are a lot of performance metrics at an application-level being presented here, such as HTTP request rates, response times, and error rates. The metrics are presented for the <span class="packt_screen">Product</span> service, that is, the service that was selected in Kiali. Click on the <span class="packt_screen">Service</span> drop-down menu in the top left corner of the page to select another service. Feel free to look around!</li>
</ol>
<p>Istio comes with a set of pre-installed Grafana dashboards; click on <span class="packt_screen">Istio/Istio Service Dashboard</span> <span>to view a list of available dashboards. Now, select the </span><span class="packt_screen">Istio Mesh Dashboard</span><span>. You will see a web page that looks similar to the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8d9b08eb-3211-45ee-a1b1-04e82a48aeaa.png" style="width:68.83em;height:38.50em;" width="2483" height="1389" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/8d9b08eb-3211-45ee-a1b1-04e82a48aeaa.png"></p>
<p style="padding-left: 60px"><span>This dashboard gives a very good overview of the microservices that are parts of the service mesh, as well as their current status in terms of requests per second, response times, and success rate.</span></p>
<p>As we've already mentioned, the Istio dashboards give a very good overview at an application level. But there is also a need for monitoring the metrics of hardware usage per microservice. In the next section, we will learn about how existing dashboards can be importedâspecifically, a dashboard showing Java VM metrics for a Spring Boot 2-based application.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Importing existing Grafana dashboards</h1>
                </header>
            
            <article>
                
<p>As we've already mentioned, Grafana has an active community that shares reusable dashboards. They can be explored at <a href="https://grafana.com/grafana/dashboards">https://grafana.com/grafana/dashboards</a>. We will try out a dashboard, called <strong><span>JVM</span></strong> (<strong>Micrometer</strong>)<span>,</span> that's tailored for getting a lot of valuable JVM-related metrics from Spring Boot 2 applications. The URL to the dashboard is <a href="https://grafana.com/grafana/dashboards/4701">https://grafana.com/grafana/dashboards/4701</a>. It is very easy to import a dashboard in Grafana. Perform the following steps to import this dashboard:</p>
<ol>
<li>Import <span>the dashboard named <span class="packt_screen">JVM (Micrometer)</span> by following these steps:</span>
<ol>
<li>On the Grafana web page, click on the <span class="packt_screen">+</span> sign in the left-hand side menu and then select <span class="packt_screen">Import</span>.</li>
<li>On the <span class="packt_screen">Import</span> page, paste the dashboard ID <kbd>4701</kbd> into the <span class="packt_screen">Grafana.com Dashboard</span> field and press the Tab key to leave the field.</li>
<li>On the <span class="packt_screen">Import</span> page that will be displayed, click on the <span class="packt_screen">Prometheus</span> drop-down menu and select <span class="packt_screen">Prometheus</span>.</li>
<li>Now, by clicking on the <span class="packt_screen">Import</span> button, the<span><span class="packt_screen"> JVM (Micrometer)</span> dashboard </span>will be imported and rendered.</li>
</ol>
</li>
<li>Inspect the<span class="packt_screen">&nbsp;<span>JVM (Micrometer)</span></span><span>&nbsp;</span><span>dashboard by following these steps:</span>
<ol>
<li>To get a good view of the metrics, c<span>lick on the t</span>ime picker <span>on the top-right. This will allow you to select a proper time interval:</span>
<ol>
<li><span>S</span><span>elect <span class="packt_screen">Last 5 minutes</span> as the range. </span><span>Click on the time picker again and set the refresh rate to <span class="packt_screen">5 seconds.</span></span></li>
<li><span>Click on the <span class="packt_screen">Apply</span> button after specifying the refresh rate.</span></li>
</ol>
</li>
<li>In the <span class="packt_screen">Application</span> drop-down menu, which can be found on the top-left of the page, select the <span class="packt_screen">product-composite</span> microservice.</li>
</ol>
</li>
</ol>
<ul>
<li style="list-style-type: none">
<ol start="3">
<li>Since we are running a load test using Siege in the background, we will see a lot of metrics. The following is a sample screenshot:</li>
</ol>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/21a2ab89-07a0-4a0a-9836-4ddb9fbac91a.png" width="2504" height="1300" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/21a2ab89-07a0-4a0a-9836-4ddb9fbac91a.png"></p>
<p style="padding-left: 60px">In this dashboard, we can find all types of Java VM relevant metrics for, among others, CPU, memory, and I/O usage, as well as HTTP-related metrics such as requests/second, average duration, and error rates. Feel free to explore these metrics on your own! </p>
<p>Being able to import existing dashboards is of great value when we want to get started quickly. However, what's even more important is to know how to create our own dashboard. We will learn about this in the next section.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Developing your own Grafana dashboards</h1>
                </header>
            
            <article>
                
<p>Getting started with developing Grafana dashboards is straightforward. The important thing for us to understand is <span>what metrics Prometheus makes available for us.</span></p>
<p>In this section, we will learn how to examine the available metrics. Based on these, we will create a dashboard that can be used to monitor some of the more interesting metrics.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Examining Prometheus metrics</h1>
                </header>
            
            <article>
                
<p>In the <em>Changes in source code for collecting application metrics</em> section, we configured Prometheus to collect metrics from our microservices. We can actually make a call to the same endpoint and see what metrics Prometheus collects. Run the following command:</p>
<pre><strong>curl http://product-composite.hands-on.svc.cluster.local:4004/actuator/prometheus -s</strong></pre>
<p>Expect a lot of output from the command, as in the following example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6ca8a2ed-3cb0-4233-84eb-18c1ba2f8f0b.png" width="1137" height="262" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/6ca8a2ed-3cb0-4233-84eb-18c1ba2f8f0b.png"></p>
<p><span>Among all of the metrics that are reported, there are two very interesting ones:</span></p>
<ul>
<li><kbd>resilience4j_retry_calls</kbd><span>: Resilience4j reports on how its retry mechanism operates. It reports four different values for successful and failed requests, with and without retries.</span></li>
<li><kbd>resilience4j_circuitbreaker_state</kbd><span>:&nbsp;</span><span>Resilience4j reports on the state of the circuit breaker.</span></li>
</ul>
<p><span>Note that the metrics have a label named <kbd>application</kbd>, which contains the name of the microservice. This field comes from the configuration of the</span> <kbd>management.metrics.tags.application</kbd> <span>property, which we did in the <em>Changes in source code for collecting application metrics</em> section.</span></p>
<p class="mce-root">These metrics <span>seem interesting to monitor.</span> None of the dashboards we have used so far use metrics from Resilience4j. I<span>n the next section, we will create a dashboard for these metrics.</span></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Creating the dashboard</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to create a dashboard that visualizes the <span>Resilience4j</span> metrics we described in the previous section.</p>
<p>We will set up the dashboard in the following subsections:</p>
<ul>
<li><span>Creating an empty dashboard</span></li>
<li><span>Creating a new panel for the circuit breaker</span> <span>metric</span></li>
<li><span>Creating a new panel for the retry</span><span> metric</span></li>
<li><span>Arranging the panels</span></li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Creating an empty dashboard</h1>
                </header>
            
            <article>
                
<p>Perform the following steps to create an empty dashboard:</p>
<ol>
<li><span>In the Grafana web page, click on</span> the <span class="packt_screen">+</span><span> sign in the left-hand menu and then select</span> <span>d</span>ashboard.<span><br></span></li>
<li>A web page named <span class="packt_screen">New dashboard</span> will be displayed:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/177f95ee-296f-4b8e-9877-a27a1d127388.png" width="808" height="140" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/177f95ee-296f-4b8e-9877-a27a1d127388.png"></p>
<ol start="3">
<li>Click on the dashboard settings button (it has a gear as its icon), as shown in the preceding screenshot. Then, follow these steps:<br>
<ol>
<li>Specify the name of the dashboard in the <span class="packt_screen">Name</span> field and set the value to <span class="packt_screen">Hands-on Dashboard</span>.</li>
<li>Click on the <span class="packt_screen">Save</span> button.</li>
</ol>
</li>
<li>Click on the time picker to select the default values for the dashboard, as follows:<br>
<ol>
<li>Select <span class="packt_screen">Last 5 minutes</span> as the range.</li>
<li>Click on the time picker again and specify <span class="packt_screen">5 seconds</span> as the refresh rate in the <span class="packt_screen">Refreshing every</span><span class="packt_screen">&nbsp;</span>field at the bottom of the panel.</li>
<li>Click on the <span class="packt_screen">Apply</span> button after specifying a refresh rate.</li>
<li>Click on the <span class="packt_screen">Save</span> button from the menu at the top of the page.</li>
<li>Enable <span class="packt_screen">Save current time range</span> and click on the <span class="packt_screen">Save</span> button in the <span class="packt_screen">Save Changes</span> dialog window.</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Creating a new panel for the circuit breaker metric</h1>
                </header>
            
            <article>
                
<p><span>Perform the following steps to c</span>reate a new panel for the circuit breaker <span>metric</span><span>:</span></p>
<ol>
<li>Click on the <span class="packt_screen">Add panel</span> button at the top-left of the page <span>(it has an icon of a graph with a</span> <span class="packt_screen">+</span> <span>sign next to it).</span></li>
<li><span>Click on the <span class="packt_screen">Add Query</span> button. A</span> page <span>will be displayed </span>where the new panel can be configured.</li>
<li><span>In the query field, under the <span class="packt_screen">A</span> letter, specify the name of the circuit breaker metric, that is,</span> <span class="packt_screen">resilience4j_circuitbreaker_state</span>.</li>
<li>In the <span class="packt_screen">Legend</span> field, specify the format, that is, <span class="packt_screen">{{application}}.{{namespace}}</span>. This will create a legend in the panel where the involved microservices will be labeled with its name and namespace.</li>
<li>The filled in values should look as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e086766e-4246-4776-a512-e23c79ef4d16.png" style="width:33.42em;height:8.25em;" width="856" height="212" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/e086766e-4246-4776-a512-e23c79ef4d16.png"></p>
<ol start="6">
<li>Click on the third tab, named <span class="packt_screen">General</span>, from the left-hand side menu and set the <span class="packt_screen">Title</span> field to <span class="packt_screen">Circuit Breaker</span>.</li>
<li>Press the back button on the top-left of the page to get back to the dashboard.</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Creating a new panel for the retry metric</h1>
                </header>
            
            <article>
                
<p><span>Here, we will repeat the same procedure that we went through for adding a panel for the preceding circuit breaker metric, but instead, we will specify the values for the retry metrics:</span></p>
<ol>
<li>In the query field, specify <span><kbd>rate(resilience4j_retry_calls[30s])</kbd>. Since the retry metric is a counter, its value will only go up. An ever-increasing metric is rather uninteresting to monitor. The <strong>rate</strong> function is used to convert the retry metric into a rate per second metric. The time window specified, that is, 30 s, is used by the rate function to calculate the average values of the rate.</span></li>
<li>For the legend, specify <kbd>{{application}}.{{namespace}} ({{kind}})</kbd>. Just like the output for the preceding Prometheus endpoint, we will get four metrics for the retry mechanism. To separate them in the legend, the <kbd>kind</kbd> attribute needs to be added.</li>
</ol>
<ol start="3">
<li>Note that Grafana immediately starts to render a graph in the panel editor based on the specified values.</li>
<li>Specify <kbd><span>Retry</span></kbd> <span>as the title.</span></li>
<li>Press the back button to get back to the dashboard.</li>
</ol>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Arranging the panels</h1>
                </header>
            
            <article>
                
<p><span>Perform the following steps to a</span>rrange the panels on the dashboard<span>:</span></p>
<ol>
<li><span>You can resize a panel by dragging its lower right-hand corner to the preferred size.</span></li>
<li>You can also move a panel by dragging its header to the desired position.</li>
<li>The following is an example layout of the two panels:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b77f5f50-990c-482e-afcc-126be94bac9b.png" style="width:42.08em;height:16.75em;" width="790" height="314" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/b77f5f50-990c-482e-afcc-126be94bac9b.png"></p>
<ol start="4">
<li>Finally, click on the <span class="packt_screen">Save</span> button at<span> the top of the page. </span>A <span class="packt_screen">Save Changes</span> dialog will show up; enter an optional description and hit the <span class="packt_screen">Save</span> button.</li>
</ol>
<p>With the dashboard created we are ready to try it out: in the next section, we will try out both metrics.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the new dashboard</h1>
                </header>
            
            <article>
                
<p>Before we start testing the new dashboard, we must stop the load test tool, Siege. For this, go<span> to the command window where Siege is running and press <em>Ctrl + C</em> to stop it.</span></p>
<p>Let's start by testing how to monitor the circuit breaker. Afterward, we will try out the retry metrics.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing the circuit breaker metrics</h1>
                </header>
            
            <article>
                
<p>If we force the circuit breaker to open up, its state will change from <strong>closed</strong> to <strong>open</strong>, and then eventually to the <strong>half-open</strong> state. This should be reported in the <span class="packt_screen">circuit breaker</span> panel.</p>
<p>Open the circuit, just like we did in <a href="23795d34-4068-4961-842d-989cde26b642.xhtml">Chapter 13</a>, <em>Improving Resilience Using Resilience4j</em>, in the <em>Trying out the circuit breaker and retry mechanism</em> section; that is, make three requests to the API in a row, all of which will fail. Run the following commands:</p>
<pre><strong>ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)</strong><br><strong>for ((n=0; n&lt;3; n++)); do curl -o /dev/null -skL -w "%{http_code}\n" https://minikube.me/product-composite/2?delay=3 -H "Authorization: Bearer $ACCESS_TOKEN" -s; done</strong></pre>
<p>We can expect three 500 as a response, indicating three errors in a row, that is, what it takes to open the circuit breaker!</p>
<div class="packt_infobox">At some rare occasions, I have noticed that the circuit breaker metrics are not reported in Grafana. If they don't show up after a minute, simply rerun the preceding command to reopen the c<span>ircuit breaker</span><span>&nbsp;</span>again.</div>
<p>Expect the value for the circuit breaker metric to rise to <kbd>1</kbd>, indicating that the circuit is open. After a while, it should rise to <kbd>2</kbd>, indicating that the circuit is now half-open. This demonstrates that we can monitor that the c<span>ircuit breaker</span><span>&nbsp;</span>opens up if there are problems. Close the c<span>ircuit breaker</span><span> again by issuing three successful requests to the API with the following command:</span></p>
<pre><strong>for ((n=0; n&lt;3; n++)); do curl -o /dev/null -skL -w "%{http_code}\n" https://minikube.me/product-composite/2?delay=0 -H "Authorization: Bearer $ACCESS_TOKEN" -s; done</strong></pre>
<p><span>We will get three <kbd>200</kbd> as responses. Note that the circuit breaker metric goes back to <span class="packt_screen">0</span> again in the dashboard; that is, it's closed.</span></p>
<p>After this test, the Grafana dashboard should look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1050575f-3831-4835-a706-bf25efa841f4.png" width="1073" height="266" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/1050575f-3831-4835-a706-bf25efa841f4.png"></p>
<p>From the preceding screenshot, we can see that the retry mechanism also reports metrics that succeeded and failed. </p>
<p>Now that we have seen the circuit breaker metrics in action, let's see the retry metrics in action!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Testing the retry metrics</h1>
                </header>
            
            <article>
                
<p>To trigger the retry mechanism, we will use the <kbd>faultPercentage</kbd> parameter we used in previous chapters. To avoid triggering the circuit breaker, we need to use relatively low values for the parameter. Run the following command:</p>
<pre><strong>while true; do curl -o /dev/null -s -L -w "%{http_code}\n" -H "Authorization: Bearer $ACCESS_TOKEN" -k https://minikube.me/product-composite/2?faultPercent=10; sleep 3; done</strong></pre>
<p><span>The preceding command will </span>call the API once every third second. It specifies that 10% of the requests shall fail so that the retry mechanism kicks in and retries the failed request. After a few minutes, the dashboard should report metrics such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/21697539-9aba-49a7-9102-1efeca3cd906.png" width="1047" height="274" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/21697539-9aba-49a7-9102-1efeca3cd906.png"></p>
<p>In the preceding screenshot, we can see that the majority of the requests have been executed successfully, without any retries. Approximately 10% of the requests have been retried by the retry mechanism and successfully executed after the retry. Before proceeding to the next section, remember to stop the request loop that we started for the preceding retry test!</p>
<p>In the next section, we will learn how to set up alarms in Grafana, based on these metrics.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up alarms in Grafana</h1>
                </header>
            
            <article>
                
<p><span>Being able to monitor the circuit breaker and retry metrics is of great value, but even more important is the capability to define automated alarms on these metrics. Automated alarms relieve us from monitoring the metrics manually.</span></p>
<p>Grafana comes with built-in support for defining alarms and sending notifications to a number of destinations. In this section we will define alerts on the circuit breaker and configure Grafana to send emails to the test mail server when alerts are raised. The local test mail server was installed in section <em><span>Installing a local mail server for tests</span></em>.</p>
<p><span>In the next section, we will define a mail-based notification channel that will be used by the alert in the section after this.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up a mail-based notification channel</h1>
                </header>
            
            <article>
                
<p>To configure a mail-based notification channel in Grafana, perform the following steps:</p>
<ol>
<li><span>On the Grafana web page, on the menu to the left, click on the <span class="packt_screen">Alerting</span></span><span> menu choice (with an alarm bell as its icon) and s</span><span>elect</span> <span class="packt_screen"><span>No</span>tification channels.</span></li>
<li>Click on the <span class="packt_screen">Add channel</span> button.</li>
<li>Set the name to <kbd>mail</kbd>.</li>
<li>Select the type to <kbd>Email</kbd>.</li>
<li>Enable <span class="packt_screen">Send on all alerts.</span></li>
<li>Enable <span class="packt_screen">Include image.</span></li>
</ol>
<ol start="7">
<li>Enter an email address of your choice. Emails will only be sent to the local test mail server, independent of what email address that's specified. The configuration of the notification channel should look as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/868fa4cb-c3fb-4419-8640-e3e10655d322.png" style="width:20.42em;height:25.67em;" width="394" height="495" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/868fa4cb-c3fb-4419-8640-e3e10655d322.png"></p>
<ol start="8">
<li>Click on the <span class="packt_screen">Send Test</span> button to send a test mail.</li>
<li>Click on the <span class="packt_screen">Save</span> button.</li>
<li>Click on the <span class="packt_screen">Dashboard</span> button in the left-hand side menu and then on the home button.</li>
<li>Select <span class="packt_screen">Hands-on Dashboard</span> from the list to get back to the dashboard.</li>
<li>Check the test mail server's web page to ensure that we have received a test email. You should receive the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/81a70ae5-3443-432f-935e-0987ae320c54.png" style="width:40.50em;height:9.50em;" width="1058" height="248" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/81a70ae5-3443-432f-935e-0987ae320c54.png"></p>
<p>With a <span>notification channel in place, we are ready to define an alert on the circuit breaker.</span></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Setting up an alarm on the circuit breaker</h1>
                </header>
            
            <article>
                
<p>To create an alarm on the c<span>ircuit breaker, we need to create the alert and then add an alert list to the dashboard, where we can see what alert events have occurred over time.</span></p>
<p>Perform the following steps to create an alert for the circuit breaker:</p>
<ol>
<li>In the <span class="packt_screen">Hands-on Dashboard</span>, click on the header of the <span><span class="packt_screen">circuit breaker</span></span> panel. A drop-down menu will appear.</li>
<li>Select the <span class="packt_screen">Edit</span> menu option.</li>
<li>Select the <span class="packt_screen">Alert</span> tab in the tab list to the left (shown as an alarm bell icon).</li>
<li>Click on the <span class="packt_screen">Create Alert</span> button.</li>
<li>In the <span class="packt_screen">Evaluate every</span> field, set the value to <kbd>10s</kbd>.</li>
<li>In the <span><span class="packt_screen">For</span></span> field, set the value to <kbd>0m</kbd>.</li>
<li>In the <span class="packt_screen">Conditions</span> section, specify the following values:
<ul>
<li>Set the <span class="packt_screen">WHEN</span> field to <kbd>max()</kbd>.</li>
<li><span>Set the </span><span class="packt_screen">OF</span> field to <kbd>query(A, 1m, now)</kbd>.</li>
<li><span>Set the </span><span class="packt_screen">IS ABOVE</span> field to <kbd>0.5</kbd>.</li>
</ul>
</li>
<li><span>Scroll down to confirm that the notification has been sent to the default notification channel, that is, the mail channel we defined previously. </span>The alarm definition should look as follows<span>:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/42d25ca0-c993-40b4-8b9c-85487788edda.png" style="width:40.75em;height:13.17em;" width="752" height="243" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/42d25ca0-c993-40b4-8b9c-85487788edda.png"></p>
<ol start="9">
<li>Click on the back button (left arrow) to get back to the dashboard.</li>
</ol>
<p>Then, we need to perform the following steps to create an alarm list:</p>
<ol>
<li>Click on the <span class="packt_screen">Add panel</span> button at the top of the page.</li>
<li>Select <span class="packt_screen">Choose Visualization</span> in the <span class="packt_screen">New Panel</span> page.</li>
</ol>
<ol start="3">
<li>Among the presented visualizations, select <span class="packt_screen">Alert List</span>. Click on it twice to display an <span class="packt_screen">Options</span> list.</li>
<li>Select the <span class="packt_screen">Show</span> option called <span class="packt_screen">Recent state changes.</span></li>
<li>Enable <span class="packt_screen">Alerts from this dashboard</span>. The settings should look as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/104910bd-eef6-4656-be83-50c72c884211.png" style="width:23.50em;height:12.08em;" width="426" height="219" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/104910bd-eef6-4656-be83-50c72c884211.png"></p>
<ol start="6">
<li>Click on the back button to get back to the dashboard.</li>
<li>Rearrange the panel to suit your needs.</li>
<li><span>Save the changes to the dashboard.</span></li>
</ol>
<p class="mce-root"><span>Here is a sample layout with the alarm list added:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/901da6a7-bf7d-40d5-8ef3-ab3a19a31e89.png" width="1425" height="429" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/901da6a7-bf7d-40d5-8ef3-ab3a19a31e89.png"></p>
<p>We can see that the circuit breaker reports the metrics as healthy (with a green heart) and that the alert list contains an OK event for the circuit breaker.</p>
<p>Now, it's time to try out the alarm!</p>
<p class="mce-root"></p>
<p class="mce-root"></p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Trying out the circuit breaker alarm</h1>
                </header>
            
            <article>
                
<p>Here, we will repeat the tests from the <em>Testing the circuit breaker metrics</em> section, but this time, we expect alarms to be raised and emails to be sent as well! Let's get started:</p>
<ol>
<li>Start by opening the <span>circuit breaker:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>for ((n=0; n&lt;3; n++)); do curl -o /dev/null -skL -w "%{http_code}\n" https://minikube.me/product-composite/2?delay=3 -H "Authorization: Bearer $ACCESS_TOKEN" -s; done</strong></pre>
<p style="padding-left: 60px">The dashboard should report the circuit as open as it did previously. After a minute, an alarm should be raised and an email should also be sent. Expect the dashboard to look similar to the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c59df830-87fe-4b2c-986a-42f792e063d6.png" width="1551" height="289" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/c59df830-87fe-4b2c-986a-42f792e063d6.png"></p>
<p style="padding-left: 60px">Take note of the alarm icon in the header of the <span class="packt_screen">circuit breaker</span> panel (a red broken heart). The red line marks the time of the alert event and that an alert has been added to the alert list.</p>
<ol start="2">
<li>In the test mail server, you should see an email, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/80de707b-9f84-480d-8419-01872070ef0b.png" width="1050" height="225" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/80de707b-9f84-480d-8419-01872070ef0b.png"></p>
<ol start="3">
<li>Great; we got alarms, just like we expected! Now, close the circuit, making the problem go away with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>for ((n=0; n&lt;3; n++)); do curl -o /dev/null -skL -w "%{http_code}\n" https://minikube.me/product-composite/2?delay=0 -H "Authorization: Bearer $ACCESS_TOKEN" -s; done</strong></pre>
<p>The metric should go back to normal, that is <kbd>0</kbd>, and after a minute, the alert should go green again.</p>
<p>Expect the dashboard to look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/5c851716-2af5-4b61-a141-a7aa0c6f25ba.png" width="1551" height="289" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/5c851716-2af5-4b61-a141-a7aa0c6f25ba.png"></p>
<p>Note that the alarm icon in the header of the <span class="packt_screen">circuit breaker</span> panel is green again; the green line marks the time for the OK event and that an OK event has been added in the alert list.</p>
<p>In the test mail server, you should see an email, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9adcc46b-e266-4566-8fe9-4c45566b7133.png" width="1054" height="227" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/9adcc46b-e266-4566-8fe9-4c45566b7133.png"></p>
<p>That completes how to monitor microservices using Prometheus and Grafana.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned how to use Prometheus and Grafana to <span>collect and monitor alerts on performance metrics. </span></p>
<p>We saw that, for collecting performance metrics, we can use Prometheus in a Kubernetes environment. We then learned how Prometheus can automatically collect metrics from a pod when a few Prometheus annotations are added to the pod's definition. In order to produce metrics in our microservices, we used Micrometer.</p>
<p>Then, we saw how we can monitor the collected metrics using Grafana dashboards. Both of the dashboards that come with Kiali, as well as the dashboards that were shared by the Grafana community. We also learned how to develop our own dashboards where we used metrics from Resilience4j to monitor the usage of its circuit breaker and retry mechanisms.</p>
<p>Finally, we learned how to define alerts on metrics in Grafana and how to use Grafana to send out alert notifications. We used a local test mail server to receive <span>alert notifications from</span> Grafana as emails.</p>
<p>I hope this book has helped you learn how to develop microservices using all the amazing features of Spring Boot, Spring Cloud, Kubernetes, and Istio and that you feel encouraged to try them out!</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What changes did we need to make to the source code in the microservices to make them produce metrics that are consumed by Prometheus?</li>
<li>What is the <kbd>management.metrics.tags.application</kbd> config parameter <span>used for?</span></li>
<li><span>If you want to analyze a support case regarding high CPU consumption, which of the dashboards in this chapter would you start with?</span></li>
<li><span>If you want to analyze a support case regarding slow API responses, which of the dashboards in this chapter would you start with?</span></li>
<li><span>What is the problem with counter-based metrics such as Resilience4J's retry metrics and what can be done so that we can monitor them in a useful way?</span></li>
</ol>
<ol start="6">
<li>Why does the metric for the circuit breaker report <span class="packt_screen">1</span> for a short while before it reports <span class="packt_screen">2</span>? See the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4ea782fb-939e-4629-8a5d-b0d2f181f8d1.png" style="width:33.83em;height:17.67em;" width="504" height="263" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/4ea782fb-939e-4629-8a5d-b0d2f181f8d1.png"></p>
<p class="CDPAlignCenter CDPAlign">.</p>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Other Books You May Enjoy</h1>
                </header>
            
            <article>
                
<p>If you enjoyed this book, you may be interested in these other books by Packt:</p>
<p><span><a href="https://www.packtpub.com/web-development/mastering-spring-5-1-second-edition" target="_blank"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ebfaa5a1-46e6-4790-9831-958c2e7e1073.jpeg" style="width:10.75em;height:13.25em;" width="567" height="700" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/ebfaa5a1-46e6-4790-9831-958c2e7e1073.jpeg"></a></span></p>
<p style="font-size: 16px"><strong>Mastering Spring 5 - Second Edition</strong><br>
Ranga Rao Karanam</p>
<p style="font-size: 16px">ISBN: 978-1-78961-569-2</p>
<ul>
<li>Explore Spring Framework 5.1 features such as AOP, transaction management, task scheduling, and scripting</li>
<li>Build REST APIs and microservices with Spring and Spring Boot</li>
<li>Develop a secure REST API with Spring Security</li>
<li>Build your first full stack React application</li>
<li>Write efficient unit tests with Spring and Spring Boot</li>
<li>Understand the advanced features that Spring Boot offers for developing and monitoring applications</li>
<li>Use Spring Cloud to deploy and manage applications on the cloud</li>
</ul>
<p><span><a href="https://www.packtpub.com/web-development/hands-full-stack-development-spring-boot-2-and-react-second-edition" target="_blank"><img src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f3e49f6a-ac5b-48f5-a764-c4e79651c8b8.png" style="width:10.75em;height:13.25em;" width="567" height="700" data-mfp-src="https://learning.oreilly.com/library/view/hands-on-microservices-with/9781789613476/assets/f3e49f6a-ac5b-48f5-a764-c4e79651c8b8.png"></a></span></p>
<p style="font-size: 16px"><strong>Hands-On Full Stack Development with Spring Boot 2 and React - Second Edition</strong><br>
Juha Hinkula</p>
<p style="font-size: 16px">ISBN: 978-1-83882-236-1</p>
<ul>
<li>Create a RESTful web service with Spring Boot</li>
<li>Grasp the fundamentals of dependency injection and how to use it for backend development</li>
<li>Discover techniques for securing the backend using Spring Security</li>
<li>Understand how to use React for frontend programming</li>
<li>Benefit from the Heroku cloud server by deploying your application to it</li>
<li>Delve into the techniques for creating unit tests using JUnit</li>
<li>Explore the Material UI component library to make more user-friendly user interfaces</li>
</ul>


            </article>

            
        </section>
    <section>

                            <header>
                    <h1 class="header-title">Leave a review - let other readers know what you think</h1>
                </header>
            
            <article>
                
<p>Please share your thoughts on this book with others by leaving a review on the site that you bought it from. <span>If you purchased the book from Amazon, please leave us an honest review on this book's Amazon page.</span> This is vital so that other potential readers can see and use your unbiased opinion to make purchasing decisions, we can understand what our customers think about our products, and our authors can see your feedback on the title that they have worked with Packt to create. It will only take a few minutes of your time, but is valuable to other potential customers, our authors, and Packt. Thank you!</p>


            </article>

            
        </section>
    
undefined
