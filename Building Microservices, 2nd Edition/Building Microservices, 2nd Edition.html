<figure data-type="cover">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/cover.png" width="1200" height="1574" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/cover.png">
</figure>
<section data-type="titlepage" epub:type="titlepage" data-pdf-bookmark="Building Microservices"><div class="preface" id="idm46534878182744">
<h1>Building Microservices</h1>
<p class="edition">Second Edition</p>
<p class="subtitle">Designing Fine-Grained Systems</p>

<aside data-type="sidebar" epub:type="sidebar" class="preview-edition"><div class="sidebar" id="idm46534878210168">With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</div></aside>

<p class="author">Sam Newman</p>
</div></section>
<section data-type="copyright-page" epub:type="copyright-page" data-pdf-bookmark="Building Microservices"><div class="preface" id="idm46534878389272"><!--TITLE-->
<h1>Building Microservices</h1>
<!--AUTHOR-->

<p class="author">by <span class="firstname">Sam</span> <span class="surname">Newman</span></p>
<!-- COPYRIGHT -->

<p class="copyright">Copyright © 2021 Sam Newman. All rights reserved.</p>

<p class="printlocation">Printed in the United States of America.</p>
<!--PUBLISHER-->

<p class="publisher">Published by <span class="publishername">O’Reilly Media, Inc.</span>, 1005 Gravenstein Highway North, Sebastopol, CA 95472.</p>

<p>O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (<a href="http://oreilly.com"><em>http://oreilly.com</em></a>). For more information, contact our corporate/institutional sales department: 800-998-9938 or <span data-type="email"><em>corporate@oreilly.com</em></span>.</p>
<!--STAFF LIST-->

<ul class="stafflist">
<li><span class="staffrole">Acquisitions Editor:</span> Melissa Duffiel</li>
	<li><span class="staffrole">Development Editor:</span> Nicole Taché</li>
	<li><span class="staffrole">Production Editor:</span> Deborah Baker</li>
	<li><span class="staffrole">Interior Designer:</span> David Futato</li>
	<li><span class="staffrole">Cover Designer:</span> Karen Montgomery</li>
	<li><span class="staffrole">Illustrator:</span> Kate Dullea</li>
</ul>
<!-- PRINTINGS -->

<ul class="printings">
	<li><span class="printedition">July 2021:</span> Second Edition</li>
</ul>
<!-- REVISIONS -->

<div>
<h1 class="revisions">Revision History for the Early Release</h1>

<ul class="releases">
	<li><span class="revdate">2020-05-22:</span> First Release</li>
	<li><span class="revdate">2020-08-27:</span> Second Release</li>
	<li><span class="revdate">2020-10-14:</span> Third Release</li>
</ul>
</div>
<!-- ERRATA -->

<p class="errata">See <a href="http://oreilly.com/catalog/errata.csp?isbn=9781492034025">http://oreilly.com/catalog/errata.csp?isbn=9781492034025</a> for release details.</p>
<!--LEGAL-->

<div class="legal">
<p>The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. <em>Building Microservices</em>, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.</p>

<p>While the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.</p>
</div>

<div class="copyright-bottom">
<p class="isbn">978-1-492-03395-0</p>

<p class="printer">[LSI]</p>
</div>
</div></section>
<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 1. What Are Microservices?"><div class="chapter" id="intro-chapter">
<h1><span class="label">Chapter 1. </span>What Are Microservices?</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534877814552">
<h5>Work In Progress</h5>
<p>Please note that the text below is currently being reworked for the 2nd edition of the book, and is not in a complete state. This will be Chapter 1 of the final book.</p>

<p>If you have any feedback on the book, or suggestions for the 2nd edition, then please contact me on <a href="mailto:book-feedback@samnewman.io">book-feedback@samnewman.io</a> and/or complete a short survey here: <a href="https://oreil.ly/Bldg_MicroServices_survey"><em class="hyperlink">https://oreil.ly/Bldg_MicroServices_survey</em></a>.</p>
</div></aside>

<p>Microservices have become an increasingly popular architecture choice in the five years since I wrote the first edition of this book. I can’t claim credit for the subsequent explosion in popularity, but the rush of people to make use of microservice architectures means that while many of the ideas I captured previously are now tried and tested, new ideas have also come into the mix, at the same time as earlier practices have fallen out of favour. So it’s once again time to distill down the essence of microservice architecture, highlighting the core concepts that make them work.</p>

<p>This book as a whole is designed to give a broad overview of the impact that microservices have on various aspects of software delivery. To start us off, this chapter will take a look at the core ideas behind microservices, the prior art that brought us here, and explore some of the reasons why these architectures are being used so widely.</p>






<section data-type="sect1" data-pdf-bookmark="Microservices At a Glance"><div class="sect1" id="idm46534878176248">
<h1>Microservices At a Glance</h1>

<p><em>Microservices</em> are independently releasable services that are modelled around a business domain. A service encapsulates functionality and makes it accessible to other services via networks—you construct a more complex system from these building blocks. One service might represent inventory, another order management, and yet another shipping, but together they might constitute an entire ecommerce system. Microservices are an architecture choice that is focused on giving you many options for solving the problems you might face.</p>

<p>They are a <em>type</em> of service-oriented architecture, albeit one that is opinionated about how service boundaries should be drawn, and one in which independent deployability is key. They are technology agnostic, which is one of the advantages they offer.</p>

<p>From the outside, a single microservice is treated as a black box. It hosts business functionality on one or more network endpoints (for example, a queue or a REST API, as shown in <a data-type="xref" href="#microservice-overview">Figure 1-1</a>), over <span class="keep-together">whatever</span> protocols are most appropriate. Consumers, whether they’re other microservices or other sorts of programs, access this functionality via these networked endpoints. Internal implementation details (for example, like the technology the service is written in or the way data is stored) are entirely hidden from the outside world. This means microservice architectures avoid the use of shared databases in most circumstances; instead, each microservice encapsulates its own database where required.</p>

<figure><div id="microservice-overview" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0101.png" alt="A microservice exposing its functionality over a REST API and a queue" width="698" height="668" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0101.png">
<h6><span class="label">Figure 1-1. </span>A microservice exposing its functionality over a REST API and a queue</h6>
</div></figure>

<p>Microservices embrace the concept of information hiding.<sup><a data-type="noteref" id="idm46534877372888-marker" href="ch01.html#idm46534877372888" class="totri-footnote">1</a></sup> <em>Information hiding</em> describes hiding as much information as possible inside a component and exposing as little as possible via external interfaces. This allows for clear separation between what can change easily and what is more difficult to change. Implementation that is hidden from external parties can be changed freely as long as the networked interfaces the microservice exposes don’t change in a backward-incompatible fashion. Changes inside a microservice boundary (as shown in <a data-type="xref" href="#microservice-overview">Figure 1-1</a>) shouldn’t affect an upstream consumer, enabling independent releasability of functionality. This is essential in allowing our microservices to be worked on in isolation and released on demand. Having clear, stable service boundaries that don’t change when the internal implementation changes results in systems that have looser coupling and stronger cohesion.</p>
<aside data-type="sidebar" epub:type="sidebar" class="less_space"><div class="sidebar" id="idm46534878164888">
<h5>Are Service-Oriented Architecture and Microservices Different Things?</h5>
<p><em>Service-oriented architecture</em> (SOA) is a design approach in which multiple services collaborate to provide a certain end set of capabilities. A <em>service</em> here typically means a completely separate operating system process. Communication between these services occurs via calls across a network rather than method calls within a process boundary.</p>

<p>SOA emerged as an approach to combat the challenges of large monolithic applications. It is an approach that aims to promote the reusability of software; two or more end-user applications, for example, could use the same services. It aims to make it easier to maintain or rewrite software, as theoretically we can replace one service with another without anyone knowing, as long as the semantics of the service don’t change too much.</p>

<p>SOA at its heart is a sensible idea. However, despite many efforts, there is a lack of good consensus on how to do SOA <em>well</em>. In my opinion, much of the industry has failed to look holistically enough at the problem and present a compelling alternative to the narrative set out by various vendors in this space.</p>

<p>Many of the problems laid at the door of SOA are actually problems with things like communication protocols (e.g., SOAP), vendor middleware, a lack of guidance about service granularity, or the wrong guidance on picking places to split your system. A cynic might suggest that vendors co-opted (and in some cases drove) the SOA movement as a way to sell more products, and those selfsame products in the end undermined the goal of SOA.</p>

<p>Much of the conventional wisdom around SOA doesn’t help you understand how to split something big into something small. It doesn’t talk about how big is too big. It doesn’t talk enough about real-world, practical ways to ensure that services do not become overly coupled. The number of things that go unsaid is where many of the pitfalls associated with SOA originate.</p>

<p>I’ve seen plenty of examples of SOA where teams were striving to make the services smaller, but still had everything coupled to a database and had to deploy everything together. Service Oriented? Yes. But it’s not microservices.</p>

<p>The microservice approach has emerged from real-world use, taking our better understanding of systems and architecture to do SOA well. You should think of microservices as a specific approach for SOA in the same way that Extreme Programming (XP) or Scrum are specific approaches for Agile software development.</p>
</div></aside>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Key Concepts of Microservices"><div class="sect1" id="idm46534878158488">
<h1>Key Concepts of Microservices</h1>

<p>A few core ideas are important to understand when exploring microservices. Given that some of these aspects are often overlooked, I think it’s vital to explore these concepts further to help ensure that you better understand just what it is that makes microservices work.</p>








<section data-type="sect2" data-pdf-bookmark="Independently Deployability"><div class="sect2" id="idm46534878250568">
<h2>Independently Deployability</h2>

<p><em>Independent deployability</em> is the idea that we can make a change to a microservice, deploy it, and release that change to our users, without having to deploy any other services. More important, it’s not just the fact that we can do this, it’s that this is <em>actually</em> how you manage deployments in your system. It’s a discipline you adopt as your default release approach. This is a simple idea that is nonetheless complex in execution.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you take only one thing from this book, and the concept of microservices in general, it should be this: ensure that you embrace the concept of independent deployability of your microservices. Get into the habit of deploying and releasing changes to a single microservice into production without having to deploy anything else. From this, many good things will follow.</p>
</div>

<p>To ensure independent deployability, we need to make sure our services are <em>loosely coupled</em>: we need to be able to change one service without having to change anything else. This means we need explicit, well-defined, and stable contracts between services. Some implementation choices make this difficult—the sharing of databases, for example, is especially problematic.</p>

<p>Independent deployability in and of itself is clearly incredibly valuable. But to achieve independent deployability, there are so many other things you have to get right that in turn have their own benefits. So you can also see the focus on independent deployability as a forcing function - by focusing on this as an outcome you’ll also achieve a number of ancillary benefits.</p>

<p>The desire for loosely coupled services with stable interfaces guides our thinking about how we find service boundaries in the first place.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Modelled Around a Business Domain"><div class="sect2" id="idm46534878451144">
<h2>Modelled Around a Business Domain</h2>

<p>Techniques like domain-driven design can allow you to structure your code to better represent the real-world domain that the software operates in.<sup><a data-type="noteref" id="idm46534878449560-marker" href="ch01.html#idm46534878449560" class="totri-footnote">2</a></sup> With microservice architectures, we use this same idea to define our service boundaries. By modelling services around business domains, we can make it easier to roll out new functionality, and make it easier to recombine microservices in different ways to deliver new functionality to our users.</p>

<p>Rolling out a feature that requires changes to one or more microservices is expensive. You need to coordinate the work across each service (and potentially across separate teams) and carefully manage the order in which the new versions of these services are deployed. That takes a lot more work than making the same change inside a single service (or, for that matter, a monolith). It therefore follows that we want to find ways to make cross-service changes as infrequent as possible.</p>

<p>I often see layered architectures, as typified in <a data-type="xref" href="#ch01-musiccorp-three-tiered">Figure 1-2</a> by the three-tiered architecture. Here, each layer in this architecture represents a different service boundary, with each service boundary based on related technical functionality. If I need to make a change to just the presentation layer in this example, that would be fairly efficient. However, experience has shown that changes in functionality typically span multiple layers in these types of architectures—requiring changes in presentation, application, and data tiers. This problem is exacerbated if the architecture is even more layered than the simple example in <a data-type="xref" href="#ch01-musiccorp-three-tiered">Figure 1-2</a>; often each tier may be split into further layers.</p>

<figure><div id="ch01-musiccorp-three-tiered" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/three-tiered.png" alt="MusicCorp's systems as a traditional three-tiered architecture" width="1445" height="1029" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/three-tiered.png">
<h6><span class="label">Figure 1-2. </span>A traditional three-tiered architecture</h6>
</div></figure>

<p>By making our services end-to-end slices of business functionality, as shown in <a data-type="xref" href="#end-to-end-slices">Figure 1-3</a>, we ensure that our architecture is arranged to make changes to business functionality as efficient as possible. Each service, if needed, can encapsulate presentation, business logic, and data storage. Arguably, with microservices we have made a decision to prioritize high cohesion of business functionality over high cohesion of technical functionality.</p>

<figure><div id="end-to-end-slices" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0103.png" alt="Each microservice, if required, can encapsulate presentation, business and data storage functionality" width="909" height="840" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0103.png">
<h6><span class="label">Figure 1-3. </span>Each microservice, if required, can encapsulate presentation, business logic, and data storage functionality</h6>
</div></figure>

<p>We come back to the interplay of domain-driven design and how this interacts with organizational design later in this chapter.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Owning Their Own State"><div class="sect2" id="idm46534878450808">
<h2>Owning Their Own State</h2>

<p>One of the things I see people having the hardest time with is the idea that microservices should not share databases. If one service wants to access data held by another service,  it should go and ask that service for the data it needs. This gives the service the ability to decide what is shared and what is hidden. This allows us to clearly separate functionality that can change freely (our internal implementation) from the functionality that we want to change infrequently (the external contract that the consumers use).</p>

<p>If we want to make independent deployability a reality, we need to ensure that we limit making backward-incompatible changes to our microservices. If we break compatibility with upstream consumers, we will force them to change as well. Having a clean delineation between internal implementation detail and an external contract for a microservice can help reduce the need for backward-incompatible changes.</p>

<p>Hiding of internal state in a microservice is analogous with the practice of encapsulation in object-oriented (OO) programming. Encapsulation of data in OO systems is an example of information hiding in action.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Don’t share databases unless you really need to. And even then, do everything you can to avoid it. In my opinion, sharing databases is one of the worst things you can do if you’re trying to achieve independent deployability.</p>
</div>

<p>As discussed in the previous section, we want to think of our services as end-to-end slices of business functionality that, where appropriate, encapsulate user interface (UI), business logic, and data. This is because we want to reduce the effort needed to change business-related functionality. The encapsulation of data and behavior in this way gives us high cohesion of business functionality. By hiding the database that backs our service, we also ensure that we reduce coupling. We come back to coupling and cohesion in <a data-type="xref" href="ch02.html#modelling-services-chapter">Chapter 2</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Size"><div class="sect2" id="idm46534877973976">
<h2>Size</h2>

<p>“How big should a microservice be?” is one of the most common questions I hear. Considering the word “micro” is right there in the name, this comes as no surprise. However, when you get into what makes microservices work as a type of architecture, the concept of size is actually one of the least interesting things.</p>

<p>How do you measure size? Lines of code? That doesn’t make much sense to me. Something that might require 25 lines of code in Java could be written in 10 lines of Clojure. That’s not to say Clojure is better or worse than Java; it’s simply that some languages are more expressive than others.</p>

<p>James Lewis, technical director at ThoughtWorks, has been known to say “a microservice should be as big as my head”. On first glance, this is doesn’t seem terribly helpful. After all, how big is James’ head exactly? The rationale behind this statement is that a microservice should be kept to the size where it can be easily understood. The challenge here of course is that people’s ability to understand something isn’t the same, and as such you’ll need to make your own judgement regarding what size works for you. An experienced team may be able to better manage a larger codebase than another. So perhaps the read James’ quote here as “A microservice should be as big as <em>your</em> head”.</p>

<p>The closest I think I get to “size” having any meaning in terms of microservices is something <em>Microservice Patterns</em> author Chris Richardson once said—that the goal of microservices is to have “as small an interface as possible.” That aligns with the concept of information hiding again, but it does represent an attempt to find meaning in the term “microservices” that wasn’t there initially. When the term was first used to define these architectures, the focus, at least initially, was not specifically on size of the interfaces.</p>

<p>Ultimately, the concept of size is highly contextual. Speak to a person who has worked on a system for 15 years, and they’ll feel that their system with 100,000 lines of code is really easy to understand. Ask the opinion of someone brand-new to the project, and they’ll feel it’s much too big. Likewise, ask a company that has just embarked on its microservice transition, having perhaps 10 or fewer microservices, and you’ll get a different answer than you would from a similar-sized company where microservices have been the norm for many years, and it now has hundreds.</p>

<p>I urge people not to worry about size. When you are first starting out, it’s much more important that you focus on two key things. First, how many microservices can you handle? As you have more services, the complexity of your system will increase, and you’ll need to learn new skills (and perhaps adopt new technology) to cope with this. It’s for this reason that I am a strong advocate for incremental migration to a microservice architecture. Second, how do you define microservice boundaries to get the most out of them, without everything becoming a horribly coupled mess? These are the topics that are much more important to focus on when you start your journey.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Flexibility"><div class="sect2" id="idm46534877973704">
<h2>Flexibility</h2>

<p>James Lewis, has been known to say that “microservices buy you options.” Lewis was being deliberate with his words—they <em>buy</em> you <em>options</em>. They have a cost, and you must decide whether the cost is worth the options you want to take up. The resulting flexibility on a number of axes—organizational, technical, scale, robustness—can be incredibly appealing.</p>

<p>We don’t know what the future holds, so we’d like an architecture that can theoretically help us solve whatever problems we might face further down the road. Finding a balance between keeping your options open and bearing the cost of architectures like this can be a real art.</p>

<p>Think of adopting microservices as less like flicking a switch, and more like turning a dial. As you turn up the dial, and you have more microservices, you have increased flexibility. But you likely ramp up the pain points too. This is yet another reason I strongly advocate incremental adoption of microservices. By turning up the dial gradually, you are better able to assess the impact as you go, and stop if required.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Alignment of Architecture and Organization"><div class="sect2" id="idm46534877802456">
<h2>Alignment of Architecture and Organization</h2>

<p>Music Corp, an ecommerce company that sells CDs online, uses the simple three-tiered architecture shown earlier and depicted again in <a data-type="xref" href="#ch01-musiccorp-three-tiered_2">Figure 1-4</a>. We’ve decided to move Music Corp kicking and screaming into the 21st century, and as part of that, we’re assessing the existing system architecture. We have a web-based UI, a business logic layer in the form of a monolithic backend, and data storage in a traditional database. These layers, as is common, are owned by different teams. We’ll be coming back to the trials and tribulations of Music Corp throughout the book.</p>

<figure><div id="ch01-musiccorp-three-tiered_2" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/three-tiered.png" alt="MusicCorp's systems as a traditional three-tiered architecture" width="1445" height="1029" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/three-tiered.png">
<h6><span class="label">Figure 1-4. </span>Music Corp’s systems as a traditional three-tiered architecture</h6>
</div></figure>

<p>We want to make a simple change to our functionality: we want to allow our customers to specify their favorite genre of music. This change requires us to change the UI to show the genre choice UI, the backend service to allow for the genre to be surfaced to the UI and for the value to be changed, and the database to accept this change. These changes will need to be managed by each team and deployed in the correct order, as outlined in <a data-type="xref" href="#ch01-musiccorp-three-tiered-change">Figure 1-5</a>.</p>

<figure><div id="ch01-musiccorp-three-tiered-change" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0105.png" alt="Making a change across all three tiers is more involved" width="1205" height="830" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0105.png">
<h6><span class="label">Figure 1-5. </span>Making a change across all three tiers is more involved</h6>
</div></figure>

<p>Now this architecture isn’t bad. All architecture ends up getting optimized around a set of goals. The three-tiered architecture is so common partly because it is universal—everyone has heard about it. So picking a common architecture that you might have seen elsewhere is often one reason we keep seeing this pattern. But I think the biggest reason we see this architecture again and again is because it is based on how we organize our teams.</p>

<p>The now famous Conway’s law states the following:</p>
<blockquote>
<p>Any organization that designs a system…will inevitably produce a design whose structure is a copy of the organization’s communication structure</p>
<p data-type="attribution">Melvin Conway, <cite>How Do Committees Invent?</cite></p>
</blockquote>

<p>The three-tiered architecture is a good example of this in action. In the past, the primary way IT organizations grouped people was in terms of their core competency: database admins were in a team with other database admins; Java developers were in a team with other Java developers; and frontend developers (who nowadays know exotic things like JavaScript and native mobile application development) were in yet another team. We group people based on their core competency, so we create IT assets that can be aligned to those teams.</p>

<p>So that explains why this architecture is so common. It’s not bad; it’s just optimized around one set of forces—how we traditionally grouped people, around familiarity. But the forces have changed. Our aspirations around our software have changed. We now group people in poly-skilled teams, to reduce hand-offs and silos. We want to ship software much more quickly than ever before. That is driving us to make different choices about the way we organize our teams, and therefore to organize them in terms of the way we break our systems apart.</p>

<p>Most changes that we are asked to make to our system relate to changes in business functionality. But in <a data-type="xref" href="#ch01-musiccorp-three-tiered-change">Figure 1-5</a>, our business functionality is, in effect, spread across all three tiers, increasing the chance that a change in functionality will cross layers. This is an architecture that has high cohesion of related technology but low cohesion of business functionality. If we want to make it easier to make changes, instead we need to change how we group code—we choose cohesion of business functionality rather than technology. Each service may or may not then end up containing a mix of these three layers, but that is a local service implementation concern.</p>

<p>Let’s compare this with a potential alternative architecture, illustrated in <a data-type="xref" href="#ch01-customer-service">Figure 1-6</a>. We have a dedicated Customer service, which exposes a UI to allow customers to update their information, and the state of the customer is also stored within this service. The choice of a favorite genre is associated with a given customer, so this change is much more localized. In <a data-type="xref" href="#ch01-customer-service">Figure 1-6</a>, we also show the list of available genres being fetched from a Catalog service, likely something that would already be in place. We also see a new Recommendations service accessing our favorite genre information, something that could easily follow in a subsequent release.</p>

<figure><div id="ch01-customer-service" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0106.png" alt="A dedicated Customer service may make it much easier to record the favorite musical genre for a customer" width="1443" height="484" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0106.png">
<h6><span class="label">Figure 1-6. </span>A dedicated Customer service can make it much easier to record the favorite musical genre for a customer</h6>
</div></figure>

<p>In such a situation, our Customer service encapsulates a thin slice of each of the three tiers—it has a bit of UI, a bit of application logic, and a bit of data storage—but these layers are all encapsulated in the single service. Our business domain becomes the primary force driving our system architecture, hopefully making it easier to make changes,  as well as making it easier for us to align our teams to lines of business within the organization.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="The Monolith"><div class="sect1" id="idm46534877801896">
<h1>The Monolith</h1>

<p>We’ve spoken about microservices, but microservices are most often discussed as an architectural approach that is an alternative to monolithic architecture. To better help distinguish the microservice architecture, and to help you better understand whether microservices are worth considering, I should also discuss what exactly I mean by <em>monoliths</em>.</p>

<p>When I talk about monoliths throughout this book, I am primarily referring to a unit of deployment. When all functionality in a system must be deployed together, we consider it a monolith. Arguably, multiple architectures fit this definition, but I’m going to discuss those I see most often: the single-process monolith, the modular monolith, and the distributed monolith.</p>








<section data-type="sect2" data-pdf-bookmark="The Single-Process Monolith"><div class="sect2" id="idm46534878381416">
<h2>The Single-Process Monolith</h2>

<p>The most common example that comes to mind when discussing monoliths is a system in which all of the code is deployed as a <em>single process</em>, as in <a data-type="xref" href="#ch01-single-process-monolith">Figure 1-7</a>. You may have multiple instances of this process for robustness or scaling reasons, but fundamentally all the code is packed into a single process. In reality, these single-process systems can be simple distributed systems in their own right because they nearly always end up reading data from or storing data into a database, or presenting information to web or mobile applications.</p>

<figure><div id="ch01-single-process-monolith" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0107.png" alt="A single process monolith. All code is packaged into a single process." width="582" height="601" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0107.png">
<h6><span class="label">Figure 1-7. </span>In a single-process monolith, all code is packaged into a single process</h6>
</div></figure>

<p>Although this fits most people’s understanding of a classic monolith, most systems I encounter are somewhat more complex than this. You may have two or more monoliths that are tightly coupled to one another, potentially with some vendor software in the mix.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Modular Monolith"><div class="sect2" id="idm46534878375400">
<h2>The Modular Monolith</h2>

<p>As a subset of the single-process monolith, the <em>modular monolith</em> is a variation in which the single process consists of separate modules. Each can be worked on independently, but all still need to be combined together for deployment, as shown in <a data-type="xref" href="#ch01-modular-monolith">Figure 1-8</a>. The concept of breaking software into modules is nothing new; modular software has its roots in work done around structured programming from the 1970s, and even further back than that. Nonetheless, this is still not an approach that I see enough organizations properly engage with.</p>

<figure><div id="ch01-modular-monolith" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0108.png" alt="A modular process monolith. The code inside the process is broken down into modules." width="582" height="535" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0108.png">
<h6><span class="label">Figure 1-8. </span>In a modular monolith, the code inside the process is divided into modules</h6>
</div></figure>

<p>For many organizations, the modular monolith can be an excellent choice. If the module boundaries are well defined, it can allow for a high degree of parallel work, while avoiding the challenges of the more distributed microservice architecture by having a much simpler deployment topology. Shopify is a great example of an organization that has used this technique as an alternative to microservice decomposition, and it seems to work really well for that company.<sup><a data-type="noteref" id="idm46534878369192-marker" href="ch01.html#idm46534878369192" class="totri-footnote">3</a></sup></p>

<p>One of the challenges of a modular monolith is that the database tends to lack the decomposition we find in the code level, leading to significant challenges if you want to pull apart the monolith in the future. I have seen some teams attempt to push the idea of the modular monolith further, having the database decomposed along the same lines as the modules, as shown in <a data-type="xref" href="#ch01-modular-monolith-and-db">Figure 1-9</a>.</p>

<figure><div id="ch01-modular-monolith-and-db" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0109.png" alt="A modular monolith with a decomposed database." width="601" height="535" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0109.png">
<h6><span class="label">Figure 1-9. </span>A modular monolith with a decomposed database</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Distributed Monolith"><div class="sect2" id="idm46534878363720">
<h2>The Distributed Monolith</h2>
<blockquote>
<p>A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable.<sup><a data-type="noteref" id="idm46534878361960-marker" href="ch01.html#idm46534878361960" class="totri-footnote">4</a></sup></p>
<p data-type="attribution">Leslie Lamport</p>
</blockquote>

<p>A <em>distributed monolith</em> is a system that consists of multiple services, but for whatever reason, the entire system must be deployed together. A distributed monolith might well meet the definition of an SOA, but all too often, it fails to deliver on the promises of SOA. In my experience, distributed monoliths have all the disadvantages of a distributed system, <em>and</em> the disadvantages of a single-process monolith, without having enough upsides of either. Encountering a number of distributed monoliths in my work has in large part influenced my own interest in microservice architecture.</p>

<p>Distributed monoliths typically emerge in an environment in which not enough focus was placed on concepts like information hiding and cohesion of business functionality. Instead, highly coupled architectures cause changes to ripple across service boundaries, and seemingly innocent changes that appear to be local in scope break other parts of the system.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Monoliths and Delivery Contention"><div class="sect2" id="idm46534878356840">
<h2>Monoliths and Delivery Contention</h2>

<p>As more and more people work in the same place, they get in one another’s way. For example, different developers wanting to change the same piece of code; different teams wanting to push functionality live at different times (or delay deployments); confusion around who owns what, and who makes decisions. A multitude of studies have been done that show the challenges of confused lines of ownership.<sup><a data-type="noteref" id="idm46534878355128-marker" href="ch01.html#idm46534878355128" class="totri-footnote">5</a></sup> I refer to this problem as <em>delivery</em> <span class="keep-together"><em>contention</em></span>.</p>

<p>Having a monolith doesn’t mean you will definitely face the challenges of delivery contention any more than having a microservice architecture means that you won’t ever face the problem. But a microservice architecture does give you more concrete boundaries around which ownership lines can be drawn in a system, giving you much more flexibility regarding how to reduce this problem.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Advantages of Monoliths"><div class="sect2" id="idm46534878351208">
<h2>Advantages of Monoliths</h2>

<p>Some monoliths, such as the single-process or modular monoliths, have a whole host of advantages too. Its much simpler deployment topology can avoid many of the pitfalls associated with distributed systems. This can result in much simpler developer workflows, and monitoring, troubleshooting, and activities like end-to-end testing can be greatly simplified as well.</p>

<p>Monoliths can also simplify code reuse within the monolith itself. If we want to reuse code within a distributed system, we need to decide whether we want to copy code, break out libraries, or push the shared functionality into a service. With a monolith, our choices are much simpler, and many people like that simplicity—all the code is there; just use it!</p>

<p>Unfortunately, people have come to view the monolith as something to be avoided—as something inherently problematic. I’ve met multiple people for whom the term <em>monolith</em> is synonymous with <em>legacy</em>. This is a problem. A monolithic architecture is a choice, and a valid one at that. I’d go further and say that in my option it is the sensible default choice as an architectural style. In other words, I am looking for a reason to be convinced to use microservices, rather than looking for a reason not to use them.</p>

<p>If we fall into the trap of systematically denigrating the monolith as a viable option for delivering our software, we’re at risk of not doing right by ourselves or by the users of our software.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Enabling Technology"><div class="sect1" id="idm46534878345912">
<h1>Enabling Technology</h1>

<p>As I touched on earlier, I don’t think you need to adopt lots of new technology when you first start using microservices. In fact, that can be counterproductive. Instead, as you ramp up your microservice architecture, you should be constantly on the lookout for issues caused by your increasingly distributed system, and then look for technology that might help.</p>

<p>That said, technology has played a large part in the adoption of microservices as a concept. Understating the tools that are available to you to help get the most out of this architecture is going to be a key part to making any implementation of microservices a success. In fact, I would go as far to say that microservices require an understanding of the supporting technology to such a degree that previous distinctions between logical and physical architecture can be problematic - if you are involved in helping shape a microservice architecture, you’ll need a breadth of understanding of these two worlds.</p>

<p>We’ll be exploring a lot of this technology in detail in subsequent chapters, but before that, let’s briefly introduce some of the enabling technology that might help you should you decide to make use of microservices.</p>








<section data-type="sect2" data-pdf-bookmark="Log Aggregation and Distributed Tracing"><div class="sect2" id="idm46534878342296">
<h2>Log Aggregation and Distributed Tracing</h2>

<p>With the number of processes you are managing increasing, it can be difficult to understand how your system is behaving in a production setting. This, in turn, can make troubleshooting much more difficult. We’ll be exploring these ideas in more depth in [Link to Come], but at a bare minimum, I strongly advocate for the implementation of a log aggregation system as a prerequisite for adopting a microservice architecture.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Be cautious in taking on too much new technology when you start off with microservices. That said, a log aggregation tool is so essential that you should consider it a pre-requisite for adopting microservices.</p>
</div>

<p>These systems allow you to collect and aggregate logs from across all your services, providing you a central place from which logs can be analyzed, and even made part of an active alerting mechanism. Many options in this space can cater to numerous situations. I’m a big fan of <a href="https://www.humio.com">Humio</a> for several reasons, but the simple logging services provided by the main public cloud vendors might be good enough to get you started.</p>

<p>These log aggregation tools can be made even more useful by implementing correlation IDs, in which a single ID is used for a related set of service calls—for example, the chain of calls that might be triggered due to user interaction. By logging this ID as part of each log entry, isolating the logs associated with a given flow of calls becomes much easier, making troubleshooting much easier.</p>

<p>As your system grows in complexity, it becomes essential to consider tools that allow you to better explore what your system is doing, providing the ability to analyze traces across multiple services, detect bottlenecks, and ask questions of your system that you didn’t know you would want to ask in the first place. Open source tools can provide some of these features. One example is <a href="https://www.jaegertracing.io">Jaeger</a>, which focuses on the distributed tracing side of the equation.</p>

<p>But products like <a href="https://lightstep.com">LightStep</a> and <a href="https://honeycomb.io">Honeycomb</a> (shown in <a data-type="xref" href="#honeycomb">Figure 1-10</a>), take these ideas further. They represent a new generation of tools moving beyond traditional monitoring approaches, making it much easier to explore the state of your running system. You might already have more conventional tools in place, but you really should look at the capabilities these products provide. They’ve been built from the ground up to solve the sorts of problems that operators of microservice architectures have to deal with.</p>

<figure><div id="honeycomb" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0110.png" alt="A distributed trace shown in Honeycomb, allowing you identify where time is being spent for operations that can span multiple microservices" width="982" height="599" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0110.png">
<h6><span class="label">Figure 1-10. </span>A distributed trace shown in Honeycomb, allowing you to identify where time is being spent for operations that can span multiple microservices</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Containers and Kubernetes"><div class="sect2" id="idm46534875704456">
<h2>Containers and Kubernetes</h2>

<p>Ideally, you want to run each microservice instance in isolation. This ensures that issues in one microservice can’t affect another—for example, by gobbling up all the CPU. Virtualization is one way to create isolated execution environments on existing hardware, but normal virtualization techniques can be quite heavy when we consider the size of our microservices. <em>Containers</em>, on the other hand, provide a much more lightweight way to provision isolated execution for service instances, resulting in faster spin-up times for new container instances, along with being much more cost effective for many architectures.</p>

<p>After you begin playing around with containers, you’ll also realize that you need something to allow you to manage these containers across lots of underlying machines. Container orchestration platforms like <em>Kubernetes</em> do exactly that, allowing you to distribute container instances in such a way as to provide the robustness and throughput your service needs, all while allowing you to make efficient use of the underlying machines. In <a data-type="xref" href="ch07.html#deployment_chapter">Chapter 7</a> we’ll come back and explore the concepts of operational isolation, containers, and kubernetes.</p>

<p>Don’t feel the need to rush to adopt Kubernetes, or even containers, for that matter. They absolutely offer significant advantages over more traditional deployment techniques, but it’s difficult to justify if you have only a few services. After the overhead of managing deployment begins to become a significant headache, start considering containerization of your service and the use of Kubernetes. But if you do end up doing that, do your best to ensure that someone else is running the Kubernetes cluster for you, perhaps by making use of a managed service on a public cloud provider. Running your own Kubernetes cluster can be a significant amount of work!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Streaming"><div class="sect2" id="idm46534875698536">
<h2>Streaming</h2>

<p>Although with microservices we are moving away from monolithic databases, we still need to find ways to share data between services. This is happening at the same time as organizations are wanting to move away from batch reporting operations, toward more real-time feedback, allowing them to react more quickly. Products that allow for the easy streaming and processing of what can often be large volumes of data have therefore become popular with people using microservice architectures.</p>

<p>Apache <a href="https://kafka.apache.org">Kafka</a> has become the de facto choice for many for streaming data in a microservice environment, and for good reason. Capabilities like message permanence, compaction, and the ability to scale to handle large volumes of messages can be incredibly useful. Kafka has also started adding stream-processing capabilities in the form of KSQL, but you can also use it with dedicated stream-processing solutions like Apache <a href="https://flink.apache.org">Flink</a>. <a href="https://debezium.io">Debezium</a> is an open source tool developed to help stream data from existing datasources over Kafka, helping ensure that traditional datasources can become part of a stream-based architecture. In <a data-type="xref" href="ch03.html#integration-chapter">Chapter 3</a> we’ll look at how streaming technology can play a part in microservice integration.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Public Cloud and Serverless"><div class="sect2" id="idm46534875692392">
<h2>Public Cloud and Serverless</h2>

<p>Public cloud providers, or more specifically the main three—Google Cloud, Microsoft Azure, and Amazon Web Services (AWS)—provide a huge array of managed services and deployment options for managing your application. As your microservice architecture grows, more and more work will be pushed into the operational space. Public cloud providers offer a host of managed services, from managed database instances or Kubernetes clusters, to message brokers or distributed filesystems. By making use of these managed services, you are offloading a large amount of this work to a third party that is arguably better able to deal with these tasks.</p>

<p>Of particular interest among the public cloud offerings are the products that sit under the banner of <em>serverless</em>. These products hide away the underlying machines, allowing you to work at a higher level of abstraction. Examples of serverless products include message brokers, storage solutions, and databases. Function as a Service (FaaS) platforms are of special interest because they provide a nice abstraction around the deployment of code. Rather than worrying about how many servers you need to run your service, you just deploy your code and let the underlying platform handle spinning up instances of your code on demand. We’ll be looking at servleress in more detail in <a data-type="xref" href="ch07.html#deployment_chapter">Chapter 7</a>.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Advantages of Microservices"><div class="sect1" id="idm46534875687848">
<h1>Advantages of Microservices</h1>

<p>The advantages of microservices are many and varied. Many of these benefits can be laid at the door of any distributed system. Microservices, however, tend to achieve these benefits to a greater degree primarily because they take a more opinionated stance in the way service boundaries are defined. By combining the concepts of information hiding and domain-driven design along with the power of distributed systems, they can help deliver significant gains over other forms of distributed architectures.</p>








<section data-type="sect2" data-pdf-bookmark="Technology Heterogeneity"><div class="sect2" id="idm46534875685672">
<h2>Technology Heterogeneity</h2>

<p>With a system composed of multiple, collaborating services, we can decide to use different technologies inside each one. This allows us to pick the right tool for each job rather than having to select a more standardized, one-size-fits-all approach that often ends up being the lowest common denominator.</p>

<p>If one part of our system needs to improve its performance, we might decide to use a different technology stack that is better able to achieve the performance levels required. We might also decide that the way we store our data needs to change for different parts of our system. For example, for a social network, we might store our users’ interactions in a graph-oriented database to reflect the highly interconnected nature of a social graph, but perhaps the posts the users make could be stored in a document-oriented data store, giving rise to a heterogeneous architecture like the one shown in <a data-type="xref" href="#a10-mixed-tech">Figure 1-11</a>.</p>

<figure><div id="a10-mixed-tech" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0111.png" alt="Microservices can allow you to more easily embrace different technologies" width="1025" height="416" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0111.png">
<h6><span class="label">Figure 1-11. </span>Microservices can allow you to more easily embrace different technologies</h6>
</div></figure>

<p>With microservices, we are also able to more quickly adopt technology and to understand how new advancements might help us. One of the biggest barriers to trying out and adopting new technology is the risks associated with it. With a monolithic application, if I want to try a new programming language, database, or framework, any change will affect much of my system. With a system consisting of multiple services, I have multiple new places to try out a new piece of technology. I can pick a service that is perhaps lowest risk and use the technology there, knowing that I can limit any potential negative impact. Many organizations find this ability to more quickly absorb new technologies to be a real advantage.</p>

<p>Embracing multiple technologies doesn’t come without overhead, of course. Some organizations choose to place some constraints on <span class="keep-together">language</span> choices. Netflix and Twitter, for example, mostly use the Java Virtual Machine (JVM) as a platform because those companies have a very good understanding of the reliability and performance of that system. They also develop libraries and tooling for the JVM that make operating at scale much easier, but make it more difficult for non-Java-based services or clients. But neither Twitter nor Netflix use only one technology stack for all jobs.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Robustness"><div class="sect2" id="idm46534875676872">
<h2>Robustness</h2>

<p>A key concept in improving robustness of your application is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem, and the rest of the system can carry on working. Service boundaries become your obvious bulkheads. In a monolithic service, if the service fails, everything stops working. With a monolithic system, we can run on multiple machines to reduce our chance of failure, but with microservices, we can build systems that handle the total failure of some of the constituent services and degrade functionality accordingly.</p>

<p>We do need to be careful, however. To ensure that our microservice systems can properly embrace this improved robustness, we need to understand the new sources of failure that distributed systems have to deal with. Networks can and will fail, as will machines. We need to know how to handle this and what impact (if any) those failures should have on the end users of our software.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Scaling"><div class="sect2" id="idm46534875673976">
<h2>Scaling</h2>

<p>With a large, monolithic service, we need to scale everything together. Perhaps one small part of our overall system is constrained in performance, but if that behavior is locked up in a giant monolithic application, we need to handle scaling everything as a piece. With smaller services, we can scale just those services that need scaling, allowing us to run other parts of the system on smaller, less powerful hardware, as illustrated in <a data-type="xref" href="#a10-scaling">Figure 1-12</a>.</p>

<figure><div id="a10-scaling" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0112.png" alt="You can target scaling at just those microservices that need it" width="1357" height="740" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0112.png">
<h6><span class="label">Figure 1-12. </span>You can target scaling at just the microservices that need it</h6>
</div></figure>

<p>Gilt, an online fashion retailer, adopted microservices for this exact reason. Starting in 2007 with a monolithic Rails application, by 2009 Gilt’s system was unable to cope with the load being placed on it. By splitting out core parts of its system, Gilt was better able to deal with its traffic spikes, and today it has more than 450 microservices, each one running on multiple separate machines.</p>

<p>When embracing on-demand provisioning systems like those provided by AWS, we can even apply this scaling on demand for those pieces that need it. This allows us to control our costs more effectively. It’s not often that an architectural approach can be so closely correlated to an almost immediate cost savings.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Ease of Deployment"><div class="sect2" id="idm46534875667304">
<h2>Ease of Deployment</h2>

<p>A one-line change to a million-line monolithic application requires the entire application to be deployed in order to release the change. That could be a large-impact, high-risk deployment. In practice, deployments such as these end up happening infrequently because of understandable fear. Unfortunately, this means that our changes continue to build up between releases, until the new version of our application entering production has masses of changes. And the bigger the delta between releases, the higher the risk that we’ll get something wrong!</p>

<p>With microservices, we can make a change to a single service and deploy it independently of the rest of the system. This allows us to get our code deployed faster. If a problem does occur, it can be quickly isolated to an individual service, making fast rollback easy to achieve. It also means that we can get our new functionality out to customers faster. This is one of the main reasons organizations like Amazon and Netflix use these architectures—to ensure that they remove as many impediments as possible to getting software out the door.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Organizational Alignment"><div class="sect2" id="idm46534875664280">
<h2>Organizational Alignment</h2>

<p>Many of us have experienced the problems associated with large teams and large codebases. These problems can be exacerbated when the team is distributed. We also know that smaller teams working on smaller codebases tend to be more productive.</p>

<p>Microservices allow us to better align our architecture to our organization, helping us minimize the number of people working on any one codebase to hit the sweet spot of team size and productivity. Microservices also allow us to change ownership of services as the organization changes—enabling us to maintain the alignment between architecture and organization in the future.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Composability"><div class="sect2" id="idm46534875661608">
<h2>Composability</h2>

<p>One of the key promises of distributed systems and service-oriented architectures is that we open up opportunities for reuse of  functionality. With microservices, we allow for our functionality to be consumed in different ways for different purposes. This can be especially important when we think about how our consumers use our software.</p>

<p>Gone is the time when we could think narrowly about either our desktop website or mobile application. Now we need to think of the myriad ways that we might want to weave together capabilities for the web, native application, mobile web, tablet app, or wearable device. As organizations move away from thinking in terms of narrow channels to more holistic concepts of customer engagement, we need architectures that can keep up.</p>

<p>With microservices, think of us opening up seams in our system that are addressable by outside parties. As circumstances change, we can build applications in different ways. With a monolithic application, I often have one coarse-grained seam that can be used from the outside. If I want to break that up to get something more useful, I’ll need a hammer!</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Microservice Pain Points"><div class="sect1" id="idm46534875658024">
<h1>Microservice Pain Points</h1>

<p>Microservice architectures bring a host of benefits, as we’ve already seen. But they also bring a host of complexity. If you are considering adopting a microservice architecture, it’s important that you do so being able to compare the good with the bad. In reality, most of these pain points can be laid at the door of distributed systems, and so would just as likely to be evident in a distributed monolith as a microservice architecture.</p>

<p>We’ll be covering many of these issues in depth throughout the rest of the book - in fact I’d argue that the bulk of this book is about dealing with the pain, suffering, and horror of owning a microservice architecture.</p>








<section data-type="sect2" data-pdf-bookmark="Developer Experience"><div class="sect2" id="idm46534875655208">
<h2>Developer Experience</h2>

<p>As you have more and more services, the developer experience can begin to suffer. More resource-intensive runtimes like the JVM can limit the number of microservices that can be run on a single developer machine. I could probably run four or five JVM-based microservices as separate processes on my laptop, but could I run 10 or 20? Probably not. Even with less-taxing runtimes, there is a limit to the number of things you can run locally, which inevitably will start conversations about what to do when you can’t run the entire system on one machine. This can become even more complicated if you are using cloud services that you cannot run locally.</p>

<p>Extreme solutions can involve “developing in the cloud,” where developers move away from being able to develop locally anymore. I’m not a fan of this, because feedback cycles can suffer greatly. Instead, I think limiting the scope of which parts of a system a developer needs to work on is likely to be a much more straightforward approach. However, this might be problematic if you want to embrace more of a “collective ownership” model in which any developer is expected to work on any part of the system.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Technology Overload"><div class="sect2" id="idm46534875652008">
<h2>Technology Overload</h2>

<p>The sheer weight of new technology that has sprung up to enable the adoption of microservice architectures can be overwhelming. I’ll be honest and say that a lot of this technology has just been re-branded as “microservice friendly,” but some advances have legitimately helped in dealing with the complexity of these sorts of architectures. There is a danger, though, that this wealth of new toys can lead to a form of technology fetishism. I’ve seen so many companies adopting microservice architecture also deciding that now is the best time to introduce vast arrays of new, and often alien, <span class="keep-together">technology</span>.</p>

<p>Microservices may well give you the <em>option</em> for each microservice to be written in a different programming language, have it run on a different runtime, or use a different database - but these are options, not requirements. You have to carefully balance the breadth and complexity of the technology you use against the costs that a diverse array of technology can bring.</p>

<p>When you start adopting microservices, some fundamental challenges are inescapable: you’ll need to spend a lot of time understanding issues around data consistency, latency, service modelling, and the like. If you’re trying to understand how these ideas change the way you think about software development at the same time that you’re embracing a huge amount of new technology, you’ll have a hard time of it. It’s also worth pointing out that the bandwidth taken up by trying to understand all of this new technology will also reduce the time you have for actually shipping features to your users.</p>

<p>As you (gradually) increase the complexity of your microservice architecture, look to introduce new technology as you need it. You don’t need a Kubernetes cluster when you have three services! In addition to ensuring that you’re not overloaded with the complexity of these new tools, this gradual increase has the added benefit of allowing you to gain new, better ways of doing things that will no doubt emerge over time.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Reporting"><div class="sect2" id="idm46534875645848">
<h2>Reporting</h2>

<p>With a monolithic system, you typically have a monolithic database. This means that stakeholders who want to analyze all of the data together, often involving large join operations across data, have a ready-made schema against which to run their reports. They can just run them directly against the monolithic database, perhaps against a read replica, as shown in <a data-type="xref" href="#ch05-monolithic-reporting-database">Figure 1-13</a>.</p>

<figure><div id="ch05-monolithic-reporting-database" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0113.png" alt="Reporting being carried out directly on the database of a monolith" width="924" height="702" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/wams_0113.png">
<h6><span class="label">Figure 1-13. </span>Reporting carried out directly on the database of a monolith</h6>
</div></figure>

<p>With a microservice architecture, we have broken up this monolithic schema. That doesn’t mean that the need for reporting across all of our data has gone away; we’ve just made it much more difficult because now our data is scattered across multiple logically isolated schemas.</p>

<p>More modern approaches to reporting, such as using streaming to allow for real-time reporting on large volumes of data, can work well with a microservice architecture but typically require the adoption of new ideas and associated technology. Alternatively, you might simply need to publish data from your microservices into central reporting databases (or perhaps less structured data lakes) to allow for reporting use cases.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Monitoring and Troubleshooting"><div class="sect2" id="idm46534875639336">
<h2>Monitoring and Troubleshooting</h2>

<p>With a standard monolithic application, we can have a fairly simplistic approach to monitoring. We have a small number of machines to worry about, and the failure mode of the application is somewhat binary—the application is often either all up or all down. With a microservice architecture, do we understand the impact if just a single instance of a service goes down?</p>

<p>With a monolithic system, if our CPU is stuck at 100% for a long time, we know that’s a big problem. With a microservice architecture with tens or hundreds of processes, can we say the same thing? Do we need to wake someone up at 3 a.m. when just one process is stuck at 100% CPU?</p>

<p>Luckily, there are a whole host of ideas in this space that can help. If you’d like to explore this concept in more detail, I recommend <em><span class="keep-together">Distributed</span> Systems Observability</em> by Cindy Sridharan (O’Reilly) as an excellent starting point, although we’ll also be taking our own look in [Link to Come].</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Security"><div class="sect2" id="idm46534875633976">
<h2>Security</h2>

<p>With a single-process monolithic system, much of our information flowed within that process. Now, more information flows over networks between our services. This can make our data more vulnerable to being observed in transit, but also potentially manipulated as part of man-in-the-middle attacks. This means that you might need to direct more care to protecting data in transit and to ensuring that your microservice endpoints are protected such that only authorized parties are able to make use of them. [Link to Come] is dedicated entirely to looking at the challenges in this space.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Testing"><div class="sect2" id="idm46534875631032">
<h2>Testing</h2>

<p>With any type of automated functional test, you have a delicate balancing act. The more functionality a test executes—the broader the scope of the test—the more confidence you have in your application. On the other hand, the larger the scope of the test, the harder it is to set up test data and supporting fixtures, the longer it can take to run, and the harder it can be to work out what is broken when it fails. In [Link to Come] I’ll share a number of techniques for making testing work in this more challenging environment.</p>

<p>End-to-end tests for any type of system are at the extreme end of the scale in terms of functionality they cover, and we are used to them being more problematic to write and maintain than smaller-scoped unit tests. Often this is worth it, though, because we want the confidence that comes from having an end-to-end test use our systems in the same way a user might.</p>

<p>But with a microservice architecture, the scope of our end-to-end tests becomes <em>very</em> large. We would now need to run tests across multiple services, all of which need to be deployed and appropriately configured for the test scenarios. We also need to be prepared for the false negatives that occur when environmental issues, such as service instances dying or network time-outs of failed deployments, cause our tests to fail.</p>

<p>These forces mean that as your microservice architecture grows, you will get a diminishing return on investment when it comes to end-to-end testing. The testing will cost more but won’t manage to give you the same level of confidence that it did in the past. This will drive you toward new forms of testing, such as contract-driven testing, as well as exploring release remediation techniques and ideas like testing in production.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Latency"><div class="sect2" id="idm46534877774136">
<h2>Latency</h2>

<p>With a microservice architecture, processing that might previously have been done locally on one processor can now end up being split across multiple separate microservices. Information that previously flowed within only a single process now needs to be serialized, transmitted, and deserialized over networks that you might be <span class="keep-together">exercising</span> more than ever before. All of this can result in worsening latency of your system.</p>

<p>Although it can be difficult to measure the exact impact on latency of operations at the design or coding phase, this is another reason it’s important to undertake any microservice migration in an incremental fashion. Make a small change and then measure the impact. This assumes that you have some way of measuring the end-to-end latency for the operations you care about—distributed tracing tools like <a href="http://jaegertracing.io">Jaeger</a> can help here. But you also need to have an understanding of what acceptable latency is for these operations too. Sometimes making an operation slower is perfectly acceptable, as long as it is still fast enough!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data Consistency"><div class="sect2" id="idm46534877769752">
<h2>Data Consistency</h2>

<p>Shifting from a monolithic system, in which data is stored and managed in a single database, to a much more distributed system, in which multiple processes manage state in different databases, causes potential challenges with respect to consistency of data. Whereas in the past you might have relied on database transactions to manage state changes, you’ll need to understand that similar safety cannot easily be provided in a distributed system. The use of distributed transactions in most cases proves to be highly problematic in coordinating state changes.</p>

<p>Instead, you might need to start using concepts like <em>sagas</em> (something I’ll detail at length in <a data-type="xref" href="ch03.html#integration-chapter">Chapter 3</a>) and eventual consistency to manage and reason about state in your system. These ideas can require fundamental changes in the way you think about data in your systems, something that can be quite daunting when migrating existing systems. Yet again, this is another good reason to be cautious in how quickly you decompose your application. Adopting an incremental approach to decomposition so that you are able to assess the impact of changes to your architecture in production is really important.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Should I Use Microservices?"><div class="sect1" id="idm46534875657400">
<h1>Should I Use Microservices?</h1>

<p>Despite the drive in some quarters to make microservice architectures the default approach for software, I feel that because of the numerous challenges I’ve outlined, adopting them still requires careful thought. You need to assess your own problem space, skills, and technology landscape and understand what you are trying to achieve before deciding whether microservices are right for you. They are <em>an</em> architectural approach, not <em>the</em> architectural approach. Your own context should play a huge part in deciding whether you want to go down that path.</p>

<p>That said, I do want to outline a few situations that would typically tip me away from—or toward—picking microservices.</p>








<section data-type="sect2" data-pdf-bookmark="Who They Might Not Work For"><div class="sect2" id="idm46534877761928">
<h2>Who They Might Not Work For</h2>

<p>Given the importance of defining stable service boundaries, I feel that microservice architectures are often a bad choice for brand-new products or startups. In either case, the domain that you are working with is typically undergoing significant change as you iterate on the fundamentals of what you are trying to build. This shift in domain models will, in turn, result in more changes being made to service boundaries, and coordinating changes across service boundaries is an expensive undertaking. In general, I feel it more appropriate to wait until enough of the domain model has stabilized before looking to define service boundaries.</p>

<p>I do see a temptation for startups to go microservice first. The reasoning goes, “If we’re really successful, we’ll need to scale!” The problem is that you don’t necessarily know if anyone is even going to want to use your new product. And even if you do become successful enough to require a highly scalable architecture, the thing you end up delivering to your users might be very different from what you started building in the first place. Uber initially focused on limos, and Flickr spun out of attempts to create a multiplayer online game. The process of finding product market fit means that you might end up with a very different product at the end than the one you thought you’d build when you started.</p>

<p>Startups also typically have fewer people available to build the system, which creates more challenges with respect to microservices. Microservices bring with them sources of new work and complexity, and this can tie up valuable bandwidth. The smaller the team, the more pronounced this cost will be. When working with smaller teams with just a handful of developers I’m always very hesitant in suggesting microservices for this reason.</p>

<p>The challenge of microservices for startups is compounded by the fact that normally your biggest constraint is people. For a small team, a microservice architecture can be difficult to justify because there is work required just to handle the deployment and management of the microservices themselves. Some people have described this as the “microservice tax.” When that investment benefits lots of people, it’s easier to justify. But if one person out of your five-person team is spending their time on these issues, that’s a lot of valuable time not being spent building your product. It’s much easier to move to microservices later, after you understand where the constraints are in your architecture and what your pain points are—then you can focus your energy on using microservices in the most sensible places.</p>

<p>Finally, I encounter a surprising number of organizations creating software that will be deployed and managed by their customers. As we’ve already covered, microservice architectures can push a lot of complexity into the deployment and operational domain. If you are running the software yourselves, you are able to offset this new complexity by adopting new technology, developing new skills, and changing working practices. This isn’t something you can expect your customers to do. If they are used to receiving your software as a Windows installer, it’s going to come as an awful shock to them when you send out the next version of your software and say, “Just put these 20 pods on your Kubernetes cluster!” In all likelihood, they will have no idea what a pod, Kubernetes, or a cluster even is.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Where They Work Well"><div class="sect2" id="idm46534877755032">
<h2>Where They Work Well</h2>

<p>Probably the single biggest reason that I see organizations adopt microservices is to allow for more developers to work on the same system without getting in each other’s way. Get your architecture and organizational boundaries right, and you allow more people to work independently from one anothers, reducing delivery contention. A five-person startup is likely to find a microservice architecture a drag. A hundred-person scale-up that is growing rapidly is likely to find that its growth is much easier to accommodate with a microservice architecture properly aligned around its product development efforts.</p>

<p>Software as a Service (SaaS) applications are, in general, also a good fit for a microservice architecture. These products are typically expected to operate 24-7, which creates challenges when it comes to rolling out changes. The independent releasability of microservice architectures is a huge boon in this area. Further, the microservices can be scaled up or down, as required. This means that as you establish a sensible baseline for your system’s load characteristics, you get more control over ensuring that you can scale your system in the most cost-effective way possible.</p>

<p>The technology-agnostic nature of microservices ensures that you can get the most out of cloud platforms. Public cloud vendors <span class="keep-together">provide</span> a wide array of services and deployment mechanisms for your code. You can much more easily match the requirements of specific services to the cloud services that will best help you implement them. For example, you might decide to deploy one service as a set of functions, another as a managed virtual machine (VM), and another on a managed Platform as a Service (PaaS) platform.</p>

<p>Although it’s worth noting that adopting a wide range of technology can often be a problem, being able to try out new technology easily is a good way to rapidly identify new approaches that might yield benefits. The growing popularity of FaaS platforms is one such example. For the appropriate workloads, it can drastically reduce the amount of operational overhead, but at present, it’s not a deployment mechanism that would be suitable in all cases.</p>

<p>Microservices also present clear benefits for organizations looking to provide services to their customers over a variety of new channels. A lot of digital transformation efforts seem to involve trying to unlock functionality hidden away in existing systems. The desire is to create new customer experiences that can support the needs of users via whatever interaction mechanism makes the most sense.</p>

<p>Above all, a microservice architecture is one that can give you a lot of flexibility as you continue to evolve your system. That flexibility has a cost, of course, but if you want to keep your options open regarding changes you might want to make in the future, it could be a price worth paying.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46534877747784">
<h1>Summary</h1>

<p>Microservice architectures can give you a huge degree of flexibility in choosing technology, handling robustness and scaling, organizing teams, and more. This flexibility is in part why many people are embracing microservice architectures. But microservices bring with them a significant degree of complexity, and you need to ensure that this complexity is warranted. For many, they have become a default system architecture, to be used in virtually all situations. On the contrary, I still think that they are an architectural choice whose use must be justified by the problems you are trying to solve; often, simpler approaches can deliver much more easily.</p>

<p>Nonetheless, many organizations, especially larger ones, have shown how effective microservices can be. When the core concepts of microservices are properly understood and implemented, they can help create empowering, productive architectures that can help systems become more than the sum of their parts.</p>

<p>I hope this chapter has served as a good introduction into these topics. Next, we’re going to look at how we define microservice boundaries, exploring the topics of structured programming and domain-driven design along the way.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46534877372888"><sup><a href="ch01.html#idm46534877372888-marker" class="totri-footnote">1</a></sup> This concept was first outlined by David Parnas in 1971, in “<a href="https://oreil.ly/rDPWA">Information Distributions Aspects of Design Methodology</a>,” <em>Proceedings of IFIP Congress ’71</em>.</p><p data-type="footnote" id="idm46534878449560"><sup><a href="ch01.html#idm46534878449560-marker" class="totri-footnote">2</a></sup> For an in-depth introduction to domain-driven design, see <em>Domain-Driven Design</em> by Eric Evans (Addison-Wesley Professional), or for a more condensed overview, <em>Domain-Driven Design Distilled</em> by Vaughn Vernon (Addison-Wesley Professional).</p><p data-type="footnote" id="idm46534878369192"><sup><a href="ch01.html#idm46534878369192-marker" class="totri-footnote">3</a></sup> For an overview of Shopify’s thinking behind the use of a modular monolith rather than microservices, "<a href="https://oreil.ly/Rpi1e">Deconstructing the Monolith</a>" by Kirsten Westeinde has some useful insights.</p><p data-type="footnote" id="idm46534878361960"><sup><a href="ch01.html#idm46534878361960-marker" class="totri-footnote">4</a></sup> <a href="https://oreil.ly/2nHF1">Email message</a> sent to a DEC SRC bulletin board at 12:23:29 PDT on May 28, 1987.</p><p data-type="footnote" id="idm46534878355128"><sup><a href="ch01.html#idm46534878355128-marker" class="totri-footnote">5</a></sup> Microsoft Research has carried out studies in this space, and I recommend all of them, but as a starting point, I suggest <a href="https://oreil.ly/0ahXX">“Don’t Touch My Code! Examining the Effects of Ownership on Software Quality”</a> by Christian Bird, et al.</p></div></div></section>
<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. How to Model Microservices"><div class="chapter" id="modelling-services-chapter">
<h1><span class="label">Chapter 2. </span>How to Model Microservices</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534877742760">
<h5>Work In Progress</h5>
<p>Please note that the text below is currently being reworked for the 2nd edition of the book, and is not in a complete state. This will be Chapter 2 of the final book.</p>

<p>If you have any feedback on the book, or suggestions for the 2nd edition, then please contact me on <a href="mailto:book-feedback@samnewman.io">book-feedback@samnewman.io</a> and/or complete a short survey here: <a href="https://oreil.ly/Bldg_MicroServices_survey"><em class="hyperlink">https://oreil.ly/Bldg_MicroServices_survey</em></a>.</p>
</div></aside>
<blockquote data-type="epigraph" epub:type="epigraph">
  <p><em>My opponent’s reasoning reminds me of the heathen, who, being asked on what the world stood, replied, “On a tortoise.” But on what does the tortoise stand? “On another tortoise.”</em></p>
  <p data-type="attribution">Joseph Barker (1854)</p>
</blockquote>

<p>So you know what microservices are, and hopefully have a sense of their key benefits. You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll be looking at some foundational concepts such as information hiding, coupling, and cohesion and understand how they’ll shift our thinking about drawing boundaries around our microservices. We’ll also be looking at different forms of decomposition you might use, as well as focusing more deeply on domain-driven design as a being a hugely useful technique in this space.</p>

<p>We’ll look at how to think about the boundaries of your microservices so as to maximize the upsides and avoid some of the potential downsides. But first, we need something to work with.</p>






<section data-type="sect1" data-pdf-bookmark="Introducing MusicCorp"><div class="sect1" id="idm46534877734664">
<h1>Introducing MusicCorp</h1>

<p>Books about ideas work better with examples. Where possible, I’ll be sharing stories from real-world situations, but I’ve found it’s also useful to have a fictional scenario with which to work. Throughout the book, we’ll be returning to this scenario, seeing how the concept of microservices works within this world.</p>

<p>So let’s turn our attention to the cutting-edge online retailer MusicCorp. MusicCorp was recently a brick-and-mortar retailer, but after the bottom dropped out of the gramophone record business it focused more and more of its efforts online. The company has a website, but feels that now is the time to double-down on the online world. After all, those smart phones for music are just a passing fad (Zunes are way better, obviously) and music fans are quite happy to wait for CDs to arrive at their doorsteps. Quality over convenience, right? And while they may have just learned that Spotify is in fact a digital music service rather than some sort of skin treatment for teenagers, MusicCorp are pretty happy with their own focus, and are sure all of this streaming business will blow over soon.</p>

<p>Despite being a little behind the curve, MusicCorp has grand ambitions. Luckily, it has decided that its best chance of taking over the world is by making sure it can make changes as easily as possible. Microservices for the win!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="What Makes a Good Microservice Boundary?"><div class="sect1" id="idm46534877730856">
<h1>What Makes a Good Microservice Boundary?</h1>

<p>Before the team from MusicCorp tears off into the distance, creating service after service in an attempt to deliver eight-track tapes to all and sundry, let’s put the brakes on and talk a bit about the most important underlying idea we need to keep in mind. We want our microservices to be able to be changed, deployed, and their functionality released to our users in an independent fashion. The ability to change one microservice in isolation from another is vital. So what things do we need to bear in mind when we think about how we draw the boundaries around them?</p>

<p>In essence, microservices are just another form of modular decomposition, albeit one that has network-based interaction between the models and all the associated challenges that brings. Luckily, this means we can rely on a lot of prior art in the space of modular software and structured programming to help guide us in terms of working out how to define our boundaries. With that in mind, let’s look more deeply at three key concepts which we touched on briefly in <a data-type="xref" href="ch01.html#intro-chapter">Chapter 1</a> and which are vital to grasp when it comes to working out what makes for a good microservice boundary - information hiding, cohesion, and coupling.</p>








<section data-type="sect2" data-pdf-bookmark="Information Hiding"><div class="sect2" id="idm46534877726712">
<h2>Information Hiding</h2>

<p>We introduced information hiding in <a data-type="xref" href="ch01.html#intro-chapter">Chapter 1</a> - a concept developed by David Parnas to look at the most effective way to define module boundaries. Information hiding describes a desire to hide as many details as possible behind a module (or in our case microservice) boundary. Parnas looked at the benefits that modules should theoretically give us<sup><a data-type="noteref" id="idm46534877723816-marker" href="ch02.html#idm46534877723816" class="totri-footnote">1</a></sup>, namely:</p>
<dl>
<dt>Improved Development Time</dt>
<dd>
<p>By allowing modules to be developed independently, we can allow for more work to be done in parallel, and reduce the impact of adding more developers to a project.</p>
</dd>
<dt>Comprehensability</dt>
<dd>
<p>Each module can be looked at in isolation, and understood in isolation. This in turn makes it easier to understand what the system as a whole does.</p>
</dd>
<dt>Flexibility</dt>
<dd>
<p>Modules can be changed independently from one another, allowing for changes to be made to the functionality of the system without requiring other modules to change. In addition, modules can be combined in different ways to deliver new functionality.</p>
</dd>
</dl>

<p>This list of desirable characteristics nicely complements what we are trying to achieve with microservice architectures - and indeed I now see microservices as just another form of modular architecture. Adrian Colyer has actually looked back at a number of David  Parnas’ papers from this period and examined them with respect to microservices, and his summaries are well worth reading<sup><a data-type="noteref" id="idm46534877716056-marker" href="ch02.html#idm46534877716056" class="totri-footnote">2</a></sup>.</p>

<p>The reality as Parnas explored through much of his work, is that having modules doesn’t result in you actually achieving these outcomes. A lot depends on <em>how</em> the module boundaries are formed. From his own research information hiding was a key technique to help get the most out of our modular architectures, and with a modern eye, the same applies to microservices too.</p>

<p>From another of Parnas’ papers<sup><a data-type="noteref" id="idm46534877711064-marker" href="ch02.html#idm46534877711064" class="totri-footnote">3</a></sup>, we have this gem:</p>
<blockquote>
<p>The connections between modules are the assumptions which the modules make about each other.</p>
<p data-type="attribution">David Parnas</p>
</blockquote>

<p>By reducing the number of assumptions that one module (or microservice) makes about another, we directly impact the connections between them. By keeping the number of assumptions small, it is easier to ensure that we can change one module without impacting others. If a developer changing a module has a clear understanding as to how the module is used by others, it will be easier for them to make changes safely in such a way that upstream callers won’t also have to change.</p>

<p>With microservices, this applies as well, except that we also have the opportunity to deploy that changed microservice without having to deploy anything else, arguably amplifying the three desireable characteristics that Parnas describes of improved development time, comprehensability and flexibility.</p>

<p>The implications of information hiding play out in so many ways, and I’ll pick up this theme throughout the book.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Cohesion"><div class="sect2" id="idm46534877726184">
<h2>Cohesion</h2>

<p>One of the most succinct<a data-type="indexterm" data-primary="cohesion" data-secondary="defined" id="idm46534877705256"></a> definitions I’ve heard for describing cohesion is this: “the code that changes together, stays together.” For our purposes, this is a pretty good definition. As we’ve already discussed, we’re optimizing our microservice architecture around ease of making changes in business functionality—so we want the functionality grouped in such a way that we can make changes in as few places as possible.</p>

<p>We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release that change as soon as possible. If we have to change that behavior in lots of different places, we’ll have to release lots of different services (perhaps at the same time) to deliver that change. Making changes in lots of different places is slower, and deploying lots of services at once is risky—both of which we want to avoid.</p>

<p>So we want to find boundaries within our problem domain that help ensure that related behavior is in one place, and that communicate with other boundaries as loosely as possible. If the related functionality is spread across the system, we say that cohesion is weak - whereas for our microservice architectures we’re aiming for strong cohesion.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Coupling"><div class="sect2" id="idm46534877701784">
<h2>Coupling</h2>

<p>When services are loosely coupled, a change to one service should not require a change to another. The whole point of a microservice is being able to make a change to one service and deploy it, without needing to change any other part of the system. This is really quite important.</p>

<p>What sort of things cause tight coupling? A classic mistake is to pick an integration style that tightly binds one service to another, causing changes inside the service to require a change to consumers.</p>

<p>A loosely coupled service knows as little as it needs to about the services with which it collaborates. This also means we probably want to limit the number of different types of calls from one service to another, because beyond the potential performance problem, chatty communication can lead to tight coupling.</p>

<p>Coupling though comes in many forms, and I’ve seen a number of misunderstandings about the nature of coupling as it pertains to a service-based architecture. With that in mind, I think it’s important that we explore this topic in more detail, something we’ll do shortly.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Interplay of Coupling And Cohesion"><div class="sect2" id="idm46534877697688">
<h2>The Interplay of Coupling And Cohesion</h2>

<p>As we’ve already touched on, the concepts of coupling and cohesion are obviously related. Logically, if related functionality is spread across our system, changes to this functionality will ripple across those boundaries, implying tighter coupling. Constantine’s Law<sup><a data-type="noteref" id="idm46534877695976-marker" href="ch02.html#idm46534877695976" class="totri-footnote">4</a></sup>, named for structured design pioneer Larry Constantine, sums this up neatly:</p>
<blockquote>
<p>A structure is stable if cohesion is strong and coupling is low.</p>
<p data-type="attribution">Constantine’s Law, <cite>Albert Endres and Dieter Rombach</cite></p>
</blockquote>

<p>The concept here of stability is important to us. For our microservice boundaries to deliver on the promise of independent deployability, allowing us to work on microservices in parallel and reduce the amount of co-ordination between teams working on these services, we need some degree of stability in the boundaries themselves. If the contract that a microservice exposes is constantly changing in a backwards incompatible fashion, then this will cause upstream consumers to constantly have to change too.</p>

<p>Based on this thinking, if we can keep cohesion strong and coupling loose, then stability should follow. The one wrinkle here is that sometimes parts of your system may be going through so much change that stability might be impossible. We’ll look at one such example later in this chapter when I share the experiences of the product development team behind SnapCI.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Types Of Coupling"><div class="sect1" id="idm46534877691144">
<h1>Types Of Coupling</h1>

<p>It’s possible that you could infer from the overview above that all coupling is bad. This isn’t strictly true. Ultimately, some coupling in our system will be unavoidable. What we want to do is reduce how much coupling we have.</p>

<p>A lot of work has been done to look at the different forms of coupling in the context of structured programming, which was largely considering modular (non-distributed, monolithic) software. Many of these different models for assessing coupling overlap or clash, and in any case they speak primarily about things at the code level, rather than considering service-based interactions. As microservices are a style of modular architecture (albeit with the added complexity of distributed systems), we can use a lot of these original concepts and apply them in the context of our microservice-based systems.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534877688008">
<h5>Prior Art In Structured Programming</h5>
<p>Much of our work in computing involves building on the work that came before. It is sometimes impossible to recognize everything that came before, but I have aimed with this second edition to highlight prior art where I can, partly to give credit where credit is due, partly as a way of ensuring that I lay down some breadcrumbs for those readers who want to explore certain topics in more detail, but also to show that many of these ideas are tried and tested.</p>

<p>When it comes to building on the work that came before, there are few areas in this book that have quite as much prior art as structured programming. We’ve already’ mentioned Larry Constantine, and his book “Structured Design<sup><a data-type="noteref" id="idm46534877685576-marker" href="ch02.html#idm46534877685576" class="totri-footnote">5</a></sup>" with Edward Yourdon is considered one of the most important texts in this area. Meilir Page-Jones’s “Practical Guide to Structured Systems Design”<sup><a data-type="noteref" id="idm46534877684712-marker" href="ch02.html#idm46534877684712" class="totri-footnote">6</a></sup> was also useful. Unfortunately, one thing all of these books have in common is how hard they can be to get hold of now, as they are out of print and aren’t made available in ebook form. Yet another reason to support your local library!</p>
</div></aside>

<p>Not all the ideas map cleanly, so I have done my best to synthesize a working model for the different types of coupling for microservices. Where these ideas map cleanly to previous definitions, I’ve stuck with those terms. In other places I have had to come up with new terms or blend in ideas from elsewhere. So please consider what follows to be built on top of a lot of prior art in this space, which I am attempting to give more meaning in the context of microservices.</p>

<p>In <a data-type="xref" href="#different_coupling_examples">Figure 2-1</a> we see a brief overview of the different types of coupling, with them organized from low (desirable) coupling to high (undesirable).</p>

<figure><div id="different_coupling_examples" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/different_coupling_examples.png" alt="different coupling examples" width="1558" height="454" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/different_coupling_examples.png">
<h6><span class="label">Figure 2-1. </span>The different types of coupling, loose to tight coupling</h6>
</div></figure>

<p>Next, we’ll look at each form of coupling in turn, and look at examples that show how they may manifest themselves in our microservice architecture.</p>








<section data-type="sect2" data-pdf-bookmark="Domain Coupling"><div class="sect2" id="idm46534878150424">
<h2>Domain Coupling</h2>

<p>Domain Coupling describes the situation where one microservice needs to interact with another microservice, because it needs to make use of the functionality that the other microservice provides<sup><a data-type="noteref" id="idm46534878148648-marker" href="ch02.html#idm46534878148648" class="totri-footnote">7</a></sup>.</p>

<p>In <a data-type="xref" href="#domain_coupling_example">Figure 2-2</a>, we see part of how orders for CDs are managed inside MusicCorp. In this example, <code>Order Processor</code> calls the <code>Warehouse</code> microservice to reserve stock, and the <code>Payment</code> microservice to take payment. The <code>Order Processor</code> is therefore dependent, and coupled, on the <code>Warehouse</code> and <code>Payment</code> microservices for this operation. We see no such coupling between <code>Warehouse</code> and <code>Payment</code> though, as they don’t interact.</p>

<figure><div id="domain_coupling_example" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/domain_coupling_example.png" alt="[" width="912" height="758" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/domain_coupling_example.png">
<h6><span class="label">Figure 2-2. </span>An example of Domain Coupling, where Order Processor needs to make use of the functionality provided by other microservices</h6>
</div></figure>

<p>In a microservice architecture, this type of interaction is largely unavoidable. A microservice-based system relies on multiple microservices collaborating in order for it to do its work. We still want to keep this to a minimum though - whenever you see a single microservice depending on multiple downstream services in this way it can be a cause for concern - it might imply a microservice that is doing too much.</p>

<p>As a general rule, domain coupling is considered to be a loose form of coupling, although even here we can hit problems. A microservice which needs to talk to lots of downstream microservices might point to a situation where too much logic has been centralized. Domain Coupling can also become problematic as more complex sets of data are sent between services - this can often point to the more problematic forms of coupling we’ll explore shortly.</p>

<p>Just remember the importance of information hiding. Only share what you absolutely have to, and only send the absolute minimum amount of data that you need.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="temporal_coupling_overview">
<h5>A Brief Note On Temporal Coupling</h5>
<p>Another form of coupling you may have heard of is <em>temporal coupling</em>. Technically speaking, this type of coupling doesn’t fit into the model we are exploring here, as it primarily speaks to runtime concerns.</p>

<p>In a situation where one microservice needs to call another microservice in a synchronous way, we say that these microservices are temporally coupled. They both need to be up and available and communicate with each other at the same time in order for the operation to complete. So in <a data-type="xref" href="#temporal_coupling">Figure 2-3</a>, where MusicCorp’s <code>Order Processor</code> is making a synchronous HTTP call to the <code>Warehouse</code> service, for the operation to complete <code>Warehouse</code> needs to be up and available at the same time the call is made.</p>

<figure><div id="temporal_coupling" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/temporal_coupling.png" alt="temporal coupling" width="1195" height="391" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/temporal_coupling.png">
<h6><span class="label">Figure 2-3. </span>An example of Temporal Coupling, where Order Processor makes a synchronous HTTP call to the Warehouse microservice</h6>
</div></figure>

<p>If for some reason <code>Warehouse</code> isn’t currently reachable by the <code>Order Processor</code>, then the operation fails, as we can’t reserve the CDs to be sent out. <code>Order Processor</code> will also have to block and wait for a response from <code>Warehouse</code> as well, potentially causing issues in terms of resource contention.</p>

<p>Temporal coupling isn’t always bad, it’s just something to be aware of. As you have more microservices, and more complex interactions between them, the challenges of temporal coupling can increase to such a point that it becomes more difficult to scale your system and keep it working. One of the ways to avoid temporal coupling is to use some form of asynchronous communication, such as a message broker. We’ll be coming back to the concept of temporal coupling and the  associated issues in much more detail in <a data-type="xref" href="ch03.html#integration-chapter">Chapter 3</a></p>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pass Through Coupling"><div class="sect2" id="idm46534878126776">
<h2>Pass Through Coupling</h2>

<p>Pass through coupling<sup><a data-type="noteref" id="idm46534878125080-marker" href="ch02.html#idm46534878125080" class="totri-footnote">8</a></sup> describes a situation where one microservice passes data to another microservice purely because it is needed by some other further downstream microservice. In many ways it’s one of the most problematic forms of implementation coupling, as it implies that the caller knows not just that the microservice it is invoking calls yet another microservice, but also potentially that it needs to know how that one-step-removed microservice works.</p>

<p>As an example of pass through coupling, let’s look deeper at part of how MusicCorp’s order processing works, in <a data-type="xref" href="#pass_through_coupling">Figure 2-4</a>. Here, we have an <code>Order Processor</code>, which is sending a request to <code>Warehouse</code> to prepare an order for dispatch. As part of the request payload, we send along a <code>Shipping Manifest</code>. This <code>Shipping Manifest</code> consists not just of the address of the customer, but also the shipping type. The <code>Warehouse</code> just passes this manifest on to the downstream <code>Shipping</code> microservice.</p>

<figure><div id="pass_through_coupling" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/tramp_coupling.png" alt="tramp coupling" width="1650" height="729" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/tramp_coupling.png">
<h6><span class="label">Figure 2-4. </span>Pass through coupling, where data is passed to a microservice purely because another downstream service needs it</h6>
</div></figure>

<p>The major issue with pass through coupling is that a change to the required data downstream can cause a more significant upstream change. In our example, if the <code>Shipping</code> now needs the format or content of the data to be changed, then both <code>Warehouse</code> and <code>Order Processor</code> would likely need to change.</p>

<p>There are a few ways this can be fixed. The first is to consider if it makes sense for the calling microservice to just bypass the intermediary. In our example, this might mean <code>Order Processor</code> speaks directly to <code>Shipping</code>. Now, in this specific situation, this causes some other headaches. Our <code>Order Processor</code> is increasing its domain coupling, as <code>Shipping</code> is yet another microservice it needs to know about - if that was the only issue, this might still be fine, as domain coupling is a looser form of coupling of course. This solution gets more complex here though as stock has to be reserved with <code>Warehouse</code> before we dispatch the package using <code>Shipping</code>, and after the shipping has been done we need to update the stock accordingly. This pushes more complexity and logic into the <code>Order Processor</code> which was previously hidden inside <code>Warehouse</code>.</p>

<figure><div id="pass_through_coupling_shipping_direct" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/tramp_coupling_shipping_direct.png" alt="tramp coupling shipping direct" width="1141" height="925" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/tramp_coupling_shipping_direct.png">
<h6><span class="label">Figure 2-5. </span>One way to work around pass through coupling involves communicating directly with the downstream service</h6>
</div></figure>

<p>For this specific example, I might consider a simpler (albeit more nuanced) change, namely to totally hide the requirement for a <code>Shipping Manifest</code> from <code>Order Processor</code>. The idea of delegating the work of both managing stock and arranging for dispatch of the package to our <code>Warehouse</code> service makes sense, but we don’t like the fact that we have leaked some lower-level implementation, namely the fact that the <code>Shipping</code> microservice wants a <code>Shipping Manifest</code>. One way to hide this detail would be to have <code>Warehouse</code> take in the required information as part of its contract, and then have it construct the Shipping Manifest locally. Now, this means that if the <code>Shipping</code> service changes its service contract, as long as the required data is collected by the <code>Warehouse</code>, then this change will be invisible from the viewpoint of the <code>Order Processor</code>.</p>

<figure><div id="pass_through_coupling_hiding" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/tramp_coupling_hiding.png" alt="tramp coupling hiding" width="1650" height="729" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/tramp_coupling_hiding.png">
<h6><span class="label">Figure 2-6. </span>Hiding the need for a Shipping Manifest from the Order Processor</h6>
</div></figure>

<p>Whilst this will help protect the <code>Warehouse</code> microservice from some changes to <code>Shipping</code>, there are some things that would still require all parties to change. Let’s consider the idea that we want to start shipping internationally. As part of this, the <code>Shipping</code> service needs a <code>Customs Declaration</code> as part of the <code>Shipping Manifest</code>. If this is an optional parameter, then we could deploy a new version of the <code>Shipping</code> microservice without issue. If this was a required parameter though, then the <code>Warehouse</code> would need to create one. It might be able to do this with existing information that it has (or is given), but if it required additional information this might require additional information to be passed to it by the <code>Order Processor</code>.</p>

<p>Although in this case we haven’t eliminated the need for a change to be made across all three microservices, we have been given much more power about when and how these changes could be made. If we had the tight (pass through) coupling of the initial example, adding this new required <code>Customs Declaration</code> may require a lock-step rollout of all three microservices. At least by hiding this detail we could much more easily phase deployment.</p>

<p>One final approach which could help reduce the pass through coupling would be for the order processor to still send the shipping manifest to the <code>Shipping</code> microservice via the <code>Warehouse</code>, but to have the <code>Warehouse</code> be totally unaware of the structure of the <code>Shipping Manifest</code> itself. The <code>Order Processor</code> sends the manifest as part of the order request, but the <code>Warehouse</code> makes no attempt to look at or process the field - it just treats it like a blob of data and doesn’t care about the contents. Instead it just sends it along. A change in the format of the the <code>Shipping Manifest</code> would still require a change to both the <code>Order Processor</code> and <code>Shipping</code> microservice, but as the <code>Warehouse</code> doesn’t care about what is actually in the manifest itself it doesn’t need to change.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Common Coupling"><div class="sect2" id="idm46534878094744">
<h2>Common Coupling</h2>

<p>Common coupling occurs when two or more microservices make use of a common set of data. A simple and common example of this form of coupling would be multiple microservices making use of the same shared database, but this could also manifest itself in the use of shared memory or a shared filesystem.</p>

<p>The main issue with common coupling is that changes to the structure of the data can impact multiple microservices at once. Consider the example of some of MusicCorp’s services in <a data-type="xref" href="#common_coupling_reference_data">Figure 2-7</a>. As we discussed earlier, MusicCorp operate around the world, so need various bits of information about the countries in which they operate. Here, multiple services are all reading static reference data from a shared database. If the schema of this database changed in a backwards-incompatible way, it would require changes to each of the consumers of the database. In practice, shared data like this tends to be very difficult to change as a result.</p>

<figure><div id="common_coupling_reference_data" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common_coupling_reference_data.png" alt="common coupling reference data" width="979" height="658" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common_coupling_reference_data.png">
<h6><span class="label">Figure 2-7. </span>Multiple services accessing shared static reference data related to countries from the same database</h6>
</div></figure>

<p>The example in <a data-type="xref" href="#common_coupling_reference_data">Figure 2-7</a> is, relatively speaking, fairly benign. This is because by its very nature static reference data doesn’t tend to change often, and also because this data is read-only - as a result I tend to be relaxed about sharing static reference data in this way. Common coupling though becomes more problematic if the structure of the common data changes more frequently, or if multiple microservices are reading and writing to the same data.</p>

<p><a data-type="xref" href="#common_coupling_order_update1">Figure 2-8</a> shows us a situation where the <code>Order Processor</code> and <code>Warehouse</code> service are both reading and writing from a shared <code>Order</code> table, to help manage the process of dispatching CDs to MusicCorp’s customers. Both microservices are updating the <code>STATUS</code> column. The <code>Order Processor</code> can set the <code>PLACED</code>, <code>PAID</code>, and <code>COMPLETED</code> statuses, whereas the <code>Warehouse</code> will apply <code>PICKING</code> or <code>SHIPPED</code> statuses.</p>

<figure><div id="common_coupling_order_update1" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common_coupling_order_update.png" alt="common coupling order update" width="1633" height="762" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common_coupling_order_update.png">
<h6><span class="label">Figure 2-8. </span>An example of common coupling where both Order Processor and Warehouse are updating the same order record</h6>
</div></figure>

<p>Although you might consider <a data-type="xref" href="#common_coupling_order_update1">Figure 2-8</a> to be a somewhat contrived example, this nonetheless straightforward example of common coupling helps illustrate a core problem. Conceptually, we have both the <code>Order Processor</code> and the <code>Warehouse</code> microservices managing different aspects of the lifecycle of an order. When making changes in <code>Order Processor</code>, can I be sure that I am not changing the order data in such a way that it breaks <code>Warehouse</code>’s view of the world, or vice-versa?</p>

<p>One way to ensure that the state of something is changed in a correct fashion, would be to create a finite state machine. A state machine can be used to manage the transition of some entity from one state to another, ensuring invalid state transitions are prohibited. In <a data-type="xref" href="#state_machine_example">Figure 2-9</a>, see the allowed transitions of state for an order in MusicCorp. An order can go from <code>PLACED</code> to <code>PAID</code>, but not straight from <code>PLACED</code> to <code>PICKING</code> (this state machine likely wouldn’t be sufficient for the real-world business processes involved in full end-to-end buying and shipping of goods, but I wanted to give a simple example to illustrate the idea).</p>

<figure><div id="state_machine_example" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/state_machine_example.png" alt="state machine example" width="2158" height="575" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/state_machine_example.png">
<h6><span class="label">Figure 2-9. </span>An overview of the allowable state transitions for an order in MusicCorp</h6>
</div></figure>

<p>The problem in this specific example is that both <code>Warehouse and +Order Processor</code> share responsibilities for managing this state machine. How do we ensure that they are both in agreement as to what transitions are allowed? There are ways to manage processes like this across microservice boundaries, and we will return to this topic when we discuss Sagas in [Link to Come].</p>

<p>A potential solution here would be to ensure that one single microservice manages the order state. In <a data-type="xref" href="#common_coupling_order_service">Figure 2-10</a>, either <code>Warehouse</code> or <code>Order Processor</code> can send status update requests to the <code>Order</code> service. Here, the <code>Order</code> microservice is the source of truth for any given order. In this situation, it is really important that we see the requests from <code>Warehouse</code> and <code>Order Processor</code> as just that - <em>requests</em>. In this scenario, it is the job of the <code>Order</code> service to manage the acceptable state transitions associated with an order aggregate. As such, if it received a request from <code>Order Processor</code> to move a status from <code>PLACED</code> straight to <code>COMPLETED</code> it is free to reject that request if that is an invalid change.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Make sure you see a request that is sent to a microservice as something that the downstream microservice can reject if it is invalid.</p>
</div>

<p>An alternative approach I see in such cases is to implement the <code>Order</code> service as little more than a wrapper around database CRUD operations, where requests just map directly to database updates. This is akin to an object having private fields but public getters and setters - the behavior has leaked from the microservice to upstream consumers (reducing cohesion), and we’re back in the world of managing acceptable state transitions across multiple different services.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If you see a microservice that just looks like a thin wrapper around database CRUD operations, that is a sign that you may have weak cohesion and tighter coupling, as logic that should be in that service to manage the data is instead spread elsewhere in your system.</p>
</div>

<figure><div id="common_coupling_order_service" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common_coupling_order_service.png" alt="common coupling order service" width="1112" height="1100" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common_coupling_order_service.png">
<h6><span class="label">Figure 2-10. </span>Both Order Processor and Warehouse can request changes are made to an order, but the Order microservice decides what requests are acceptable</h6>
</div></figure>

<p>Sources of common coupling are also potential sources of resource contention. Multiple microservices making use of the same file system or database could overload that shared resource, potentially causing significant problems if the shared resource becomes slow or even entirely unavailable. Shared databases are especially prone to this problem, as multiple consumers can run arbitrary queries against the database itself, which in turn can have wildly different performance characteristics. I’ve seen more than one database brought to its knees by an expensive SQL query - I may have even been the culprit once or twice<sup><a data-type="noteref" id="idm46534877682408-marker" href="ch02.html#idm46534877682408" class="totri-footnote">9</a></sup>.</p>

<p>So common coupling is <strong>sometimes</strong> ok, but often not. Even when it’s benign, it means that we are limited in what changes can be made to the shared data, but it often speaks to a lack of cohesion in our code. It can also cause us problems in terms of operational contention too. It’s for those reasons that we consider common coupling to be one of the least desirable forms of coupling, but it can get worse.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Content Coupling"><div class="sect2" id="idm46534878089336">
<h2>Content Coupling</h2>

<p>Content coupling describes a situation where an upstream service reaches into the internals of a downstream service and changes its internal state. The most common manifestation of this is an external service directly accessing another microservice’s database and changing it directly. The difference between content coupling and common coupling are subtle. On the face of it, in both cases two or more microservices are reading and writing to the same set of data. With common coupling, you understand that you are making use of a shared, external dependency. You know it’s not under your control. With content coupling, the lines of ownership become less clear, and it becomes more difficult for developers to change a system.</p>

<p>Let’s revisit our earlier example from MusicCorp. In <a data-type="xref" href="#content_coupling_order_service">Figure 2-11</a>, we have an <code>Order</code> service which is supposed to manage the allowable state changes to orders in our system. The <code>Order Processor</code> is sending requests to the <code>Order</code> service, delegating not just the exact change in state that will be made, but also delegating to the <code>Order</code> service responsibility for deciding what state transitions are allowable. On the other hand, the <code>Warehouse</code> service is directly updating the table where order data is stored, bypassing any functionality in the <code>Order</code> service which might check for allowable changes. We have to hope that the <code>Warehouse</code> service has a consistent set of logic to ensure that only valid changes are made. Best case, this represents a duplication of logic. Worst case, the checking around allowable changes in <code>Warehouse</code> is different to that in the <code>Order</code> service, and as a result we could end up with orders in very odd, confusing states.</p>

<figure><div id="content_coupling_order_service" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/content_coupling_order_service.png" alt="content coupling order service" width="1233" height="983" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/content_coupling_order_service.png">
<h6><span class="label">Figure 2-11. </span>An example of content coupling, where the Warehouse is directly accessing the internal data of the Order service</h6>
</div></figure>

<p>In this situation, we also have the issue that the internal data structure of our order table is exposed to an outside party. When changing the <code>Order</code> service, we now have to be extremely careful about making changes to that particular table - that’s even assuming it’s obvious to us that this table is being directly accessed by an outside party. The easy fix here is to have the <code>Warehouse</code> send requests to the <code>Order</code> service itself, where we can vet the request, but also hide the internal detail making subsequent changes to the <code>Order</code> service much easier.</p>

<p>If you are working on a microservice, it’s vital that you have a clear separation between what can be changed freely, and what cannot. To be explicit, as a developer you need to know when you are changing functionality that is part of the contract your service exposes to the outside world. You need to ensure that if you make changes here that you will not break upstream consumers. Functionality that doesn’t impact the contract your microservice exposes can be changed without concern.</p>

<p>It’s certainly the case that all the problems that occur with common coupling also apply with content coupling, but content coupling has some additional headaches which make it so problematic - problematic enough that some people refer to this form of coupling as <em>pathological coupling</em>.</p>

<p>When you allow an outside party to directly access <em>your</em> database, your database in effect becomes part of that external contract, albeit one you cannot easily reason about what can, or cannot, be changed. You’ve lost the ability to define what is shared (and therefore cannot be changed easily), and what is hidden. Information hiding has gone out of the window.</p>

<p>In short, avoid content coupling.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Alternatives to Domain-Oriented Decomposition"><div class="sect1" id="idm46534877690520">
<h1>Alternatives to Domain-Oriented Decomposition</h1>

<p>So far, we’ve looked at the interplay of cohesion and coupling as they apply to microservices that arrived pre-formed. And as I introduced in <a data-type="xref" href="ch01.html#intro-chapter">Chapter 1</a>, the primary mechanism we use for finding microservice boundaries is around the domain itself. Domain-oriented boundaries give us benefits in terms of making it easier to align to organizational structures, make it easier to combine functionality in different ways to deliver different experiences to users, and experience has shown that the boundaries also tend to be more stable than other forms of decomposition.</p>

<p>While I think that using the domain as the primary mechanism for identifying boundaries for microservices makes the most sense in general, there are other approaches that can be useful on occasion, either as an alternative to domain-oriented decompositon, or else as an additional tool.</p>








<section data-type="sect2" data-pdf-bookmark="Volatility"><div class="sect2" id="idm46534877659512">
<h2>Volatility</h2>

<p>I’ve increasingly heard of a push back against domain-oriented decomposition, often by advocates promoting instead that volatility should be the primary driver for decomposition. Volatility based decomposition has you identify the parts of your system going through more frequent change, and extract that functionality into their own services where they can be more effectively worked on. Conceptually, I don’t have a problem with this, but promoting it as the only way to do things isn’t helpful, especially when we consider the different drivers we might have that are pushing us towards microservices. If my biggest issue is related to the need to scale my application, a volatility-based decomposition is unlikely to deliver much of a benefit for example.</p>

<p>The mindset behind volatility-based decomposition is also evident in approaches like Bimodal IT. A concept put forward by Gartner, Bimodal IT neatly breaks the world down into the snappily named “Mode 1” (aka Systems Of Record) and “Mode 2” (aka Systems Of Innovation) categories based on how fast (or slow) different systems need to go. Mode 1 systems, otherwise known as we are told, don’t change much, don’t need much business involvement. Mode 2 is where the action is, with systems needing to change fast and needing a high-touch from the business. Putting aside for one moment the drastic oversimplification inherent in such a categorization scheme, it also implies a very fixed view of the world, and belies the sorts of transformations that are evident across industry as companies look to “go digital”. Parts of their system that didn’t need to change much in the past suddenly do, in order to open up new market opportunities and provide services to their customers in ways that they previously didn’t imagine.</p>

<p>Let’s come back to MusicCorp. Their first foray into what we now call digital was just having a webpage. All it offered back in the mid-90s was a listing of what was for sale, but you just had to phone up to place the order. It was little more than an advert in a newspaper. Then, online ordering was a thing - now the entire warehouse which had up until that point been just handled with paper had to be digitized. Who knows, perhaps MusicCorp will at some stage have to consider making music available digitally? Although you might consider that MusicCorp are behind the times, you can still appreciate the amount of upheaval  that companies have been going through as they understand how changing technology and customer behavior can require significant changes in parts of a business that couldn’t be easily forseen.</p>

<p>I also dislike bimodal IT as a concept, as it becomes a way for people to dump stuff that is hard to change into a nice neat box and say “we don’t need to deal with the issues in there - it’s Mode 1”. It’s yet another model that a company can adopt to ensure that nothing actually has to change. It also avoids the fact that quite often changes in functionality also require changes in “Systems of record” (Mode 1) to allow for changes in “Systems of Innovation” (Mode 2). In my experience, organizations adopting bimodal IT do end up having two speeds - slow and slower.</p>

<p>To be fair to proponents of volatility-based decomposition, many of them aren’t necessarily recommending such simplistic models as bimodal IT. In fact I find this technique to be highly useful to help determine boundaries if the main driver is about fast time to market - extracting functionality that is changing (or needs to change) frequently makes perfect sense in such a situation. But again, the goal determines the most appropriate mechanism.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data"><div class="sect2" id="idm46534877652392">
<h2>Data</h2>

<p>The nature of the data you hold and manage can drive you towards different forms of decomposition. For example you might want to limit what services handle personally identifiable information (PII), to reduce your risk of data breaches, but to also simplify oversight and implementation of things like GDPR.</p>

<p>For one of my recent clients, a payment company we’ll call PaymentCo, the use of certain types of data directly influenced the decisions we made about system decomposition. PaymentCo handle credit card data, and as a result it means that their system needed to comply to various requirements set down by Payment Card Industry (PCI) about how this data needs to be managed. As part of this, their system and processes needed to be audited. They had a need to handle the full credit card data, and at a volume that meant their system had to comply with PCI Level 1, which is the most stringent level, and requires quarterly external assessment of the systems and practices related to how this data is managed.</p>

<p>Many of the PCI requirements are common sense, but the requirement to ensure that the whole system complied with these requirements, not least the need for the system to be audited by an external party, was proving to be quite onerous. As a result, they wanted to split out the part of the system which handled the full credit card data - meaning that only a subset of the system required this additional level of oversight.  In <a data-type="xref" href="#pci_separation">Figure 2-12</a> we see a simplified form of the design we came up with. Services operating in the green zone (shown here enclosed by a dotted lone) never see full credit card information - this is limited to processes (and networks) in red zone (surrounded by dashes). The gateway diverted calls to the appropriate services (and the appropriate zone) - as the credit card information passed through this gateway, it was in effect also in the Red zone.</p>

<figure><div id="pci_separation" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/pci_separation.png" alt="pci separation" width="1679" height="1254" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/pci_separation.png">
<h6><span class="label">Figure 2-12. </span>PaymentCo, who segregate processes based on their use of credit card information in order to limit the scope ot PCI</h6>
</div></figure>

<p>As credit card information never flowed into the green zone, all services in this area could be exempted from a full PCI audit. Services in the red zone were in scope for such oversight. When working through the design we did everything we could to limit what had to be in this red zone. It’s key to note that we had to make sure that the credit card information never flowed to the green zone at all - if a microservice in the green zone could request this information, or that information was sent by a microservice in the red zone back to the green zone, then the clear lines of separation would break down.</p>

<p>Segregation of data is often driven by a variety of privacy and security concerns - we’ll come back to this topic and the example of PaymentCo later on in [Link to Come].</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Technology"><div class="sect2" id="idm46534877642760">
<h2>Technology</h2>

<p>The need to make use of different technology can also be a factor in terms of finding a boundary. You can accommodate different databases in a single running microservice, but if you wanted to mix different runtime models you may face a challenge. If you identify that part of your functionality needs to be implemented in a runtime like rust which enables you to eke out additional performance improvements, this ends up being a major forcing factor.</p>

<p>Of course we have to be aware of where this can drive us if adopted as a general means of decomposition. The classic three tiered architecture that we discussed in the opening chapter, and show again in <a data-type="xref" href="#ch03-musiccorp-three-tiered">Figure 2-13</a>, is an example where related technology is grouped together. As we’ve already explored, this is often a less than ideal architecture.</p>

<figure><div id="ch03-musiccorp-three-tiered" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/three-tiered.png" alt="three tiered" width="1445" height="1029" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/three-tiered.png">
<h6><span class="label">Figure 2-13. </span>A traditional three-tiered architecture is often driven by technological boundaries</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Organizational"><div class="sect2" id="idm46534877637064">
<h2>Organizational</h2>

<p>As we established when I introduced Conway’s law back in <a data-type="xref" href="ch01.html#intro-chapter">Chapter 1</a>, there is an inherent interplay between organizational structure and the system architecture you end up with. Quite aside from the studies that have shown this link, in my own anecdotal experience I have seen this play out time and time again. How you organize yourselves ends up driving your systems architecture, for good or ill. When it comes to helping us define our service boundaries, we have to consider this as a key part of our decision making.</p>

<p>Defining a service boundary whose ownership would cut across multiple different teams is unlikely to yield the outcomes we would desire - as we’ll explore further in [Link to Come], shared ownership of microservices is a fraught affair. It therefore follows that we must take account of the existing organizational structure when considering where and when to define boundaries, and in some situations perhaps even consider changing the organizational structure to support the architecture we want.</p>

<p>Even when we do work within an existing organizational structure, there is a danger that we will not get our boundaries in the right place. Many years ago, a few colleagues and I were working with a client in California, helping the company adopt some cleaner code practices and move more toward automated testing. <span class="no-kerning">We’d</span> started with some of the low-hanging fruit, such as service decomposition, when we noticed something much more worrying. I can’t go into too much detail as to what the application did, but it was a public-facing application with a large, global customer base.</p>

<p>The team, and system, had grown. Originally one person’s vision, the system had taken on more and more features, and more and more users. Eventually, the organization decided to increase the capacity of the team by having a new group of developers based in Brazil take on some of the work. The system got split up, with the front half of the application being essentially stateless, implementing the public-facing website, as shown in <a data-type="xref" href="#a30-onion">Figure 2-14</a>. The back half of the system was simply a remote procedure call (RPC) interface over a data store. Essentially, imagine <span class="no-kerning">you’d</span> taken a repository layer in your codebase and made this a separate service.</p>

<figure><div id="a30-onion" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/a30-onion.png" alt="a30 onion" width="1029" height="937" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/a30-onion.png">
<h6><span class="label">Figure 2-14. </span>A service boundary split across a technical seam</h6>
</div></figure>

<p>Changes frequently had to be made to both services. Both services spoke in terms of low-level, RPC-style method calls, which were overly brittle (we’ll discuss this further in <a data-type="xref" href="ch03.html#integration-chapter">Chapter 3</a>). The service interface was also very chatty too, resulting in performance issues. This resulted in the need for elaborate RPC-batching mechanisms. I called this <em>onion architecture</em>, as it had lots of layers and made me cry when we had to cut through it.</p>

<p>Now on the face of it, the idea of splitting the previously monolithic system along geographical/organizational lines makes perfect sense, as we’ll expand on in [Link to Come]. Here, however, rather than taking a vertical, business-focused slice through the stack, the team picked what was previously an in-process API and made a horizontal slice. A better model would have been for the team in California to have one end-to-end vertical slice, consisting of the related parts of the front end and data access functionality, with the team in Brazil taking another slice.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534877622168">
<h5>Layering Inside Vs Layering Outside</h5>
<p>As you can hopefully see by now, I’m not a fan of horizontally layered architecture. Layering though can have it’s place. Within a microservice boundary, it can be totally sensible to delineate between different layers in order to make the code easier to manage. The problem occurs when this layering becomes the mechanism by which your microservice and ownership boundaries are drawn.</p>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Different Goals, Different Drivers"><div class="sect1" id="idm46534877620152">
<h1>Different Goals, Different Drivers</h1>

<p>Microservices are not the goal.<a data-type="indexterm" data-primary="migration" data-secondary="planning" data-tertiary="understanding the goal" id="ix_migplngoa"></a><a data-type="indexterm" data-primary="microservices" data-secondary="deciding whether to adopt" id="idm46534877617096"></a> You don’t “win” by having microservices. Adopting a microservice architecture should be a conscious decision, one based on rational decision-making. You should be thinking of adopting a microservice architecture in order to achieve something that you can’t currently achieve with your existing system architecture.</p>

<p>Without having a handle on what you are trying to achieve, how are you going to inform your decision-making process about what options you should take? What you are trying to achieve by adopting microservices will greatly change where you focus your time, and how aggressive you are in creating microservices. It will also help guide you in which approach makes the most sense for finding microservice boundaries. PaymentCo for example are guided by their overriding concern about data.</p>

<p>In other words, while I can share my own views on how we should define a microservice boundary, and the fact that I think modelling them around a business domain is a sensible starting point, other factors may well come into play in determining what sort of decomposition you want to pick.</p>








<section data-type="sect2" data-pdf-bookmark="Mixing Models And Exceptions"><div class="sect2" id="idm46534877613896">
<h2>Mixing Models And Exceptions</h2>

<p>As I hope is clear so far, I am not dogmatic in terms of how you find these boundaries. If you follow the guidelines of information hiding and appreciate the interplay of coupling and cohesion, then chances are that you’ll avoid some of the worst pitfalls of whatever mechanism you pick. I happen to think that by focusing on these ideas that you are <em>more</em> likely to end up with a domain-oriented architecture, but that is by the by. The fact is though that there can often be reasons to mix models, even if “domain-oriented” is what you decide to pick as your main mechanism for defining microservice boundaries.</p>

<p>The different mechanisms we’ve outlined so far also have a lot of potential interplay between them. Being too narrow in your choices here will cause you to follow the dogma, rather than do the right thing. Volatility-based decomposition can make a lot of sense if your focus is on improving the speed of delivery, but if this causes you to extract a service which crosses organizational boundaries, then expect your pace of change to suffer due to delivery contention.</p>

<p>I might define a nice <code>Warehouse</code> service based on my understanding of the business domain, but if part of that system needs to be implemented in C++, and another part in Kotlin, then you’ll need to decompose further along those technical lines.</p>

<p>For me, organizational and domain-driven service boundaries are my starting point. It’s the usual tool I pick up, because as a general rule of thumb it’s the one that tends to work best. But it’s just that, a general model. It’s also extremely rare that the domain is the only factor driving this decision making. Typically, a number of the factors we outlined above come into play, and which ones influence your own decisions will be based on what problems you are trying to solve. You need to look at your own specific circumstances to determine what works best for you - and hopefully I’ve given you a few different options here to consider. Just remember, if someone says “The only way to do this is X!” they are likely just selling you more dogma. You can do better than that.</p>

<p>With all that said, let’s dive deeper into the topic of domain modelling, by exploring domain-driven design in a little more detail.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Just Enough Domain-Driven Design"><div class="sect1" id="idm46534877607400">
<h1>Just Enough Domain-Driven Design</h1>

<p>So as we see, modeling our services around a business domain has significant advantages for our microservice architecture.<a data-type="indexterm" data-primary="business domains" data-secondary="domain-driven design" id="idm46534877605688"></a><a data-type="indexterm" data-primary="domain-driven design (DDD)" id="idm46534877604824"></a> The question is how to come up with that model—and this is where domain-driven design (DDD) comes in.<a data-type="indexterm" data-primary="domains" data-seealso="business domains; domain-driven design" id="idm46534877603832"></a></p>

<p>The desire to have our programs better represent the real world in which the programs themselves will operate is not a new idea.<a data-type="indexterm" data-primary="programming languages" id="idm46534877602344"></a> Object-oriented programming languages like Simula were developed to allow us to model real domains. But it takes more than program language capabilities for this idea to really take shape.<a data-type="indexterm" data-primary="Domain-Driven Design (Evans)" id="idm46534877601304"></a></p>

<p>Eric Evans’ Domain-Driven Design<sup><a data-type="noteref" id="idm46534877600232-marker" href="ch02.html#idm46534877600232">10</a></sup> presented a series of important ideas that helped us better represent the problem domain in our programs. A full exploration of these ideas is outside the scope of this book, but I’ll provide a brief overview of the most important ideas in the context of microservice architectures.</p>








<section data-type="sect2" data-pdf-bookmark="Ubiquitous Language"><div class="sect2" id="idm46534877598520">
<h2>Ubiquitous Language</h2>

<p>Ubiquitous language refers to the idea that we should strive to use the same terms in our code as the users use . The idea is that by having a common language between the delivery team and the actual people who use the system it will be easier to model the real-world domain, and should also improve communication.</p>

<p>As a counter-example, I recall a situation when working at a large, global bank. We were working in the area of corporate liquidity, a fancy term that basically refers to the ability to move cash between different accounts held by the same corporate entity. The product owner was really great to work with, and she had a fantastic deep understanding of the various products that she wanted to bring to market. When working with her, we’d have discussions about things like haircuts and end-of-day sweeps, all things which made a lot of sense in her world and which had meaning to her customers.</p>

<p>The code on the other hand had none of this language in there. At some point previously, a decision had been made to use a standard data model for the database. It was widely refereed to as “The IBM banking model”, but I never got to grips as to whether or not this was a standard IBM product, or just the creation of a consultant from IBM. By defining the loose concept of an “arrangement”, the theory went that any banking operation could be modeled. Taking out a loan? That was an arrangement. Buying a share? That’s an arrangement! Applying for a credit card? Guess what - that’s an arrangement too!</p>

<p>The data model had polluted the code to such an extent that the codebase was shorn of all real understanding of the system we were building. We weren’t building a generic banking application. We were building a system specifically to manage corporate liquidity. The problem was that we had to map the rich domain language of the product owner to the generic code concepts - meaning a lot of work in helping translate. Our business analysts were often just spending their time explaining the same concepts over and over again as a result.</p>

<p>By working the real world language into the code, things became much easier. A developer picking up a story written using the terms that had come straight from the product owner was much more likely to understand their meaning and work out what needed to be done.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Aggregate"><div class="sect2" id="idm46534877592744">
<h2>Aggregate</h2>

<p>In DDD, an <em>aggregate</em> is a somewhat confusing concept, with many different definitions out there.<a data-type="indexterm" data-primary="domain-driven design (DDD)" data-secondary="aggregates" id="idm46534877590568"></a><a data-type="indexterm" data-primary="aggregates" id="idm46534877589576"></a> Is it just an arbitrary collection of objects? The smallest unit I should take out of a database? The model that has always worked for me is to first consider an aggregate as a representation of a real domain concept—think of something like an Order, Invoice, Stock Item, etc. Aggregates typically have a life cycle around them, which opens them up to being implemented as a state machine.</p>

<p>As an example in the MusicCorp domain, an Order aggregate might contain multiple line items that represent the items in the order. Those line items only have meaning as part of the overall Order aggregate.</p>

<p>We want to treat aggregates as self-contained units; we want to ensure that the code that handles the state transitions of an aggregate are grouped together, along with the state itself. So one aggregate should be managed by one microservice, although a single microservice might own management of multiple aggregates.</p>

<p>In general though, think of an aggregate as something which has state, has identity, and has a lifecycle that will be managed as part of the system. They typically refer to real-world concepts.</p>

<p>When thinking about aggregates and microservices, a single microservice will handle the life cycle and data storage of one or more different types of aggregates.<a data-type="indexterm" data-primary="microservices" data-secondary="aggregates and" id="idm46534877585784"></a> If functionality in another service wants to change one of these aggregates, it needs to either directly request a change in that aggregate, or else have the aggregate itself react to other things in the system to initiate its own state transitions, perhaps by subscribing to events issued by other microservices.</p>

<p>The key thing to understand here is that if an outside party requests a state transition in an aggregate, the aggregate can say no. You ideally want to implement your aggregates in such a way that illegal state transitions are impossible.</p>

<p>Aggregates can have relationships with other aggregates. In <a data-type="xref" href="#ch01-customer-to-order">Figure 2-15</a>, we have a Customer aggregate, which is associated with one or more Orders, and one or more Wishlists. Each of these aggregates could be managed by the same microservice, or different microservice.</p>

<figure><div id="ch01-customer-to-order" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/ch01-customer-to-order.png" alt="ch01 customer to order" width="1333" height="491" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/ch01-customer-to-order.png">
<h6><span class="label">Figure 2-15. </span>One Customer aggregate may be associated with one or more Order or Wishlist aggregates</h6>
</div></figure>

<p>If these relationships between aggregates exist inside the scope of a single microservice, the relationships could easily be stored using something like a foreign key relationship if using a relational database. If the relationships between these aggregates span microservice boundaries though, we need some way to model this relationship. In <a data-type="xref" href="#customer_references">Figure 2-16</a>, we have an entry in a financial ledger being made against a customer - this could represent a Payment aggregate. In the ledger table, we store a reference for the customer, here in the form of a URI which we might use if building a REST-based system<sup><a data-type="noteref" id="idm46534877578344-marker" href="ch02.html#idm46534877578344">11</a></sup>. We’ll revisit the topic of cross-service relationships of this nature in <a data-type="xref" href="ch03.html#integration-chapter">Chapter 3</a> to explore the nature and use of these references in more detail.</p>

<figure><div id="customer_references" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/customer_references.png" alt="customer references" width="1350" height="829" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/customer_references.png">
<h6><span class="label">Figure 2-16. </span>An example of how a relationship between two aggregates in different microservices can be implemented</h6>
</div></figure>

<p>There are lots of ways to break a system into aggregates, with some choices being highly subjective. You may, for performance reasons or ease of implementation, decide to reshape aggregates over time. To start with, though, I consider implementation concerns to be secondary, initially letting the mental model of the system users be my guiding light on initial design until other factors come into play.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Bounded Context"><div class="sect2" id="idm46534877592184">
<h2>Bounded Context</h2>

<p>A <em>bounded context</em> typically represents a larger organizational boundary inside an organization.<a data-type="indexterm" data-primary="domain-driven design (DDD)" data-secondary="bounded contexts" id="idm46534877571736"></a><a data-type="indexterm" data-primary="bounded contexts" id="idm46534877570744"></a> Within the scope of that boundary, explicit responsibilities need to be carried out. That’s all a bit wooly, so let’s look at another specific example.</p>

<p>At Music Corp, our warehouse is a hive of activity—managing orders being shipped out (and the odd return), taking delivery of new stock, having forklift truck races, and so on. Elsewhere, the finance department is perhaps less fun-loving, but still has an important function inside our organization, handling payroll, paying for shipments, and the like.</p>

<p>Bounded contexts hide implementation detail. There are internal concerns—for example, the types of forklift trucks used is of little interest to anyone other than the folks in the warehouse. These internal concerns should be hidden from the outside world—they don’t need to know, nor should they care.</p>

<p>From an implementation point of view, bounded contexts contain one or more aggregates.<a data-type="indexterm" data-primary="aggregates" data-secondary="in bounded contexts" id="idm46534877567864"></a> Some aggregates may be exposed outside the bounded context; others may be hidden internally. As with aggregates, bounded contexts may have relationships with other bounded contexts—when mapped to services, these dependencies become inter-service dependencies.</p>

<p>Let’s return for a moment to the MusicCorp business. Our domain is the whole business in which we are operating. It covers everything from the warehouse to the reception desk, from finance to ordering. We may or may not model all of that in our software, but that is nonetheless the domain in which we are operating. Let’s think about parts of that domain that look like the bounded contexts that Evans refers to.</p>










<section data-type="sect3" data-pdf-bookmark="Hidden Models"><div class="sect3" id="idm46534877565448">
<h3>Hidden Models</h3>

<p>For MusicCorp, we can then consider the finance department and the warehouse to be two separate bounded contexts. They both have an explicit interface to the outside world (in terms of inventory reports, pay slips, etc.), and they have details that only they need to know about (forklift trucks, calculators).</p>

<p>Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though—for example it needs to know about stock levels to keep the accounts up to date. <a data-type="xref" href="#a30-context-diagram">Figure 2-17</a> shows an example context diagram. We see concepts that are internal to the warehouse, like Picker (people who pick orders), shelves that represent stock locations, and so on. Likewise, entries in the general ledger is integral to finance but is not shared externally here.</p>

<figure><div id="a30-context-diagram" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/shared_model.png" alt="shared model" width="2562" height="1058" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/shared_model.png">
<h6><span class="label">Figure 2-17. </span>A shared model between the finance department and the warehouse</h6>
</div></figure>

<p>To be able to work out the valuation of the company, though, the finance employees need information about the stock we hold. The stock item then becomes a shared model between the two contexts. However, note that we don’t need to blindly expose everything about the stock item from the warehouse context. In <a data-type="xref" href="#stock_item_representations">Figure 2-18</a>, we see how <code>Stock Item</code> inside the warehouse bounded context contains references to the shelf locations, but the shared representation just contains a a count. So there is the internal-only representation, and the external representation we expose. Often, when you have different internal and external representations, it may be beneficial to name them differently to avoid confusion - in this situation, one approach could be to call the shared <code>Stock Item</code> a <code>Stock Count</code> instead.</p>

<figure><div id="stock_item_representations" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/stock_item_representations.png" alt="stock item representations" width="1133" height="570" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/stock_item_representations.png">
<h6><span class="label">Figure 2-18. </span>A model which is shared can decide to hide information that should not be shared externally</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Shared Models"><div class="sect3" id="idm46534877565176">
<h3>Shared Models</h3>

<p>We can also have concepts which appear in more than one bounded context. In <a data-type="xref" href="#a30-context-diagram">Figure 2-17</a> we saw that a customer exists in both locations. What does this mean? Is the customer copied? The way to think about this is that conceptually, both finance and warehouse needs to know something about our customer. Finance need to know about the financial payments made to a customer, whereas the Warehouse needs to know about the customer to the extent that it knows what packages have been sent to allow for deliveries to be traced.</p>

<p>When you have a situation like this, a shared model like customer can have different meanings in the different bounded contexts, and therefore might be called different things. We might be happy to keep the name “customer” in Finance, but in Warehouse we might call them a “recipient”, as that is the role they play in that context. We store information about the customer in both locations, but the information is different. Finance stores information about the customer’s financial payments (or refunds), the warehouse stores information related to the goods shipped. We still may need to link both local concepts to a global customer, and we may want to look up common, shared information about that customer like their name or email address - we could use a technique like that shown in <a data-type="xref" href="#customer_references">Figure 2-16</a> to achieve this.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Mapping Aggregates and Bounded Contexts to Microservices"><div class="sect2" id="idm46534877946008">
<h2>Mapping Aggregates and Bounded Contexts to Microservices</h2>

<p>Both the aggregate and the bounded context give us units of cohesion with well-defined interfaces with the wider system.<a data-type="indexterm" data-primary="domain-driven design (DDD)" data-secondary="mapping aggregates and bounded contexts to microservices" id="idm46534877944568"></a><a data-type="indexterm" data-primary="aggregates" data-secondary="mapping with bounded contexts to microservices" id="idm46534877943576"></a><a data-type="indexterm" data-primary="bounded contexts" data-secondary="mapping with aggregates to microservices" id="idm46534877942600"></a><a data-type="indexterm" data-primary="microservices" data-secondary="mapping aggregates and bounded contexts to" id="idm46534877941624"></a> The aggregate is a self-contained state machine that focuses on a single domain concept in our system, with the bounded context representing a collection of associated aggregates, again with an explicit interface to the wider world.</p>

<p>Both can therefore work well as service boundaries. When starting out, as I’ve already mentioned, you want to reduce the number of services you work with. As a result, you should probably target services that encompass entire bounded contexts. As you find your feet, and decide to break these services into smaller services, look to split them around aggregate boundaries.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Turtles All the Way Down"><div class="sect2" id="idm46534877939240">
<h2>Turtles All the Way Down</h2>

<p>At the start, you will probably identify a number of coarse-grained bounded contexts. But these bounded contexts can in turn contain further bounded contexts. For example, you could decompose the warehouse into capabilities associated with order fulfillment, inventory management, or goods receiving. When considering the boundaries of your microservices, first think in terms of the larger, coarser-grained contexts, and then subdivide along these nested contexts when you’re looking for the benefits of splitting out these seams.</p>

<p>A trick here is that even if you decide to split a service that models an entire bounded context into smaller services later on, you can still hide this decision from the outside world—perhaps by presenting a coarser-grained API to consumers. The decision to decompose a service into smaller parts is arguably an implementation decision, so we might as well hide it if we can. in <a data-type="xref" href="#nested-services">Figure 2-19</a> we see an example of this. We’ve split <code>Warehouse</code> down into <code>Inventory</code> and <code>Shipping</code>. As far as the outside world is concerned, there is still just the <code>Warehouse</code> microservice. Internally though, we’ve further decomposed things to allow <code>Inventory</code> to manage <code>Stock Items</code> and have <code>Shipping</code> manage <code>Shipments</code>. Remember, we want to keep the ownership of a single aggregate inside a single microservice.</p>

<figure><div id="nested-services" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/nested-services.png" alt="nested services" width="2204" height="1158" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/nested-services.png">
<h6><span class="label">Figure 2-19. </span>The Warehouse service internally has been split into a Finance and Warehouse microservice</h6>
</div></figure>

<p>This is another form of information hiding - we’ve hidden a decision about internal implementation in such a way that if this implementation detail changes again in the future then our consumers will be unaware.</p>

<p>Another reason to prefer the nested approach could be to chunk up your architecture to simplify testing. For example, when testing services that consume the warehouse, I don’t have to stub each service inside the warehouse context, just the more coarse-grained API. This can also give you a unit of isolation when considering larger-scoped tests. I may, for example, decide to have end-to-end tests where I launch all services inside the warehouse context, but for all other collaborators I might stub them out. We’ll explore more about testing and isolation in [Link to Come].</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Dangers Of Premature Decomposition"><div class="sect2" id="idm46534877927032">
<h2>The Dangers Of Premature Decomposition</h2>

<p>There is a danger in creating decomposing microservices based on an unclear understanding of the domain. One such example comes from my previous company, ThoughtWorks. One of their products was called SnapCI, a hosted continuous integration and continuous delivery tool (we’ll discuss those concepts later in [Link to Come]). The team had previously worked on another similar tool, Go-CD, a now open source continuous delivery tool that can be deployed locally rather than being hosted in the cloud.</p>

<p>Although there was some code reuse very early on between the SnapCI and Go-CD projects, in the end SnapCI turned out to be a completely new codebase. Nonetheless, the previous experience of the team in the domain of CD tooling emboldened them to move more quickly in identifying boundaries, and building their system as a set of microservices.</p>

<p>After a few months, though, it became clear that the use cases of SnapCI were subtly different enough that the initial take on the service boundaries wasn’t quite right. This led to lots of changes being made across services, and an associated high cost of change. Eventually the team merged the services back into one monolithic system, giving them time to better understand where the boundaries should exist. A year later, the team was then able to split the monolithic system apart into microservices, whose boundaries proved to be much more stable. This is far from the only example of this situation I have seen. Prematurely decomposing a system into microservices can be costly, especially if you are new to the domain. In many ways, having an existing codebase you want to decompose into microservices is much easier than trying to go to microservices from the beginning for this very reason.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Communication in Terms of Business Concepts"><div class="sect2" id="idm46534877922184">
<h2>Communication in Terms of Business Concepts</h2>

<p>The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality—capabilities—that are exposed to our customers. If our systems are decomposed along the bounded contexts that represent our domain, the changes we want to make are more likely to be isolated to one, single microservice boundary. This reduces the number of places we need to make a change, and allows us to deploy that change quickly.</p>

<p>It’s also important to think of the communication between these microservices in terms of the same business concepts. The modeling of your software after your business domain shouldn’t stop at the idea of bounded contexts. The same terms and ideas that are shared between parts of your organization should be reflected in your interfaces. It can be useful to think of forms being sent between these microservices, much as forms are sent around an organization.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Event-storming"><div class="sect1" id="idm46534877606776">
<h1>Event-storming</h1>

<p><em>Event Storming</em>, a technique developed by Alberto Brandolini, is a collaborative brainstorming exercise designed to help surface a domain-model. Rather than having an architect sit in a corner and come up with their own representation of what the domain model is<sup><a data-type="noteref" id="idm46534877917224-marker" href="ch02.html#idm46534877917224">12</a></sup>, event storming brings together technical and non-technical stakeholders in a joint exercise. The idea is that by making the development of the domain model a joint activity, that you end up with a shared, joined-up view of the world.</p>

<p>It’s worth mentioning at this point that while the domain models defined using event storming can be used to implement event-driven systems, and indeed the mapping is very straightforward, you can also use such a domain model to build a more request/response oriented system too.</p>








<section data-type="sect2" data-pdf-bookmark="Logistics"><div class="sect2" id="idm46534877915464">
<h2>Logistics</h2>

<p>Alberto has some very specific views as to how event storming should be run, and on some of these points I am very much in agreement. Firstly, get everyone in a room together. This is often the most difficult step - getting people’s calendars to line up can be a problem, as can finding a room big enough. Those issues were all true in a pre-covid world, but as I write this during the virus-related lockdown in the UK, I’m aware that this might be even more problematic in the future. The key here though is to have all stakeholders present at the same time. You want representatives for all parts of the domain that you plan to model - users, subject matter experts, product owners, whoever is best placed to help represent that part of the domain.</p>

<p>Once in a room together, Alberto suggests the removal of all chairs, in order to make sure that everyone gets up and gets involved. As someone with a bad back, while this is something I understand, it may not work for everyone. One thing I do agree with Alberto about is the need to have a large space where the modelling can be done. A common solution here is to pin large rolls of brown paper to the walls of the room, allowing for all the walls to be used for capturing information.</p>

<p>The main modelling tool is post-it notes to capture different concepts, with different coloured post-it notes representing different concepts.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Process"><div class="sect2" id="idm46534877911528">
<h2>The Process</h2>

<p>The exercise starts with the participants identifying the <em>domain events</em>. These represent things that happen in the system - they are the facts that you care about. “Order Placed” would be a good event that we would care about in the context of MusicCorp, as would “Payment Received”. These are captured on orange post-it notes. It is at this point that I have another disagreement with Alberto’s structure here, as the events are far and away the most numerous things you’ll be capturing, and orange post-it notes are surprisingly hard to get hold off<sup><a data-type="noteref" id="idm46534877908952-marker" href="ch02.html#idm46534877908952">13</a></sup>.</p>

<p>Next, participants identify the commands that cause these events to happen. Commands are decisions made by a human to do something (a user of the software). Here you are trying to understand the boundary of the system, and identify the key human actors in the system. Commands are captured on blue post-it notes.</p>

<p>For the techies in the event storming session, at this stage they should be listening to what their non-technical colleagues come up with here. A key part of this exercise is not to let any current implementation warp the perception of what the domain is (that comes later). At this stage you want to create a space where you can get the concepts out of the heads of the key stakeholders, and get these ideas out into the open.</p>

<p>With events and commands captured, aggregates come next. The events you have at this stage are useful sharing not just what happens in the system, but also it starts to highlight what the potential aggregates might be. Think of the aforementioned domain event “Order Placed”. The noun here - Order - could well be a potential aggregate. And “Placed” is something that can happen to an order, so this may well be part of the life-cycle of the aggregate. Aggregates are represented by yellow post-it notes, and the commands and events associated with that aggregate are moved and clustered around the aggregate. This also helps you understand how aggregates are related to each other - events from one aggregate might trigger behavior in another.</p>

<p>With the aggregates identified, they are then grouped into bounded contexts. Bounded contexts most commonly follow a company’s organizational structure, and the participants of the exercise are well placed to understand what aggregates are used by which parts of the organization.</p>

<p>There is more to event storming than this - it was just meant as a brief overview. For a more detailed overview of how Event Storming works I’d suggest you read the (currently in progress) book “Event Storming<sup><a data-type="noteref" id="idm46534877904104-marker" href="ch02.html#idm46534877904104">14</a></sup>" by Alberto</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46534877901992">
<h1>Summary</h1>

<p>In this chapter, you’ve learned a bit about what makes a good microservice boundary, and how to find seams in our problem space that give us the dual benefits of both low coupling and strong cohesion. Having a detailed understanding of your domain can be a vital tool in helping us find these seams, and by aligning our microservices to these boundaries we ensure that the resulting system has every chance of keeping those virtues intact. We’ve also got a hint about how we can subdivide our microservices further, something we’ll explore in more depth later. And we also introduced MusicCorp, the example domain that we will use throughout this book.</p>

<p>The ideas presented in Eric Evans’s <em>Domain-Driven Design</em> are very useful to us in finding sensible boundaries for our services, and I’ve just scratched the surface here. I recommend Vaughn Vernon’s book <em>Implementing Domain-Driven Design</em> (Addison-Wesley) to help you understand the practicalities of this approach.</p>

<p>Although this chapter has been mostly high-level, we need to get much more technical in the next. There are many pitfalls associated with implementing interfaces between services that can lead to all sorts of trouble, and we will have to take a deep dive into this topic if we are to keep our systems from becoming a giant, tangled mess.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46534877723816"><sup><a href="ch02.html#idm46534877723816-marker" class="totri-footnote">1</a></sup> Parnas, David, “On the criteria to be used in decomposing systems into modules”, 1971 <a href="https://kilthub.cmu.edu/articles/On_the_criteria_to_be_used_in_decomposing_systems_into_modules/6607958"><em class="hyperlink">https://kilthub.cmu.edu/articles/On_the_criteria_to_be_used_in_decomposing_systems_into_modules/6607958</em></a></p><p data-type="footnote" id="idm46534877716056"><sup><a href="ch02.html#idm46534877716056-marker" class="totri-footnote">2</a></sup> The obvious starting point is Adrian’s summary of “On the criteria…” <a href="https://blog.acolyer.org/2016/09/05/on-the-criteria-to-be-used-in-decomposing-systems-into-modules/"><em class="hyperlink">https://blog.acolyer.org/2016/09/05/on-the-criteria-to-be-used-in-decomposing-systems-into-modules/</em></a>, but the coverage of Parnas’ earlier work “Information Distribution Aspects of Design Methodology” contains some great insights along with commentary from Parnas himself: <a href="https://blog.acolyer.org/2016/10/17/information-distribution-aspects-of-design-methodology/"><em class="hyperlink">https://blog.acolyer.org/2016/10/17/information-distribution-aspects-of-design-methodology/</em></a></p><p data-type="footnote" id="idm46534877711064"><sup><a href="ch02.html#idm46534877711064-marker" class="totri-footnote">3</a></sup> Parnas, David, “Information distribution aspects of design methodology”, 1971</p><p data-type="footnote" id="idm46534877695976"><sup><a href="ch02.html#idm46534877695976-marker" class="totri-footnote">4</a></sup> In my book, Monolith To Microservices, I attributed this to Larry Constantine himself. While the statement neatly sums up much of Constantine’s work in this space, the quote should really be attributed to Albert Endres and Dieter Rombach from their book “A Handbook of Software and Systems Engineering”.</p><p data-type="footnote" id="idm46534877685576"><sup><a href="ch02.html#idm46534877685576-marker" class="totri-footnote">5</a></sup> Constantine, Larry and Edward Yourdon, Structured Design, Yourdon Press</p><p data-type="footnote" id="idm46534877684712"><sup><a href="ch02.html#idm46534877684712-marker" class="totri-footnote">6</a></sup> Page-Jones, Meilir, Practical Guide to Structured Systems Design (Yourdon Press Computing)</p><p data-type="footnote" id="idm46534878148648"><sup><a href="ch02.html#idm46534878148648-marker" class="totri-footnote">7</a></sup> This concept is similar to the Domain Application Protocol which defines the rules by which components interact in a REST-based system.</p><p data-type="footnote" id="idm46534878125080"><sup><a href="ch02.html#idm46534878125080-marker" class="totri-footnote">8</a></sup> Pass through coupling is my name for what was originally described as Tramp coupling by M. Page-Jones: The Practical Guide to Structured Systems Design. I chose to use a different term here due to the fact that I found the original term somewhat problematic, and not terribly meaningful to a wider audience</p><p data-type="footnote" id="idm46534877682408"><sup><a href="ch02.html#idm46534877682408-marker" class="totri-footnote">9</a></sup> OK, more than once or twice. A <strong>lot</strong> more than once or twice…</p><p data-type="footnote" id="idm46534877600232"><sup><a href="ch02.html#idm46534877600232-marker">10</a></sup> Eric Evans, <em>Domain-Driven Design: Tackling Complexity in the Heart of Software</em> (Boston: Addison-Wesley, 2004).</p><p data-type="footnote" id="idm46534877578344"><sup><a href="ch02.html#idm46534877578344-marker">11</a></sup> I know some people object to the use of templated URIs in REST systems, and I understand why - I just want to keep things simple for this example</p><p data-type="footnote" id="idm46534877917224"><sup><a href="ch02.html#idm46534877917224-marker">12</a></sup> I mean no disrespect if this is you - I’ve done this myself more than once</p><p data-type="footnote" id="idm46534877908952"><sup><a href="ch02.html#idm46534877908952-marker">13</a></sup> I mean, why not yellow? It’s the most common colour!</p><p data-type="footnote" id="idm46534877904104"><sup><a href="ch02.html#idm46534877904104-marker">14</a></sup> Event  Storming, Alberto Brandolini, Leanpub (work in progress) <a href="https://leanpub.com/introducing_eventstorming"><em class="hyperlink">https://leanpub.com/introducing_eventstorming</em></a></p></div></div></section>
<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Microservice Communication Styles"><div class="chapter" id="integration-chapter">
<h1><span class="label">Chapter 3. </span>Microservice Communication Styles</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534877896136">
<h5>Work In Progress</h5>
<p>Please note that the text below is currently being reworked for the 2nd edition of the book, and is not in a complete state. This will be Chapter 3 of the final book.</p>

<p>If you have any feedback on the book, or suggestions for the 2nd edition, then please contact me on <a href="mailto:book-feedback@samnewman.io">book-feedback@samnewman.io</a> and/or complete a short survey here: <a href="https://oreil.ly/Bldg_MicroServices_survey"><em class="hyperlink">https://oreil.ly/Bldg_MicroServices_survey</em></a>.</p>
</div></aside>

<p>Getting communication between microservices right is problematic for many, in great part due to the fact that I feel that people gravitate towards a chosen technological approach without first considering the different types of communication you might want. In this chapter, I’ll try and tease apart the different styles of communication, to help you understand the pros and cons of each, and also help you understand which approach will best fit your problem space.</p>

<p>We’ll be looking at synchronous blocking and asynchronous non-blocking communication mechanisms, as well as comparing request-response collaboration with event-driven collaboration.</p>

<p>By the end of this chapter you should be much better prepared to understand the different options available to you, and will have a foundational knowledge that will help when we start looking at more detailed implementation concerns in the following chapters.</p>






<section data-type="sect1" data-pdf-bookmark="From In-Process To Inter-Process"><div class="sect1" id="idm46534877889720">
<h1>From In-Process To Inter-Process</h1>

<p>OK, let’s get the easy stuff out of the way first - or at least what I <em>hope</em> is the easy stuff. Namely, calls <em>between</em> different processes across a network (inter-process) are <strong>very</strong> different to calls <em>within</em> a single process (in-process). At one level, we can ignore this distinction. It’s easy, for example, to think of one object making a method call on another object, then just map this interaction to two microservices communicating via a network. Putting aside the fact that microservices aren’t just objects, this thinking can get us into a lot of trouble.</p>

<p>Let’s look at some of these differences now, and how they might change how you think about the interactions between your microservices.</p>








<section data-type="sect2" data-pdf-bookmark="Performance"><div class="sect2" id="idm46534877885224">
<h2>Performance</h2>

<p>The performance of an in-process call and an inter-process call is fundamentally different. When I make an in-process call, the underlying compiler and runtime can carry out a whole host of optimizations to reduce the impact of the call, including inlining the invocation so it’s as though there was never a call in the first place. No such optimizations are possible with inter-process calls. Packets have to be sent. Expect the overhead of an inter-process call to be significant compared to the overhead of an in-process call. The former is very measurable - just round-tripping a single packet in a data centre is measured in milliseconds - whereas the overhead of making a method call is something you don’t need to worry about.</p>

<p>This can often lead you to want to rethink APIs. An API that makes sense in-process may not make sense in inter-process situations. I can make 1000 calls across an API boundary in-process without concern. Do I want to make 1000 network calls between two microservices? Perhaps not.</p>

<p>When I pass a parameter into a method, the data structure I pass in typically doesn’t move - what’s more likely is that I pass around a pointer to a memory location. Passing in an object or data structure to another method doesn’t necessitate more memory to be allocated in order to copy the data.</p>

<p>When making calls between microservices over a network on the other hand, the data actually has to be serialized into some form that can be transmitted over a network. The data then needs to be sent, and deserialized at the other end. We therefore may need to be more mindful about the size of payloads being sent between processes. When was the last time you were aware of how big a data structure was that you were passing around inside a process? The reality is that you likely didn’t need to know - now, you do. This might lead you to reduce the amount of data being sent or received (perhaps not a bad thing if we think about information hiding), pick more efficient serialization mechanisms, or even offload data to a file system and pass around pointers to that data instead.</p>

<p>These differences may not cause you issues straight away, but you certainly need to be aware of them. I’ve seen a lot of attempts to hide from the developer the fact that a network call is even taking place. Our desire to create abstractions to hide detail is a big part of what allows us to do more things more efficiently, but sometimes we create abstractions that hide too much. A developer needs to be aware if they are doing something that will result in a network call, otherwise do not be surprised if you end up with some nasty performance bottlenecks further down the line.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Changing Interfaces"><div class="sect2" id="idm46534877879080">
<h2>Changing Interfaces</h2>

<p>When we consider changes to an interface inside a process, the act of rolling out the change is straightforward. Both the code implementing the interface, and the code calling the interface, are all packaged together in the same process. In fact if I change a method signature using an IDE with refactoring capability, often the IDE itself will automatically refactor calls to this changing method. Rolling out such a change can be done in an atomic fashion - both sides of the interface are packaged together in a single process.</p>

<p>With communication between microservices, however, the microservice exposing an interface, and the consuming microservices using that interface, are separately deployable microservices. When making a backwards incompatible change to a microservice interface, we either need to do a lock-step deployment with consumers, making sure they are updated to use the new interface, or else find some way to phase the rollout of the new microservice contract. We’ll explore this concept in more detail later in this chapter.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Error handling"><div class="sect2" id="idm46534877876120">
<h2>Error handling</h2>

<p>Within a process, if I call a method, the nature of the errors tends to be pretty straightforward. Simplistically, the errors are either expected and easy to handle, or else they are catastrophic to the point where we just propagate the error up the call stack. Errors, on the whole, are deterministic.</p>

<p>With a distributed system, the nature of errors can be different. You are vulnerable to a host of errors that are outside of your control. Networks time out. Downstream microservices might be temporarily unavailable. Networks get disconnected, containers get killed due to consuming too much memory, and in extreme situations, bits of your data centre can catch fire<sup><a data-type="noteref" id="idm46534877873432-marker" href="ch03.html#idm46534877873432" class="totri-footnote">1</a></sup>.</p>

<p>Many of these errors are often transient in nature - they are short-lived problems that might go away, and therefore are things you might want to retry - think of a simple network timeout. Other problems can’t be dealt with easily. As a result, it can become important to have a richer set of semantics for returning errors in a way that can allow for clients to take appropriate action.</p>

<p>HTTP is an example of a protocol that understands the importance of this. Every HTTP response has a code, with the 400 and 500 series codes being reserved for errors. 400 series error codes are request errors - essentially, a downstream service is telling the client that there is something wrong with the original request. As such, it’s probably something you should give up with - is there any point retrying a <code>404 Not Found</code> for example? The 500 series response codes relate to downstream issues, a subset of which indicate to the client that the issue might be temporary. A <code>503 Service Unavailable</code> for example indicates that the downstream serer is unable to handle the request, but that this could be a temporary state. In which case, an upstream client might decide to retry this request. On the other hand, if a client received a <code>501 Not Implemented</code> response, a retry is unlikely to help much.</p>

<p>Whether or not you pick a HTTP-based protocol for communication between microservices, if you have a rich set of semantics around the nature of the error, you’ll make it easier for clients to carry out compensating actions, which in turn should help you build more robust systems.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Technology for Inter-process Communication: So Many Choices"><div class="sect1" id="idm46534877868536">
<h1>Technology for Inter-process Communication: So Many Choices</h1>
<blockquote>
<p>“And in a world where we have too many choices and too little time, the obvious thing to do is just ignore stuff.”</p>
<p data-type="attribution">Seth Godin</p>
</blockquote>

<p>The range of technology available to us for inter-process communication is vast. As a result, we can often be overburdened with choice. Often, I find people just gravitate to technology which is familiar to them, or perhaps just the latest hot technology they learned about from a conference. The problem with this is that when you buy into a specific technology choice, you are often buying into a set of ideas (and constraints) that come along for the ride. These constraints might not be the right ones for you - and the mindset behind the technology may not actually line up with the problem you are trying to solve.</p>

<p>If you’re trying to build a website, single page app technology like Angular or React is a bad fit. Likewise, trying to use Kafka for request-response really isn’t a good idea, as it was designed for more event-based interactions (topics we’ll get to in just a moment). And yet I see technology used in the wrong place time and time again. People pick the new shiny tech (like microservices!) without considering whether or not it fits their problem.</p>

<p>When it comes to the bewildering array of technology available to us for communication between microservices, I therefore think it is important to talk first about the style of communication you want, and only then look for the right technology to implement these styles. With that in mind, let’s take a look at a model I’ve been using for several years to help distinguish between the different approaches for microservice-to-microservice communication, which in turn can help you filter the technology options you’ll want to look at.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Styles of Microservice Communication"><div class="sect1" id="idm46534877862728">
<h1>Styles of Microservice Communication</h1>

<p>In <a data-type="xref" href="#comms-styles">Figure 3-1</a> we see an outline for the model I use for thinking about different styles of communication. This model is not meant to be entirely exhaustive (I’m not trying to present a grand unified theory of inter-process communication here), more that it provides a good high-level overview for considering the different styles of communication which are most widely used for microservice architectures.</p>

<figure><div id="comms-styles" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/comms-styles.png" alt="Different styles of inter-microservice communication along with example implementing technologies" width="1516" height="616" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/comms-styles.png">
<h6><span class="label">Figure 3-1. </span>Different styles of inter-microservice communication along with example implementing technologies</h6>
</div></figure>

<p>We’ll look at each element in more detail shortly, but first I’d like to briefly outline the different elements of this model.</p>
<dl>
<dt>Synchronous Blocking</dt>
<dd>
<p>A microservice makes a call to another microservice and blocks operation waiting for the response.</p>
</dd>
<dt>Asynchronous Non-Blocking</dt>
<dd>
<p>The microservice emitting a call is able to carry on processing whether or not the call is received.</p>
</dd>
<dt>Request-response</dt>
<dd>
<p>A Microservice sends a request to another microservice asking for something to be done. It expects to receive a response to the request informing it of the result.</p>
</dd>
<dt>Event-Driven</dt>
<dd>
<p>Microservices emit events, which other microservices consume and react to accordingly. The microservice emitting the event is unaware of which microservices, if any, consume the events it emits.</p>
</dd>
<dt>Common Data</dt>
<dd>
<p>Not often seen as a communication style, microservices collaborate via some shared data source.</p>
</dd>
</dl>

<p>When using this model to help teams decide on the right approach, I spend a lot of time understanding the context in which they are operating. Their needs in terms of reliable communication, acceptable latency and volume of communication are all going to play a part in making a technology choice. But in general, I tend to start with deciding if synchronous or asynchronous communication is more appropriate for the given situation. If synchronous communication is an option, then I am firmly in the world of request-response communication. If asynchronous communication makes more sense, then I have a second choice to make, which is whether or not event-driven, request-response-based or common data-based communication is more appropriate. As we’ll explore, event-driven communication is fundamentally asynchronous, but request-response calls can be implemented synchronously or asynchronously.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Pattern: Synchronous Blocking"><div class="sect1" id="sync-blocking-call">
<h1>Pattern: Synchronous Blocking</h1>

<p>With a synchronous blocking call, a microservice sends a call of some kind to a downstream process (likely another microservice), and blocks until the call has completed, and potentially until a response has been received. In <a data-type="xref" href="#sync-example">Figure 3-2</a>, the <code>Order Processor</code> sends a call to the <code>Loyalty</code> microservice to inform it that some points should be added to a customer’s account.</p>

<figure><div id="sync-example" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/sync-example.png" alt="Order Processor sends a synchronous call to the Loyalty microservice, blocks and waits for a response" width="808" height="475" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/sync-example.png">
<h6><span class="label">Figure 3-2. </span>Order Processor sends a synchronous call to the Loyalty microservice, blocks and waits for a response</h6>
</div></figure>

<p>Typically, a synchronous blocking call is one that is waiting for a response from the downstream process. This may be because the result of the call is needed for some further operation, or just because it wants to make sure the call worked and if not carry out some sort of retry. As a result, virtually all synchronous blocking calls I see would also constitute being a request-response call, something we’ll look at shortly.</p>








<section data-type="sect2" data-pdf-bookmark="Advantages"><div class="sect2" id="idm46534877841192">
<h2>Advantages</h2>

<p>There is something simple and familiar about a blocking, synchronous call. Many of us learned to program in a fundamentally synchronous style, reading a piece of code like a script, with each line executing in turn, with the next line of code waiting its turn to do something. Most of the situations where you would have used inter-process calls were probably done so in a synchronous, blocking style. Running a SQL query on a database for example, or making a HTTP request of a downstream API.</p>

<p>When moving from a less distributed architecture, like that of a single process monolith, it can make sense to stick with those ideas that are familiar when there is so much else going on that is brand new.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Disadvantages"><div class="sect2" id="idm46534877838456">
<h2>Disadvantages</h2>

<p>The main challenge with synchronous calls is the inherent temporal coupling that occurs, a topic we explored briefly in <a data-type="xref" href="ch02.html#modelling-services-chapter">Chapter 2</a>. When the <code>Order Processor</code> makes a call to <code>Loyalty</code> in the example above, the <code>Loyalty</code> microservice needs to be reachable in order for the call to work. If the <code>Loyalty</code> microservice is unavailable, then the call will fail and <code>Order Processor</code> needs to work out what kind of compensating action to carry out - this might involve an immediate retry, buffering the call to retry later, or perhaps giving up altogether.</p>

<p>As the sender of the call is blocking and waiting for the downstream microservice to respond, it also follows that if the downstream microservice responds slowly, or if there is an issue with the latency of the network, then the sender of the call will be blocked for a prolonged period of time waiting for a response. If the <code>Loyalty</code> microservice is under significant load, and is responding slowly to requests, this in turn will cause the <code>Order Processor</code> to respond slowly.</p>

<p>The use of synchronous calls can therefore make a system more vulnerable to cascading issues caused by downstream outages more readily than asynchronous calls.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Where To Use It"><div class="sect2" id="idm46534877830968">
<h2>Where To Use It</h2>

<p>For simple microservice architectures, I don’t have a massive problem with the use of synchronous, blocking calls. Their familiarity for many people is an advantage when getting to grips with distributed systems.</p>

<p>For me, where these types of calls start to be problematic is when you start having more chains of calls - in <a data-type="xref" href="#fraud-detection-long-chain">Figure 3-3</a> for example, we have an example flow from MusicCorp, where we are checking a payment for potentially fraudulent activity. The <code>Order Processor</code> calls the <code>Payment</code> service to take payment. The <code>Payment</code> service in turn wants to check with the <code>Fraud Detection</code> microservice as to whether or not this should be allowed. The <code>Fraud Detection</code> microservice in turn needs to get information from the <code>Customer</code> microservice.</p>

<figure><div id="fraud-detection-long-chain" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/fraud-detection-long-chain.png" alt="Checking for potentially fraudulent behavior as part of order processing flow" width="1066" height="566" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/fraud-detection-long-chain.png">
<h6><span class="label">Figure 3-3. </span>Checking for potentially fraudulent behavior as part of order processing flow</h6>
</div></figure>

<p>If all of these calls are synchronous and blocking, there are a number of issues we might face. A issue in any of the four involved microservices, or in the network calls between them, could cause the whole operation to fail - we arguably have a greater surface area for failure. This is quite aside from the fact that these kinds of long chains can cause significant <em>resource contention</em>. Behind the scenes, the <code>Order Processor</code> likely has a network connection open waiting to hear back from <code>Payment</code>. <code>Payment</code> in turn has a network connection open waiting for a response from <code>Fraud Detection</code> and so on. Having a lot of connections that need to be kept open can have an impact on the running system - you are much more likely to experience issues where you run out of available connections, or suffer from increased network congestion as a result.</p>

<p>To improve this situation, we could re-examine the interactions between the microservices in the first place. For example, maybe we take the use of the <code>Fraud Detection</code> out of the main purchase flow, as shown in <a data-type="xref" href="#fraud-detection-background">Figure 3-4</a>, and instead have it run in the background. If it finds a problem with a specific customer their records are updated accordingly, and this is something that could be checked earlier in the payment process. Effectively, this means we’re doing some of this work in parallel. By reducing the length of the call chain we’ll see the overall latency of the operation improve, and take one of our microservices (Fraud Detection) out of the critical path for the purchase flow, giving us one fewer dependencies to worry about for what is a critical operation.</p>

<figure><div id="fraud-detection-background" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/fraud-detection-background.png" alt="Moving fraud detection to a background process can reduce the concerns around the length of the call chain" width="704" height="679" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/fraud-detection-background.png">
<h6><span class="label">Figure 3-4. </span>Moving fraud detection to a background process can reduce the concerns around the length of the call chain</h6>
</div></figure>

<p>We could also of course replace the use of blocking calls with some style of non-blocking interaction without changing the workflow here, something we’ll explore next.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Pattern: Asynchronous Non-blocking"><div class="sect1" id="async-non-blocking-call">
<h1>Pattern: Asynchronous Non-blocking</h1>

<p>With asynchronous communication, the act of sending a call out over the network doesn’t block the microservice issuing the call. It is able to carry on with any other processing without having to wait for a response. If a response is needed, it is able to handle that response when it returns. Non-blocking asynchronous communication comes in many forms, but we’ll be looking in more detail at the three most common styles I see in microservice architecture. They are:</p>
<dl>
<dt>Communication Though Common Data</dt>
<dd>
<p>The upstream microservice changes some common data, which one or more microservices later make use of.</p>
</dd>
<dt>Request-Response</dt>
<dd>
<p>A microservice sends a request to another microservice asking it to do something. When the requested operation completes, successfully or not, the upstream microservice receives the response.</p>
</dd>
<dt>Event-Driven Interaction</dt>
<dd>
<p>A microservice broadcasts an event, which can be thought of as a factual statement as to something that has happened. Other microservices can listen for the events they are interested in and react accordingly.</p>
</dd>
</dl>








<section data-type="sect2" data-pdf-bookmark="Advantages"><div class="sect2" id="idm46534876013416">
<h2>Advantages</h2>

<p>With non-blocking asynchronous communication the microservice making the initial call, and the microservice (or microservices) receiving the call, are decoupled temporarly. The microservices that receive the call do not need to be reachable at the same time the call is made. This means we avoid the concerns of temporal decoupling that we discussed in <a data-type="xref" href="ch02.html#modelling-services-chapter">Chapter 2</a> (see <a data-type="xref" href="ch02.html#temporal_coupling_overview">“A Brief Note On Temporal Coupling”</a>).</p>

<p>This style of communication is also beneficial if the functionality being triggered by a call will take a long time to process. Let’s come back to our example of MusicCorp, and specifically the process of sending out a package. In <a data-type="xref" href="#warehouse-async">Figure 3-5</a>, the <code>Order Processor</code> has taken payment, and decided that it is time to dispatch the package, so it sends a call to the <code>Warehouse</code> microservice. The process of finding the CDs, taking them off the shelf, packaging them up, and having them picked up, could take many hours, potentially days, depending on how the actual dispatch process works. It makes sense therefore for the <code>Order Processor</code> to issue a non-blocking asynchronous call to the <code>Warehouse</code>, and have the <code>Warehouse</code> call back to the <code>Order Processor</code> later on to inform it of progress. This is a form of asynchronous request-response communication.</p>

<figure><div id="warehouse-async" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/warehouse-async.png" alt="The Order Processor kicks off the process to package and ship an order, which is done in an asynchronous fashion" width="887" height="683" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/warehouse-async.png">
<h6><span class="label">Figure 3-5. </span>The Order Processor kicks off the process to package and ship an order, which is done in an asynchronous fashion</h6>
</div></figure>

<p>If we tried doing something similar with synchronous blocking calls, then we’d either have to restructure the interactions between <code>Order Processor</code> and <code>Warehouse</code> - it wouldn’t be feasible for <code>Order Processor</code> to open a connection, send a request, block any further operations in the calling the thread, and wait for what might be hours or days waiting for a response.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Disadvantages"><div class="sect2" id="idm46534876000744">
<h2>Disadvantages</h2>

<p>The main downsides of non-blocking asynchronous communication, relative to blocking synchronous communication, is the level of complexity and range of choice. As we’ve already outlined, there are different styles of asynchronous communication to choose from - which is right for you? When we start digging into how these different styles of communication are implemented, there is a potentially bewildering list of technology we could look at.</p>

<p>If asynchronous communication doesn’t map to your mental models of computing, adopting an asynchronous style of communication will be challenging at first. And as we’ll explore further when we look at detail at the various styles of asynchronous communication, there are a lot of different, interesting ways in which you can get yourself into a <strong>lot</strong> of trouble.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534875997736">
<h5>Async/await, and When Asynchronous Is Still Blocking</h5>
<p>As with many areas of computing, we can use the same term in different contexts to have very different meaning. A style of programming, which appears to be especially popular in JavaScript, is the use of constructs like async/await to work with a potentially asynchronous source of data, but work with it in a blocking, synchronous style.</p>

<p>In <a data-type="xref" href="#example-async-await">Example 3-1</a> we see a very simple example of this in action. The currency exchange rates fluctuate frequently through the day, and we recieve these via a message broker. We define a <code>Promise</code>. Generically, a promise is something that will resolve to a state at some point in the future. In our case, our <code>eurToGbp</code> will eventually resolve to being the next Euro to GBP exchange rate.</p>
<div id="example-async-await" data-type="example">
<h5><span class="label">Example 3-1. </span>An example of working with a potentially asyncrhnous call in a blocking, sychronous fashion.</h5>

<pre data-type="programlisting" data-code-language="javascript"><code class="nx">async</code><code> </code><code class="kd">function</code><code> </code><code class="nx">f</code><code class="p">(</code><code class="p">)</code><code> </code><code class="p">{</code><code>

  </code><code class="kd">let</code><code> </code><code class="nx">eurToGbp</code><code> </code><code class="o">=</code><code> </code><code class="k">new</code><code> </code><code class="nb">Promise</code><code class="p">(</code><code class="p">(</code><code class="nx">resolve</code><code class="p">,</code><code> </code><code class="nx">reject</code><code class="p">)</code><code> </code><code class="o">=&gt;</code><code> </code><code class="p">{</code><code>
    </code><code class="c1">//code to fetch latest exchange rate between USD and GBP
</code><code>    </code><code class="p">...</code><code>
  </code><code class="p">}</code><code class="p">)</code><code class="p">;</code><code>

  </code><code class="kd">var</code><code> </code><code class="nx">latestRate</code><code> </code><code class="o">=</code><code> </code><code class="nx">await</code><code> </code><code class="nx">usdToGbp</code><code class="p">;</code><code> </code><a class="co" id="co_microservice_communication_styles_CO1-1" href="#callout_microservice_communication_styles_CO1-1"><img src="https://learning.oreilly.com
  /library/view/building-microservices-2nd/9781492034018/assets/1.png" alt="1" width="12" height="12" data-mfp-src="https://learning.oreilly.com
 /library/view/building-microservices-2nd/9781492034018/assets/1.png"></a><code>
  </code><code class="nx">process</code><code class="p">(</code><code class="nx">latestRate</code><code class="p">)</code><code class="p">;</code><a class="co" id="co_microservice_communication_styles_CO1-2" href="#callout_microservice_communication_styles_CO1-2"><img src="https://learning.oreilly.com
  /library/view/building-microservices-2nd/9781492034018/assets/2.png" alt="2" width="12" height="12" data-mfp-src="https://learning.oreilly.com
 /library/view/building-microservices-2nd/9781492034018/assets/2.png"></a><code>
</code><code class="p">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_microservice_communication_styles_CO1-1" href="#co_microservice_communication_styles_CO1-1"><img src="https://learning.oreilly.com
/library/view/building-microservices-2nd/9781492034018/assets/1.png" alt="1" width="12" height="12" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/1.png"></a></dt>
<dd><p>Wait until the latest USD to GBP exchange rate is fetched</p></dd>
<dt><a class="co" id="callout_microservice_communication_styles_CO1-2" href="#co_microservice_communication_styles_CO1-2"><img src="https://learning.oreilly.com
/library/view/building-microservices-2nd/9781492034018/assets/2.png" alt="2" width="12" height="12" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/2.png"></a></dt>
<dd><p>Won’t run until the promise is fulfilled</p></dd>
</dl></div>

<p>When we reference <code>eurToGbp</code> using <code>await</code>, we block until <code>latestOrder</code>’s state is fulfilled - <code>process</code> isn’t reached until we resolve the state of <code>eurToGbp</code> <sup><a data-type="noteref" id="idm46534877030632-marker" href="ch03.html#idm46534877030632" class="totri-footnote">2</a></sup>.</p>

<p>Even though our exchange rates are being received in an asynchronous fashion, from the use of <code>await</code> in this context means we are <em>blocking</em> until the state of <code>latestOrder</code> is resolved. So even if the underlying technology we are using to get the order status could be considered to be asynchronous in nature (for example waiting for the order stat), from the point of our code, this is inherently a synchronous, blocking interaction.</p>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Where To Use It"><div class="sect2" id="idm46534877026680">
<h2>Where To Use It</h2>

<p>Ultimately, when considering if asynchronous communication is right for you, you also have to consider which <em>type</em> of asynchronous communication you want to pick, as each as it’s own tradeoffs. In general though, there are some specific use cases that would have me reaching for some form of asynchronous communication. Long running processes are an obvious candidate, as we explored in <a data-type="xref" href="#warehouse-async">Figure 3-5</a> above. Also, situations where you have long call chains you can’t easily restructure could be a good candidate. We’ll dive deeper into this though when we look at three of the most common forms of asynchronous communication - request-response calls, event-driven communication, and communication through common data.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Pattern: Communication Through Common Data"><div class="sect1" id="common-data-sharing">
<h1>Pattern: Communication Through Common Data</h1>

<p>A style of communication which spans a multitude of implementations is communication through common data. This pattern is used in a situation where one microservice puts data into a defined location, and another microservice (or potentially multiple) then make use of this data. It can be as simple as one microservice dropping a file in a location, and at some point later on another microservice picking that file up and doing something with it. This integration style is fundamentally asynchronous in nature.</p>

<p>An example of this is shown in <a data-type="xref" href="#common-data">Figure 3-6</a>, where the <code>New Product Importer</code> creates a file that is then read by the downstream <code>Inventory</code> and <code>Catalog</code> microservices.</p>

<figure><div id="common-data" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common-data.png" alt="One microservice writes out a file which other microservices make use of" width="850" height="608" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common-data.png">
<h6><span class="label">Figure 3-6. </span>One microservice writes out a file which other microservices make use of</h6>
</div></figure>

<p>This pattern is in some ways the most common general inter-process communication pattern that you’ll see, and yet we sometimes fail to see it as a communication pattern at all - largely I think because the communication between processes is often so indirect as to be hard to spot.</p>








<section data-type="sect2" data-pdf-bookmark="Implementation"><div class="sect2" id="idm46534873469896">
<h2>Implementation</h2>

<p>To implement this pattern, you need some sort of persistent store for the data. A file system in many cases can be enough. I’ve built many systems which just periodically scan a file system, note the presence of a new file, and react on it accordingly. You could also use some sort of robust distributed memory store as well of course. It’s worth noting that any downstream microservice which is going to act on this data will need it’s own mechanism to identify that new data is available - polling is a frequent solution to this problem.</p>

<p>Two common examples of this pattern are the data lake and the data warehouse. In both cases, these solutions are typically designed to help processing large volumes of data, but arguably they exist at opposite ends of the spectrum regarding coupling. With data lake, sources upload raw data in whatever format they see fit, and downstream consumers of this raw data are expected to know how to process that information. With a data warehouse, the warehouse itself is a structured data store. Microservices pushing data to the data warehouse need to know the structure of the data warehouse - if the structure changes in a backwards compatible way, then these producers will need to be updated.</p>

<p>With both the data warehouse or data lake, the assumption is that the flow of information is in a single direction. One microservice publishes data to the common data store, and downstream consumers read that data and carry out appropriate actions. This unidirectional flow can make it easier to reason about the flow of information. A more problematic implementation can be the use of a shared database where multiple microservices both read and write to the same data store, an example of which we discussed in <a data-type="xref" href="ch02.html#modelling-services-chapter">Chapter 2</a> when we explored common coupling - <a data-type="xref" href="#common_coupling_order_update">Figure 3-7</a> shows both the <code>Order Processor</code> and <code>Warehouse</code> updating the same record.</p>

<figure><div id="common_coupling_order_update" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common_coupling_order_update.png" alt="Order Processor and Warehouse both update the same row in the order table" width="1633" height="762" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/common_coupling_order_update.png">
<h6><span class="label">Figure 3-7. </span>An example of common coupling where both Order Processor and Warehouse are updating the same order record</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Advantages"><div class="sect2" id="idm46534873461144">
<h2>Advantages</h2>

<p>This pattern can be implemented very simply, using commonly understood technology. If you can read or write to a file, or read and write to a database, you can use this pattern. The use of prevalent and well understood technology also enables interoperability between different types of systems, including older mainframe applications or customizable of the shelf software (COTS) products. Data volumes are also less of a concern here - if you’re sending lots of data in one big go, this pattern can work well.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Disadvantages"><div class="sect2" id="idm46534873459144">
<h2>Disadvantages</h2>

<p>Downstream consuming microservices will typically be aware that there is new data to process via some sort of polling mechanism, or else perhaps through a periodically triggered timed job. That means that this mechanism is unlikely to be useful in low-latency situations. You can of course combine this pattern with some other sort of call, informing a downstream microservice that new data is available. For example I could write a file to a shared filesystem, then send a call to the interested microservice informing it that there is new data that it may want. This can close the gap between data being published and data being processed. In general though, if you’re using this pattern for very large volumes of data, it’s less likely that low latency is high on your list of requirements. If you are interested in sending larger volumes of data and have them processed more in “real time”, then using some sort of streaming technology like Kafka would be a better fit.</p>

<p>Another big disadvantage, and something that should be fairly obvious if you remember back to our exploration of common coupling in  <a data-type="xref" href="#common_coupling_order_update">Figure 3-7</a>, is that the common data store becomes a potential source of coupling. If that data store changes structure in some way, it can break communication between microservices.</p>

<p>The robustness of the communication will also come down to the robustness of the underlying data store. This isn’t a disadvantage strictly speaking, but something to be aware of. If you’re dropping a file on a file system, you might want to make sure that the filesystem itself isn’t going to fail in interesting ways.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Where To Use It"><div class="sect2" id="idm46534873454296">
<h2>Where To Use It</h2>

<p>Where this pattern really shines is in enabling interoperability between processes which might have restrictions in what technology they can use. Having an existing system talk to your microservice’s GRPC interface or subscribe to its Kafka topic might well be more convenient from the point of view of the microservice, but not from the point of view of a consumer. Older systems may have limitations on what technology they can support, and may have high costs of change. Even old mainframe systems should be able to read data out of a file on the other hand. This does of course all depend on using data store technology which is widely supported - I could also implement this pattern using something like a redis cache. But can your old mainframe system talk to redis?</p>

<p>Another major sweet spot for this pattern is when sharing large volumes of data. If you need to send a multi gigabyte file onto a file system, or load in a few million rows into a database, then this pattern is the way to go.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Pattern: Request-Response Communication"><div class="sect1" id="request-response-call">
<h1>Pattern: Request-Response Communication</h1>

<p>With request-response, a microservice sends a request to a downstream service asking it to do something, and expects to receive a response with the result of the request. This interaction can be undertaken via a synchronous blocking call, or could be implemented in an asynchronous non-blocking fashion. A simple example of this interaction is shown in <a data-type="xref" href="#fetch-stock-level">Figure 3-8</a>, where the <code>Chart</code> microservice, which collates the best selling CDs for different genres, sends a request to the <code>Inventory</code> service asking for the current stock levels for some CDs.</p>

<figure><div id="fetch-stock-level" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/fetch-stock-level.png" alt="The Chart microservice sends a request to Inventory asking for stock levels" width="1033" height="420" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/fetch-stock-level.png">
<h6><span class="label">Figure 3-8. </span>The Chart microservice sends a request to Inventory asking for stock levels</h6>
</div></figure>

<p>Retrieving data from other microservices like this is a common use case for a request-response call. Sometimes though, you just need to make sure something gets done. In <a data-type="xref" href="#reserve-stock">Figure 3-9</a>, the <code>Warehouse</code> microservice is sent a request from <code>Order Processor</code>, asking it to reserve stock. The <code>Order Processor</code> just needs to know that stock has been successfully reserved if it wants to carry on with taking payment. If the stock can’t be reserved - perhaps because an item is no longer available - then the payment can be cancelled. Using request-response calls in situations where calls need to be completed in a certain order like this is common place.</p>

<figure><div id="reserve-stock" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/reserve-stock.png" alt="Order Processor needs to ensure stock can be reserved before payment can be taken" width="1145" height="754" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/reserve-stock.png">
<h6><span class="label">Figure 3-9. </span>Order Processor needs to ensure stock can be reserved before payment can be taken</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534873440648">
<h5>Commands vs Requests</h5>
<p>I’ve heard some people talk about sending commands, rather than requests, specifically in the context of asynchronous request-response communication. The intent behind the term command is arguably the same as that of request - namely an upstream microservice is asking a downstream microservice to do something.</p>

<p>Personally speaking though, I much prefer the term request. Command implies a directive that must be obeyed, and it can lead to the situation where people feel that a command has to be acted on. A request implies something that can be rejected. It is right that a microservice examines each request on its merits, and based on it’s own internal logic decides if the request should be auctioned. If the request it has been sent violates internal logic, it should be rejected. Although it’s a subtle nuance, I don’t feel that the term command conveys the same meaning.</p>

<p>Although I’ll stick to using request over command, whatever term you decide to use, just remember that a microservice gets to reject the request/command if appropriate.</p>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Implementation: Synchronous vs Asynchronous"><div class="sect2" id="idm46534873437112">
<h2>Implementation: Synchronous vs Asynchronous</h2>

<p>Request-response calls like this can be implemented in either a blocking synchronous, or non-blocking asynchronous style. With a synchronous call, what you’d typically see is a network connection being opened with the downstream microservice, with the request being sent along this connection. The connection is kept open, waiting for the downstream microservice to respond. In this case, the microservice sending the response doesn’t really need to know anything about the microservice that sent the request - it’s just sending stuff back over an inbound connection.</p>

<p>With a asynchronous request response, things are less straight forward. Let’s revisit the process associated with reserving stock. In <a data-type="xref" href="#reserve-stock-message">Figure 3-10</a> the request to reserve stock is sent as a message over some sort of message broker (we’ll explore message brokers later in this chapter). Rather than the message going directly to the <code>Inventory</code> microservice from <code>Order Processor</code>, it instead sits in a queue. The <code>Inventory</code> consumes messages from this queue when it is able. It reads the request, carries out the associated work of reserving the stock, and now it needs to send the response back to a queue that the <code>Order Processor</code> is reading from. The <code>Inventory</code> microservice needs to know where to route the response. In our example, it sends this response back over another queue which is in turn consumed by <code>Order Processor</code>.</p>

<figure><div id="reserve-stock-message" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/reserve-stock-message.png" alt="Using a queue to send stock reservation requests" width="1304" height="862" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/reserve-stock-message.png">
<h6><span class="label">Figure 3-10. </span>Using a queue to send stock reservation requests</h6>
</div></figure>

<p>So with a non-blocking asynchronous interaction, the microservice that receives the request either needs to implicitly know where to route the response, or else be told where the response should go. When using a queue, we have the added benefit that multiple requests could be buffered up in the queue waiting to be handled. This can help in situations where the requests can’t be handled quickly enough. The microservice can consume the next request when it is ready, rather than being overwhelmed by too many calls. A lot of course then depends on the queue absorbing these requests.</p>

<p>When a microservice receives a response in this way, it might need to relate the response to the original request. This can be challenging as a lot of time may have passed, and depending on the nature of the protocol being used, the response may not come back to the same instance of the microservice that sent the request. In our example of reserving stock as part of placing an order, we’d need to know how to associate the stock reserved response with a given order, so we can carry on processing that particular order. An easy way to handle this would be to store any state associated with the original request into a database, such that when the response comes in, the receiving instance can reload any associated state and act accordingly.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534873426104">
<h5>Parallel vs Sequential Calls</h5>
<p>When working with request-response interactions, you’ll often encounter a situation where you need to make multiple calls before you can continue with some processing.</p>

<p>Consider a situation where MusicCorp needs to check on the  price for a given item from three different stockists, which we do by issuing API calls. We want to get the prices back from all three stockists before deciding which one we want to order new stock from. We could decide to make the three calls in sequence - waiting for each one to finish, before proceeding with the next. In such a situation, we’d be waiting for the sum of latencies of each of the calls. If the API call to each provider took 1 second to return, we’d be waiting 3 seconds before we can decide who we should order from.</p>

<p>A better option would be to run these three requests in parallel - then the overall latency of the operation would be based on the slowest API call, rather than the sum of latencies of each API call.</p>

<p>Reactive extensions, and mechanisms like async/await can be very useful to help run calls in parallel, and this can result in significant improvements in the latency of some operations.</p>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Where To Use It"><div class="sect2" id="idm46534873421960">
<h2>Where To Use It</h2>

<p>Request-response calls make perfect sense for any situation where the result of a request is needed before further processing can take place. It also fits really well in situations where a microservice wants to know if a call didn’t work, so that it can carry out some sort of compensating action, like a retry. If that fits your situation, request-response is a sensible approach - the only remaining question then is to decide on a synchronous vs asynchronous implementation, with the same tradeoffs we discussed earlier.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Pattern: Event-Driven Communication"><div class="sect1" id="event-driven">
<h1>Pattern: Event-Driven Communication</h1>

<p>Event-driven communication looks quite odd compared to request-response calls. Rather than a microservice asking some other microservice to do something, instead a microservice emits events which may or may not be received by other microservices. It is an inherently asynchronous interaction, as the event listeners will be running on their own thread of execution.</p>

<p>An event is a statement about something that has occurred, nearly always something that has happened inside the world of the microservice that is emitting the event. The microservice emitting the event has no knowledge of the intent of other microservices to use the event, and indeed may not even be aware that any other microservice exists. It emits the event when required, and that is the end of it’s responsibilities.</p>

<p>In <a data-type="xref" href="#warehouse-events">Figure 3-11</a>, we see the <code>Warehouse</code> emitting events related to the process of packaging up of an order. These events are received by two microservices, <code>Notifications</code> and <code>Inventory</code>, and they react accordingly. The <code>Notifications</code> microservice sends an email to update our customer about changes in order status, where the <code>Inventory</code> microservice can update stock levels as items are packaged into customer orders.</p>

<figure><div id="warehouse-events" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/warehouse-events.png" alt="The Warehouse emits events which some downstream microservices care about" width="1291" height="700" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/warehouse-events.png">
<h6><span class="label">Figure 3-11. </span>The Warehouse emits events which some downstream microservices care about</h6>
</div></figure>

<p>This is an inversion of responsibilities, when compared to a request-response model. With events, the <code>Warehouse</code> is just broadcasting events, assuming that interested parties will react accordingly. It is unaware of who the recipients of the events are, making event-driven interactions much more loosely coupled in general. When compared to a request-response call though, this is an inversion of responsibility that it can take a while to get your head around. With request-response, I might instead expect <code>Warehouse</code> to tell the <code>Notifications</code> microservice to send emails when appropriate. In such a model, the <code>Warehouse</code> would need to know what events require notifying a customer about. With an event-driven interaction, we are instead pushing that responsibility into the <code>Notifications</code> microservice.</p>

<p>This distribution of responsibility we see with our event-driven interactions can mirror the same distribution of responsibility we see with organizations trying to create more autonomous teams. Rather than holding all the responsibility centrally, instead we want to push it into the teams themselves to allow them to operate in a more autonomous fashion - a concept we will revisit in [Link to Come]. Here, we are pushing responsibility from <code>Warehouse</code> into <code>Notifications</code> and <code>Payment</code> - this can help us reduce the complexity of microservices like <code>Warehouse</code>, and lead to a more even distribution of “smarts” in our system. We’ll explore that idea in more detail when we compare choreography and orchestration later.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534873404104">
<h5>Events &amp; Messages</h5>
<p>On occasion I’ve seen the term messages and events get confused. An event is a fact - a statement that something happened, along with some information about exactly what happened. A message is a thing we send over an asynchronous communication mechanism, like a message broker.</p>

<p>With event-driven collaboration, we want to broadcast that event, and a typical way to implement that broadcast mechanism would be to put that event into a message. The message is the medium, the event is the payload.</p>

<p>Likewise, we might want to send a request as the payload of a message - in which case we would be implementing a form of asynchronous request-response.</p>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Implementation"><div class="sect2" id="idm46534873401000">
<h2>Implementation</h2>

<p>There are two main parts we need to consider here: a way for our microservices to emit events, and a way for our consumers to find out those events have happened.</p>

<p>Traditionally, message brokers like RabbitMQ try to handle both problems. Producers use an API to publish an event to the broker. The broker handles subscriptions, allowing consumers to be informed when an event arrives. These brokers can even handle the state of consumers, for example by helping keep track of what messages they have seen before. These systems are normally designed to be scalable and resilient, but that doesn’t come for free. It can add complexity to the development process, because it is another system you may need to run to develop and test your services. Additional machines and expertise may also be required to keep this infrastructure up and running. But once it does, it can be an incredibly effective way to implement loosely coupled, event-driven architectures. In general, I’m a fan.</p>

<p>Do be wary, though, about the world of middleware, of which the message broker is just a small part. Queues in and of themselves are perfectly sensible, useful things. However, vendors tend to want to package lots of software with them, which can lead to more and more smarts being pushed into the middleware, as evidenced by things like the Enterprise Service Bus. Make sure you know what you’re getting: keep your middleware dumb, and keep the smarts in the endpoints.</p>

<p>Another approach is to try to use HTTP as a way of propagating events. ATOM is a REST-compliant specification that defines semantics (among other things) for publishing feeds of resources. Many client libraries exist that allow us to create and consume these feeds. So our customer service could just publish an event to such a feed when our customer service changes. Our consumers just poll the feed, looking for changes. On one hand, the fact that we can reuse the existing ATOM specification and any associated libraries is useful, and we know that HTTP handles scale very well. However, HTTP is not good at low latency (where some message brokers excel), and we still need to deal with the fact that the consumers need to keep track of what messages they have seen and manage their own polling schedule.</p>

<p>I have seen people spend ages implementing more and more of the behaviors that you get out of the box with an appropriate message broker to make ATOM work for some use cases. For example, the Competing Consumer pattern describes a method whereby you bring up multiple worker instances to compete for messages, which works well for scaling up the number of workers to handle a list of independent jobs (we’ll come back to that later in [Link to Come]). However, we want to avoid the case where two or more workers see the same message, as we’ll end up doing the same task more than we need to. With a message broker, a standard queue will handle this. With ATOM, we now need to manage our own shared state among all the workers to try to reduce the chances of reproducing effort.</p>

<p>If you already have a good, resilient message broker available to you, consider using it to handle publishing and subscribing to events. But if you don’t already have one, give ATOM a look, but be aware of the sunk-cost fallacy. If you find yourself wanting more and more of the support that a message broker gives you, at a certain point you might want to change your approach.</p>

<p>In terms of what we actually send over these asynchronous protocols, the same considerations apply as with synchronous communication. If you are currently happy with encoding requests and responses using JSON, stick with it.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="What’s In An Event?"><div class="sect2" id="idm46534873392280">
<h2>What’s In An Event?</h2>

<p>In <a data-type="xref" href="#customer-registration-events">Figure 3-12</a>, we see an event being broadcast from the <code>Customer</code> microservice, informing interested parties that a new customer has registered with the system. Two of the downstream microservices, <code>Loyalty</code> and <code>Notifications</code> care about this event. The <code>Loyalty</code> microservice reacts to receiving the event by setting up an account for the new customer so that they can start earning points, whereas the <code>Notifications</code> microservice sends an email to the newly registered customer welcoming them to the wondrous delights of MusicCorp.</p>

<figure><div id="customer-registration-events" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/customer-registration-events.png" alt="The customer microservice fires an event when a new customer is created. The Loyalty and Notification microservices receive this event" width="1220" height="600" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/customer-registration-events.png">
<h6><span class="label">Figure 3-12. </span>Notifications and Loyalty microservices receive an event when a new customer is registered.</h6>
</div></figure>

<p>With a request, we are asking a microservice to do something, and providing the required information for the requested operation to be carried out. With an event we are broadcasting a fact that other parties <strong>might</strong> be interested in, but as the microservice emitting an event can’t and shouldn’t know who receives the event, how do we know what information other parties might need from the event? So what, exactly, should be inside the event?</p>










<section data-type="sect3" data-pdf-bookmark="Just An ID"><div class="sect3" id="idm46534873383752">
<h3>Just An ID</h3>

<p>One option, is for the event to just contain an identifier for the newly registered customer, as shown in <a data-type="xref" href="#customer-registration-events-callback">Figure 3-13</a>. The <code>Loyalty</code> microservice only needs this identifier to create the matching loyalty account, so it has all the information it needs. However, while the <code>Notifications</code> microservice knows that it needs to send a welcome email when this type of event is received, it will need additional information to do its job - at least an email address, and probably the name of the customer as well to give the email that personal touch. As this information isn’t in the event that the <code>Notifications</code> microservice receives then it has no choice but to fetch this information from the <code>Customer</code> microservice, something we see in <a data-type="xref" href="#customer-registration-events-callback">Figure 3-13</a>.</p>

<figure><div id="customer-registration-events-callback" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/customer-registration-events-callback.png" alt="After receiving the customer registration event, the notification microservice needs to call back to the Customer microservice to fetch additional information" width="1079" height="645" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/customer-registration-events-callback.png">
<h6><span class="label">Figure 3-13. </span>The Notification microservice needs to request further details from the Customer microservice as they aren’t in the event</h6>
</div></figure>

<p>There are some downsides with this approach. Firstly, the <code>Notification</code> microservice now has to know about the <code>Customer</code> microservice, adding additional domain coupling. While domain coupling, as we discussed in <a data-type="xref" href="ch02.html#modelling-services-chapter">Chapter 2</a>, is on the looser end of the coupling spectrum, we’d still like to avoid it where possible. If the event that <code>Notification</code> received contained all the information it needed, then this call back wouldn’t be required. This call back from the receiving microservice can also lead to the other major downside - namely that in a situation with a large number of receiving microservices, the microservice emitting the event might get a barrage of requests as a result. Imagine if five different microservices all received the same customer creation event, and all needed to request additional information - they’d all need to immediately send a request to the <code>Customer</code> microservice to get what they needed. As the number of microservices interested in a particular event increases, the impact of these calls could become significant.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Fully Detailed Events"><div class="sect3" id="idm46534873371960">
<h3>Fully Detailed Events</h3>

<p>The alternative, which I prefer, is to put everything into an event that you would be happy otherwise sharing via an API. If you’d let the <code>Notifications</code> microservice ask for the email address and name of a given customer, why not just put that in the event in the first place? In <a data-type="xref" href="#customer-registration-events-detailed">Figure 3-14</a>, we see this approach - <code>Notification</code> is now more self-sufficient, and able to do its job without needing to communicate with the <code>Customer</code> microservice. In fact, it might never need to know the <code>Customer</code> microservice even exists.</p>

<figure><div id="customer-registration-events-detailed" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/customer-registration-events-detailed.png" alt="The Customer Registered event contains the ID, name, and email, so the Notifications and Loyalty microservices don't need to make additional round trips to the Customer microservice." width="1233" height="641" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/customer-registration-events-detailed.png">
<h6><span class="label">Figure 3-14. </span>An event with more information in it can allow receiving microservices to act without requiring further calls to the source of the events.</h6>
</div></figure>

<p>In addition to the fact that events with more information can allow for looser coupling, events with more information can double up as an historical record as to what happened to a given entity. This could help you as part of implementing an auditing system, or perhaps even provide the ability to reconstitute an entity at given points of time - meaning that these events could be used as part of an event sourcing, a concept we’ll explore briefly in a moment.</p>

<p>Whilst this approach is definitely my preference, it’s not without some downsides. Firstly, if the data associated with an event is large, we might have concerns about the size of the event. Now, modern message brokers (assuming you’re using one to implement your event broadcast mechanism) have fairly generous limits for message size. The default maximum size for a message in Kafka is 1MB, and the latest release of RabbitMQ has a theoretical upper limit of 512MB for a single message (down from the previous limit of 2GB!), even though one could expect there to be some interesting performance issues with large messages like this. But even the 1MB afforded to us as the maximum size of a message on Kafka gives us a lot of scope to send quite a bit of data. Ultimately, if you’re venturing into a space where you are starting to worry about the size of your events, then a hybrid approach where some information is in the event but other (larger) data can be looked up if required.</p>

<p>In <a data-type="xref" href="#customer-registration-events-detailed">Figure 3-14</a>, <code>Loyalty</code> doesn’t need to know the email address or name of the customer, and yet because it is being sent this information via the event it nonetheless receives it. This could lead to concerns if we are trying to limit the scope of which microservices can see what kind of data - for example I might want to limit what microservices can see personally identifiable information (or PII), payment card details, or similar sensitive data. A way to solve this could be to implement something like Split Horizon Communication, which we’ll explore later in [Link to Come].</p>

<p>Another consideration is that once we put data into an event, it becomes part of our contract with the outside world. We have to be aware that if we remove a field from an event that we may break external parties. Information hiding is still an important concept in event-driven collaboration - the more data we put into an event, the more assumptions external parties will have about an event. My general rule is that I am OK putting information into an event if I’d be happy sharing the same data over a request-response API.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Did It Work?"><div class="sect2" id="idm46534873358552">
<h2>Did It Work?</h2>

<p>TODO: Move to workflow discussion?</p>

<p>Some of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead to significantly more decoupled, scalable systems. And they can. But these programming styles do lead to an increase in complexity. This isn’t just the complexity required to manage publishing and subscribing to messages as we just discussed, but also in the other problems we might face. For example, when considering long-running async request-response, we have to think about what to do when the response comes back. Does it come back to the same node that initiated the request? If so, what if that node is down? If not, do I need to store information somewhere so I can react accordingly? Short-lived async can be easier to manage if you’ve got the right APIs, but even so, it is a different way of thinking for programmers who are accustomed to intra-process synchronous message calls.</p>

<p>Time for a cautionary tale. Back in 2006, I was working on building a pricing system for a bank. We would look at market events, and work out which items in a portfolio needed to be repriced. Once we determined the list of things to work through, we put these all onto a message queue. We were making use of a grid to create a pool of pricing workers, allowing us to scale up and down the pricing farm on request. These workers used the Competing Consumer pattern, each one gobbling messages as fast as possible until there was nothing left to process.</p>

<p>The system was up and running, and we were feeling rather smug. One day, though, just after we pushed a release out, we hit a nasty problem. Our workers kept dying. And dying. And dying.</p>

<p>Eventually, we tracked down the problem. A bug had crept in whereby a certain type of pricing request would cause a worker to crash. We were using a transacted queue: as the worker died, its lock on the request timed out, and the pricing request was put back on the queue—only for another worker to pick it up and die. This was a classic example of what Martin Fowler calls a <a href="http://bit.ly/1EmZMss">catastrophic failover</a>.</p>

<p>Aside from the bug itself, <span class="no-kerning">we’d</span> failed to specify a maximum retry limit for the job on the queue. We fixed the bug itself, and also configured a maximum retry. But we also realized we needed a way to view, and potentially replay, these bad messages. We ended up having to implement a message hospital (or dead letter queue), where messages got sent if they failed. We also created a UI to view those messages and retry them if needed. These sorts of problems aren’t immediately obvious if you are only familiar with synchronous point-to-point communication.</p>

<p>The associated complexity with event-driven architectures and asynchronous programming in general leads me to believe that you should be cautious in how eagerly you start adopting these ideas. Ensure you have good monitoring in place, and strongly consider the use of correlation IDs, which allow you to trace requests across process boundaries, as we’ll cover in depth in [Link to Come].</p>

<p>I’d also strongly recommend checking out <em>Enterprise Integration Patterns</em> (Addison-Wesley), which contains a lot more detail on the different messaging patterns that you may want to consider in this space.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46534873419384">
<h1>Summary</h1>

<p>In this chapter, we broke down some of the key styles of microservice communication, and discussed the various tradeoffs. There isn’t always a single <strong>right</strong> option, but hopefully I’ve detailed enough information regarding synchronous and asynchronous calls, event-driven and request-response styles of communication, to help you make the right call for your given context.</p>

<p>Where this chapter focused primarily on how one microservice talks to another, in our next chapter we look beyond that to how we can get multiple microservices collaborating to implement workflows.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46534877873432"><sup><a href="ch03.html#idm46534877873432-marker" class="totri-footnote">1</a></sup> True story</p><p data-type="footnote" id="idm46534877030632"><sup><a href="ch03.html#idm46534877030632-marker" class="totri-footnote">2</a></sup> Please note, this is very simplified - I’ve completely omitted error handling code for example. If you want to know more about async/await, specifically in JavaScript, the The Modern JavaScript Tutorial is a great place to start: <a href="https://javascript.info/"><em class="hyperlink">https://javascript.info/</em></a></p></div></div></section>
<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Implementing Microservice Communication"><div class="chapter" id="integration-technology-chapter">
<h1><span class="label">Chapter 4. </span>Implementing Microservice Communication</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534873343752">
<h5>Work In Progress</h5>
<p>Please note that the text below is currently being reworked for the 2nd edition of the book, and is not in a complete state. This will be Chapter 4 of the final book.</p>

<p>If you have any feedback on the book, or suggestions for the 2nd edition, then please contact me on <a href="mailto:book-feedback@samnewman.io">book-feedback@samnewman.io</a> and/or complete a short survey here: <a href="https://oreil.ly/Bldg_MicroServices_survey"><em class="hyperlink">https://oreil.ly/Bldg_MicroServices_survey</em></a>.</p>
</div></aside>

<p>There is a bewildering array of options out there for how one microservice can talk to another. But which is the right one: SOAP? XML-RPC? REST? GRPC?</p>

<p>Well, as we discussed in the previous chapter, your choice of technology should be driven in large part based on the style of communication you want. Deciding between blocking synchronous or non-blocking asynchronous calls, request-response or event-driven collaboration, will help you whittle down what might otherwise be a very long list of technology.</p>

<p>In this chapter, we’re going to now look at some of the common technology used for microservice communication. But new options are always coming up, so before we discuss specific technology, let’s think about what we want out of whatever technology we pick.</p>








<section data-type="sect2" data-pdf-bookmark="Make Backwards Compatibility Easy"><div class="sect2" id="idm46534873337528">
<h2>Make Backwards Compatibility Easy</h2>

<p>When making changes to our microservices, we need to make sure we don’t break compatibility with any consuming microservices. As such, we want to ensure that whatever technology we pick makes it easy to make backwards compatible changes. Simple operations like adding new fields shouldn’t break clients. We also ideally want the ability to validate that the changes we have made are backwards compatible - and have a way to get that feedback before we deploy our microservice into production.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Make Your Interface Explicit"><div class="sect2" id="idm46534873335416">
<h2>Make Your Interface Explicit</h2>

<p>It is important that the interface that a microservice exposes to the outside world is explicit. This means that it is clear to a consumer of a microservice as to what functionality that microservice exposes. But it also means that it is clear to a developer working on a microservice as to what functionality needs to remain intact for external parties - we want to avoid a situation where a change to a microservice causes an accidental breakage in compatibility.</p>

<p>Schemas can go a long way to helping ensure that the interface a microservice exposes is explicit. Some of the technology we can look at requires the use of a schema, for others the use of a schema is optional. Either way, I strongly encourage the use of a schema, as well as enough supporting documentation to be clear about what functionality a consumer can expect a microservice to provide.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Keep Your APIs Technology-Agnostic"><div class="sect2" id="idm46534873332504">
<h2>Keep Your APIs Technology-Agnostic</h2>

<p>If you have been in the IT industry for more than 15 minutes, you don’t need me to tell you that we work in a space that is changing rapidly. The one certainty <em>is</em> change. New tools, frameworks, and languages are coming out all the time, implementing new ideas that can help us work faster and more effectively. Right now, you might be a .NET shop. But what about in a year from now, or five years from now? What if you want to experiment with an alternative technology stack that might make you more productive?</p>

<p>I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between microservices technology-agnostic. This means avoiding integration technology that dictates what technology stacks we can use to implement our microservices.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Make Your Service Simple for Consumers"><div class="sect2" id="idm46534873329160">
<h2>Make Your Service Simple for Consumers</h2>

<p>We want to make it easy for consumers to use our microservice. Having a beautifully factored microservice doesn’t count for much if the cost of using it as a consumer is sky high! So let’s think about what makes it easy for consumers to use our wonderful new service. Ideally, <span class="no-kerning">we’d</span> like to allow our clients full freedom in their technology choice, but on the other hand, providing a client library can ease adoption. Often, however, such libraries are incompatible with other things we want to achieve. For example, we might use client libraries to make it easy for consumers, but this can come at the cost of increased coupling.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Hide Internal Implementation Detail"><div class="sect2" id="idm46534873326200">
<h2>Hide Internal Implementation Detail</h2>

<p>We don’t want our consumers to be bound to our internal implementation. This leads to increased coupling. This means that if we want to change something inside our microservice, we can break our consumers by requiring them to also change. That increases the cost of change—exactly what we are trying to avoid. It also means we are less likely to want to make a change for fear of having to upgrade our consumers, which can lead to increased technical debt within the service. So any technology that pushes us to expose internal representation detail should be avoided.</p>

<p>There is a whole host of technology we could look at, but rather than looking broadly at a long list of options in this space, I will highlight some of the most popular and interesting choices. Here are the options we’ll be looking at:</p>
<dl>
<dt>Remote Procedure Calls (RPC)</dt>
<dd>
<p>Frameworks that allow for local method calls to be invoked on a remote process. Common options include SOAP and GRPC.</p>
</dd>
<dt>REST</dt>
<dd>
<p>An architectural style where you expose resources (Customer, Order etc) that can be accessed using a common set of verbs (GET, POST). There is a bit more to REST than that, but we’ll get to that shortly.</p>
</dd>
<dt>GraphQL</dt>
<dd>
<p>A relatively new protocol that allows for consumers to define custom queries that can fetch information from multiple downstream microservices, filtering the results to return only what is needed.</p>
</dd>
<dt>Message Brokers</dt>
<dd>
<p>Middleware that allows for asynchronous communication either via queues or topics.</p>
</dd>
</dl>

<p>TODO: Show this tech against styles we outlined previously?</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Remote Procedure Calls"><div class="sect2" id="idm46534873316488">
<h2>Remote Procedure Calls</h2>

<p><em>Remote procedure call</em> refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technology out there. Most of the technology in this space requires an explicit schema, such as SOAP or GRPC. The use of a separate schema makes it easier to generate client and server stubs for different technology stacks, so, for example, I could have a Java server exposing a SOAP interface, and a .NET client generated from the Web Service Definition Language (WSDL) definition of the interface. Other technology, like Java RMI, calls for a tighter coupling between the client and server, requiring that both use the same underlying technology but avoid the need for a shared interface definition. All these technologies, however, have the same, core characteristic in that they make a remote call look like a local call.</p>

<p>Typically, using an RPC technology means you are buying into a serialization protocol. The RPC framework defines how data is serialized and deserialized. GRPC for example uses the protocol buffer serialization format for this purpose. Some implementations are tied to a specific networking protocol (like SOAP, which makes nominal use of HTTP), whereas others might allow you to use different types of networking protocols, which themselves can provide additional features. For example, TCP offers guarantees about delivery, whereas UDP doesn’t but has a much lower overhead. This can allow you to use different networking technology for different use cases.</p>

<p>RPC frameworks that have an explicit schema make it very easy to generate client code. This can avoid the need for client libraries, as any client can just generate their own code against this service specification. For client side code generation to work though, the client needs some way to get the schema out of band - in other words the consumer needs to have access to the schema before it plans to make calls. AVRO RPC is an interesting outlier here, as it has the option to send the full schema along with the payload, allowing for clients to dynamically interpret the schema.</p>

<p>The ease of generation of client-side code is one of the main selling points of RPC: its ease of use. The fact that I can just make a normal method call and theoretically ignore the rest is a huge boon.</p>










<section data-type="sect3" data-pdf-bookmark="Challenges"><div class="sect3" id="idm46534873311048">
<h3>Challenges</h3>

<p>As we’ve seen, RPC offers some great advantages, but it’s not without it’s downsides - and some RPC implementations can be more problematic than others. Many of these issues can be dealt with, but they deserve further exploration.</p>












<section data-type="sect4" data-pdf-bookmark="Technology Coupling"><div class="sect4" id="idm46534873309096">
<h4>Technology Coupling</h4>

<p>Some RPC mechanisms, like Java RMI, are heavily tied to a specific platform, which can limit which technology can be used in the client and server. Thrift and protocol buffers have an impressive amount of support for alternative languages, which can reduce this downside somewhat, but be aware that sometimes RPC technology comes with restrictions on interoperability.</p>

<p>In a way, this technology coupling can be a form of exposing internal technical implementation details. For example, the use of RMI ties not only the client to the JVM, but the server too.</p>

<p>To be fair, there are a number of RPC implementations that don’t have this restriction - GRPC, SOAP and Thrift are all examples that allow for interoperability between different technology stacks.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Local Calls Are Not Like Remote Calls"><div class="sect4" id="idm46534873305880">
<h4>Local Calls Are Not Like Remote Calls</h4>

<p>The core idea of RPC is to hide the complexity of a remote call. This can though lead to hiding too much. The drive in some forms of RPC to make remote method calls look like local method calls hides the fact that these two things are very different. I can make large numbers of local, in-process calls without worrying overly about the performance. With RPC, though, the cost of marshalling and un-marshalling payloads can be significant, not to mention the time taken to send things over the network. This means you need to think differently about API design for remote interfaces versus local interfaces. Just taking a local API and trying to make it a service boundary without any more thought is likely to get you in trouble. In some of the worst examples, developers may be using remote calls without knowing it, if the abstraction is overly opaque.</p>

<p>You need to think about the network itself. Famously, the first of the fallacies of distributed computing is <a href="http://bit.ly/1LbdCzY">“The network is reliable”</a>. Networks aren’t reliable. They can and will fail, even if your client and the server you are speaking to are fine. They can fail fast, they can fail slow, and they can even malform your packets. You should assume that your networks are plagued with malevolent entities ready to unleash their ire on a whim. Therefore, the failure modes you can expect are different. A failure could be caused by the remote server returning an error, or by you making a bad call. Can you tell the difference, and if so, can you do anything about it? And what do you do when the remote server just starts responding slowly? We’ll cover this topic when we talk about resiliency in [Link to Come].</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Brittleness"><div class="sect4" id="idm46534873300696">
<h4>Brittleness</h4>

<p>Some of the most popular implementations of RPC can lead to some nasty forms of brittleness, Java’s RMI being a very good example. Let’s consider a very simple Java interface that we have decided to make a remote API for our customer service. <a data-type="xref" href="#a50-customer-java-rmi">Example 4-1</a> declares the methods we are going to expose remotely. Java RMI then generates the client and server stubs for our method.</p>
<div id="a50-customer-java-rmi" data-type="example">
<h5><span class="label">Example 4-1. </span>Defining a service endpoint using Java RMI</h5>

<pre data-type="programlisting" data-code-language="java"><code class="kn">import</code> <code class="nn">java.rmi.Remote</code><code class="o">;</code>
<code class="kn">import</code> <code class="nn">java.rmi.RemoteException</code><code class="o">;</code>

<code class="kd">public</code> <code class="kd">interface</code> <code class="nc">CustomerRemote</code> <code class="kd">extends</code> <code class="n">Remote</code> <code class="o">{</code>
  <code class="kd">public</code> <code class="n">Customer</code> <code class="nf">findCustomer</code><code class="o">(</code><code class="n">String</code> <code class="n">id</code><code class="o">)</code> <code class="kd">throws</code> <code class="n">RemoteException</code><code class="o">;</code>

  <code class="kd">public</code> <code class="n">Customer</code> <code class="nf">createCustomer</code><code class="o">(</code><code class="n">String</code> <code class="n">firstname</code><code class="o">,</code> <code class="n">String</code> <code class="n">surname</code><code class="o">,</code> <code class="n">String</code> <code class="n">emailAddress</code><code class="o">)</code>
      <code class="kd">throws</code> <code class="n">RemoteException</code><code class="o">;</code>
<code class="o">}</code></pre></div>

<p>In this interface, <code>createCustomer</code> takes the first name, surname, and email address. What happens if we decide to allow the <code>Customer</code> object to also be created with just an email address? We could add a new method at this point pretty easily, like so:</p>

<pre data-type="programlisting" data-code-language="java"><code class="o">...</code>
<code class="kd">public</code> <code class="n">Customer</code> <code class="nf">createCustomer</code><code class="o">(</code><code class="n">String</code> <code class="n">emailAddress</code><code class="o">)</code> <code class="kd">throws</code> <code class="n">RemoteException</code><code class="o">;</code>
<code class="o">...</code></pre>

<p>The problem is that now we need to regenerate the client stubs too. Clients that want to consume the new method need the new stubs, and depending on the nature of the changes to the specification, consumers that don’t need the new method may also need to have their stubs upgraded too. This is manageable, of course, but to a point. The reality is that changes like this are fairly common. RPC endpoints often end up having a large number of methods for different ways of creating or interacting with objects. This is due in part to the fact that we are still thinking of these remote calls as local ones.</p>

<p>There is another sort of brittleness, though. Let’s take a look at what our <code>Customer</code> object looks like:</p>

<pre data-type="programlisting" data-code-language="java"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">Customer</code> <code class="kd">implements</code> <code class="n">Serializable</code> <code class="o">{</code>
  <code class="kd">private</code> <code class="n">String</code> <code class="n">firstName</code><code class="o">;</code>
  <code class="kd">private</code> <code class="n">String</code> <code class="n">surname</code><code class="o">;</code>
  <code class="kd">private</code> <code class="n">String</code> <code class="n">emailAddress</code><code class="o">;</code>
  <code class="kd">private</code> <code class="n">String</code> <code class="n">age</code><code class="o">;</code>
<code class="o">}</code></pre>

<p>Now, what if it turns out that although we expose the <code>age</code> field in our <code>Customer</code> objects, none of our consumers ever use it? We decide we want to remove this field. But if the server implementation removes <code>age</code> from its definition of this type, and we don’t do the same to all the consumers, then even though they never used the field, the code associated with deserializing the <code>Customer</code> object on the consumer side will break. To roll out this change, I would have to deploy both a new server and clients at the same time. This is a key challenge with any RPC mechanism that promotes the use of binary stub generation: you don’t get to separate client and server deployments. If you use this technology, lock-step releases may be in your future.</p>

<p>Similar problems occur if I want to restructure the <code>Customer</code> object even if I didn’t remove fields—for example, if I wanted to encapsulate <code>firstName</code> and <code>surname</code> into a new <code>naming</code> type to make it easier to manage. I could, of course, fix this by passing around dictionary types as the parameters of my calls, but at that point, I lose many of the benefits of the generated stubs because I’ll still have to manually match and extract the fields I want.</p>

<p>In practice, objects used as part of binary serialization across the wire can be thought of as <em>expand-only</em> types. This brittleness results in the types being exposed over the wire and becoming a mass of fields, some of which are no longer used but can’t be safely removed.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Where To Use It"><div class="sect3" id="idm46534873300072">
<h3>Where To Use It</h3>

<p>Despite its shortcomings, I actually quite like RPC, and the more modern implementations, such as GRPC, are excellent, whereas other implementations have significant issues which would cause me to give them a wide berth. Java RMI for example has a number of issues regarding brittleness and limited technology choices, and SOAP is pretty heavyweight from a developer perspective, especially when compared with more modern choices.</p>

<p>Just be aware of some of the potential pitfalls associated with RPC if you’re going to pick this model. Don’t abstract your remote calls to the point where the network is completely hidden, and ensure that you can evolve the server interface without having to insist on lock-step upgrades for clients. Finding the right balance for your client code is important, for example. Make sure your clients aren’t oblivious to the fact that a network call is going to be made. Client libraries are often used in the context of RPC, and if not structured right they can be problematic. We’ll talk more about them shortly.</p>

<p>If I was looking at options in this space, GRPC would be top of my list. Built to take advantage of HTTP/2, it has some impressive performance characteristics and good general ease of use. I also appreciate the ecosystem around GRPC, including tools like Protolock<sup><a data-type="noteref" id="idm46534872997208-marker" href="ch04.html#idm46534872997208" class="totri-footnote">1</a></sup>, something we’ll discuss later in this chapter when we discuss schemas.</p>

<p>GRPC fits a synchronous request-response model well, but can also work in conjunction with reactive extensions. It’s high on my list whenever I’m in situations where I have a good deal of control over both the client and server ends of the spectrum. If you’re having to support a wide variety of other applications that might need to talk to your microservices, the need to compile client-side code against a server-side schema can be problematic. In which case, some form of REST over HTTP API would likely be a better fit.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="REST"><div class="sect2" id="idm46534872994600">
<h2>REST</h2>

<p>Representational State Transfer (REST) is an architectural style inspired by the Web. There are many principles and constraints behind the REST style, but we are going to focus on those that really help us when we face integration challenges in a microservices world, and when we’re looking for an alternative style to RPC for our service interfaces.</p>

<p>Most important when thinking about REST is the concept of resources. You can think of a resource as a thing that the service itself knows about, like a <code>Customer</code>. The server creates different representations of this <code>Customer</code> on request. How a resource is shown externally is <span class="keep-together">completely</span> decoupled from how it is stored internally. A client might ask for a JSON representation of a <code>Customer</code>, for example, even if it is stored in a completely different format. Once a client has a representation of this <code>Customer</code>, it can then make requests to change it, and the server may or may not comply with them.</p>

<p>There are many different styles of REST, and I touch only briefly on them here. I strongly recommend you take a look at the <a href="http://bit.ly/1fh2AGt">Richardson Maturity Model</a>, where the different styles of REST are compared.</p>

<p>REST itself doesn’t really talk about underlying protocols, although it is most commonly used over HTTP. I have seen implementations of REST using very different protocols before, such as serial or USB, although this can require a lot of work. Some of the features that HTTP gives us as part of the specification, such as verbs, make implementing REST over HTTP easier, whereas with other protocols you’ll have to handle these features yourself.</p>










<section data-type="sect3" data-pdf-bookmark="REST and HTTP"><div class="sect3" id="idm46534872987048">
<h3>REST and HTTP</h3>

<p>HTTP itself defines some useful capabilities that play very well with the REST style. For example, the HTTP verbs (e.g., GET, POST, and PUT) already have well-understood meanings in the HTTP specification as to how they should work with resources. The REST architectural style actually tells us that methods should behave the same way on all resources, and the HTTP specification happens to define a bunch of methods we can use. GET retrieves a resource in an idempotent way, and POST creates a new resource. This means we can avoid lots of different <code>createCustomer</code> or <code>editCustomer</code> methods. Instead, we can simply POST a customer representation to request that the server create a new resource, and initiate a GET request to retrieve a representation of a resource. Conceptually, there is one <em>endpoint</em> in the form of a <span class="keep-together"><code>Customer</code></span> resource in these cases, and the operations we can carry out upon it are baked into the HTTP protocol.</p>

<p>HTTP also brings a large ecosystem of supporting tools and technology. We get to use HTTP caching proxies like Varnish and load balancers like mod_proxy, and many monitoring tools already have lots of support for HTTP out of the box. These building blocks allow us to handle large volumes of HTTP traffic and route them smartly, in a fairly transparent way. We also get to use all the available security controls with HTTP to secure our communications. From basic auth to client certs, the HTTP ecosystem gives us lots of tools to make the security process easier, and we’ll explore that topic more in [Link to Come]. That said, to get these benefits, you have to use HTTP well. Use it badly, and it can be as insecure and hard to scale as any other technology out there. Use it right, though, and you get a lot of help.</p>

<p>Note that HTTP can be used to implement RPC too. SOAP, for example, gets routed over HTTP, but unfortunately uses very little of the specification. Verbs are ignored, as are simple things like HTTP error codes. GRPC on the other hand has been designed to take advantage of the capabilities of HTTP/2 such as the ability to send multiple request-response streams over a single connection.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Hypermedia As the Engine of Application State"><div class="sect3" id="idm46534872966808">
<h3>Hypermedia As the Engine of Application State</h3>

<p>Another principle introduced in REST that can help us avoid the coupling between client and server is the concept of <em>hypermedia as the engine of application state</em> (often abbreviated as HATEOAS, and boy, did it need an abbreviation). This is fairly dense wording and a fairly interesting concept, so let’s break it down a bit.</p>

<p>Hypermedia is a concept whereby a piece of content contains links to various other pieces of content in a variety of formats (e.g., text, images, sounds). This should be pretty familiar to you, as it’s what the average web page does: you follow links, which are a form of hypermedia controls, to see related content. The idea behind HATEOAS is that clients should perform interactions (potentially leading to state transitions) with the server via these links to other resources. It doesn’t need to know where exactly customers live on the server by knowing which URI to hit; instead, the client looks for and navigates links to find what it needs.</p>

<p>This is a bit of an odd concept, so let’s first step back and consider how people interact with a web page, which we’ve already established is rich with hypermedia controls.</p>

<p>Think of the Amazon.com shopping site. The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact with it. We have an understanding of what a shopping cart means, even if the exact form and underlying control used to represent it has changed. We know that if we want to view the cart, this is the control we want to interact with. This is how web pages can change incrementally over time. As long as these implicit contracts between the customer and the website are still met, changes don’t need to be breaking changes.</p>

<p>With hypermedia controls, we are trying to achieve the same level of <em>smarts</em> for our electronic consumers. Let’s look at a hypermedia control that we might have for <span class="keep-together">MusicCorp</span>. We’ve accessed a resource representing a catalog entry for a given album in <a data-type="xref" href="#a50-album-listing-with-hypermedia">Example 4-2</a>. Along with information about the album, we see a number of hypermedia controls.</p>
<div id="a50-album-listing-with-hypermedia" data-type="example">
<h5><span class="label">Example 4-2. </span>Hypermedia controls used on an album listing</h5>

<pre data-type="programlisting" data-code-language="xml"><code class="nt">&lt;album</code><code class="nt">&gt;</code><code>
  </code><code class="nt">&lt;name</code><code class="nt">&gt;</code><code>Give Blood</code><code class="nt">&lt;/name&gt;</code><code>
  </code><code class="nt">&lt;link</code><code> </code><code class="na">rel=</code><code class="s">"/artist"</code><code> </code><code class="na">href=</code><code class="s">"/artist/theBrakes"</code><code> </code><code class="nt">/&gt;</code><code> </code><a class="co" id="co_implementing_microservice_communication_CO1-1" href="#callout_implementing_microservice_communication_CO1-1"><img src="https://learning.oreilly.com
  /library/view/building-microservices-2nd/9781492034018/assets/1.png" alt="1" width="12" height="12" data-mfp-src="https://learning.oreilly.com
 /library/view/building-microservices-2nd/9781492034018/assets/1.png"></a><code>
  </code><code class="nt">&lt;description</code><code class="nt">&gt;</code><code>
    Awesome, short, brutish, funny and loud. Must buy!
  </code><code class="nt">&lt;/description&gt;</code><code>
  </code><code class="nt">&lt;link</code><code> </code><code class="na">rel=</code><code class="s">"/instantpurchase"</code><code> </code><code class="na">href=</code><code class="s">"/instantPurchase/1234"</code><code> </code><code class="nt">/&gt;</code><code> </code><a class="co" id="co_implementing_microservice_communication_CO1-2" href="#callout_implementing_microservice_communication_CO1-2"><img src="https://learning.oreilly.com
  /library/view/building-microservices-2nd/9781492034018/assets/2.png" alt="2" width="12" height="12" data-mfp-src="https://learning.oreilly.com
 /library/view/building-microservices-2nd/9781492034018/assets/2.png"></a><code>
</code><code class="nt">&lt;/album&gt;</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_implementing_microservice_communication_CO1-1" href="#co_implementing_microservice_communication_CO1-1"><img src="https://learning.oreilly.com
/library/view/building-microservices-2nd/9781492034018/assets/1.png" alt="1" width="12" height="12" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/1.png"></a></dt>
<dd><p>This hypermedia control shows us where to find information about the artist.</p></dd>
<dt><a class="co" id="callout_implementing_microservice_communication_CO1-2" href="#co_implementing_microservice_communication_CO1-2"><img src="https://learning.oreilly.com
/library/view/building-microservices-2nd/9781492034018/assets/2.png" alt="2" width="12" height="12" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/2.png"></a></dt>
<dd><p>And if we want to purchase the album, we now know where to go.</p></dd>
</dl></div>

<p>In this document, we have two hypermedia controls. The client reading such a document needs to know that a control with a relation of <code>artist</code> is where it needs to navigate to get information about the artist, and that <code>instantpurchase</code> is part of the protocol used to purchase the album. The client has to understand the semantics of the API in much the same way as a human being needs to understand that on a shopping website the cart is where the items to be purchased will be.</p>

<p>As a client, I don’t need to know which URI scheme to access to <em>buy</em> the album, I just need to access the resource, find the buy control, and navigate to that. The buy control could change location, the URI could change, or the site could even send me to another service altogether, and as a client I wouldn’t care. This gives us a huge amount of decoupling between the client and server.</p>

<p>We are greatly abstracted from the underlying detail here. We could completely change the implementation of how the control is presented as long as the client can still find a control that matches its understanding of the protocol, in the same way that a shopping cart control might go from being a simple link to a more complex JavaScript control. We are also free to add new controls to the document, perhaps representing new state transitions that we can perform on the resource in question. We would end up breaking our consumers only if we fundamentally changed the semantics of one of the controls so it behaved very differently, or if we removed a control altogether.</p>

<p>The theory is that by using these controls to decouple the client and server we gain significant benefits over time that hopefully offset the increase in the time it takes to get these protocols up and running. Unfortunately, although these ideas all seem sensible in theory, I’ve found that this form of REST is rarely practiced, for reasons I’ve not entirely got to grips with. This makes HATEOS specifically a much harder concept for me to promote for those already committed to the use of REST. Fundamentally, many of the ideas in REST are predicated on creating distributed hypermedia systems, and this isn’t what most people end up building.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Challenges"><div class="sect3" id="idm46534872966216">
<h3>Challenges</h3>

<p>In terms of ease of consumption, historically you wouldn’t be able to generate client-side code for your REST over HTTP application protocol like you can with RPC implementations. This has often lead to people creating REST APIs providing client libraries for consumers to make use of. These client libraries give you a binding to the API to make client integration easier. The problem is that client libraries can cause some challenges with regards to coupling between the client and server, something we’ll discuss in <a data-type="xref" href="#a50-dry">“DRY and the Perils of Code Reuse in a Microservice World”</a>.</p>

<p>In recent years this problem has been somewhat alleviated. The OpenAPI specification<sup><a data-type="noteref" id="idm46534876354136-marker" href="ch04.html#idm46534876354136" class="totri-footnote">2</a></sup>, that grew out of the Swagger documentation format, now provides you with the ability to define enough information on a REST endpoint to allow for the generation of client-side code in a variety of languages. In my experience, I haven’t seen many teams actually making use of this functionality even if they were already using Swagger for documentation. I have a suspicion that this may be due to the difficulties of retrofitting its use into current APIs. I do also have concerns about a specification previously just being used for documentation now being used to define a more explicit contract. This can lead to a much more complex specification - comparing an OpenAPI schema with a protocol buffer schema for example is quite a stark contrast. Despite my reservations though, it’s good that this option now exists.</p>

<p>Performance may also be an issue. REST over HTTP payloads can actually be more compact than SOAP because it supports alternative formats like JSON or even binary, but it will still be nowhere near as lean a binary protocol as Thrift might be. The overhead of HTTP for each request may also be a concern for low-latency requirements. All mainstream HTTP protocols in current use require the use of Transmission Control Protocol (TCP) under the hood, which has inefficiencies compared with alternative networking protocols, and some RPC implementations can allow you to use alternative networking protocols to TCP such as User Datagram Protocol (UDP).</p>

<p>The limitations placed on HTTP due to the requirement to use TCP are being addressed. HTTP/3, which is currently in the process of being finalized, is looking to shift over to using the newer QUIC protocol. QUIC provides the same sorts of capabilities as TCP (such as improved guarantees over UDP) but has some significant improvements over TCP, which have been shown to deliver improvements in latency and reductions in bandwidth. It’s likely that HTTP/3 will take several years before it has a widespread impact on the public internet, but it seems reasonable to assume that organizations can benefit earlier than this within their own networks.</p>

<p>With respect to HATEOS specifically, you can encounter additional performance issues. As clients need to navgiate multiple controls to find the right endpoints for a given operation, this can lead to very chatty protocols - multiple round trips may be required for each operation. Ultimately, this is a trade-off. If you decide to adopt a HATEOS-style of REST, I would suggest you start with having your clients navigate these controls first, then optimize later if necessary. Remember that we have a large amount of help out of the box by using HTTP, which we discussed earlier. The evils of premature optimization have been well documented before, so I don’t need to expand upon them here. Also note that a lot of these approaches were developed to create distributed hypertext systems, and not all of them fit! Sometimes you’ll find yourself just wanting good old-fashioned RPC.</p>

<p>Despite these disadvantages, REST over HTTP is a sensible default choice for service-to-service interactions. If you want to know more, I recommend <em>REST in Practice</em> (O’Reilly)<sup><a data-type="noteref" id="idm46534872951464-marker" href="ch04.html#idm46534872951464" class="totri-footnote">3</a></sup>, which covers the topic of REST over HTTP in depth.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Where To Use It"><div class="sect3" id="idm46534872950584">
<h3>Where To Use It</h3>

<p>Due to its widespread use in the industry, a REST over HTTP based API is an obvious choice for a synchronous request-response interface if you are looking to allow access from as wide a variety of clients as possible. It would be a mistake to think of a REST over HTTP as just being a “good enough for most things” choice, but there is something to that. It’s a widely understood style of interface, that most people are familiar with, and guarantees interoperability from a huge variety of technologies.</p>

<p>Due in large part to the capabilities of HTTP, and the extent to which REST builds upon these capabilities (rather than hiding them), these APIs excel in situations where you want large scale and effective caching of requests. It’s for this reason that they are the obvious choice for exposing APIs to external parties or client interfaces. They may well suffer when compared to more efficient communication protocols, and although you can construct asynchronous interaction protocols over the top of REST-based APIs, that’s not really a great fit compared to the alternatives for general microservice-to-microservice communication.</p>

<p>Despite intellectually appreciating the goals behind HATEOS, I haven’t in my experience seen the additional work to implement this style of REST deliver worthwhile benefits in the long run, nor can I recall in the last few years  talking to any teams implementing a microservice architecture that can speak to the value of using HATEOS. My own experiences are obviously only one set of data points, and I don’t doubt that for some people it may have worked well. But this concept does not seem to have caught on as much as I thought it would. It could be that the concepts behind HATEOS are too alien for us to grasp, or it could be the lack of tools or standards in this space, or perhaps the model just doesn’t work for the sorts of systems we have ended up building.</p>

<p>So for use at the perimeter, it works fantastically well, and for synchronous request-response based communication between microservices, it’s great.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="GraphQL"><div class="sect2" id="idm46534871486312">
<h2>GraphQL</h2>

<p>In recent years, GraphQL<sup><a data-type="noteref" id="idm46534877015992-marker" href="ch04.html#idm46534877015992" class="totri-footnote">4</a></sup> has gained more popularity, due in large part to the fact that it excels in one specific area. Namely, it makes it possible for a client-side device to define queries that can avoid the need to make multiple requests to retrieve the same information. This can offer significant improvements in terms of the performance of constrained client-side devices, and also avoid the need to implement bespoke server-side aggregation.</p>

<p>TODO: Do I need a picture?</p>

<p>To take a simple example, imagine a mobile device that wants to display a page showing an overview of a customer’s latest orders. The page needs to contain some information about the customer. along with information about the 5 most recent orders the client placed. The screen only needs a few fields from the customer record, and only needs the date, value and shipped status of each order. The mobile device could issue calls to two downstream microservices to retrieve the required information, but this would involve making multiple calls, including pulling back information that isn’t actually required. Especially with mobile devices, this can be wasteful - it uses up more of a mobile device’s data plan than is needed, and can take longer.</p>

<p>GraphQL allows for the mobile device to issue a single query that can pull back all the required information. For this to work, you need a microservice which exposes a GraphQL endpoint to the client device. This GraphQL endpoint is the entry for all client queries, and exposes a schema for the client devices to use. This schema exposes the types available to the client, and a nice graphical query builder is also available to make creating these queries easier. By reducing the amount of calls and amount of data retrieved by the client device, you can deal neatly with some of the challenges that occur when building user interfaces with microservice architectures.</p>










<section data-type="sect3" data-pdf-bookmark="Challenges"><div class="sect3" id="idm46534870653592">
<h3>Challenges</h3>

<p>Early on, one challenge was lack of language support for the GraphQL specification, with JavaScript being your only choice initially. This has improved greatly, with all major technologies now having support for the specification. In fact across the board there have been significant improvements in GraphQL and the various implementations, making it a much less risky prospect than it might have been a few years ago. That said, a few challenges do remain with the technology which you might want to be aware of.</p>

<p>As the client device can issue dynamically changing queries, this can potentially cause an issue with server-side load. I’ve heard of teams who have had issues with GraphQL queries causing significant load on the server-side as a result of this. To compare GraphQL with something like SQL, we have the same issue there. An expensive SQL statement can cause significant problems for a database, potentially having a large impact on the wider system. The same problem applies with GraphQL. The difference is that at least with SQL we have tools like query planners for our databases, which can help us diagnose problematic queries, whereas a similar problem with GraphQL can be harder to track down. Server-side throttling of requests is one potential issue, but as the execution of the call may be spread across multiple microservices, this is far from straightforward.</p>

<p>Compared with normal REST-based HTTP APIs, caching is also more complex. With REST-based API, I can set one of many response headers to help client side devices, or intermediate caches like content delivery networks, cache responses so they don’t need to be requested again. This isn’t possible in the same way with GraphQL. The advice I’ve seen around this issue seems to revolve around just associating an ID with every returned resource (and remember, a GraphQL query could contain multiple resources), and then having the client device cache the request against that ID. As far as I can tell, this makes the use of Content Delivery Networks (CDNs) or caching reverse proxies incredibly difficult without additional work, or additional tooling.</p>

<p>Although I’ve seen some implementation-specific solutions to this problem (such as those found in the JavaScript Apollo implementation), caching feels like it was either consciously or unconsciously ignored as part of the initial development of GraphQL. If the queries you are issuing are highly specific in nature to a particular user, then this lack of request-level caching may not be a deal breaker of course, as your cache-hit ratio is likely to be low. I do wonder though if this limitation means that you’ll still end up with a hybrid solution for client devices, with some (more generic) requests going over normal REST-based HTTP APIs, with other requests going over GraphQL.</p>

<p>Another issue, is that while GraphQL theoretically can handle writes, it doesn’t seem to fit as well as reads. This does lead to situations where teams are using GraphQL for read, but REST for writes.</p>

<p>The last issue is something which may be entirely subjective, but I still think it’s worth raising. GraphQL makes it feel like you are just working with data, which can reinforce the idea that the microservices you are talking to are in fact just wrappers over databases. I’ve seen multiple people in fact compare GraphQL with OData, a technology which is designed as a generic API for accessing data from databases. As we’ve already discussed at length, the idea of just treating microservices as wrappers over databases can be very problematic. Microservices expose functionality over networked interfaces. Some of that functionality might require or result in data being exposed, but they should still have their own internal logic and behavior. Just because you are using GraphQL, don’t slip into thinking of your microservices as little more than an API on a database - it’s essential that your GraphQL API isn’t coupled to the underlying datastores of your microservices.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Where To Use It"><div class="sect3" id="idm46534872805064">
<h3>Where To Use It</h3>

<p>GraphQL’s sweet spot is for use at the perimeter of the system, exposing functionality to external clients. These clients are typically GUIs, and it’s an obvious fit for mobile devices given their constraints in terms of their limited ability to surface data to the end user and nature of mobile networks. GraphQL has also seen use though for external APIs, GitHub being an early adopter of GraphQL. If you have an external API which often requires external clients to make multiple calls to get the information they need, then GraphQL can help make these APIs much more efficient and friendly.</p>

<p>TODO: Reference picture in intro for GraphQL above</p>

<p>Fundamentally, GraphQL is a call aggregation mechanism, so in the context of a microservice architecture it would be used to aggregate calls over multiple downstream microservices, as we saw in &lt;&lt;&gt;&gt;. As such, it’s not something that would replace general microservice-to-microservice communication.</p>

<p>An alternative to the use of GraphQL would be to consider an alternative pattern like the Backend For Frontend (BFF) pattern - we’ll look at that and compare with GraphQL and other aggregation techniques further in [Link to Come].</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Message Brokers"><div class="sect2" id="idm46534871982280">
<h2>Message Brokers</h2>

<p>Message brokers are intermediaries, often called middleware, that sit between processes to manage communication between them. They are a popular choice to help implement asynchronous communication between microservices as they offer a variety of powerful capabilities.</p>

<p>As we discussed earlier, a message is a generic concept which defines the thing that a message broker sends. A message could contain a request, a response, or an event. Rather than one microservice directly communicating with another microservice, instead, it gives a message to a message broker, with information about how the message should be sent.</p>










<section data-type="sect3" data-pdf-bookmark="Topics and Queues"><div class="sect3" id="idm46534870634904">
<h3>Topics and Queues</h3>

<p>Brokers tend to provide either queues, topics, or both. Queues are typically point to point. A sender puts a message on a queue, and a consumer reads from that queue. With a topic-based system, multiple consumers are able to subscribe to a topic, and each subscribed consumer will receive a copy of that message.</p>

<p>A consumer could represent one or more microservices - typically modelled as a consumer group. This would be useful when you have multiple instances of a microservice, and you want any one of them to be able to receive a message. In <a data-type="xref" href="#queues">Figure 4-1</a>, we see an example where the <code>Order Processor</code> has three deployed instances, all as part of the same consumer group. When a message is put into the queue, only one member of the consumer group will receive that message - this means the queue works as a load distribution mechanism - this is an example of the Competing Consumers pattern we touched on briefly in <a data-type="xref" href="ch03.html#integration-chapter">Chapter 3</a>.</p>

<figure><div id="queues" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/queues.png" alt="Topics allow for multiple subscribers to receive the same messages, useful for event broadcast" width="1245" height="729" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/queues.png">
<h6><span class="label">Figure 4-1. </span>A queue allows for one consumer group</h6>
</div></figure>

<p>With topics, you can have multiple consumer groups. In <a data-type="xref" href="#topics">Figure 4-2</a>, an event representing an order being paid for is put onto the <code>Order Status</code> topic. A copy of that event is received by both the <code>Warehouse</code> microservice, and the <code>Notifications</code> microservice, both of which are in separate consumer groups. Only one instance of each consumer group will see that event.</p>

<figure><div id="topics" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/topics.png" alt="Topics allow for multiple subscribers to receive the same messages, useful for event broadcast" width="1558" height="1050" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/topics.png">
<h6><span class="label">Figure 4-2. </span>Topics allow for multiple subscribers to receive the same messages, useful for event broadcast</h6>
</div></figure>

<p>At first glance, a queue just looks like a topic with a single consumer group. A large part of the distinction between the two is that when sending a message over a queue, there is knowledge of what the message is being sent to. With a topic, this information is hidden from the sender of the message - they are unaware of who (if anyone) will end up receiving the message.</p>

<p>Topics are a good fit for event-based collaboration, where queues would be more appropriate for request/response communication. This should be considered as general guidance though rather than a strict rule.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Guaranteed Delivery"><div class="sect3" id="idm46534870622616">
<h3>Guaranteed Delivery</h3>

<p>So why use a broker? Fundamentally, they provide some capabilities that can be very useful for asynchronous communication. The properties they provide vary, but the most interesting feature is that of guaranteed delivery, something which all widely used brokers support in some way. Guaranteed delivery describes a commitment by the broker to ensure that the message is delivered.</p>

<p>From the point of view of the microservice sending the message, this can be very useful. If the downstream destination is unavailable, then this isn’t a problem - the broker will hold on to the message until it can be delivered. This can reduce the number of things an upstream microservice needs to worry about. When compared to a synchronous direct call, for example an HTTP request, if the downstream destination isn’t reachable, the upstream microservice will need to work out what to do with the request - should it retry the call, or give up?</p>

<p>For guaranteed delivery to work, a broker will need to ensure that any messages not yet delivered are going to be held in a durable fashion until they are able to be delivered. To deliver on this promise, a broker will normally run as some sort of cluster-based system, ensuring that the loss of a single machine doesn’t cause the message to be lost. There is typically a lot involved in running a broker correctly, partly due to the challenges in managing cluster-based software. Often, the promise of guaranteed delivery can be undermined if the broker isn’t setup correctly. As an example, RabbitMQ requires instances in a cluster to communicate over relatively low-latency networks, otherwise the instances can start to get confused about the current state of messages being handled, resulting in data loss. I’m not highlighting this particular limitation as a way of saying that RabbitMQ is in anyway bad, all brokers have restrictions as to how they need to be run to deliver the promise of guaranteed delivery. If you plan to run your own broker, make sure you read the documentation carefully.</p>

<p>It’s also worth noting that what any given broker means by guaranteed delivery can vary. Again, reading the documentation is a great start.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Trust"><div class="sect3" id="idm46534870618088">
<h3>Trust</h3>

<p>One of the big draws of a broker is the property of guaranteed delivery. But for this to work, you need to trust not only the people who created the broker, but also the way that broker has operated. If you’ve built a system that is based on the assumption that delivery is guaranteed, and that turns out not to be the case due to an issue with the underlying broker, it can cause significant issues. The hope of course is that you are offloading that work to software created by people who can do that job better than you can. Ultimately, you have to decide how much you want to trust the broker you are making use of.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Other Characteristics"><div class="sect3" id="idm46534870616056">
<h3>Other Characteristics</h3>

<p>Aside from guaranteed delivery, there are other characteristics that brokers can provide that you may find to be useful.</p>

<p>Most brokers can guarantee the order in which messages will be delivered, but this isn’t universal, and even then the scope of this guarantee can be limited. With Kafka for example, ordering is only guaranteed within a single partition. If you are unable to be certain that messages will be received in order, your consumer may need to compensate for this, perhaps by deferring processing of messages that are received out of order, until the missing messages are received.</p>

<p>Some brokers provide transactions on write - Kafka as an example allows you to write to multiple topics in a single transaction. Some brokers can also provide read transactionality, and this is something I’ve made use of when using a number of brokers via the Java Messaging Service (JMS) APIs. This can be useful if you want to ensure the message can be processed by the consumer before removing it from the broker.</p>

<p>Another, somewhat controversial feature promised by some brokers is that of exactly once delivery. One of the easier ways to provide guaranteed delivery is allowing the message to be resent. This can result in a consumer seeing the same message more than once (even if this is a rare situation). Most brokers will do what they can to reduce the chance of this, or hide this fact from the consumer, but some brokers have gone further and guarantee exactly once delivery. This is a complex topic, as I’ve spoken to some experts who state that guaranteeing this in all cases is impossible, while other experts say you basically can do this with a few simple workarounds. Either way, if your broker of choice claims to implement this, then pay <strong>really</strong> careful attention to how this is implemented. Even better, build your consumers in such a way that they are prepared for the fact that they might receive a message more than once, and can handle this situation. A very simple example would be for each message to have an ID which a consumer can check when the message is received. If a message with that ID has already been processed, the message can be ignored.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Choices"><div class="sect3" id="idm46534870610760">
<h3>Choices</h3>

<p>A variety of message brokers exist. Popular examples include RabbitMQ, ActiveMQ, and Kafka (which we’ll explore further shortly). The main public cloud vendors also provide a variety of products that play this role, from managed versions of those brokers you could install on your own infrastructure, to bespoke implementations that are specific to a given platform. AWS for example has the Simple Queue Service (SQS), Simple Notification Service (SNS), and Kinesis, all of which provide different flavours of fully managed brokers. SQS was in fact the first ever product released by AWS, launched back in 2006.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Kafka"><div class="sect3" id="idm46534870608856">
<h3>Kafka</h3>

<p>Kafka is worth highlighting as a specific broker, due in large part to its popularity in recent years. Part of this popularity is due to its use in helping moving large volumes of data around as part of implementing stream processing pipelines. This can help move from batch-oriented processing to more real-time processing.</p>

<p>There are a few characteristics of kafka which are worth highlighting. Firstly, it is designed for very large scale - it was built at LinkedIn to replace multiple existing message clusters with a single platform. Kafka is built to allow for multiple consumers and producers - I’ve spoken to one expert at a large technology company who had over 50K producers and consumers working on the same cluster. To be fair, very few organizations have problems at that level of scale, but for some organizations, the ability to scale kafka easily (relatively speaking) can be very useful.</p>

<p>Another fairly unique feature of kafka is message permanence. With a normal message broker, once the last consumer has received a message, the broker no longer needs to hold on to that message. With Kafka, messages can be stored for a configurable period. This means that messages can be stored forever. This can allow consumers to re-ingest messages that they had already processed, or allow newly deployed consumers to process messages that were sent previously.</p>

<p>Finally, Kafka has been rolling out built-in support for stream processing. Rather than using Kafka to send messages to a dedicated stream processing tool like Apache Flink, instead some tasks can be done inside Kafka itself. Using KSQL, you can define SQL-like statements that can process one or more topics on the fly. This can give you something akin to a dynamically updating materialized database view, with the source of data being Kafka topics rather than a database. These capabilities open up some very interesting possibilities about how data is managed in distributed systems. If you’d like to explore these ideas in more detail, I can recommend “Designing Event-Driven Systems” by Ben Stopford<sup><a data-type="noteref" id="idm46534870604200-marker" href="ch04.html#idm46534870604200" class="totri-footnote">5</a></sup> (I have to recommend Ben’s book, as I wrote the foreword for it!). For a deeper dive on Kafka in general, I’d suggest “Kafka: The Definitive Guide”<sup><a data-type="noteref" id="idm46534870603016-marker" href="ch04.html#idm46534870603016" class="totri-footnote">6</a></sup>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="TODO: Challenges"><div class="sect3" id="idm46534870601880">
<h3>TODO: Challenges</h3>

</div></section>













<section data-type="sect3" data-pdf-bookmark="TODO: When To Use It"><div class="sect3" id="idm46534870600872">
<h3>TODO: When To Use It</h3>

</div></section>



</div></section>











<section data-type="sect1" data-pdf-bookmark="Serialization Formats"><div class="sect1" id="idm46534870637048">
<h1>Serialization Formats</h1>

<p>Some of the technology choices we’ve looked at - specifically some of the RPC implementations - make choices for you regarding how data in serialized and deserialized. When picking GRPC for example, any data sent will be converted into protocol buffer format. Many of the technology options though give us a lot of freedom in terms of how we covert data for network calls. Pick Kafka as your broker of choice, and you can send messages in a variety of formats. So which format should you chose?</p>








<section data-type="sect2" data-pdf-bookmark="Textual Formats"><div class="sect2" id="idm46534870598216">
<h2>Textual Formats</h2>

<p>The use of standard textual formats gives clients a lot of flexibility as to how they consume resources. REST APIs mostly typically use a textual format for the request and response bodies, even if theoretically you can quite happily send binary data over HTTP. In fact, this is how GRPC works - using HTTP underneath, but sending binary protocol buffers.</p>

<p>JSON has usurped XML as the text serialization format of choice. You can point to a number of reasons why this occurred, but the main reason is that one of the main consumers of APIs is often a browser, where JSON is a great fit. JSON became popular partly as a result of the backlash against XML, and proponents cite its relative compactness and simplicity when compared to XML as another winning factor. The reality is that the size of a JSON vs XML payload is rarely a massive differential, especially as these payloads are typically compressed. It’s also worth pointing out that some of the simplicity of JSON comes at a cost - in our rush to adopt simpler protocols, schemas went out of the window (more on that later).</p>

<p>AVRO is an interesting serialization format. It takes JSON as an underlying structure and uses it to define a schema-based format. AVRO has found a lot of popularity as a format for message payloads, partly due to the ability to send the schema as part of the payload, which can make supporting multiple different messaging formats much easier.</p>

<p>Personally, though, I am still a fan of XML. Some of the tool support is better. For example, if I want to extract only certain parts of the payload (a technique we’ll discuss more in <a data-type="xref" href="#a50-versioning">“Handling Change Between Microservices”</a>) I can use XPATH, which is a well-understood standard with lots of tool support, or even CSS selectors, which many find even easier. With JSON, I have JSONPATH, but this is not as widely supported. I find it odd that people pick JSON because it is nice and lightweight, then try and push concepts into it like hypermedia controls that already exist in XML. I accept, though, that I am probably in the minority here and that JSON is the format of choice for most people!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Binary Formats"><div class="sect2" id="idm46534870592664">
<h2>Binary Formats</h2>

<p>Where textual formats have benefits like making it easy for humans to read them, and provide a lot of interoperability with different tools and technologies, the world of binary serialization protocols is where you want to be if you start getting worried about payload size, or the efficiencies of writing and reading the payloads. Protocol buffers have been around for a while, and are often used outside the scope of GRPC - they probably represent the most popular binary serialization format for microservice-based communication.</p>

<p>This space though is large, and there are a number of other formats out there that have been developed with a variety of requirements in mind. Simple Binary Encoding<sup><a data-type="noteref" id="idm46534870590424-marker" href="ch04.html#idm46534870590424" class="totri-footnote">7</a></sup>, Cap’n Proto<sup><a data-type="noteref" id="idm46534870589064-marker" href="ch04.html#idm46534870589064" class="totri-footnote">8</a></sup>, and FlatBuffers<sup><a data-type="noteref" id="idm46534870587704-marker" href="ch04.html#idm46534870587704" class="totri-footnote">9</a></sup> all come to mind. Although benchmarks abound for each of these formats, highlighting their relevant benefits compared to protocol buffers, JSON, or other formats, benchmarks suffer from a fundamental problem that they may not necessarily represent how you are going to use them. If you’re looking to eek the last few bytes out of your serialization format, or shave microseconds off the time taken to read or write these payloads, I strongly suggest you carry out your own comparison of these various formats. In my experience, the vast majority of systems rarely have to worry about such optimizations though, as they can often achieve the improvements they are looking for by sending less data, or not making the call at all. If you are building an ultra-low latency distributed system though, make sure you’re prepared to dive head first into the world of binary serialization formats.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Schemas"><div class="sect1" id="idm46534870585048">
<h1>Schemas</h1>

<p>One discussion that comes up time and again is should we use schemas to define what our endpoints expose, and what they accept? Schemas can come in lots of different types, and typically picking a serialization format will define which schema technology you can use. If you’re working with raw XML, you’d use XML Schema Definition (XSD), raw json, you’d use JSON-Schema. Some of the technology choices we’ve touched on (specifically a sizable subset of the RPC options) require the use of explicit schemas, so if you picked those technologies you’d have to make use of schemas. SOAP works through use of a schema specification called the Web Service Definition Language (WSDL), while GRPC requires the use of a protocol buffer specification. Other technology choices we’ve explored make the use of schemas optional, and this is where things get more interesting.</p>

<p>Personally speaking, I am in favour of having explicit schemas for microservice endpoints. This is for two key reasons. Firstly, it goes a long way to being an explicit representation of what a microservice endpoint exposes, and what it can accept. This makes life easier for both developers working on the microservice, but also their consumers. Schemas may not replace the need for good documentation, but they certainly can help reduce the amount of documentation required.</p>

<p>The other reason I like explicit schemas though, is how they help in terms of catching accidental breakages of microservice endpoints. We’ll explore how to handle changes between microservices in a moment, but it’s first worth exploring the different types of breakages and the role schemas can play.</p>








<section data-type="sect2" data-pdf-bookmark="Structural vs Semantic Contract Breakages"><div class="sect2" id="idm46534870581064">
<h2>Structural vs Semantic Contract Breakages</h2>

<p>Broadly speaking, we can break contract breakages down into two categories - <em>structural</em> breakages, and <em>semantic</em> breakages. Structural breakages refer to situations where the structure of the endpoint changes in such a way that a consumer is now incompatible - this could represent fields or methods being removed, or new required fields being added. Semantic breakages refer to situations where the structure of the microservices endpoint remains the same, but the behavior changes in such a way as to break consumers expectations.</p>

<p>Let’s take a simple example. You have a highly complex <code>Hard Calculations</code> microservice that exposes a <code>calculate</code> method on its endpoint. This <code>calculate</code> method takes two integers, both of which are required fields. If you changed <code>Hard Calculations</code> such that the <code>calculate</code> method now takes only one integer, then consumers would break - they’d be sending requests with two integers which the <code>Hard Calculations</code> microservice would reject. This is an example of a structural change, and in general these changes can be easier to spot.</p>

<p>Semantic changes are more problematic. This is where the structure of the endpoint doesn’t change, but the behavior of the endpoint does. Coming back to our <code>calculate</code> method, imagine that in the first version, the two provided integers are addded together and the results returned. So far so good. Now, we change <code>Hard Calculations</code> so that the <code>calculate</code> method now multiplies the integers together and returns the result. The semantics of the <code>calculate</code> method have changed in a way that could break expectations of the consumers.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Should You Use Schemas?"><div class="sect2" id="idm46534870572536">
<h2>Should You Use Schemas?</h2>

<p>By using schemas, and comparing different versions of schemas, we can catch structural breakages. Catching semantic breakages requires the use of testing. If you don’t have schemas, or have schemas but decide to not compare schema changes for compatibility, then the burden of catching structural breakages before you get to production also falls on testing. Arguably, the situation is somewhat analogous with static vs dynamic typing in programming languages. With a statically typed language, the types are fixed at compile time - if your code does something with an instance of a type that isn’t allowed (like calling a method that doesn’t exist), then the compiler can catch that mistake. This can leave you to focus testing efforts on other sorts of problems. With a dynamically typed language though, some of your testing will need to catch mistakes that a compiler picks up for statically typed languages.</p>

<p>Now, I’m pretty relaxed about static vs dynamically typed languages, and I’ve found myself to be very productive (relatively speaking) in both. Certainly, dynamically typed languages give you some significant benefits which for many people justify giving up on compile time safety. Personally speaking though, if we bring the discussion back to microservice interactions, I haven’t found that a similar balanced tradeoff exists when it comes to schema vs schemaless communication. Put simply, I think that having an explicit schema more than offsets any perceived benefit of having schema-less based communication.</p>

<p>The main argument for schemaless endpoints seems to be that schemas need more work and don’t give enough value. This IMHO is partly a failure of imagination, and partly a failure of good tooling to help schemas have more value when it comes to using them to catch structural breakages.</p>

<p>Really, the question isn’t actually if you have a schema or not - it’s whether or not that schema is <em>explicit</em>. If you are consuming data from a schemaless API, you still have expectations as to what data should be in there, and how that data should be structured. Your code that will handle the data will be written with a set of assumptions in mind as to how that data is structured. In such a case the schema is arguably totally implicit, rather than explicit<sup><a data-type="noteref" id="idm46534870567000-marker" href="ch04.html#idm46534870567000">10</a></sup>. A lot of my desire for an explicit schema is driven by the fact that I think it’s important to be as explicit as possible as to what a microservice does (or doesn’t) expose.</p>

<p>Ultimately, a lot of what schemas provide is an explicit representation of part of the structure contract between a client and server. They help make things explicit, and can greatly aid communication between teams as well as work as a safety net. In situations where the cost of change is reduced, for example where both client and server are owned by the same team, then I am more relaxed about you not having schemas.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Handling Change Between Microservices"><div class="sect1" id="a50-versioning">
<h1>Handling Change Between Microservices</h1>

<p>Probably the most common question I get about microservices, after “how big should they be?” is “how do you handle versioning?”. When this question gets asked, it’s rarely a query regarding what sort of numbering scheme you should use, it’s more about how you handle changes in the contracts between microservices.</p>

<p>How you handle change really breaks down into two topics. In a moment, we’ll look at what happens if you need to make a breaking change. But before that, we’ll look at what you can do to avoid making a breaking change in the first place.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Avoiding Breaking Changes"><div class="sect1" id="idm46534870561256">
<h1>Avoiding Breaking Changes</h1>

<p>If you want to avoid making breaking changes, there are a few key ideas which are worth exploring - many of which we’ve already touched on at the start of the chapter. If you can put these ideas into practice, you’ll find it much easier to allow for microservices to be changed independently from one another.</p>
<dl>
<dt>Expansion Changes</dt>
<dd>
<p>Add new things to a microservice interface, don’t remove old things</p>
</dd>
<dt>Tolerant Reader</dt>
<dd>
<p>When consuming a microservice interface, be flexible in what you expect.</p>
</dd>
<dt>Right Technology</dt>
<dd>
<p>Pick technology that makes it easier to make backwards compatible changes to the interface.</p>
</dd>
<dt>Explicit Interface</dt>
<dd>
<p>Be explicit about what a microservice exposes. This makes things easier for the client, and easier for the maintainers of the microservice to understand what can be changed freely.</p>
</dd>
<dt>Catch Accidental Breaking Changes Early</dt>
<dd>
<p>Have mechanisms in place to catch interface changes that will break consumers in production, before those changes are deployed.</p>
</dd>
</dl>

<p>These ideas do reinforce each other, and many build upon that key concept of information hiding that we’ve discussed frequently so far. Let’s look at each in turn.</p>








<section data-type="sect2" data-pdf-bookmark="Expansion Changes"><div class="sect2" id="idm46534870551128">
<h2>Expansion Changes</h2>

<p>Probably the easiest place to start is by only adding new things to a microservice contract, and don’t remove anything else. Consider the example of adding a new field to a payload - assuming the client is in some way tolerant of such changes, this shouldn’t have a material impact. Adding a new <code>dob</code> field to a customer record should be fine for example.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Tolerant Reader"><div class="sect2" id="idm46534870548744">
<h2>Tolerant Reader</h2>

<p>How the consumer of a microservice is implemented can have a lot to say regarding making backwards compatible changes easy. Specifically, we want to avoid client code binding too tightly to the interface of a microservice. Let’s consider an email microservice, whose job it is to send out emails to our customers from time to time. It gets asked to send an order shipped email to a customer with the ID 1234. It goes off and retrieves the customer with that ID, and gets back something like the response shown in <a data-type="xref" href="#exampleresponse-4-3">Example 4-3</a>.</p>
<div id="exampleresponse-4-3" data-type="example">
<h5><span class="label">Example 4-3. </span>Sample response from the customer service</h5>

<pre data-type="programlisting" data-code-language="xml"><code class="nt">&lt;customer&gt;</code>
  <code class="nt">&lt;firstname&gt;</code>Sam<code class="nt">&lt;/firstname&gt;</code>
  <code class="nt">&lt;lastname&gt;</code>Newman<code class="nt">&lt;/lastname&gt;</code>
  <code class="nt">&lt;email&gt;</code>sam@magpiebrain.com<code class="nt">&lt;/email&gt;</code>
  <code class="nt">&lt;telephoneNumber&gt;</code>555-1234-5678<code class="nt">&lt;/telephoneNumber&gt;</code>
<code class="nt">&lt;/customer&gt;</code></pre></div>

<p>Now to send the email, the email microservice only needs the <code>firstname</code>, <code>lastname</code>, and <code>email</code> fields. We don’t need to know the <code>telephoneNumber</code>. We want to simply pull out those fields we care about, and ignore the rest. Some binding technology, especially that used by strongly typed languages, can attempt to bind <em>all</em> fields whether the consumer wants them or not. What happens if we realize that no one is using the <code>telephoneNumber</code> and we decide to remove it? This could cause consumers to break needlessly.</p>

<p>Likewise, what if we wanted to restructure our <code>Customer</code> object to support more details, perhaps adding some further structure as in <a data-type="xref" href="#a50-restructured-customer">Example 4-4</a>? The data our email service wants is still there, and still with the same name, but if our code makes very explicit assumptions as to where the <code>firstname</code> and <code>lastname</code> fields will be stored, then it could break again. In this instance, we could instead use XPath to pull out the fields we care about, allowing us to be ambivalent about where the fields are, as long as we can find them. This pattern—of implementing a reader able to ignore changes we don’t care about—is what Martin Fowler calls a <a href="http://bit.ly/1yISOdQ">Tolerant Reader</a>.</p>
<div id="a50-restructured-customer" data-type="example">
<h5><span class="label">Example 4-4. </span>A restructured Customer resource: the data is all still there, but can our consumers find it?</h5>

<pre data-type="programlisting" data-code-language="xml"><code class="nt">&lt;customer&gt;</code>
  <code class="nt">&lt;naming&gt;</code>
    <code class="nt">&lt;firstname&gt;</code>Sam<code class="nt">&lt;/firstname&gt;</code>
    <code class="nt">&lt;lastname&gt;</code>Newman<code class="nt">&lt;/lastname&gt;</code>
    <code class="nt">&lt;nickname&gt;</code>Magpiebrain<code class="nt">&lt;/nickname&gt;</code>
    <code class="nt">&lt;fullname&gt;</code>Sam "Magpiebrain" Newman<code class="nt">&lt;/fullname&gt;</code>
  <code class="nt">&lt;/naming&gt;</code>
  <code class="nt">&lt;email&gt;</code>sam@magpiebrain.com<code class="nt">&lt;/email&gt;</code>
<code class="nt">&lt;/customer&gt;</code></pre></div>

<p>The example of a client trying to be as flexible as possible in consuming a service demonstrates <a href="http://bit.ly/1Cs7dfR">Postel’s Law</a> (otherwise known as the <em>robustness principle</em>), which states: “Be conservative in what you do, be liberal in what you accept from others.” The original context for this piece of wisdom was the interaction of devices over networks, where you should expect all sorts of odd things to happen. In the context of microservice-based interactions, it leads us to try and structure our client code to be tolerant of changes to payloads.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Right Technology"><div class="sect2" id="idm46534870514152">
<h2>Right Technology</h2>

<p>As we’ve already explored, some technology can be more brittle when it comes to allowing us to change interfaces - I’ve already highlighted my own personal frustrations with Java RMI. On the other hand, some integration implementations go out of their way to make it as easy as possible for changes to be made without breaking clients. At the simple end of the spectrum, protocol buffers, the serialization format used as part of GRPC, has the concept of field number. Each entry in a protocol buffer has to define a field number, which client code expects to find. If new fields are added, the client doesn’t care. AVRO allows for the schema to be sent along with the payload, allowing clients to potentially interpret a payload much like a dynamic type.</p>

<p>At the more extreme end of the spectrum, the REST concept of HATEOS is largely all about enabling clients to make use of REST endpoints even when they change by making use of the previously discussed hypermedia links. This does call for you to buy into the entire HATEOS mindset of course.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Explicit Interface"><div class="sect2" id="idm46534870511160">
<h2>Explicit Interface</h2>

<p>I am a <strong>big</strong> fan of a microservice exposing an explicit schema denoting what it’s endpoints do. Having an explicit schema makes it clear to consumers as to what they can expect, but it also makes it much more clear to a developer working on a microservice as to what things should remain untouched to ensure you don’t break consumers. Put another way, an explicit schema goes a long way to making the boundaries of information hiding more explicit - what’s exposed in the schema is by definition not hidden.</p>

<p>Having an explicit schema for RPC is long established, and is in fact a requirement for many RPC implementations. REST on the other hand has typically viewed the concept of a schema as optional, to the point where I find explicit schemas for REST endpoints to be vanishingly rare. This is changing, with things like the aforementioned OpenAPI specification gaining traction, and the JSON Schema specification also gaining in maturity.</p>

<p>Asynchronous messaging protocols have struggled more in this space. You can have a schema for the payload of a message easily enough, and in fact this is an area where AVRO is frequently used. However having an explicit interface needs to go further than this. If we consider a microservice that fires events, which events does it expose? There are a few attempts at making explicit schemas for event-based endpoints underway. One is AsyncAPI<sup><a data-type="noteref" id="idm46534870473048-marker" href="ch04.html#idm46534870473048">11</a></sup> which has picked up a number of big name users, but the one gaining most traction seems to be CloudEvents specification<sup><a data-type="noteref" id="idm46534870471384-marker" href="ch04.html#idm46534870471384">12</a></sup> which is backed by the Cloud Native Computing Foundation. Azure’s event grid product supports the CloudEvents format, a sign of different vendors supporting this format which should help with interoperability. This is still a fairly new space, so it will be interesting to see how things shake out over the next few years.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46534870469464">
<h5>Semantic Versioning</h5>
<p>Wouldn’t it be great if as a client you could look just at the version number of a service and know if you can integrate with it?  <a href="http://semver.org/"><em>Semantic versioning</em></a> is a specification that allows just that. With semantic versioning, each version number is in the form <code>MAJOR.MINOR.PATCH</code>. When the <code>MAJOR</code> number increments, it means that backward incompatible changes have been made. When <code>MINOR</code> increments, new functionality has been added that should be backward compatible. Finally, a change to <code>PATCH</code> states that bug fixes have been made to existing functionality.</p>

<p>To see how useful semantic versioning can be, let’s look at a simple use case. Our helpdesk application is built to work against version 1.2.0 of the customer service. If a new feature is added, causing the customer service to change to 1.3.0, our helpdesk application should see no change in behavior and shouldn’t be expected to make any changes. We couldn’t guarantee that we could work against version 1.1.0 of the customer service, though, as we may rely on functionality added in the 1.2.0 release. We could also expect to have to make changes to our application if a new 2.0.0 release of the customer service comes out.</p>

<p>You may decide to have a semantic version for the service, or even for an individual endpoint on a service if you are coexisting them as detailed in the next section.</p>

<p>This versioning scheme allows us to pack a lot of information and expectations into just three fields. The full specification outlines in very simple terms the expectations clients can have of changes to these numbers, and can simplify the process of communicating about whether changes should impact consumers. Unfortunately, I haven’t seen this approach used enough in distributed systems to understand its effectiveness in that context.</p>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Catch Accidental Breaking Changes Early"><div class="sect2" id="idm46534870462296">
<h2>Catch Accidental Breaking Changes Early</h2>

<p>It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, it’s possible that an innocent change of a microservice could cause consumers to break. As we’ve already touched on, using schemas can help us pick up structural changes, assuming we use some sort of tooling to help compare schema versions. There is a wide range of tooling out there to do this for different schema types. We have ProtoLock<sup><a data-type="noteref" id="idm46534870460408-marker" href="ch04.html#idm46534870460408">13</a></sup> for protocol buffers, json-schema-diff-validator for JSON-Schema<sup><a data-type="noteref" id="idm46534870459000-marker" href="ch04.html#idm46534870459000">14</a></sup>, or openapi-diff for the openAPI specification<sup><a data-type="noteref" id="idm46534870457512-marker" href="ch04.html#idm46534870457512">15</a></sup>. More tools seem to be cropping up all the time in this space - what you’re looking for though is something that doesn’t just report on the differences between two schemas, but something that will pass or fail based on compatibility - this would allow you to fail a CI build if incompatible schemas are found, ensuring that your microservice won’t get deployed.</p>

<p>The open source Confluent schema registry<sup><a data-type="noteref" id="idm46534870435976-marker" href="ch04.html#idm46534870435976">16</a></sup> supports JSON-schema, AVRO and protocol buffers, and is capable of comparing newly uploaded versions for backwards compatibility. Although it was built to help as part of an ecosystem where Kafka is being used, the registry isn’t tied to kafka in anyway, and you could make use of this in other situations to ensure backwards compatibility based on schema comparison.</p>

<p>Schema comparison tools can help us catch structural breakages, but what about semantic breakages? Or what if you aren’t making use of schemas in the first place? Then we’re looking at testing. This is a topic we’ll explore in more detail in [Link to Come], but I wanted to highlight consumer-driven contract testing which explicitly helps in this area. Just remember, if you don’t have schemas, expect your testing to have to do more work to catch breaking changes.</p>

<p>If you’re supporting multiple different client libraries, running tests using each library you support against the latest service is another technique that can help. Once you realize you are going to break a consumer, you have the choice to either try to avoid the break altogether or else embrace it and start having the right conversations with the people looking after the consuming <span class="keep-together">services</span>.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Managing Breaking Changes"><div class="sect1" id="idm46534870431016">
<h1>Managing Breaking Changes</h1>

<p>So you’ve gone as far as you can to ensure that the changes you’re making to a microservice’s interface are backwards compatible, but you’ve realized that you just have to make a change that will constitute a breaking change. What can you do in such a situation? You’ve got three main options:</p>
<dl>
<dt>Lock-step Deployment</dt>
<dd>
<p>Require that the microservice exposing the interface and all consumers of that interface are changed at the same time</p>
</dd>
<dt>Coexist Incompatible Microservice Versions</dt>
<dd>
<p>Run old and new versions of the microservice side by side</p>
</dd>
<dt>Emulate The Old Interface</dt>
<dd>
<p>Have your microservice expose the new interface, and also emulate the old interface</p>
</dd>
</dl>








<section data-type="sect2" data-pdf-bookmark="Lock-Step Deployment"><div class="sect2" id="idm46534870424248">
<h2>Lock-Step Deployment</h2>

<p>Of course, lock-step deployment flies in the face of independent deployability. If we want to be able to deploy a new version of our microservice with a breaking change to it’s interface, but still do this in an independent fashion, we need to give our consumers time to upgrade to the new interface. That leads us on to the next two options I’d consider.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Coexist Incompatible Microservice Versions"><div class="sect2" id="idm46534870422296">
<h2>Coexist Incompatible Microservice Versions</h2>

<p>Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older version, with newer versions seeing the new one, as shown in <a data-type="xref" href="#multiple-service-versions">Figure 4-3</a>. This is the approach used sparingly by Netflix in situations where the cost of changing older consumers is too high, especially in rare cases where legacy devices are still tied to older versions of the API. Personally, I am not a fan of this idea, and understand why Netflix uses it rarely. First, if I need to fix an internal bug in my service, I now have to fix and deploy two different sets of services. This would probably mean I have to branch the codebase for my service, and this is always problematic. Second, it means I need smarts to handle directing consumers to the right microservice. This behavior inevitably ends up sitting in middleware somewhere or a bunch of <code>nginx</code> scripts, making it harder to reason about the behavior of the system. Finally, consider any persistent state our service might manage. Customers created by either version of the service need to be stored and made visible to all services, no matter which version was used to create the data in the first place. This can be an additional source of complexity.</p>

<figure><div id="multiple-service-versions" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/multiple-service-versions.png" alt="Running multiple versions of the same service to support old endpoints" width="791" height="904" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/multiple-service-versions.png">
<h6><span class="label">Figure 4-3. </span>Running multiple versions of the same service to support old endpoints</h6>
</div></figure>

<p>Coexisting concurrent service versions for a short period of time can make perfect sense, especially when you’re doing things like blue/green deployments or canary releases (we’ll be discussing these patterns more in [Link to Come]). In these situations, we may be coexisting versions only for a few minutes or perhaps hours, and normally will have only two different versions of the service present at the same time. The longer it takes for you to get consumers upgraded to the newer version and released, the more you should look to coexist different endpoints in the same microservice rather than coexist entirely different versions. I remain unconvinced that this work is worthwhile for the average project.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Emulate The Old Interface"><div class="sect2" id="idm46534870414072">
<h2>Emulate The Old Interface</h2>

<p>If we’ve done all we can to avoid introducing a breaking interface change, our next job is to limit the impact. The thing we want to avoid is forcing consumers to upgrade in lock-step with us, as we always want to maintain the ability to release microservices independently of each other. One approach I have used successfully to handle this is to coexist both the old and new interfaces in the same running service. So if we want to release a breaking change, we deploy a new version of the service that exposes both the old and new versions of the endpoint.</p>

<p>This allows us to get the new microservice out as soon as possible, along with the new interface, but give time for consumers to move over. Once all of the consumers are no longer using the old endpoint, you can remove it along with any associated code, as shown in <a data-type="xref" href="#emulate-endpoints">Figure 4-4</a>.</p>

<figure><div id="emulate-endpoints" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/emulate-endpoints.png" alt="Coexisting different endpoint versions allows consumers to migrate gradually" width="1470" height="875" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/emulate-endpoints.png">
<h6><span class="label">Figure 4-4. </span>One microservice emulating the old endpoint and exposing the new backwards incompatible endpoint</h6>
</div></figure>

<p>When I last used this approach, we had gotten ourselves into a bit of a mess with the number of consumers we had and the number of breaking changes we had made. This meant that we were actually coexisting three different versions of the endpoint. This is not something <span class="no-kerning">I’d</span> recommend! Keeping all the code around and the associated testing required to ensure they all worked was absolutely an additional burden. To make this more manageable, we internally transformed all requests to the V1 endpoint to a V2 request, and then V2 requests to the V3 endpoint. This meant we could clearly delineate what code was going to be retired when the old endpoint(s) died.</p>

<p>This is in effect an example of the expand and contract pattern, which allows us to phase breaking changes in. We <em>expand</em> the capabilities we offer, supporting both old and new ways of doing something. Once the old consumers do things in the new way, we <em>contract</em> our API, removing the old functionality.</p>

<p>If you are going to coexist endpoints, you need a way for callers to route their requests accordingly. For systems making use of HTTP, I have seen this done with both version numbers in request headers and also in the URI itself—for example, <em>/v1/customer/</em> or <em>/v2/customer/</em>. I’m torn as to which approach makes the most sense. On the one hand, I like URIs being opaque to discourage clients from hard-coding URI templates, but on the other hand, this approach does make things very obvious and can simplify request routing.</p>

<p>For RPC, things can be a little trickier. I have handled this with protocol buffers by putting my methods in different namespaces—for example, <code>v1.createCustomer</code> and <code>v2.createCustomer</code>—but when you are trying to support different versions of the same types being sent over the network, this can become really painful.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Which Approach Do I Prefer?"><div class="sect2" id="idm46534870401512">
<h2>Which Approach Do I Prefer?</h2>

<p>For situations where the same team manages both the microservice and all consumers, I am somewhat relaxed about a lock-step release in limited situations. Assuming it really is a one-off situation, then doing this in a situation where the impact is limited to a single team can be justifiable. I am very cautious about this though, as there is the danger that a one-off activity becomes business as usual, and there goes independent deployability. Use lock-step deployments too often, and you’ll end up with a distributed monolith before long.</p>

<p>Co-existing different versions of the same microservice can be problematic, as we discussed. I’d only consider doing this in situations where we only planned to run the microservice versions side by side for a short period of time. The reality is that when you need to give consumers time to upgrade, you could be looking at weeks or more. In other situations where you might co-exist microservice versions, perhaps as part of a blue/green deployment or canary release, the durations involved are much shorter, offsetting the downsides of this approach.</p>

<p>My general preference is where possible to use emulation of old endpoints. The challenges of implementing emulation are in my opinion much easier to deal with than co-existing of microservice versions.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Social Contract"><div class="sect2" id="idm46534870397768">
<h2>The Social Contract</h2>

<p>Which approach you pick will be due in large part to the expectations consumers have of how these changes will be made. Keeping the old interface lying around can have a cost, and ideally you’d like to turn it off and remove associated code and infrastructure as soon as possible. On the other hand, you want to give consumers as much time as possible to make a change. And remember, in many cases the backwards-incompatible changes you are making are often things that have been asked for by the consumers, or will actually end up benefiting them. There is a balancing act of course, between the needs of the microservice maintainers, and the consumers - and this needs to be discussed.</p>

<p>I’ve found that in many situations, how these changes will be handled has never been discussed, leading to all sorts of challenges. As with schemas, having some degree of explicitness in how backwards-incompatible changes will be made can greatly simplify things.</p>

<p>You don’t need reams of paper and huge meetings necessarily to agree these things. But both the owner and consumer of a microservice need to be clear on a few things. Assuming you aren’t going down the route of lock-step releases, I’d suggest being clear on a few things:</p>

<ul>
<li>
<p>How will you raise the issue that the interface needs to change?</p>
</li>
<li>
<p>How will the consumers and microservice teams collaborate to agree on what the change will look like?</p>
</li>
<li>
<p>Who is expected to do the work to update the consumers?</p>
</li>
<li>
<p>When the change is agreed, how long will consumers have to shift over to the new interface before it is removed?</p>
</li>
</ul>

<p>Remember, one of the secrets to an effective microservice architecture is to embrace a consumer-first approach. Your microservices exist to be called by other consumers. Their needs are paramount, and if you are making changes to a microservice that are going to cause upstream consumers problems, this needs to be taken into account.</p>

<p>In some situations of course it might not be possible to change the consumers. I’ve heard from Netflix that they had issues (at least historically), with old set-top boxes using older versions of the Netflix APIs. These set-top boxes cannot be upgraded easily, so the old endpoints have to remain available unless and until the number of older set-top boxes drops to a level where they can have their support disabled. Decisions to stop old consumers being able to access your endpoints can sometimes end up being financial - how much money does it cost you to support the old interface, balanced against how much money you make from those consumers.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Tracking Usage"><div class="sect2" id="idm46534870387784">
<h2>Tracking Usage</h2>

<p>Even if you do agree on a time by which consumers should stop using the old interface, would you know if they had actually stopped using it? Making sure you have logging in place for each endpoint your microservice exposes can help, as can ensuring that you have some sort of client identifier so you can chat to the team in question if you need to work with them to get them to migrate away from your old interface. This could be something as simple as asking consumers to put their identifer in the user agent header when making HTTP requests, or you could require that all calls go via some sort of API Gateway where clients need keys to identify themselves.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Extreme Measures"><div class="sect2" id="idm46534870385528">
<h2>Extreme Measures</h2>

<p>So, assuming you know a consumer is still using an old interface that you want to remove, and they are dragging their heels about moving to the new version, what can you do about it? Well, the first thing to do is talk to them. Perhaps you can lend them a hand to make the changes happen. If all else fails, and they still don’t upgrade even after agreeing to, then there are some extreme techniques I’ve seen used.</p>

<p>In one large tech company, I discussed with them how they handled this issue. Internally, they had a very generous period of one year before old interfaces would be retired. I asked how they knew if consumers were still using the old interfaces, and they replied that they didn’t bother tracking that information really. After one year they just turned the old interface off. It was recognized internally that if this caused a consumer to break, then in that company it was accepted that it was the fault of the consuming microservice’s team - they’d had a year to make the change, and hadn’t done it. Of course, this approach won’t work for many (I said it was extreme!). It also leads to a large degree of inefficiency. By not knowing if the old interface was used, they denied themselves the opportunity to remove it before the year had passed. Personally, even if I was to suggest just turning the endpoint off after a certain period of time, I’d still definitely want tracking of who was going to be impacted.</p>

<p>Another extreme measure I saw was actually in the context of deprecating libraries, but it could also theoretically be used for microservice endpoints. The example given was of an old library that people were trying to retire from use inside the organization, in favour of a newer, better one. Despite lots of work, other teams were still dragging their heels. The solution was to insert a sleep in the old library, so that it responded more slowly to calls (with logging to show what was happening). Over time, the team just kept increasing the duration of the sleep, until eventually the other teams got the message. You obviously have to be extremely sure that you’ve exhausted other reasonable efforts to get consumers to upgrade before considering something like this!</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="DRY and the Perils of Code Reuse in a Microservice World"><div class="sect1" id="a50-dry">
<h1>DRY and the Perils of Code Reuse in a Microservice World</h1>

<p>One of the acronyms we developers hear a lot is DRY: don’t repeat yourself. Though its definition is sometimes simplified as trying to avoid duplicating code, DRY more accurately means that we want to avoid duplicating our system <em>behavior and knowledge</em>. This is very sensible advice in general. Having lots of lines of code that do the same thing makes your codebase larger than needed, and therefore harder to reason about. When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a mantra, in general, makes sense.</p>

<p>DRY is what leads us to create code that can be reused. We pull duplicated code into abstractions that we can then call from multiple places. Perhaps we go as far as making a shared library that we can use everywhere! It turns out though that sharing code in a microservice environment is a bit more involved than that. As always, we have more than one option to consider.</p>








<section data-type="sect2" data-pdf-bookmark="Sharing Code Via Libraries"><div class="sect2" id="idm46534870377272">
<h2>Sharing Code Via Libraries</h2>

<p>One of the things we want to avoid at all costs is overly coupling a microservice and consumers such that any small change to the microservice itself can cause unnecessary changes to the consumer. Sometimes, however, the use of shared code can create this very coupling. For example, at one client we had a library of common domain objects that represented the core entities in use in our system. This library was used by all the services we had. But when a change was made to one of them, all services had to be updated. Our system communicated via message queues, which also had to be drained of their now <em>invalid</em> contents, and woe betide you if you forgot.</p>

<p>If your use of shared code ever leaks outside your service boundary, you have introduced a potential form of coupling. Using common code like logging libraries is fine, as they are internal concepts that are invisible to the outside world. RealEstate.com.au makes use of a tailored service template to help bootstrap new service creation. Rather than make this code shared, the company copies it for every new service to ensure that coupling doesn’t leak in.</p>

<p>The really important point about sharing code via libraries is that you cannot update all uses of the library at once. Although multiple microservices might all use the same library, they do so typically by packaging that library into the microservice deployment. To upgrade the version of the library being used, you’d therefore need to redeploy the microservice. If you want to update the same library everywhere at exactly the same time, this could lead to a widespread deployment of multiple different microservices all at the same time, with all the associated headaches.</p>

<p>So, if using libraries for code reuse across microservice boundaries, you have to accept that multiple different versions of the same library might be out there at the same time. You can of course look to update all of these to the last version over time, but as long as you are OK with this fact, then by all means reuse code via libraries. If you really do need to update that code for all users of it at exactly the same time, then you’ll actually want to look at reusing code via a dedicated microservice instead.</p>

<p>There is one specific use case associated with reuse through libraries which is worth exploring further, though.</p>










<section data-type="sect3" data-pdf-bookmark="Client Libraries"><div class="sect3" id="idm46534870371208">
<h3>Client Libraries</h3>

<p>I’ve spoken to more than one team that has insisted that creating client libraries for your services is an essential part of creating services in the first place. The argument is that this makes it easy to use your service, and avoids the duplication of code required to consume the service itself.</p>

<p>The problem, of course, is that if the same people create both the server API and the client API, there is the danger that logic that should exist on the server starts leaking into the client. I should know: I’ve done this myself. The more logic that creeps into the client library, the more cohesion starts to break down, and you find yourself having to change multiple clients to roll out fixes to your server. You also limit technology choices, especially if you mandate that the client library has to be used.</p>

<p>A model for client libraries I like is the one for Amazon Web Services (AWS). The underlying SOAP or REST web service calls can be made directly, but everyone ends up using just one of the various software development kits (SDKs) that exist, which provide abstractions over the underlying API. These SDKs, though, are written by the community or AWS people other than those who work on the API itself. This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you go down the path of client libraries yourself, make sure this is the case.</p>

<p>Netflix in particular places special emphasis on the client library, but I worry that people view that purely through the lens of avoiding code duplication. In fact, the client libraries used by Netflix are as much (if not more) about ensuring reliability and scalability of their systems. The Netflix client libraries handle service discovery, failure modes, logging, and other aspects that aren’t actually about the nature of the service itself. Without these shared clients, it would be hard to ensure that each piece of client/server communications behaved well at the massive scale at which Netflix operates. Their use at Netflix has certainly made it easy to get up and running and increase productivity while also ensuring the system behaves well. However, according to at least one person at Netflix, over time this has led to a degree of coupling between client and server that has been problematic.</p>

<p>If the client library approach is something you’re thinking about, it can be important to separate out client code to handle the underlying transport protocol, which can deal with things like service discovery and failure, from things related to the destination service itself. Decide whether or not you are going to insist on the client library being used, or if you’ll allow people using different technology stacks to make calls to the underlying API. And finally, make sure that the clients are in charge of when to upgrade their client libraries: we need to ensure we maintain the ability to release our services independently of each other!</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Service Meshes and API Gateways"><div class="sect3" id="idm46534870364872">
<h3>Service Meshes and API Gateways</h3>

<p>Service Meshes and API Gateways do offer a potential way to share code between microservices without requring the creation of new client libraries, or new microservices. Put (very) simply, service meshes and API gateways can work as proxies between microservices. This can mean that they can be used to to implement some microservice-agnostic behaviour which might otherwise have to be done in code, such as service discovery or logging.</p>

<p>If you are using either an API gateway or a service mesh to implement shared, common behaviour for your microservices, it’s essential that this behaviour is totally generic - in other words, the behaviour in the proxy bares no relation to any specific behaviour of an individual microservice.</p>

<p>API Gateways and Service Meshes are topics which need to be explored more fully - we’ll come back to them both in [Link to Come].</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46534870360680">
<h1>Summary</h1>

<p>So, we’ve covered a lot of ground here - let’s break down some of what we’ve covered.</p>

<ul>
<li>
<p>Firstly, ensure that the problem you are trying to solve guides your technology choice. Based on your context, and your preferred communication style, use that to select the technology that is most appropriate to you - don’t fall into the trap of picking the technology first. The model shared again in <a data-type="xref" href="#comms-styles-again">Figure 4-5</a> can help guide your decision making, but just following this model isn’t a replacement for sitting down and thinking about your own situation.</p>
</li>
</ul>

<figure><div id="comms-styles-again" class="figure">
<img src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/comms-styles.png" alt="Different styles of inter-microservice communication along with example implementing technologies" width="1516" height="616" data-mfp-src="https://learning.oreilly.com/library/view/building-microservices-2nd/9781492034018/assets/comms-styles.png">
<h6><span class="label">Figure 4-5. </span>Different styles of inter-microservice communication along with example implementing technologies</h6>
</div></figure>

<ul>
<li>
<p>Whatever choice you make, consider the use of schemas as part of helping make your contracts more explicit, but also to help catch accidental breaking changes.</p>
</li>
<li>
<p>Where possible, strive to make changes which are backwards compatible to ensure that independent deployability remains as possibility.</p>
</li>
<li>
<p>If you do have to make backwards incompatible changes, find a way to allow consumers time to upgrade to avoid lock-step deployments.</p>
</li>
</ul>

<p>Next, we need to address the fact that most people don’t start with a microservice architecture, and look at how you can take an existing monolithic system and migrate it to a microservice architecture.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46534872997208"><sup><a href="ch04.html#idm46534872997208-marker" class="totri-footnote">1</a></sup> <a href="https://protolock.dev/"><em class="hyperlink">https://protolock.dev/</em></a></p><p data-type="footnote" id="idm46534876354136"><sup><a href="ch04.html#idm46534876354136-marker" class="totri-footnote">2</a></sup> <a href="https://github.com/OAI/OpenAPI-Specification/"><em class="hyperlink">https://github.com/OAI/OpenAPI-Specification/</em></a></p><p data-type="footnote" id="idm46534872951464"><sup><a href="ch04.html#idm46534872951464-marker" class="totri-footnote">3</a></sup> Robinson, Ian, Jim Webber, and Savas Parastatidis, “REST in Practice: Hypermedia and Systems Architecture”. O’Reilly 2010</p><p data-type="footnote" id="idm46534877015992"><sup><a href="ch04.html#idm46534877015992-marker" class="totri-footnote">4</a></sup> <a href="https://graphql.org/"><em class="hyperlink">https://graphql.org/</em></a></p><p data-type="footnote" id="idm46534870604200"><sup><a href="ch04.html#idm46534870604200-marker" class="totri-footnote">5</a></sup> Stopford, Ben. <em>Designing Event-Driven Systems</em>. O’Reilly 2017.</p><p data-type="footnote" id="idm46534870603016"><sup><a href="ch04.html#idm46534870603016-marker" class="totri-footnote">6</a></sup> Narkhede, Neha, Gwen Shapira and Todd Palino. <em>Kafka: The Definitive Guide</em>. O’Reilly 2017</p><p data-type="footnote" id="idm46534870590424"><sup><a href="ch04.html#idm46534870590424-marker" class="totri-footnote">7</a></sup> <a href="https://github.com/real-logic/simple-binary-encoding"><em class="hyperlink">https://github.com/real-logic/simple-binary-encoding</em></a></p><p data-type="footnote" id="idm46534870589064"><sup><a href="ch04.html#idm46534870589064-marker" class="totri-footnote">8</a></sup> <a href="https://capnproto.org/"><em class="hyperlink">https://capnproto.org/</em></a></p><p data-type="footnote" id="idm46534870587704"><sup><a href="ch04.html#idm46534870587704-marker" class="totri-footnote">9</a></sup> <a href="https://google.github.io/flatbuffers/"><em class="hyperlink">https://google.github.io/flatbuffers/</em></a></p><p data-type="footnote" id="idm46534870567000"><sup><a href="ch04.html#idm46534870567000-marker">10</a></sup> Martin Fowler explores this in more detail in the context of schemaless datastorage: <a href="https://martinfowler.com/articles/schemaless/"><em class="hyperlink">https://martinfowler.com/articles/schemaless/</em></a></p><p data-type="footnote" id="idm46534870473048"><sup><a href="ch04.html#idm46534870473048-marker">11</a></sup> <a href="https://www.asyncapi.com/"><em class="hyperlink">https://www.asyncapi.com/</em></a></p><p data-type="footnote" id="idm46534870471384"><sup><a href="ch04.html#idm46534870471384-marker">12</a></sup> <a href="https://cloudevents.io/"><em class="hyperlink">https://cloudevents.io/</em></a></p><p data-type="footnote" id="idm46534870460408"><sup><a href="ch04.html#idm46534870460408-marker">13</a></sup> <a href="https://github.com/nilslice/protolock"><em class="hyperlink">https://github.com/nilslice/protolock</em></a></p><p data-type="footnote" id="idm46534870459000"><sup><a href="ch04.html#idm46534870459000-marker">14</a></sup> <a href="https://www.npmjs.com/package/json-schema-diff-validator"><em class="hyperlink">https://www.npmjs.com/package/json-schema-diff-validator</em></a></p><p data-type="footnote" id="idm46534870457512"><sup><a href="ch04.html#idm46534870457512-marker">15</a></sup> Note that there are actually three different tools in this space with the same name! <a href="https://github.com/Azure/openapi-diff"><em class="hyperlink">https://github.com/Azure/openapi-diff</em></a> seems to get closest to a tool that actually passes or fails compatibility</p><p data-type="footnote" id="idm46534870435976"><sup><a href="ch04.html#idm46534870435976-marker">16</a></sup> <a href="https://github.com/confluentinc/schema-registry#documentation"><em class="hyperlink">https://github.com/confluentinc/schema-registry#documentation</em></a></p></div></div></section>